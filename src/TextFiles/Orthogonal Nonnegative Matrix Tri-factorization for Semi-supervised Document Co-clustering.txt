Orthogonal Nonnegative Matrix Tri-factorization

for Semi-supervised Document Co-clustering

Huifang Ma, Weizhong Zhao, Qing Tan, and Zhongzhi Shi

Key Lab of Intelligent Information Processing, Institute of Computing Technology,

Chinese Academy of Sciences, 100190 Beijing, China

Graduate University of the Chinese Academy of Sciences, 100049 Beijing, China

http://www.intsci.ac.cn/users/mahuifang/index.html

mahf@ics.ict.ac.cn

Abstract. Semi-supervised clustering is often viewed as using labeled
data to aid the clustering process. However, existing algorithms fail to
consider dual constraints between data points (e.g. documents) and fea-
tures (e.g. words). To address this problem, in this paper, we propose a
novel semi-supervised document co-clustering model OSS-NMF via or-
thogonal nonnegative matrix tri-factorization. Our model incorporates
prior knowledge both on document and word side to aid the new word-
category and document-cluster matrices construction. Besides, we prove
the correctness and convergence of our model to demonstrate its mathe-
matical rigorous. Our experimental evaluations show that the proposed
document clustering model presents remarkable performance improve-
ments with certain constraints.

Keywords: Semi-supervised Clustering, Pairwise Constraints, Word-
Level Constraints, Nonnegative Matrix tri-Factorization.

1 Introduction

Providing a meaningful cluster hierarchy to a document corpus has always been
a major goal for the data mining community. Approaches to solve this problem
have focused on document clustering algorithms, which are widely used in a
number of diﬀerent areas of text mining and information retrieval. One of a
latest presented approach for obtaining document cluster is Non-negative Matrix
Factorization (NMF) [1], which aimed to provide a minimum error non-negative
representation of the term-document matrix. This technique can be considered
as co-clustering [2], which aimed to cluster both the rows and columns of the
original data simultaneously by making eﬃcient use of the duality between data
points (e.g. documents) and features (e.g. words). Put it another way, document
clustering and word clustering are performed in a reinforcing manner.

However, traditional clustering algorithms fail to take advantage of knowledge
from domain experts. Incorporating the additional information can greatly enhance
the performance of clustering algorithms. In recent years, a great amount of eﬀort
has been made for clustering document corpus in a semi-supervised way, aiming to
cluster the document set under the guidance of some supervisory information.

M.J. Zaki et al. (Eds.): PAKDD 2010, Part II, LNAI 6119, pp. 189–200, 2010.
c(cid:2) Springer-Verlag Berlin Heidelberg 2010

190

H. Ma et al.

Unfortunately, traditional approaches to semi-supervised document clustering
inherently strongly depend on constraints within document themselves while
ignore the useful semantic correlation information hidden within the words of
the document corpus. We believe that adding word semantic information (such as
word clusters indicating word semantics) as additional constraints can deﬁnitely
improve document clustering performance. Thereafter how to eﬀectively combine
both document-level and word-level constraints to guide the process of document
clustering is a problem that is deﬁnitely worthy of researching.

Based on the above considerations, in this paper, we propose a novel semi-
supervised document co-clustering method via non-negative factorization of the
term-document matrix for the given document corpus. We have extended the
classical NMF approach by introducing both document-level and word-level con-
straints based on some prior knowledge. Our clustering model encodes the user’s
prior knowledge with a set of constraints to the objective function, and the
document clustering task is carried out by solving a constrained optimization
problem. Speciﬁcally, we propose a semi-supervised co-clustering framework to
cluster the words and documents simultaneously. Meanwhile, we derive iterative
algorithms to perform orthogonal non-negative tri-factorization. The correctness
and convergence of these algorithms are proved by showing that the solution
satisﬁed the KKT optimality and these algorithms are guaranteed to converge.
Experiments performed on various publicly available document datasets demon-
strate the superior performance of the proposed work.

The basic outline of this paper is as follows: Section 2 introduces related
works. Section 3 presents the semi-supervised orthogonal nonnegative matrix
tri-factorization. The experiments and results are given in Section 4. Lastly, we
conclude our paper in Section 5.

2 Related Work

This section brieﬂy reviews related work about NMF and semi-supervised doc-
ument clustering.
The classical NMF algorithms [3] aim to ﬁnd two matrix factors for a matrix
X such that X ≈ W H T , where W m×k and H n×k are both nonnegative matrices.
Ding et al.[4] made systematical analysis of NMF and introduced 3-factor NMF.
They demonstrated that the orthogonality constraint leads to rigorous clustering
interpretation. When 3-factor NMF is applied to the term-document matrix X,
each column Xj of X is an encoding of an original document and each entry xij
of vector Xj is the signiﬁcance of term i with respect to the semantics of Xj,
where i ranges across the terms in the dictionary. Thereafter, Orthogonal NMF
factorizes X into three non-negative matrices

X = F SGT ,

(1)

where G is the cluster indicator matrix for clustering of documents of X and F is
the word cluster indicator matrix for clustering of rows of X. The simultaneous
row/column clustering can be solved by optimizing

Orthogonal Nonnegative Matrix Tri-factorization

191

J =

min

F≥0,S≥0,G≥0

(cid:2)

(cid:2)X − F SGT

(cid:2)
(cid:2)2
F

s.t F T F = I, GT G = I.

(2)

The Frobenius norm is often used to measure the error between the original
matrix X and its low rank approximation F SGT . The rank of the approximation,
k, is a parameter that must be set by users.

Several formulations of co-clustering problem are proposed in the past decade
and they are superior to traditional one-side clustering. Dhillon [2] proposed a bi-
partite spectral graph partitioning approach to co-cluster words and documents.
Long et al.[5] presented a general principled model, called relation summary
network to co-cluster the heterogeneous data on a k-partite graph. As for semi-
supervised co-clustering algorithms, Chen et al.[6] presented a semi-supervised
document clustering model with simultaneous text representation and catego-
rization. Fei et al.[7] proposed a semi-supervised clustering algorithm via matrix
factorization. Li et al.[8] presented an interesting word-constrained clustering al-
gorithm. The way of incorporating word constraints is very appealing and sets a
good foundation for our model formulation. Even though these semi-supervised
algorithms have shown to be superior to traditional clustering method, very little
is known about the combination of constraints on both documents and words.
One recent work came from Li et al.[9]. They have demonstrated a non-negative
matrix tri-factorization approach to sentiment classiﬁcation with prior knowl-
edge about sentiment words in the lexicon and partial labels on documents.

3 Semi-supervised Orthogonal Nonnegative Matrix

Tri-factorization for Co-clustering

In this section, we ﬁrst describe how we integrate two diﬀerent constraints in our
model in Sect. 3.1. We then derive the OSS-NMF model, prove the correctness
and convergence of the algorithm in Sect. 3.2 and Sect. 3.3 respectively.

3.1 Incorporating Document-Level Constraints

Our model treats the prior knowledge on the word side as categorization of
words, represented by a complete speciﬁcation F0 for F . The prior knowledge
on document-level is provided in the form of two sets of pairwise constraints on
documents: two documents are known to be highly related and must be grouped
into the same document cluster; or two documents that are irrelevant and can
not be grouped into the same cluster.

We make use of set Aml to denote that must-link document pairs (di1 , dj1)

are similar and must be clustered into the same document cluster:

Aml = {(i1; j1); . . . ; (ia; ja)}; a = |Aml|.

(3)

It is easy to demonstrate that the must-link constraints represent equivalence
relation. Therefore, we can compute a collection of transitive closures from Aml.
Each pair of documents in the same transitive closure must be in the same cluster
in the clustering result.

192

H. Ma et al.

Meanwhile, cannot-link document pairs are collected into another set:

Bcl = {(i1; j1); . . . ; (ib; jb)}; b = |Bcl|,

(4)

where each pair of documents are considered dissimilar and ought not to be
clustered into the same document cluster.

We then encode the must-link document pairs as a symmetric matrix A whose
diagonal entries are all equal to one and the cannot-link document pairs as
another matrix B.

Suppose each document in the corpus either completely belongs to a particular
topic, or is more or less related to several topics. We can then regard these con-
straints as the document class posterior probability on G. A must-link pair (i1; j1)
implies that the overlap gi1kgj1k > 0 for some class k, and therefore
gi1kgj1k =
(GGT )i1j1 should be maximized. The must-link condition can be presented as

(cid:3)

k

max

G

(cid:4)

i,j∈A

(GGT )ij =

(cid:4)

ij

Aij(GGT )ij =T rGT AG.

(5)

In terms of cannot-link pairs (i2; j2), gi2kgj2k = 0 for all k. Likewise, we take
gi2kgj2k = (GT G)i2j2. Since gik are
the cannot-link constraints and minimize
nonnegative, we write this condition as:

(cid:3)

k

(cid:4)

i,j∈B

(GGT )ij =T rBGGT = 0, or min
G

T rGT BG.

(6)

3.2 Algorithm Derivation

Combining the above constraints together, we deﬁne the objective function of
OSS-NMF as:

J =

min

F≥0,S≥0,G≥0

||X − F SGT|| + α(cid:3)F − F0(cid:3)2

F + T r(−βGAGT + γGBGT ),
(7)

s.t. F F T = I, GGT = I,

where α, β and γ are positive trade-oﬀ parameters that control the degree of
enforcement of the user’s prior knowledge. The larger value the parameters take,
the stronger enforcement of the users prior knowledge we will have; vise versa.
An iterative procedure to solve the optimization problem in Eq.(7) can be

summarized as follows.

Computation of S. Optimizing Eq.(7) with respect to S is equivalent to
optimizing

J1 =

min

F≥0,S≥0,G≥0

(cid:2)

(cid:2)X − F SGT

(cid:2)
(cid:2)2
F .

Setting ∂J1

∂S = 0 leads to the following updating formula:

(cid:5)

Sik = Sik

(F T XG)ik

(F T F SGT G)ik

.

(8)

(9)

Orthogonal Nonnegative Matrix Tri-factorization

193

Computation of F . Optimizing Eq.(7) with respect to F is equivalent to
optimizing

J2 =

min

F≥0,S≥0,G≥0

(cid:2)

(cid:2)X − F SGT

(cid:2)
(cid:2)2

F + α(cid:3)F − F0(cid:3)2

F ,

s.t. F F T = I.

(10)

We present an iterative multiplicative updating solution. After introducing the
Lagrangian multiplier, the Lagrangian function is stated as

L(F ) =

(cid:2)

(cid:2)X − F SGT

(cid:2)
(cid:2)2

F + α(cid:3)F − F0(cid:3)2

F + T r[λ1(F T F − I)].

(11)

This takes the exact form as Li demonstrated in [8], thereby we can update F
as follows:

(cid:5)

Fik = Fik

(XGST + αF0)ik

(F F T XGST + αF F T F0)ik

.

(12)

Computation of G. Optimizing Eq.(7) with respect to G is equivalent to
optimizing

min

(cid:2)
(cid:2)2

(cid:2)

(cid:2)X − F SGT

J3 =

F + T r(−βGT AG + γGT BG),

F≥0,S≥0,G≥0

s.t. GGT = I.
(13)
Similar with the computation of F , we introduce the Lagrangian multiplier, thus
the Lagrangian function is

L(G) =

(cid:2)

(cid:2)X − F SGT

(cid:2)

(cid:2)2 + T r(−βGT AG + γGT BG) + T r[λ2(GT G − I)]. (14)

We show that G can be iterated as:

(cid:5)

Gik = Gik

(X T F S + βAG)ik

(G(SF T F ST + λ2) + γBG)ik

.

(15)

The detailed analysis of computation of G is shown in the optimization section.
When the iteration starts, we update one factor with others ﬁxed.

3.3 Algorithm Correctness and Convergence

To prove the correctness and convergence of our algorithm, we will make use of
optimization theory, matrix inequalities and auxiliary functions that used in [3].

Correctness

Theorem 1. If the update rule of S, F and G in Eq.(9), Eq.(12) and Eq.(15)
converge, then the ﬁnal solution satisﬁes the KKT optimality condition, i.e., the
algorithm converges correctly to a local optima.

194

H. Ma et al.

Proof: Following the standard theory of constrained optimization, we intro-
duce the Lagrangian multipliers λ1, λ2 and construct the following Lagrangian
function:

0 + F0F T

0 )]

L = (cid:3)X − F SGT(cid:3) + α(cid:3)F − F0(cid:3) + T r[λ1(F T F − I)]
+ T r[−βGAGT + γGBGT + λ2(GT G − I)]
= T r[X T X − 2GT X T F S + GT GSF T F S + α(F F T − 2F F T
− βGT AG + γGT BG + λ1(F T F − I) + λ2(GT G − I)].

(16)
The correctness of updating rules for S in Eq.(9) and F in Eq.(12) have been
proved in [8]. Therefore, we only need to proof the correctness of updating rules
for G. Fixing F , S, we can get that the KKT complementary condition for the
non-negativity of G

[−2X T F S + 2G(SF T F ST + λ2) − 2βAG + 2γBG]ikGik = 0.

(17)
We then obtain the Lagrangian multiplier, it is obvious that at convergence the
solution satisfy

[−2X T F S + 2G(SF T F ST + λ2) − 2βAG + 2γBG]ikG2

(18)
We can see that this is identical to the KKT condition. The above equation
denotes that either the ﬁrst factor equals to zero, or Gik is zero. If the ﬁrst
factor is zero, the two equations are identical. If Gik is zero, then G2
ik is zero
as well, vice versa. Thus, we have proved that if the iteration converges, the
converged solution satisﬁes the KKT condition, i.e., it converges correctly to a
local minima.

ik = 0.

Proof is completed.

Convergence. We demonstrate that the above objective function decreased
monotonically under these three updating rules. Before we proof the convergence
of the algorithm, we need to construct the auxiliary function similar to that used
in Lee and Seung [3]. We ﬁrst introduce the deﬁnition of auxiliary function.
Deﬁnition 1. A function Z(H, H
satisﬁes

(cid:5)) is called an auxiliary function of L(H) if it

(cid:5)

Z(H, H

) ≥ L(H), Z(H, H) = L(H).

(19)
(cid:5)) is an auxiliary function, then L is non-increasing under

Lemma 1. If Z(H, H
the update

Z(H, H t).

H (t+1) = arg min

(20)
By construction L(H (t)) = Z(H (t), H (t)) ≥ Z(H (t+1), H (t)) ≥ L(H (t+1)),
L(H (t+1)) is monotonic decreasing (non-increasing).
Lemma 2. For any nonnegative matrices A ∈ Rn×n,B ∈ Rk×k,S ∈ Rn×k,S
Rn×k , A, B are symmetric, the following inequality holds[10]:

(cid:5) ∈

H

n(cid:4)

k(cid:4)

(AS

i=1

p=1

(cid:5)

B)ipS2
ip
(cid:5)
S
ip

≥ tr(ST ASB).

(21)

Orthogonal Nonnegative Matrix Tri-factorization

195

Theorem 2. The above iterative algorithms converge.
Proof: To proof the algorithm converges, the key step is to ﬁnd an appropriate
(cid:5)) of L(G) in Eq.(14). We show that the following
auxiliary function Z(G, G
function

(cid:5)
Z(G, G

) =

(cid:3)

)(X T F S)ik +

(cid:5)(SF T F S + λ2)]ikG2

ik

[G

(cid:5)
G
ik

ik [−2G
(cid:5)
ik(1 + log Gik
(cid:5)
G
ik
− βG
(cid:5)
(cid:5)
ik(AG

)ik(1 + log G2
ik
(cid:5)
G
ik

) + γ

(cid:5))ikG2
(BG
(cid:5)
G
ik

ik

].

(22)

is its corresponding auxiliary function.
First, it is obvious that when G = G

(cid:5), the equality holds. Second, the inequal-
(cid:5)) ≥ L
(cid:5)(G). This is based on the following: a) The ﬁrst term
ity holds Z(G, G
(cid:5)) are always smaller than the corresponding terms in
and third term in Z(G, G
(cid:5)(G) because of the inequality z ≥ 1 + log(z) ∀z > 0; b) The second and last
L
(cid:5)(G), due
term in Eq.(24) are always bigger than the corresponding terms in L
(cid:5)(G).
to Lemma 2. Putting these together, we can guarantee that Z(G, G

(cid:5)) ≥ L

(23)

(24)

To ﬁnd the minimum of Z(G, G

(cid:3)

=

(cid:5))ik

(cid:5))
∂Z(G, G
∂Gik

(cid:5)
ik [−2 G
ik
Gik
(cid:5)
− 2β
ik(AG
G
Gik
(cid:5))
and the Hessian matrix of Z(G, G
(cid:5)
ik [2 G
ik
G2
ik
(cid:5)
ik(AG
G
G2
ik

(cid:5))
∂2Z(G, G
∂Gik∂Gjl

+ 2β

=

(cid:3)

(cid:5)), we take

(X T F S)ik + 2

(cid:5)(SF T F S + λ2)]ikGik

[G

(cid:5)
G
ik

+ 2γ

(cid:5))ikGik
(BG
(cid:5)
G
ik

]

(cid:5)(SF T F S + λ2)]ik

[G

(X T F S)ik + 2

(cid:5))ik

(cid:5)
G
ik

(cid:5))ik

+ 2γ

(BG
(cid:5)
G
ik
is a diagonal matrix with positive diagonal elements.
minimum of Z. The minimum value is obtained by setting ∂Z(G,G(cid:2)
(cid:5)
(SF T F ST ) + λ2 + γBG

(cid:5)) is a convex function of G. Therefore, we can obtain the global
= 0, we get

Thus Z(G, G

)ik.

(25)

]δijδkl

(cid:5)
(G

(X T F S + βAG)ik = Gik
(cid:5)
G
ik

(cid:5)
G
ik
Gik

∂Gik

)

We can thereafter derive the updating rule of Eq.(16)

(cid:5)

(X T F S + βAG)ik

Gik = Gik

(26)
(cid:5)(G) decreases monotonically, where the Lagrangian
Under this updating rule, L
multiplier k-by-k matrix λ2 for enforcing the orthogonality and GT G = I is
given by

(G(SF T F ST + λ2) + γBG)ik

.

λ2 = GT X T F S + βGT AG − γGT BG − SF T F ST .

(27)

Proof is completed.

196

H. Ma et al.

4 Experiments

This section provides empirical evidence to show the beneﬁts of our model OSS-
NMF. We compared our method with Constrained-Kmeans[11], Information-
Theoretic Co-clustering, which is referred to as IT-Co-clustering[12], ONMF-W
denoting Orthogonal NMF with word-level constraints[8], ONMF-D representing
Orthogonal NMF with document-level constraints. Constrained K-means is the
representative semi-supervised data clustering method; Information-Theoretic
Co-clustering is one of the most popular co-clustering method; ONMF-W and
ONMF-D are two derived algorithms from our approach.

The requirement of word constraints is the speciﬁcation of word catego-
rization. Similar with Li [8], we took advantage of the ACM term taxonomy,
which come naturally and strictly decide the taxonomy of computer society. The
document-level constraints were generated by randomly selecting pairs of doc-
uments. If the labels of this document pair are the same, then we generated a
must link. In contrast, if the labels are diﬀerent, a cannot link is generated. The
amounts of constraints were determined by the size of input data. Incorporating
dual constraints on our model, we believe that our approach should perform
better given reasonable amount of labeled data.

4.1 Datasets
Three diﬀerent datasets widely used as benchmark data sets in clustering liter-
ature were used.
Citeseer dataset: Citeseer collection was made publicly available by
Lise Getoor’s research group at University of Maryland. We end up with a sam-
pling of Citeseer data containing 3312 documents. These data are classiﬁed into
one of the following six classes: Agents, Artiﬁcial Intelligence, Data Base, Infor-
mation Retrieval, Machine Learning, Human Computer Interaction.
DBLP Dataset: This dataset is downloaded from DBLP Computer Science
Bibliography spanning from 1999 to 2004. We extract the paper titles to form
our dataset from 5 categories, which contains 2170 documents.
URCS Technical Reports: This dataset is composed of abstracts of tech-
nical reports published in the Department of Computer Science at Rochester
University. There are altogether 512 reports abstracts grouped according to 4
categories.

We pre-processed each document by tokenizing the text into bag-of-words.
Then we applied stopwords removing and stemmed words. In particular, words
that occur in less than three documents are removed. We used the weighted
term-frequency vector to represent each document.

Evaluation Metrics

4.2
We adopt the clustering accuracy and normalized mutual information as our per-
formance measures. These performance measures are standard measures widely

Orthogonal Nonnegative Matrix Tri-factorization

197

used for clustering. Clustering accuracy measures the cluster performance from
the one-to-one relationship between clusters and classes point of view, which is
deﬁned as:

N(cid:3)

Acc = max

i=1

δ(map(ri), di)

N

,

(28)

where ri denotes the cluster label of a document and di denotes the true class
label, N is the total number of documents, δ(x, y) is a function which equals one
if x = y and equals zero otherwise, map(ri) is the permutation function which
maps each cluster label to the corresponding label of the data set.

NMI measures how closely the clustering algorithm could reconstruct the

underlying label distribution in the data. It is deﬁned as:

(cid:5); Z)

I(Z

(H(Z(cid:5)) + H(Z))/2 ,

N M I =
(cid:5); Z) = H(Z)−H(Z|Z
(cid:5) and Z, H(Z) is the Shannon entropy of Z, and H(Z|Z

(cid:5)) is the mutual information between the random
(cid:5)) is the
(cid:5). In general, the larger the N M I value is, the

(29)

where I(Z
variables Z
conditional entropy of Z given Z
better the clustering quality is.

4.3 Clustering Results

Considering the document constraints are generated randomly, we run each al-
gorithm 20 times for each dataset and took the average as statistical results. To
give these algorithms some advantage, we set the number of clusters equal to
the real number of all the document clusters and word clusters.

Overall Evaluation. Table 1 shows the cluster accuracy and normalized mu-
tual information of all the algorithms on all the data sets. From the experimental
comparisons, we observe that our proposed method OO-SNMF eﬀectively com-
bined prior knowledge from the word side with constraints on the document side
for improving clustering results. Moreover, our model outperforms most of the
clustering methods on all the data sets. In summary, the experimental results
match favorably with our hypotheses and encouraged us to further explore the
reasons.

The superiority of our model arises in the following three aspects: (1) the
mechanism of tri-factorization for term-document matrix allows setting diﬀerent
classes of terms and documents, which is in line with the real applications; (2) co-
clustering the terms and documents with both constraints leads to improvement
in the clustering of documents; (3) last but not least, the constraints on word-
level are quite diﬀerent from that of document-level, which means our model can
incorporate distinguished semantic information on both sides for clustering.

Eﬀect of the Size of Words. In this section, we describe the eﬀect of the
size of words on clustering. These words can be used to represent the underlying

198

H. Ma et al.

Table 1. Comparison of four algorithms on diﬀerent datasets

(a) Clustering Accuracy

(b) Normalized Mutual Information

Data Sets

IT-Co-clustering

Citeseer DBLP URCS
Constrained-Kmeans 0.5124 0.4215 0.5923
0.5765 0.4873 0.6214
0.5514 0.4812 0.6052
0.6142 0.5321 0.6812
0.7235 0.6823 0.8368

ONMF-W
ONMF-D
OSS-NMF

Data Sets

IT-Co-clustering

Citeseer DBLP URCS
Constrained-Kmeans 0.5813 0.5312 0.6358
0.6521 0.5821 0.7389
0.6722 0.6312 0.7548
0.7214 0.6523 0.7964
0.8345 0.7643 0.9124

ONMF-W
ONMF-D
OSS-NMF

‘concept’ of the corresponding category cluster. We follow the term frequency
criteria to select word. The performance results with diﬀerent numbers of words
on all of the datasets are demonstrated.

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55

y
c
a
r
u
c
c
A

 
r
e

t
s
u
C

l

0.9

0.85

0.8

0.75

0.7

I

M
N

0.65

0.6

0.55

0.5

0.45

Citeseer
DBLP
URCS

Citeseer
DBLP
URCS

0.5

100

200

300

400

500
Number of Words

600

700

800

900

1000

0.4

100

200

300

400

500
Number of Words

600

700

800

900

1000

(a) Cluster Accuracy with diﬀerent num-
bers of words on 3 dataset.

(b) NMI with diﬀerent numbers of words
on 3 dataset.

Fig. 1. Accuracy and NMI results with diﬀerent numbers of words on 3 dataset

Both Accuracy and NMI show clear beneﬁts of having more words: the perfor-
mance increases as the amount of words grows, as shown in Fig.1. This indicates
the addition of word semantic information can greatly help the clustering per-
formance. It also shows a great variation with the increase of words. When the
size of words increases beyond a certain value, the quality of clustering ﬂuctuates
and suddenly drops and then becomes stable.

Experiments on Pairwise Constraint of Documents. We conducted ex-
periments for our framework by varying the number of pairwise constraints and
size of words. Results from all these document collections indicate that gener-
ally as more and more constraints are added to the dataset being clustered, the
performance of the clustering method becomes better, conﬁrming previous dis-
cussion on the eﬀect of increase of more labeled data. Due to the limitation of
this paper, we only present NMI and Cluster Accuracy on Citeseer in Fig.2.

Our ﬁnding can be summarized as follows: (1) As long as the constraints are
provided, our model always outperforms the traditional constrained methods.
(2) The model performs much better with the increase of constraints.

Orthogonal Nonnegative Matrix Tri-factorization

199

0.85

0.8

0.75

0.7

0.65

0.6

0.55

y
c
a
r
u
c
c
A

 
r
e

t
s
u
C

l

0.5

50

200 words
400 words
600 words
800 words
1000 words

200 words
400 words
600 words
800 words
1000 words

0.85

0.8

0.75

0.7

0.65

I

M
N

100

150

200

250

300

350

400

450

500

Number of Pairwise Relations on Dataset Citeseer

0.6

50

100

150

200

250

300

350

400

450

500

Number of Pairwise Relations on Dataset Citeseer

(a) Cluster Accuracy on Citeseer

(b) NMI results on Citeseer

Fig. 2. Accuracy and NMI results with diﬀerent numbers of words and pairwise doc-
uments on Citeseer

5 Conclusions and Future Work

In this paper, we consider the problem of semi-supervised document co-clustering.
We have presented a novel orthogonal semi-supervised nonnegative matrix tri-
factorization model. We also have provided theoretical analysis of the correctness
and convergence of the algorithm. The ability of our proposed algorithm to inte-
grate double constraints makes it eﬃcient for document co-clustering.

Our work leads to several questions. We incorporated the word prior knowl-
edge as a speciﬁcation of the initial word cluster. It would also be interesting to
make use of pairwise constraints on the word side. In particular, a further in-
teresting direction is to actively select informative document pairs for obtaining
user judgments so that the clustering performance can be improved with as few
supervised data as possible.

Acknowledgments. This work is supported by the National Science Foun-
dation of China (No. 60933004, 60903141, 60775035), the National Basic Re-
search Priorities Programme (No. 2007CB311004), 863 National High-Tech Pro-
gram (No.2007AA-01Z132), and National Science and Technology Support Plan
(No.2006BAC08B06).

References

1. Xu, W., Liu, X., Gong, Y.: Document Clustering Based on Non-negative Matrix
Factorization. In: Proceedings of the 26th ACM SIGIR Conference on Research
and Development in Information Retrieval, Toronto, Canada, pp. 267–273 (2003)
2. Dhillon, I.S.: Co-clustering Documents and Words Using Bipartite Spectral Graph
Partitioning. In: Proceedings of the 7th ACM SIGKDD International Confer-
ence on Knowledge Discovery and Data Mining, San Francisco, California, USA,
pp. 269–274 (2001)

200

H. Ma et al.

3. Lee, D., Seung, H.S.: Algorithms for Non-negative Matrix Factorization. In: Pro-
ceedings of 15th Annual Conference on Neural Information Processing Systems,
Vancouver, British Columbia, Canada, vol. 13, pp. 556–562 (2001)

4. Ding, C., Li, T., Peng, W., Park, H.: Orthogonal Nonnegative Matrix Tri-
factorizations for Clustering. In: Proceedings of the 12th ACM SIGKDD Inter-
national Conference on Knowledge Discovery and Data Mining, Philadelphia, PA,
USA, pp. 126–135 (2006)

5. Long, B., Zhang, Z., Wu, X., Yu, P.S.: Spectral Clustering for Multi-type Rela-
tional Data. In: Proceedings of 23rd International Conference on Machine Learning,
Pittsburgh, Pennsylvania, USA, pp. 585–592 (2006)

6. Chen, Y.H., Wang, L.J., Dong, M.: Semi-supervised Document Clustering with
Simultaneous Text Representation and Categorization. In: Buntine, W., Grobelnik,
M., Mladeni´c, D., Shawe-Taylor, J. (eds.) ECML PKDD 2009. LNCS (LNAI),
vol. 5781, pp. 211–226. Springer, Heidelberg (2009)

7. Wang, F., Li, T., Zhang, C.S.: Semi-Supervised Clustering via Matrix Factor-
ization. In: Proceedings of The 8th SIAM Conference on Data Mining, Atlanta,
Geogia, pp. 1–12 (2008)

8. Li, T., Ding, C., Zhang, Y., Shao, B.: Knowledge Transformation from Word Space
to Document Space. In: Proceedings of the 31st Annual International ACM SI-
GIR conference on research and development in information retrieval, Singapore,
pp. 187–194 (2008)

9. Li, T., Zhang, Y., Sindhwani, W.: A Non-negative Matrix Tri-factorization Ap-
proach to Sentiment Classiﬁcation with Lexical Prior Knowledge. In: Proceedings
of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP,
Suntec, Singapore, pp. 244–252 (2009)

10. Ding, C.H., Li, T., Jordan, M.I.: Convex and Semi-nonnegative Matrix Factor-
izations. IEEE Transactions on Pattern Analysis and Machine Intelligence 99(1),
195–197 (2008)

11. Wagstaﬀ, K., Cardie, C., Rogers, S., Schroedl, S.: Constrained K-means Clustering
with Background Knowledge. In: Proceedings of the 18th International Conference
on Machine Learning, Williamstown, MA, USA, pp. 577–584 (2001)

12. Dhillon, I., Mallela, S., Modha, D.S.: Information-Theoretic Co-clustering. In: Pro-
ceedings of the 9th ACM SIGKDD International Conference on Knowledge Dis-
covery and Data Mining, Washington, DC, USA, pp. 89–98 (2003)


