Finding Well-Clusterable Subspaces for High

Dimensional Data

A Numerical One-Dimension Approach

Chuanren Liu1, Tianming Hu2,(cid:2), Yong Ge3, and Hui Xiong1

1 Rutgers University, New Jersey, USA
{chuanren.liu,hxiong}@rutgers.edu

2 Dongguan University of Technology, Guangdong, China

tmhu@ieee.org

3 UNC Charlotte, North Carolina, USA

yong.ge@uncc.edu

Abstract. High dimensionality poses two challenges for clustering algo-
rithms: features may be noisy and data may be sparse. To address these
challenges, subspace clustering seeks to project the data onto simple yet
informative subspaces. The projection process should be fast and the pro-
jected subspaces should be well-clusterable. In this paper, we describe a
numerical one-dimensional subspace approach for high dimensional data.
First, we show that the numerical one-dimensional subspaces can be con-
structed eﬃciently by controlling the correlation structure. Next, we pro-
pose two strategies to aggregate the representatives from each numerical
one-dimensional subspace into the ﬁnal projected space, where the clus-
tering problem becomes tractable. Finally, the experiments on real-world
document data sets demonstrate that, compared to competing methods,
our approach can ﬁnd more clusterable subspaces which align better with
the true class labels.

Keywords: numerical one-dimension, clusterable subspace, subspace
learning.

1

Introduction

People often face a dilemma when analyzing high dimensional data. On one
hand, more features imply more information available for the learning task.
On the other hand, irrelevant/contradicting features introduce noise and may
mislead the learning algorithms. This diﬃculty has been studied extensively in
the literature from diﬀerent perspectives including dimension reduction, feature
selection, model ensembling, etc.

Among them, multiple subspace learning is a promising paradigm to address
the high dimensional diﬃculty. In this approach, we construct multiple simple

(cid:2) Corresponding Author. This research was partially supported by National Science
Foundation via grant CCF-1018151 and IIS-1256016. Also, it was supported in part
by Natural Science Foundation of China (No. 61100136 and 71329201).

V.S. Tseng et al. (Eds.): PAKDD 2014, Part II, LNAI 8444, pp. 311–323, 2014.
c(cid:2) Springer International Publishing Switzerland 2014

312

C. Liu et al.

yet informative subspaces of the original high dimensional data. For example,
principle component analysis (PCA) chooses the subspaces that best preserve
the variance of the data. Then we can either build learning models in the ag-
gregated space, or build models collaboratively in each of the subspaces. This
paradigm brings several desirable advantages. First, we can construct the sub-
spaces by grouping related features together and separating contradicting fea-
tures simultaneously. This is superior to simple feature reduction which may
lose information carried by contradicting features. Second, such collaborative
learning mode in the aggregated space is superior to separately learning one
submodel at a time and ﬁnally combining them. In fact, this mode share some
spirit with multi-source learning [3] in the literature. In the language of multi-
source learning, directly learning the original high dimensional data is actually
the early-source-combination based approach, which might be too diﬃcult for a
single model. At the other extreme, directly assembling the separately learned
submodels is actually the late-source-combination based approach, which might
make very limited or even no information to be shared among diﬀerent submod-
els. The aggregated/collaborative learning mode is actually the intermediate-
source-combination approach, which can balance between the learning diﬃculty
of too many features for individual models and the ensemble diﬃculty of many
too isolated and non-cooperative models.

Along this line, in this paper, we focus on the task of subspace learning for
clustering high dimensional data. Speciﬁcally, we ﬁrst construct numerical one-
dimensional subspaces consisting of highly related features. In theory, such sub-
spaces can substantially alleviate the unstable diﬃculties often encountered by
clustering algorithms such as K-means. In practice, we show such subspaces
can be eﬃciently constructed by leveraging correlation coeﬃcients. Next, by fur-
ther exploiting the one-dimension nature, we propose strategies to aggregate
the representatives from the numerical one-dimensional subspaces into the ﬁnal
projected space. Finally, we use real-world document data sets to compare our
approach with several competing methods in terms of performance lift and clus-
tering separability. The experimental results demonstrate that our approach can
ﬁnd more clusterable subspaces which align better with the true class labels.

The rest of the paper is organized as follows. Section 2 summarizes recent
works related to subspace learning. In Section 3, we show the numerical one-
dimensional subspaces can be constructed by controlling the correlation struc-
ture. In Section 4, we propose strategies to build the ﬁnal projected subspace by
aggregating the representatives from the numerical one-dimensional subspaces.
Section 5 validates the eﬀectiveness of our idea on real-world document data
sets. Section 6 concludes this paper with some remarks on future work.

2 Related Work

Our work can be categorized as dimension reduction for clustering. Although
there have been extensive studies of dimension reduction techniques in the lit-
erature, few of them are designed specially for the general clustering problems.

Finding Well-Clusterable Subspaces for High Dimensional Data

313

In [10], the idea of grouping correlated features was exploited for the regression of
the DNA microarray data. Speciﬁcally, the authors deﬁned the “supergenes” by
averaging the genes within the correlated feature subspaces and then used them
to ﬁt the regression models. In our case of unsupervised clustering, however, we
do not have response for learning, which was used in [10] to analyze the accuracy
improvement of the regression with the averaged features. Instead, we show that
the subspaces of correlated features are actually of numerical one-dimension,
which speaks to the improved clustering stability. Furthermore, empirical stud-
ies on real-world data sets suggest that they enjoy higher clustering separability
which aligns better with the true class labels. In [1], another approach of dimen-
sion reduction, random projection, was exploited for the clustering problems. It
is shown that any set of N points in D dimensions can be projected into O(K/2)
dimensions, for  ∈ (0, 1/3), where optimal K-means can be preserved. In the
later experiments, we will compare our methods with this baseline approach.

Another category of related work includes the validation measures of the clus-
tering results. [17] gave an organized study of the external validation measures.
Normalization solutions and major properties of several measures were provided.
Later, [9] investigated more widely used internal clustering validation measures.
Recently, [5] studied the eﬀectiveness of the validation measures with respect to
diﬀerent distance metrics. It is shown that the validation measures might bias-
edly prefer some distance metrics. Thus, we should be careful with the choice of
validation measures involving distance computation.

3 Numerical 1-Dimensional Subspace Construction

In this section, we ﬁrst use the simpleness of 1-dimensional clustering to intro-
duce the motivation of our work. Then we show how to construct numerical
1-dimensional subspaces by controlling the correlation structure of the features.

3.1 1-Dimensional Clustering

The clustering problem can be formulated as:

Problem 1. Given a set of observations X, and the number of clusters K, the

optimal clustering solution C = {C1,··· , CK} minimizes the so-called within-

cluster sum of squares (WCSS):

WCSS(X|C) =

K(cid:2)

(cid:2)

x∈X∩Ck

k=1

(cid:3)x − μk(cid:3)2

where μk is the centroid of cluster Ck.

The most common solver for this problem, K-means [18], can only achieve local
optima, which are not stable. Indeed, we might have more than one solutions,
which are often inconsistent with one another. However, there is a special place
where K-means yields more stable clustering results: 1-dimensional space.

314

C. Liu et al.

Proposition 1. For any two K-means clustering solutions on a 1-dimensional
2 ,···} and C2 = {C2
data set, C1 = {C1
i ∈
1 , C1
2 < ··· for j = 1, 2, there are no data points x1 and x2 such
j
j
C
i where c
1 < c
that x1 ∈ C1
1 , x2 ∈ C1
2 but x1 ∈ C2
2 , x2 ∈ C2
1 .

2 ,···}, with cluster centers c

1 , C2

j

j

The proof is straightforward and is omitted due to space limit. In other words,
K-means clustering is very simple in 1-dimensional space, which is equivalent to
ﬁnding the cut points. This can also be intuitively visualized in the clustergram
[12], as we will see later in Figure 1. In short, the clustergram examines how
data points in each cluster are assigned to new clusters in the next round as
the number of clusters increase. When Proposition 1 holds, it is expected that
there are few cross lines connecting the consecutive solutions. However, few data
are so perfectly “1-dimensional” in reality. Hence, in the following, we seek 1-
dimension-like subspaces, where Proposition 1 can be preserved approximately.

3.2 Numerical 1-Dimensional Subspace

In 1-dimension-like subspaces (subset of features), it is observed that, if most of
the variation of the data can be captured by the ﬁrst principle component, then
K-means is roughly equivalent to clustering in 1-dimensional space (along the
ﬁrst principle direction). In this case, Proposition 1 will still hold under the mild
assumption that all cluster centers can be roughly connected by a line parallel
to the ﬁrst principle direction. Speciﬁcally, note that, if data point x is closer
to cluster center c, its projection (cid:4)x, v(cid:5) is also closer to c on the axis of the
ﬁrst principle direction v. Formally, this notion is captured by the numerical
1-dimensional space deﬁne below [11, 7]:

Deﬁnition 1. A data set X is numerical 1-dimensional with error , if and only

if σ2 ≤ σ1, where σ1 ≥ σ2 ≥ ··· are singular values of X (standardized to be of

zero-mean and unit-variance along each feature).

At ﬁrst glance, we need to perform singular value decomposition many times to
ﬁnd such subspaces, which is expensive in high dimensional space. Nevertheless,
as we will show below, the error  is bounded with a term of correlation among
features, which can be leveraged to construct the desired subspaces eﬃciently.

Theorem 1. If the average correlation of diﬀerent features in the d-dimensional
data set X is ρ > 0, then X is numerical 1-dimensional with error

(cid:3)

 ≤

(1 − ρ)d − 1 + ρ

ρd + 1 − ρ

<

(cid:4)

1 − ρ
ρ

.

Proof. Suppose matrix X ∈ R
N×d is already standardized to be of zero-mean and
unit-variance along each feature (column). Then the feature correlations of X can
be expressed by C = 1
X where the diagonal coeﬃcients are all 1. With the
singular value decomposition (SVD) X = U ΣV
where U, V are unitary matrices

(cid:5)
N X

(cid:5)

Finding Well-Clusterable Subspaces for High Dimensional Data

315

and the diagonal coeﬃcients of Σ are σ1, σ2,··· , we have C = 1
(cid:5)

N V Σ

(cid:5)

ΣV

(cid:5)

where

Σ

Σ = diag(σ2

1, σ2

2,··· ). It follows that
1 + σ2

(σ2

1
N

2) ≤ tr(C) = d.

Let J be the column vector with 1 as all coeﬃcients, then on one hand we have

(cid:5)

N J

CJ = (V
≤ σ2

1

(cid:5)

(cid:5)
J)
(cid:2)

(cid:5)

Σ
(cid:2)
(

(cid:5)

J) =

Σ(V

(cid:2)

(cid:2)
(

vji)2σ2
i

i

j

vji)2 = σ2

1(V

(cid:5)

(cid:5)
J)

(V

(cid:5)

J) = σ2

1J

(cid:5)

J = dσ2
1.

i

j

On the other hand, with the average of non-diagonal coeﬃcients in C, ρ, we
have J

i,j cij ≥ (d2 − d)ρ + d. Hence, it follows that

CJ =

(cid:5)

(cid:5)

≥ ρd + 1 − ρ
≤ (1 − ρ)d − 1 + ρ

1
N
1
N

σ2
1

σ2
2

and this concludes our proof.

Theorem 1 suggests that, with a proper threshold of average correlation, the
agglomerative hierarchical clustering over the feature set with average linkage
can unambiguously group the original space into numerical 1-dimensional sub-
spaces with error lower than the desired level. The standard Euclidean distance
between features can be used as the linkage when the data matrix is of zero-mean
and unit-variance along each feature. In the general case, the computational com-
plexity of the agglomerative average linkage algorithm for D-dimensional data
is O(D3), which is not eﬃcient for big data applications. However, we note that
Theorem 1 still holds if we denote ρ as the minimal correlation between fea-
tures. This leads to the complete linkage clustering for which the computational
complexity can be reduced to roughly O(D2). We will use this procedure in our
experiments and denote it by F = N 1dSpaces(X, ) in the following discussions,
where X is the data matrix,  is the maximal error of numerical 1-dimensional
subspaces, and F is the constructed subspaces.

The eﬀectiveness of the subspace construction algorithm can be visualized in
Figure 1, as mentioned earlier. Speciﬁcally, for a given high dimensional data set
X, we can produce a clusgtergram by directly applying a clustering algorithm,
such as K-means with increasing number of clusters. Then we can construct the
numerical 1-dimensional subspaces F , and produce the same clusgergram in
each subspace S in F . The results show that, in the subspaces, there are few
cross lines connecting the consecutive solutions.

4 Collaborative Ensemble of Subspaces

Now we have constructed subspaces where the clustering problem can be ap-
proached stably. However, clustering algorithms directly applied to the isolated

316

C. Liu et al.

Clustergram of the PCA−weighted Mean of

the clusters k−mean clusters vs number of clusters (k)

s
r
e

l

t
s
u
c
 
e
h

t
 
f

 

 

t

o
n
a
e
M
d
e
h
g
e
w
A
C
P

 

i

0
0
0
2

0
0
5
1

0
0
0
1

0
0
5

0

0
0
5
−

2

3

4

5

6

7

8

9

10

Number of clusters (k)

Clustergram of the PCA−weighted Mean of

the clusters k−mean clusters vs number of clusters (k)

s
r
e
t
s
u
c
 
e
h

l

t
 
f

o

 

2

3

4

5

6

7

8

9

10

 

t

n
a
e
M
d
e
h
g
e
w
A
C
P

 

i

Clustergram of the PCA−weighted Mean of

the clusters k−mean clusters vs number of clusters (k)

0
1

.

8
0

.

6
0

.

4

.

0

2
0

.

0
0

.

2

3

4

5

6

7

8

9

10

0
1

.

8

.

0

6
0

.

4

.

0

2
0

.

0

.

0

s
r
e
t
s
u
c
 
e
h

l

t
 
f

o

 

 

n
a
e
M
d
e
h
g
e
w
A
C
P

 

t

i

Number of clusters (k)

Number of clusters (k)

Fig. 1. Comparison of clustergram, where cluster means of consecutive cluster solu-
tions are connected with parallelograms whose widths are proportional to the size of
data assigned from the previous clusters. The top ﬁgure shows the clustergram of the
high dimensional space. The bottom ﬁgures show the clustergram of two numerical
1-dimensional subspaces.

subspaces might produce degenerated solutions, since no information is shared
between the subspaces. On the other hand, since each subspace S is numer-
ically only of 1 dimension, it can be approximated by a few observation fea-
tures. A natural way to this end is to investigate the SVD S = UΣV
, where

Σ = diag(σ1,··· , σs) is a diagonal matrix consisting of s positive singular values
of S: σ1 ≥ ··· ≥ σs. In general, we can transform S to SV by the principal direc-

(cid:5)

tions in V. Then, guaranteed by Theorem 1, we can use only the ﬁrst principal
component Sv where v is the ﬁrst principal direction in V corresponding to σ1.
Note that, this is often computationally more eﬃcient, since we only need the
ﬁrst singular vector and it is not necessary to fully decompose S. Also, when the
number of features are small in S, the computation can be further boosted by
S as in Theorem 1. This collaborative strategy of subspace en-
decomposing S
semble is detailed in Algorithm 1, where mSpace(F ) denotes the combination of
the projected components of the multiple subspaces in F , and mCluster denotes
the clustering problem solver applied to mSpace(F ).

(cid:5)

In addition to the above strategy of aggregating projected components, we can
also progressively approximate the subspaces in the light of [8]. Speciﬁcally, sup-
pose we have the approximation (cid:6)Sd for the ﬁrst d subspaces S1, S2,··· , Sd. To
(cid:5)
approximate the next new subspace Sd+1, we compute the SVD S = UΣV
,
where S = ((cid:6)Sd, Sd+1) is concatenation of (cid:6)Sd and Sd+1. Then the new ap-
proximation (cid:6)Sd+1 = SP where P are the top d + 1 principal directions in V.

Finding Well-Clusterable Subspaces for High Dimensional Data

317

Algorithm 1. The multiple subspaces clustering algorithm

Signature: C = mCluster(X, K, )
Input: The data matrix X; The number of clusters K; The maximal error of numerical

1-dimensional subspaces .

Output: The clustering C.
1. Construct subspaces F ← N 1dSpaces(X, ).
2. for Each subspace S ∈ F do
3.
4.
5. end for
6. Construct (cid:2)X = mSpace(F) by combining the approximated subspaces in F.
7. Solve Problem 1 in the space (cid:2)X with the parameter K, e.g., compute the Kmeans

Compute the ﬁrst singular vector v of S.
Replace S in F by Sv.

clustering C ← kmeans( (cid:2)X, K).

Table 1. The characteristics of data sets

data
#doc
#term
#class

MinClass
MaxClass
Min/Max

fbis
2463
2000

17
38
506
0.075

k1a
2340
4707
20
9

494
0.018

la1
3204
6188

6

273
943
0.290

re0
1504
2886

13
11
608
0.018

re1
1657
3758
25
10
371
0.027

wap
1560
8460
20
5

341
0.015

The details are given in Algorithm 2, where pSpace(F ) denotes the approxima-
tion described above for the subspaces in F , and pCluster denotes the clustering
problem solver applied to pSpace(F ).

5 Experimental Evaluation

5.1 Experimental Data Sets

For evaluation, we used six real data sets from diﬀerent domains, all of which
are available at the website of CLUTO [4]. Some characteristics of these data
sets are shown in Table 1. One can see diverse characteristics in terms of size
(#doc), dimension (#term), number of clusters (#class) and cluster balance are
covered by the investigated data sets. The cluster balance is measured by the
ratio MinClass/MaxClass, where MinClass and MaxClass are the sizes of the
smallest class and the largest class, respectively.

5.2 Comparison of Performance Lift

To see how much improvements can be achieved by the subspaces, regardless
which solver of Problem 1 is used, we compute the performance lift [14, 5] in

318

C. Liu et al.

Algorithm 2. The progressive subspaces clustering algorithm

Signature: C = pCluster(X, K, )
Input: The data matrix X; The number of clusters K; The maximal error of numerical

1-dimensional subspaces .

Output: The clustering C.
1. Construct subspaces F = {S1, S2,· ··} ← N 1dSpaces(X, ).
2. Order subspaces in F by the descending order of numerical 1-dimensional error.
3. Initialize the pSpace(F) as (cid:2)X ← (), i.e., empty space.
4. d ← 0.
5. repeat
d ← d + 1.
6.
S ← ( (cid:2)X, Sd).
7.
Compute the ﬁrst d singular vectors P of S.
8.
(cid:2)X ← SP.
9.
10. until d reaches the number of subspaces in F
11. Solve Problem 1 in the space (cid:2)X with the parameter K, e.g., compute the Kmeans

clustering C ← kmeans( (cid:2)X, K).

(cid:5)T

t=1

the approximated subspaces. Speciﬁcally, the performance lift can be deﬁned
by the expectation: lift(X|Y ) = E[ WCSS(X|C)
WCSS(X|Y ) ] where C is a random clustering
assignments for the data set X and Y is the true class labels. The performance
lift actually represents the diﬀerence between the ground truth of the clustering
structure and the random clustering solution. The higher the lift is, the easier it
will be for the solver of Problem 1 to ﬁnd the optimal solutions. Thus, we can
use this lift to see which subspaces help most. To estimate the lift(X|Y ), we can
generate T (e.g., 10) random clustering assignments {C1,··· , CT}, and compute
WCSS(X|Ct)
the average: 1
WCSS(X|Y ) . In Figure 2, we show the performance lifts in
T
diﬀerent approximated subspaces for all the data sets.
Speciﬁcally, we generate T = 10 random clustering assignments to estimate
the performance lift. By controlling the error  used in F = N 1dSpaces(X, ),
we can construct approximation mSpace(F ) and pSpace(F ) with diﬀerent di-
mensions, e.g., d = 100, 200,··· , 1000. For comparison, we also compute the per-
formance lifts with top d principal components constructed by simple PCA, as
denoted by “P C” in Figure 2. The line denoted by “RP ” stands for Random Pro-
jection [1], which constructs the low dimensional approximation of X ∈ R
N×D
√
by XΩ where Ω ∈ R
d with
equal probability. We can see that mSpace, pSpace, and P C are all eﬀective to
boost the performance lift. Also, while mSpace and pSpace outperform others
consistently, mSpace achieves signiﬁcantly higher lift of performance.

D×d is random matrix with entries +1/

√
d or −1/

Finding Well-Clusterable Subspaces for High Dimensional Data

319

t
f
i
l

e
c
n
a
m
r
o
f
r
e
P

t
f
i
l

e
c
n
a
m
r
o
f
r
e
P

t
f
i
l

e
c
n
a
m
r
o
f
r
e
P

1.5

1.4

1.3

1.2

1.1

1.14

1.12

1.1

1.08

1.06

1.04

1.02

1.25

1.2

1.15

1.1

fbis

k1a

mSpace
pSpace

PC
RP

mSpace
pSpace

PC
RP

t
f
i
l

e
c
n
a
m
r
o
f
r
e
P

1.25

1.2

1.15

1.1

1.05

200

400

600

800

1,000

200

400

600

800

1,000

Dimensions

la1

Dimensions

re0

mSpace
pSpace

PC
RP

mSpace
pSpace

PC
RP

t
f
i
l

e
c
n
a
m
r
o
f
r
e
P

1.14

1.12

1.1

1.08

1.06

200

400

600

800

1,000

200

400

600

800

1,000

Dimensions

re1

Dimensions

wap

mSpace
pSpace

PC
RP

mSpace
pSpace

PC
RP

t
f
i
l

e
c
n
a
m
r
o
f
r
e
P

1.2

1.15

1.1

1.05

200

400

600

800

1,000

200

400

600

800

1,000

Dimensions

Dimensions

Fig. 2. The performance lift in diﬀerent subspaces

320

C. Liu et al.

y
t
i
l
i

b
a
r
a
p
e
s

g
n
i
r
e
t
s
u
C

l

y
t
i
l
i

b
a
r
a
p
e
s

g
n
i
r
e
t
s
u
C

l

y
t
i
l
i

b
a
r
a
p
e
s

g
n
i
r
e
t
s
u
C

l

1.8

1.6

1.4

1.2

1

1.4

1.3

1.2

1.1

1

1.3

1.2

1.1

1

100 dimensional space

mSpace
pSpace

PC
RP

fbis

k1a

la1

re0

re1

wap

500 dimensional space

mSpace
pSpace

PC
RP

fbis

k1a

la1

re0

re1

wap

1000 dimensional space

mSpace
pSpace

PC
RP

fbis

k1a

la1

re0

re1

wap

Fig. 3. The clustering separability in diﬀerent subspaces

Finding Well-Clusterable Subspaces for High Dimensional Data

321

5.3 Comparison of Clustering Separability

Table 2. The clustering separability in 500 dimensional subspaces constructed with
diﬀerent methods on ‘la1’

Cluster ID

1
2
3
4
5
6

Cluster Label
Entertainment

Financial
Foreign
Metro

National
Sports

Average

mSpace
1.1596
1.0628
1.0160
1.0287
1.0225
1.1200
1.0684

pSpace
1.2010
1.0287
1.0176
1.0442
1.0418
1.0410
1.0624

PC

1.1482
1.0372
1.0223
1.0395
1.0347
1.0440
1.0543

RP

1.0328
1.0299
1.0355
1.0136
1.0191
1.0390
1.0283

tion {C1,··· , CK}, we can compute the ratio

Adopted in [4, 5], one can investigate the data separability for the unsuper-
vised clustering problem. Speciﬁcally, for each cluster Ci in the clustering solu-
EDis(Ci)
IDis(Ci) of the average external
distance, EDis(Ci), over the average internal distance, IDis(Ci). The average
internal distance IDis(Ci) is the average distance between the instances in Ci,
and the average external distance EDis(Ci) is the average distance between the
instances in Ci and the instances in the rest of the clusters Cj where j (cid:9)= i. The
higher the ratio is for a cluster, the more compact and isolated the cluster will
be, which, in turn, makes it easier for a clustering solver to identify the cluster.
The ratio results of data set ‘la1’ are listed in Table 2, which clearly indicates
that mSpace and pSpace provide better clustering separability. The last row
also reports the average separability for the 6 clusters, where mSpace performs
best. Besides, Figure 3 shows the average cluster separability for all of the six
data sets. One can see that mSpace performs best on all data sets with few
exceptions where pSpace performs better.

5.4 Analysis of Computational Cost
To reduce the dimensionality of the data matrix X ∈ R
N×D to d, our meth-
ods ﬁrst construct the numerical 1-dimensional spaces. In the general case, the
complexity of this step is O(D2), as we discussed in Section 3. To construct
mSpace, we need to perform SVD further d times in the subspaces, each of O(N )
time, since most of the subspaces are of very low dimensions and we only need
the ﬁrst principal component. Thus the total computational cost for mSpace is
O(D2 + dN ). For pSpace, the computational complexity of the progressive SVD
is costly O(d2N ) and the total cost is O(D2 + d3N ). For PCA, we have the com-
putational cost of O(N 2D) when N ≤ D or O(N D2) when D ≤ N . In our experi-
ments, since most of the data sets are very high dimensional, we have the order of
computational cost for the evaluated methods: RP < P C < mSpace < pSpace,
which aligns with the order of performance.

322

C. Liu et al.

6 Concluding Remarks

In this paper, we proposed a numerical one-dimension approach to high di-
mensional data clustering. An eﬃcient correlation-based method was provided
to construct the numerical one-dimensional subspace, which is well-clusterable
and thus makes the clustering stable. Also, we discussed two strategies to col-
laboratively aggregate them into the ﬁnal projected space. The experiments on
real-world data sets demonstrated that such transformed data aligns better with
the true class labels with respect to clustering.

This paper focused on the collaborative ensembling of the one-dimensional
subspaces. For the future work, we plan to investigate collaboratively building
clustering submodels directly in each of these one-dimensional subspaces. In the
literature, this is related to the areas of multiple clustering [19, 6], clustering
kernel [16] and clustering ensembles [13, 2, 15], where there are still many open
problems to be answered.

References

1. Boutsidis, C., Zouzias, A., Drineas, P.: Random projections for k-means clustering.

In: NIPS, pp. 298–306 (2010)

2. Fred, A.L.N., Jain, A.K.: Combining multiple clusterings using evidence accumu-
lation. IEEE Transactions on Pattern Analysis and Machine Intelligence 27(6),
835–850 (2005)

3. G¨onen, M., Alpaydın, E.: Multiple kernel learning algorithms. The Journal of Ma-

chine Learning Research 12, 2211–2268 (2011)

4. Karypis, G.: CLUTO: Data clustering software,

http://glaros.dtc.umn.edu/gkhome/views/cluto

5. Liu, C., Hu, T., Ge, Y., Xiong, H.: Which distance metric is right: An evolutionary

k-means view. In: SDM, pp. 907–918 (2012)

6. Liu, C., Xie, J., Ge, Y., Xiong, H.: Stochastic unsupervised learning on unla-
beled data. Journal of Machine Learning Research - Proceedings Track 27, 111–122
(2012)

7. Liu, G., Lin, Z., Yu, Y.: Robust subspace segmentation by low-rank representation.

In: ICML, vol. 3 (2010)

8. Liu, J., Chen, S., Zhou, Z.-H.: Progressive principal component analysis. In: Yin,
F.-L., Wang, J., Guo, C. (eds.) ISNN 2004. LNCS, vol. 3173, pp. 768–773. Springer,
Heidelberg (2004)

9. Liu, Y., Li, Z., Xiong, H., Gao, X., Wu, J.: Understanding of internal clustering

validation measures. In: ICDM, pp. 911–916. IEEE (2010)

10. Park, M.Y., Hastie, T., Tibshirani, R.: Averaged gene expressions for regression.

Biostatistics 8(2), 212–227 (2007)

11. Rangan, A.V.: Detecting low-rank clusters via random sampling. Journal of Com-

putational Physics 231(1), 215–222 (2012)

12. Schonlau, M.: The clustergram: A graph for visualizing hierarchical and non-

hierarchical cluster analyses. The Stata Journal 3, 316–327 (2002)

13. Strehl, A., Ghosh, J.: Cluster ensembles—a knowledge reuse framework for com-
bining multiple partitions. The Journal of Machine Learning Research 3, 583–617
(2003)

Finding Well-Clusterable Subspaces for High Dimensional Data

323

14. Strehl, A., Ghosh, J., Mooney, R.: Impact of similarity measures on web-page
clustering. In: Workshop on Artiﬁcial Intelligence for Web Search (AAAI 2000),
pp. 58–64 (2000)

15. Topchy, A., Jain, A.K., Punch, W.: Clustering ensembles: Models of consensus
and weak partitions. IEEE Transactions on Pattern Analysis and Machine Intelli-
gence 27(12), 1866–1881 (2005)

16. Weston, J., Leslie, C., Zhou, D., Elisseeﬀ, A., Noble, W.S.: Semi-supervised protein
classiﬁcation using cluster kernels. In: Thrun, S., Saul, L., Sch¨olkopf, B. (eds.) NIPS
(2003)

17. Wu, J., Xiong, H., Chen, J.: Adapting the right measures for k-means clustering.

In: SIGKDD, pp. 877–886. ACM (2009)

18. Wu, X., et al.: Top 10 algorithms in data mining. Knowledge and Information

Systems 14(1), 1–37 (2008)

19. Zhang, J., et al.: Pattern classiﬁcation of large-scale functional brain networks:
Identiﬁcation of informative neuroimaging markers for epilepsy. PloS One 7(5),
e36733 (2012)


