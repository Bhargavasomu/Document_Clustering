Vocabulary Filtering for Term Weighting

in Archived Question Search

Zhao-Yan Ming, Kai Wang, and Tat-Seng Chua

Department of Computer Science,

School of Computing, National University of Singapore
{mingzy,kwang,chuats}@comp.nus.edu.sg

Abstract. This paper proposes the notion of vocabulary ﬁltering in a term weight-
ing framework that consists of three ﬁlters at the document level, collection level,
and vocabulary level. While term frequency and document frequency along with
their variations are respectively the dominant term weighting factors at the docu-
ment level and collection level, vocabulary level factors are seldom considered
in current models. In a way, stopword removal can be seen as a vocabulary
level ﬁlter, but it is not well integrated into the current term-weighting models.
In this paper, we propose a vocabulary ﬁltering and multi-level term weighting
model by integrating point-wise divergence based measure into the commonly
used TF-IDF model. With our proposed model, the speciﬁcity of the vocabu-
lary is captured as a new factor in term weighting, and stopwords are naturally
handled within the model rather than being removed according to a separately
constructed list. Experiments conducted on searching for similar questions in a
large community-based question answering archive show that: (a)our proposed
term weighting model with multiple levels is consistently better than those with
single level for retrieval task; (b)the proposed vocabulary ﬁlter well distinguishes
salient and trivial terms, and can be utilized to construct stopword lists.

1 Introduction

As large Community-based Question Answering (cQA) archives are built up through
user collaboration, the knowledge is accumulated and made ready for sharing. Research
on archived questions has emerged recently [5,14,15]. An application that facilitates
knowledge sharing and diversity maintaining is Archived Question Search (AQS). It is
a function that makes the huge resource reusable by returning relevant answered ques-
tions given a new question as a query. If good matches are found, the lag time involved
with waiting for a personal response can be avoided, thus improving user satisfaction
and avoiding repeating questions.

As a speciﬁc application of IR, AQS in cQA repository is distinct from the search
of web pages or news articles because it deals with long queries and short documents
that are both in the form of questions (for simplicity, we call query questions in AQS as
queries, and candidate questions to be searched as documents thereafter):

Long query: each AQS queries consist of natural language sentences that are supposed
to be understood and answered by other community members. This kind of queries is

M.J. Zaki et al. (Eds.): PAKDD 2010, Part I, LNAI 6118, pp. 383–390, 2010.
c(cid:2) Springer-Verlag Berlin Heidelberg 2010

384

Z.-Y. Ming, K. Wang, and T.-S. Chua

usually longer, noisier, and more verbose than keyword queries. Thus the salient terms
and trivial terms are weaved together and the information needs are usually more speciﬁc.

Short document: AQS documents are essentially the same as its queries since both are
cQA questions. Shorter document length means that most terms might appear only once
in a document, resulting in term frequency (tf ) approximates a weak binary factor.

This characteristic of cQA data makes the existing term weighting schemes, such as
TF-IDF model [13], Okapi BM25 [10], and Divergence From Randomness [2], less ca-
pable if directly applied. The major difﬁculty is that documents and collections statistics
are not adequate to provide enough information. The proper functioning of the existing
term weighting schemes is under the assumption that documents are long and queries
are short. The same difﬁculty may also be encountered by other forms of community
content,like blogs and forums, which have the concise and noise-prone nature.

This motivates us to explore beyond the document and collection. We propose the
notion of vocabulary ﬁltering as a complementary dimension of term weighting. By def-
inition, a vocabulary refers to the body of words used on a particular setting or in a par-
ticular domain. Although different vocabularies may share a similar set of terms, their
respective weights are speciﬁc. Given the underlying setting or domain, the weights can
be determined, independent of any speciﬁc collection and document that instantiates the
vocabulary. In a way, stopword removal can be seen as a simple binary vocabulary ﬁlter,
in that it assigns 0 or 1 score to a term in additional to a weighting scheme.

We propose to measure term saliency in a vocabulary by estimating a heuristic
evaluation function that accepts terms’ point-wise divergence feature as input. The
assumption under the point-wise divergence feature is that terms that have distinct
distributions in a speciﬁc vocabulary vs. the general vocabulary are important. For
instance, we expect “ipod” to have a much higher frequency in a vocabulary about
music & music players than it does in the general vocabulary, whereas the universal
stopword “the” would have similar frequencies in the two vocabularies. The vocabulary
ﬁlter, in the form of a heuristic term saliency evaluation function, is integrated into the
existing term weighting schemes as an enhancement.

2 Related Work

Archived Question Search (AQS) in cQA repository was investigated recently by [5]
and [15] using translation-based language model. The translation probability trained on
similar collections can be seen as a form of collection-level ﬁltering of term weights. We
attribute the success of their proposed model to the integration of the collection-level
evidence into the document-level language model.

In related research on term weighting models, the TF-IDF model has been widely
used and accepted. Recently, the justiﬁcation and interpretation of TF-IDF has been
studied in [1,4,12], from the perspectives of information theory, probabilistic language
modeling, binary independence retrieval, and Poisson distribution. [11] tried to interpret
the Okapi BM25 model from the perspective of poisson model, the language model, the
TF-IDF model, and a Divergence From Randomness model. However, none of these ef-
forts attempt to separate the evidences between document-level and the collection-level
in term weighting. Since our investigation is focused on the vocabulary-level evidences,

Vocabulary Filtering for Term Weighting in Archived Question Search

385

it further raises question on the roles of document level, collection level as well as vo-
cabulary level analysis in term weighting and IR effectiveness.

As for the vocabulary-level ﬁltering for term weighting, the building of a customized
stopword list [7,8] and the extraction of domain-speciﬁc keywords [3] can be seen as the
most relevant work. [6] evaluated the interestingness of a term using the KL divergence
and the JS divergence from the distribution of the human interested corpora. This work
inspires the method we use for vocabulary ﬁltering.

3 Proposed Vocabulary-Level Filter

Our goal to build a vocabulary-level ﬁlter is to quantitatively measure term saliency for
a speciﬁc vocabulary. We thus emphasize the speciﬁcity of vocabularies in construct-
ing the vocabulary-level ﬁlters. Term distribution in a speciﬁc collection is biased as
compared to that in a general collection. This speciﬁcity of term distribution in a col-
lection reﬂects the speciﬁcity of its vocabulary, and enables us to highlight the speciﬁc
important terms for a vocabulary.

3.1 Divergence Feature for Vocabulary Filtering

To capture the vocabulary level term importance, we propose to take a novel point-wise
divergence feature for each individual term, rather than divergence of two distributions.
We see the term distribution of a vocabulary as the background knowledge to instanti-
ate vocabulary ﬁltering. More broadly, the combining of all vocabularies consists of a
general background that can be used to compare against a speciﬁc vocabulary.

Jensen-Shannon(JS) divergence is a well adopted distance measure between two
probability distributions. It is deﬁned as the mean of the relative entropy of each distri-
bution to the mean distribution, with the following formula:
(cid:2)

(cid:2)

DJS(S||G) =

1
2

ps log

ps

1
2 (ps + pg)

+

1
2

pg log

pg

1
2(ps + pg)

(1)

where S and G denote the speciﬁc and general vocabularies, and ps(ti) and pg(ti)
denote their corresponding probability distribution.

As we evaluate the divergence at term level rather than at the whole sample set, we

examine the point-wise function as follows:

dJS(t) =

ps(t) log

2ps(t)

ps(t)+pg (t) + pg(t) log

2pg (t)

ps(t)+pg (t)

2

(2)

The point-wise JS function is an appropriate choice since it is symmetric and ranges
over (cid:2)+. Speciﬁcally, dJS(t) assigns a point-wise divergence score to term t highest,
when either ps(t) is much higher than pg(t), or pg(t) is much higher than ps(t), which
means the specialized terms in the vocabulary and generally recognized content repre-
sentative terms are ranked high; lower, when ps(t) and pg(t) get closer to each other.
These properties suggest that dJS emphasizes divergence at both the most frequent
terms in the speciﬁc vocabulary and the most frequent terms in the general vocabu-
lary. ps(t) and pg(t) are estimated using the Maximum Likelihood Estimator over the
speciﬁc vocabulary and the general vocabulary respectively.

386

Z.-Y. Ming, K. Wang, and T.-S. Chua

3.2 Estimating Term Saliency from Divergence Feature

Given a point-wise divergence feature, we aim to estimate the term saliency score,
which can be integrated with any existing term weighting scheme. More speciﬁcally,
we deﬁne a mapping function fv : dJS → Wv, which produces as output an estimation
Wv(denotes the term saliency score on (cid:2)+) given dJS as input.

We propose a heuristic evaluation function based on logistic function L(x) as below.

fv(x) = 1 + τ L(x),

where L(x) = 1

(3)
1+e−x is the logistic function that maps (cid:2)+ to [0.5, 1) monotonically.
Logistic function grows more slowly as |x| increases. This property enables us to
control the rate of normalization by shifting the curve along the horizontal axis. Thus a
tuning parameter α is introduced.Equation 4 represents the ﬁnal form of the the heuristic
evaluation function of vocabulary level term saliency.
1

fv(dJS) = 1 + τ

1 + e−(dJS+α)

(4)

where the parameters τ and α are tuned in Section 5.2. Since there is no direct obser-
vation of term saliency available, we tune the parameters by using the retrieval perfor-
mance as an indirect guidance.

4 Three-Level Filtering for Term Weighting

After discussing the method for term weighting accounts for the vocabulary-level infor-
mativeness of a term, we next investigate on how it can be naturally integrated into an
IR framework.

Term weighting lies in the core of current bag-of-word IR algorithms. Terms weights
discriminate the importance of terms for content representation as document descrip-
tors. The general form of the bag-of-words retrieval function with regards to term
weighting is given as:

(cid:2)

Score(Q, D) =

W (ti)

(5)

ti∈(Q∩D)

where ti is the ith query term that appears in both the query Q and the document D, and
W (·) is the term weighting model. Generally, W (·) is a function that takes in evidences
such as term frequency, document length, document frequency, etc.

Figure 1 illustrates the pipeline of our proposed three-level ﬁltering framework.
Given a term ti as input, the pipeline of three ﬁlters outputs Wt(ti), a quantity that
indicates ti’s importance. Accordingly, we break down the term weighting model into
three components. The scoring function in Equation (5) is thus rewritten as

Score(Q, D) =

fv(ti) × fc(ti) × fd(ti)

(6)

(cid:2)

ti∈(Q∩D)

where fv(ti), fc(ti), and fd(ti) are the Vocabulary-Level Filter, Collection-Level Fil-
ter, and Document-Level Filter respectively. The sequence of ﬁlters in the pipeline is
naturally decided by the scope of the ﬁlters and the implementation process.

Vocabulary Filtering for Term Weighting in Archived Question Search

387

Fig. 1. A general framework for term weighting schemes is represented in a pipeline of three
ﬁlters, the vocabulary level ﬁlter, collection level ﬁlter, and document level ﬁlter

5 Experiment

5.1 Data Collection and Evaluation Method

We assembled a collection of 325,274 questions posted on Yahoo! Answers(YA) cate-
gory Consumer Electronics from March 2008 to December 2008 using YA APIs1.
The archived questions have an average length of approximately 60 words, consisting
of “subject” and “content”. The statistic for replicating the term distribution in the gen-
eral vocabulary was acquired from a project called Web Term Document Frequency and
Rank, a joint effort of the UC Berkeley and Stanford WebBase Projects2.

For evaluation, 100 questions were assembled by randomly selecting questions from
the whole collection. 93 questions were ﬁnally used after manually removing the noisy
and redundant ones. The top 20 results of the 93 queries by all the experimented models
were labeled by two independent assessors to be relevant or not. The evaluation system,
as well as the testing set and the archive, are publicly accessible3.

We use Terrier [9] for indexing and retrieving, and Porter Stemmer to stem the cQA
collection, the queries, and the general web vocabulary. The evaluation metrics are
Mean Average Precision(MAP) and Mean Reciprocal Rank (MRR).

5.2 Archived Question Search with Vocabulary Filters

Experimental Setup.
In this suit of experiment, we use vocabulary ﬁltering to re-
place the role of stopword lists in order to test the overall quality of the new retrieval
function. The efﬁciency aspect of stopword removal is not considered here. The com-
parison systems are (1)D.(tf ); (2) C.D.(idf -tf ); (3) V.D. (js-tf ); and (4) V.C.D.
(js-idf -tf ), where V. denotes the proposed vocabulary-level ﬁlter fv(dJS); C. denotes
the collection-level ﬁltering (fc(t) = ln(1 + N
df )), D. denotes the document-level ﬁlter-
ing (fd(t) = 1 + ln tf

dl ).

Parameter Tuning for Term Salience Function. A small set of 20 queries are used
for tuning the parameters of the term salience estimation function as in in Equation 4.
For simplicity, we ﬁx τ to be 1.0 as it does not inﬂuence the shape of the function.
Therefore, only the optimal α is studied in this section.

Figure 2 shows the inﬂuence of α on MAP for V.C.D. term weighting scheme. At
the point α = 2.0, the MAP starts to approach its maximum. This is because the logistic

1 http://developer.yahoo.com/answers/
2 http://www.comp.nus.edu.sg/∼rpnlpir/
3 http://www.comp.nus.edu.sg/∼g0601820/aqs/

388

Z.-Y. Ming, K. Wang, and T.-S. Chua

V.C.D.

P
A
M

 0.364

 0.3635

 0.363

 0.3625

 0.362

 0.3615

 0

 0.5

 1

 1.5

 2

 2.5

 3

 3.5

 4

alpha

Fig. 2. MAP at different normalization level of α for V.C.D.

function begins to saturate around 2 and grows slowly thereafter as the input increases
on (cid:2)+. This slow growth satisﬁes the requirement for a deep normalization on dJS(t).
It is worth noticing that this optimal α ranges over [2.0, 2.9]. This relatively wide range
suggests the stability and tolerance of the proposed heuristic term salience estimation
function. For the rest of the experiments, α is set to be 2.0.

Table 1. The MAP and MRR of V.C.D. and improvement over two baselines D. and C.D.

MAP

D.
C.D.
0.1874 0.3182
V.D.
0.2942 56.99% -7.54%
V.C.D 0.3622 93.28% 13.83%

MRR

D.
C.D.
0.5856 0.6987
V.D.
0.7396 26.30% 5.85%
V.C.D 0.7616 30.05% 9.00%

Overall Results and Discussion. Table 1 presents the overall results in term of MAP
and MRR. MAP is based on the top 20 returned results and MRR evaluates the quality
of the top results returned. We draw following observations from the table:

1) Vocabulary Filter in conjunction with collection and document level ﬁlters (i.e.
V.C.D), boosts tf and idf.tf signiﬁcantly in both MAP and MRR. MAP comparison
in Table 1 shows that V.C.D. improves over D. by 93.28%, and C.D. by 13.83%. The
consistent improvement suggests that vocabulary level evidence is complimentary to
collection level and document factors for term weighting. The scale of improvement
over C. is higher than that over C.D., which indicates V.D. is a much stronger baseline
than D.. In other words, collection level evidence is also critical for measuring term
importance. The only negative improvement is V.D. over C.D., which shows that V.D.
is not as effective as the classical tf.idf model. This also suggests that V., C., and D.
are three orthogonal factors critical for term weighting.

2) Vocabulary Filter improves the top results when the baselines are already very
high. By examining MRR comparison in Table 1, we ﬁnd that two baseline systems both
have MRR of over 0.5, which suggests that YA archives have considerable number of
similar questions and it is relatively easy to ﬁnd a similar one with a high ranking. The
two systems with V., i.e., V.D. and V.C.D. both have MRR of over 0.7, which shows
that both systems have most of their top retrieval results correct. We also notice that
V.D. has a higher MRR than C.D., while the latter has higher MAP, which conﬁrms
our assertion of the orthogonal of V., C., and D..

Vocabulary Filtering for Term Weighting in Archived Question Search

389

Table 2. Highest and lowest ranked 10 terms in music & music players category by fv(dJ S)

ipod
itune
page

1
2
3
4 my
5

song

Top 10

Lowest 10

home
servic
provid

-1
6
-2
7
-3
8
9 mail
-4
10 copyright -5

us
of
in
by
new

at
-6
thi
-7
on
-8
-9
all
-10 be

5.3 Study on Rank Terms Using Vocabulary Filtering

To examine the effect of the two divergence kernels, we utilize them to rank terms from
a subcategory of ConsumerElectronics, i.e., the music & music players question
archive. From the top 10 terms in Table 2, we ﬁnd that the vocabulary ﬁlter has suc-
cessfully captured the salient terms of the recent music & music players vocabulary,
such as “ipod”, “itune”, and “sync”. We may guess that if the archive was collected
years earlier, the top terms might be “walkman”, “tape” and the like. It suggests that
the vocabularies are evolving, or more generally, are speciﬁc. We notice that terms like
“my” is ranked high, because of the user-collaborative nature of the YA archive. fv(djs)
also ranks some general terms high, such as “mail” and “copyright”. This is because
“mail” and “copyright” have high probabilities in the general web vocabulary, but low
probabilities in the music & music players vocabulary. They are considered to be
informative though relatively less frequent in the speciﬁc vocabulary. This shows that
fv(dJS) is capable of capturing term importance in a vocabulary.

The stopwords are expected be among the lowest in a descending ranked list. Left
part of Table 2 lists the lowest 10 terms by fv(dJS). Generally this lowest 10 terms meet
our expectation of the commonly recognized stopwords. We thus think of eliminating
the lowest ranked terms from indexing, as what stopword removal does, for the purpose
of improving the efﬁciency of the whole retrieval system. As a complementary explo-
ration, we construct stopword lists by taking the lowest 5%, 10%, and 15% fv(dJS)
ranked terms. In stead of implementing the full-ﬂedged V.C.D. ﬁltering term weight-
ing scheme, we use the 3 stopword lists of different size upon C.D. term weighting
scheme and ﬁnd that the retrieval performance at 5% removal actually improves over
those without stopword removal and with standard stopword list removal. Moreover,
10% removal is slightly worse than 5% removal since less terms are used for indexing,
but still acceptable considering that the efﬁciency is improved at a small price.

6 Conclusions

In this paper, we proposed a novel notion of vocabulary-ﬁltering to capture term im-
portance by using the whole vocabulary as the background knowledge. JS divergences
are utilized to characterize a speciﬁc vocabulary by contrasting its term distribution to
that of of a general vocabulary. The normalized vocabulary ﬁlters are integrated into a
framework that consists of a pipeline of three ﬁlters at the document level, collection
level, and vocabulary level. Our proposed model has been empirically shown to be sig-
niﬁcantly better than TF-IDF model in tackling the archived question search problem.

390

Z.-Y. Ming, K. Wang, and T.-S. Chua

In future work, we plan to explore the use of vocabulary ﬁltering and the three-level
term weighting schemes in other text processing tasks like document clustering and
categorization.

References

1. Aizawa, A.: An information-theoretic perspective of tf-idf measures. Inf. Process. Man-

age. 39(1), 45–65 (2003)

2. Amati, G., Joost, C., Rijsbergen, V.: Probabilistic models for information retrieval based on

divergence from randomness. ACM TOIS 20, 357–389 (2002)

3. Frank, E., Paynter, G.W., Witten, I.H., Gutwin, C., Nevill-manning, C.G.: Domain-speciﬁc

keyphrase extraction, pp. 668–673. Morgan Kaufmann Publishers, San Francisco (1999)

4. Hiemstra, D.: A probabilistic justiﬁcation for using tf idf term weighting in information re-

trieval. International Journal on Digital Libraries 3, 131–139 (2000)

5. Jeon, J., Croft, W.B., Lee, J.H.: Finding similar questions in large question and answer

archives. In: Proc. of CIKM, pp. 84–90. ACM, New York (2005)

6. Kor, K.-W., Chua, T.-S.: Interesting nuggets and their impact on deﬁnitional question an-

swering. In: Proc. of SIGIR, pp. 335–342. ACM, New York (2007)

7. Lo, R., He, B., Ounis, I.: Automatically building a stopword list for an information retrieval

system. In: Proc. of Dutch-Belgian DIR, Utrecht, Netherlands (2005)

8. Makrehchi, M., Kamel, M.S.: Automatic extraction of domain-speciﬁc stopwords from la-
beled documents. In: Macdonald, C., Ounis, I., Plachouras, V., Ruthven, I., White, R.W.
(eds.) ECIR 2008. LNCS, vol. 4956, pp. 222–233. Springer, Heidelberg (2008)

9. Ounis, I., Amati, G., Plachouras, V., He, B., Macdonald, C., Lioma, C.: Terrier: A High
Performance and Scalable Information Retrieval Platform. In: Proc. of SIGIR Workshop on
OSIR (2006)

10. Robertson, S.E., Walker, S.: Some simple effective approximations to the 2-poisson model
for probabilistic weighted retrieval. In: Proc. of SIGIR, pp. 232–241. Springer, New York
(1994)

11. Robertson, S.E., Zaragoza, H.: The probabilistic relevance model: Bm25 and beyond. In:

Tutorial of SIGIR, ACM, New York (2008)

12. Roelleke, T., Wang, J.: Tf-idf uncovered: a study of theories and probabilities. In: Proc. of

SIGIR, pp. 435–442. ACM, New York (2008)

13. Salton, G., Yang, C.: On the speciﬁcation of term values in automatic indexing. Journal of

Documentation 29, 351–372 (1973)

14. Wang, K., Ming, Z., Chua, T.-S.: A syntactic tree matching approach to ﬁnding similar ques-
tions in community-based qa services. In: Proc. of SIGIR, pp. 187–194. ACM, New York
(2009)

15. Xue, X., Jeon, J., Croft, W.B.: Retrieval models for question and answer archives. In: Proc.

of SIGIR, pp. 475–482. ACM, New York (2008)


