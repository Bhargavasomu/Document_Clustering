A Novel Weighted Ensemble Technique

for Time Series Forecasting

Ratnadip Adhikari and R.K. Agrawal

Jawaharlal Nehru University, New Delhi-110067, India

School of Computer and Systems Sciences,
{adhikari.ratan,rkajnu}@gmail.com

Abstract. Improvement of time series forecasting accuracy is an active
research area having signiﬁcant importance in many practical domains.
Extensive works in literature suggest that substantial enhancement in
accuracies can be achieved by combining forecasts from diﬀerent mod-
els. However, forecasts combination is a diﬃcult as well as a challenging
task due to various reasons and often simple linear methods are used
for this purpose. In this paper, we propose a nonlinear weighted ensem-
ble mechanism for combining forecasts from multiple time series models.
The proposed method considers the individual forecasts as well as the
correlations in pairs of forecasts for creating the ensemble. A successive
validation approach is formulated to determine the appropriate combi-
nation weights. Three popular models are used to build up the ensemble
which is then empirically tested on three real-world time series. Obtained
forecasting results, measured through three well-known error statistics
demonstrate that the proposed ensemble method provides signiﬁcantly
better accuracies than each individual model.

Keywords: Time Series Forecasting, Ensemble Technique, Box-Jenkins
Models, Artiﬁcial Neural Networks, Elman Networks.

1

Introduction

Time series forecasting has indispensable importance in many practical data
mining applications. It is an ongoing dynamic area of research and over the
years various forecasting models have been developed in literature [1,2]. A major
concern in this regard is to improve the prediction accuracy of a model without
sacriﬁcing its ﬂexibility, robustness, simplicity and eﬃciency. However, this is not
at all an easy task and so far no single model alone can provide best forecasting
results for all kinds of time series data [3,4].

Combining forecasts from conceptually diﬀerent methods is a very eﬀective
way to improve the overall forecasting precisions. The earliest use of this prac-
tice started in 1969 with the monumental work of Bates and Granger [5]. Till
then, numerous forecasts combination methods have been developed in litera-
ture [6,7,8]. The precious role of model combination in time series forecasting can
be credited to the following facts: (a) by an adequate ensemble technique, the

P.-N. Tan et al. (Eds.): PAKDD 2012, Part I, LNAI 7301, pp. 38–49, 2012.
c(cid:2) Springer-Verlag Berlin Heidelberg 2012

A Novel Weighted Ensemble Technique for Time Series Forecasting

39

forecasting strengths of the participated models aggregate and their weaknesses
diminish, thus enhancing the overall forecasting accuracy to a great extent, (b)
often, there is a large uncertainty about the optimal forecasting model and in
such situations combination strategies are the most appropriate alternatives to
use, and (c) combining multiple forecasts can eﬃciently reduce errors arising
from faulty assumptions, bias, or mistakes in the data [3].

The simple average is the most widely used forecasts combining technique. It
is easy to understand, implement and interpret. However, this method is often
criticized because it does not utilize the relative performances of the contributing
models and is quite sensitive to the extreme errors [1,3]. As a result, other
forms of averaging, e.g. trimmed mean, Winsorized mean, median, etc. have
been studied in literature [9]. Another common method is the weighted linear
combination of individual forecasts in which the weights are determined from
the past forecast errors of the contributing models. But, this method completely
ignores the possible relationships between two or more participating models and
hence is not so adequate for combining nonstationary and chaotic data. Various
modiﬁcations of this linear combination technique have also been suggested by
researchers [9,10,11].

In this paper, we propose a weighted nonlinear framework for combining mul-
tiple time series models. Our approach is partially motivated by the work of
Freitas and Rodrigues [12]. The proposed technique considers individual fore-
casts from diﬀerent methods as well as the correlations between pairs of fore-
casts for combining. We consider three models, viz. Autoregressive Integrated
Moving Average (ARIMA), Artiﬁcial Neural Network (ANN) and Elman ANN
to build up the ensemble. An eﬃcient methodology, based on a successive valida-
tion approach is formulated for ﬁnding the appropriate combination weights. The
eﬀectiveness of the proposed technique is tested on three real-world time series
(one stationary and two nonstaionary ﬁnancial data). The forecasting accura-
cies are evaluated in terms of the error measures: Mean Absolute Error (MAE),
Mean Squared Error (MSE), and Average Relative Variance (ARV).

The rest of the paper is organized as follows. Section 2 describes a number
of common forecasts combination techniques. Our proposed ensemble scheme is
presented in Sect. 3. In Sect. 4, we describe the three time series forecasting
models, which are used here to build up the ensemble. Experimental results are
reported in Sect. 5 and ﬁnally Sect. 6 concludes this paper.

2 Forecasts Combination Methods
Let the actual time series be Y = {y1, y2, . . . , yN} and ˆY(i) = {ˆy(i)
N }
, . . . , ˆy(i)
be its forecast obtained from the ith method (i = 1, 2, . . . , N ). Then the series
obtained from linearly combining these n forecasted series is given by:

, ˆy(i)

1

2

⎧
⎪⎪⎨

⎪⎪⎩

N },
, . . . , ˆy(c)

1
k + w2 ˆy(2)

k + ··· + wn ˆy(n)

k =

, ˆy(c)
2

ˆY(c) = {ˆy(c)
ˆy(c)
k = w1 ˆy(1)
∀k = 1, 2, . . . , N

n(cid:6)

i=1

wi ˆy(i)

k

(1)

40

R. Adhikari and R.K. Agrawal

1/n (i = 1, 2, . . . , n) [9,10].

where, wi is the weight assigned to the ith forecasting method. To ensure un-
biasedness, sometimes it is assumed that the weights add up to unity. Diﬀerent
combination techniques are developed in literature which are based on diﬀerent
weight assignment schemes; some important among them are discussed here:
• In the simple average, all models are assigned equal weights, i.e. wi =
• In the trimmed average, individual forecasts are combined by a simple arith-
metic mean, excluding the worst performing k% of the models. Usually, the
value of k is selected from the range of 10 to 30. This method is sensible only
when n ≥ 3 [9,10].
• In the Winsorized average, the i smallest and largest forecasts are selected
• In the error-based combining, an individual weight is chosen to be inversely
• In the outperformance method, the weight assignments are based on the
• In the variance-based method, the optimal weights are determined by mini-

proportional to the past forecast error of the corresponding model [3].

number of times a method performed best in the past [11].

smallest and largest forecasts, respectively [9].

and set to the (i + 1)

th

mizing the total Sum of Squared Error (SSE) [7,10].

All the combination techniques, discussed above are linear in nature. The lit-
erature on nonlinear forecast combination methods is very limited and further
research works are required in this area [10].

3 The Proposed Ensemble Technique

Our ensemble technique is an extension of the usual linear combination method
in order to deal with the possible correlations between pairs of forecasts and is
partially motivated from the work of Freitas and Rodrigues [12].

3.1 Mathematical Description

For simplicity, here we describe our ensemble technique for combining forecasts
from three methods; but, it can be easily generalized . Let, the actual test dataset

T

of a time series be Y = [y1, y2, . . . , yN ]
being
its forecast obtained from the ith method (i = 1, 2, 3). Let, μ(i) and σ(i) be the
mean and standard deviation of ˆY(i) respectively. Then the combined forecast
of Y is deﬁned as: ˆY(c) =

with ˆY(i) =

, where

, . . . , ˆy(c)
N

(cid:8)T

(cid:7)
ˆy(c)
1

, ˆy(c)
2

, . . . , ˆy(i)

N

(cid:7)
ˆy(i)

1

, ˆy(i)

2

(cid:8)T

ˆy(c)
k = w0 + w1 ˆy(1)

k + w2 ˆy(2)
k v(3)

k + w3 ˆy(3)
k
k v(1)
k + θ3v(3)
k

k + θ2v(2)

+ θ1v(1)

k v(2)

(2)

v(i)
k =

(cid:9)
k − μ(i)
y(i)

(cid:10)

/

(cid:10)2

(cid:9)

σ(i)

, ∀i = 1, 2, 3; k = 1, 2, . . . , N.

A Novel Weighted Ensemble Technique for Time Series Forecasting

41

In (2), the nonlinear terms are included in calculating ˆy(c)
to take into account
k
the correlation eﬀects between two forecasts. It should be noted that for com-
bining n methods, there will be

nonlinear terms in (2).

(cid:12)

(cid:11)

n
2

3.2 Optimization of the Combination Weights

The combined forecast deﬁned in (2) can be written in vector form as follows:

ˆY(c)

k = Fw + Gθ

(3)

where,

w = [w0, w1, w2, w3]T , θ = [θ1, θ2, θ3]T .

(cid:7)

1| ˆY(1)| ˆY(2)| ˆY(3)

F =

(cid:8)
N×4

.

⎡

1 = [1, 1, . . . , 1]T .
v(2)
v(3)
1
1
...
N v(3)
N v(2)

v(1)
v(2)
1
1
...
v(1)
N v(2)

⎢
⎢
⎣

G =

v(3)
v(1)
1
1
...
N v(1)
N v(3)
N

⎤

⎥
⎥
⎦

.

N×3

The weights are to be optimized by minimizing the forecast SSE, given by:

(cid:9)

N(cid:6)

(cid:10)2

yk − ˆy(c)

k

SSE =

k=1

= (Y − Fw − Gθ)T (Y − Fw − Gθ)
= YTY − 2wTb + wTVw+
2wTZθ − 2θTd + θTUθ

(4)

where,

(cid:19)

(cid:19)

V =

d =

(cid:20)
FTF

GTY

4×4
(cid:20)
3×1

, b =
, U =

(cid:20)

(cid:19)
FTY
(cid:19)

4×1
(cid:20)
GTG
3×3

.

, Z =

(cid:19)
(cid:20)
FTG

,

4×3

Now from (∂/∂w) (SSE) = 0 and (∂/∂θ) (SSE) = 0, we get the following system
of linear equations:

(cid:21)

Vw + Zθ = b
ZTw + Uθ = d

By solving (5), the optimal combination weights can be obtained as:

(cid:21)

(cid:11)

θopt =
wopt = V

U − ZTV
−1Z
−1 (b − Zθopt)

(cid:12)−1 (cid:11)

d − ZTV

(cid:12)

−1b

These optimal weights are determinable if and only if all the matrix inverses,
involved in (6) are well-deﬁned.

(5)

(6)

42

R. Adhikari and R.K. Agrawal

3.3 Approach for Weights Determination

The optimal weights in the proposed ensemble technique solely depend on the
knowledge of the forecast SSE value. But, in practical applications it is unknown
in advance, since the dataset Y to be forecasted is unknown. Due to this reason,
we suggest a robust mechanism for estimating the combination weights from the
training data. Here, we divide the available time series into a suitable number of
pairs of training and validation subsets and determine the optimal weights for
each pairs; the desired combination weights are then calculated as the mean of
all these pairwise optimal weights. In this way, the past forecasting performances
of the participating models are eﬀectively utilized for weights determination.

The necessary steps of our ensemble scheme are outlined in Alg. 1.

Algorithm 1. Weighted nonlinear ensemble of multiple forecasts
Inputs: The training data: Y = [y1, y2, . . . , yN ]T of the associated time series and its

n forecasts, obtained from the models: Mi (i = 1, 2, . . . , n).

(cid:2)

(cid:3)T

Output: The combined forecast vector ˆY(c) =
Steps:
1. Select base size, validation window and the positive integer k, such that:

2 , . . . , ˆy(c)

ˆy(c)
1 , ˆy(c)

N

.

base size + k × validation window = N.

2. Set j ← 1.
3. W ← empty, Θ ← empty

(cid:4)

// initially set both the ﬁnal weight matrices W and Θ as the empty matrices of
orders n × 1 and
4. while j ≤ k do
5.

Deﬁne:

(cid:5) × 1, respectively.

n
2

α = base size + (j − 1) × validation window.
Ytrain = [y1, y2, . . . , yα]
Yvalidation = [yα+1, yα+2, . . . , yα+validation window]
// this step selects a pair of training and validation subsets of Y.

T .

T .

6.

Train each model Mi (i = 1, 2, . . . , n) on Ytrain to forecast the corresponding
Yvalidation dataset.
Determine the optimal combination weight vectors wk and θk using (6).

7.
8. W = [W|wk], Θ = [Θ|θk]// augment the currently found weight vectors to the

corresponding weight matrices.
j ← j + 1.
9.
10. end while
11. Calculate the ﬁnal weight vectors wcomb and θcomb as:

wcomb = mean (W, row-wise) .
θcomb = mean (Θ, row-wise) .

12. Use wcomb and θcomb to calculate the combined forecast vector according to (3).

A Novel Weighted Ensemble Technique for Time Series Forecasting

43

4 Three Time Series Forecasting Models

In this paper, we consider three popular time series forecasting methods to build
up our proposed ensemble. These three methods are brieﬂy described here.

4.1 Autoregressive Integrated Moving Average (ARIMA)

The ARIMA models are the most widely used methods for time series forecasting,
which are developed by Box and Jenkins in 1970 [2]. These models are based
on the assumption that the successive observations of a time series are linearly
generated from the past values and a random noise process. Mathematically, an
ARIMA(p, d, q) model is represented as follows:

φ (L) (1 − L)

d yt = θ (L) t .

(7)

where,

φ (L) = 1 − p(cid:22)

i=1

φiLi, θ (L) = 1 +

q(cid:22)

j=1

θjLj, and Lyt = yt−1 .

The terms p, d, q are the model orders, which respectively refer to the autore-
gressive, degree of diﬀerencing and moving average processes; yt is the actual
time series and t is a white noise process. In this model, a nonstationary time
series is transformed to a stationary one by successively (d times) diﬀerencing
it [2,4]. A single diﬀerencing is often suﬃcient for practical applications. The
ARIMA(0, 1, 0), i.e. yt − yt−1 = t is the popular Random Walk (RW) model

which is frequently used in forecasting ﬁnancial and stock-market data [4].

4.2 Artiﬁcial Neural Networks (ANNs)

ANNs are the most eﬃcient computational intelligence models for time series
forecasting [10]. Their outstanding characteristic is the nonlinear, nonparametric,
data-driven and self-adaptive nature [4,13]. The Multilayer Perceptrons (MLPs)
are the most popular ANN architectures in time series forecasting. MLPs are
characterized by a feedforward network of an input layer, one or more hidden
layers and an output layer, as depicted in Fig. 1. Each layer contains a number
of nodes which are connected to those in the immediate next layer by acyclic
links. In practical applications, usually a single hidden layer is used [4,10,13].

The notation (p, h, q) is commonly used to refer an ANN with p input, h hidden
and q output nodes. The forecasting performance of an ANN model depends on
a number of factors, e.g. the selection of a proper network architecture, training
algorithm, activation functions, signiﬁcant time lags, etc. However, no rigorous
theoretical framework is available in this regard and often some experimental
guidelines are followed [13]. In this paper, we use popular model selection criteria,
e.g. Akaike Information Criterion (AIC) and Bayesian Information Criterion
(BIC) [13,14] for selecting suitable ANN structures. The Resilient Propagation

44

R. Adhikari and R.K. Agrawal

(RP) [15,16] is applied as the network training algorithm and the logistic and
identity functions are used as the hidden and output layer activation functions,
respectively.

Input Layer Hidden Layer  Output Layer

Output 

k
r
o
w
t
e
n
 
e
h
t
 
o
t
 
s
t
u
p
n
I

Bias 

…
…
…
…
…
…
…
…
…
…
…
…
…
…
…

…
…
…
…
…
…
…
…
…
…
…
…
…
…
…

Bias 

Fig. 1. Example of a multilayer feedforward ANN

4.3 Elman Artiﬁcial Neural Networks (EANNs)

Elman networks belong to the class of recurrent neural networks in which one
extra layer, known as the context layer is introduced to recognize the spatial
and temporal patterns in the input data [17]. The Elman networks contain two
types of connections: feedforward and feedback. At every step, the outputs of
the hidden layer are again fed back to the context layer, as shown in Fig. 2. This
recurrence makes the network dynamic, so that it can perform non linear time-
varying mappings of the associated nodes [16,17]. Unlike MLPs, there seems to
be no general model selection guidelines in literature for the Elman ANNs [10].
However, it is a well-known fact that EANNs require much more hidden nodes
than the simple feedforward ANNs in order to adequately model the temporal
relationships [10,16]. In this paper, we use 24 hidden nodes and the training
algorithm traingdx [16] for ﬁtting EANNs.

Input Nodes 

Context Nodes 

 

 
s
e
d
o
N
n
e
d
d
i
H

Output Nodes 

Feedforward Connection 

Feedback Connection 

Fig. 2. Architecture of an Elman network

A Novel Weighted Ensemble Technique for Time Series Forecasting

45

5 Experiments and Discussions

To empirically examine the performances of our proposed ensemble technique,
three important real-world time series are used in this paper. These are the
Wolfs sunspots, the daily closing price of S & P 500 index and the exchange
rates between US Dollar (USD) and Indian Rupee (INR) time series. These time
series are obtained from the Time Series Data Library (TSDL) [18], the Yahoo!
Finance [19] and the Paciﬁc FX database [20], respectively and are described
in Table 1. The natural logarithms of the S & P data are used in our analysis.
All three time series are divided into suitable training and testing sets. The
training sets are used for ﬁtting the three forecasting models as well as to build
up the proposed ensemble; the testing sets are used to evaluate the out-of-sample
forecasting performances of the ﬁtted models and the ensemble.

The experiments in this paper are performed using MATLAB. For ﬁtting
ANN and EANN models, the neural network toolbox [16] is used. Forecasting
eﬃcacies of the models are evaluated through three well-known error statistics,
viz. Mean Absolute Error (MAE), Mean Squared Error (MSE), and Average
Relative Variance (ARV), which are deﬁned below:

MAE =

1
n
(cid:23)

ARV =

n(cid:22)

|yt − ˆyt| , MSE =

t=1
n(cid:22)

t=1

(yt − ˆyt)2

(cid:23)

n(cid:22)

(cid:24)

/

t=1

(yt − ˆyt)2 ,

1
n

n(cid:22)

t=1

(μ − ˆyt)2

(cid:24)

,

where, yt and ˆyt are the actual and forecasted observations, respectively; N is
the size and μ is the mean of the test set. For an eﬃcient forecasting model, the
values of these error measures are expected to be as less as possible.

The sunspots series is stationary with an approximate cycle of 11 years, as
can be seen from Fig. 3(a). Following Zhang [4], the ARIMA(9, 0, 0) (i.e. AR(9))

Table 1. Description of the time series datasets

Time Series 

Description 

Sunspots 

The  annual  number  of  observed  sunspots 
(1700–1987). 

S & P 500

Daily closing price of S & P 500 index (2 Jan. 
2004–31 Dec. 2007)  

Exchange Rate 

USD  to  INR  exchange  rates  (1  July  2009–16 
Sept. 2011) 

Size 

Total size: 288 
Training: 171 
Testing: 67 

Total size: 1006 
Training: 800 
Testing: 206 

Total size: 681 
Training: 565 
Testing: 116 

46

R. Adhikari and R.K. Agrawal

and the (7, 5, 1) ANN models are ﬁtted to this time series. The EANN model is
ﬁtted with same numbers of input and output nodes as the ANN, but with 24
hidden nodes. For combining, we take base size = 41, validation window = 20
and the number of iterations k = 9.

The S & P and exchange rate are nonstationary ﬁnancial series and both ex-
hibit quite irregular patterns which can be observed from their respective time
plots in Fig. 4(a) and Fig. 5(a). The RW-ARIMA model is most suitable for
these type of time series1. For ANN modeling, the (8, 6, 1) and (6, 6, 1) net-
work structures are used for S & P and exchange rate, respectively. As usual,
the ﬁtted EANN models have the same numbers of input and output nodes as
the corresponding ANN models, but 24 hidden nodes. For combining, we take
base size = 200, validation window = 50, k = 12 for the S & P data and
base size = 165, validation window = 40, k = 10 for the exchange rate data.

In Table 2, we present the forecasting performances of ARIMA, ANN, EANN,

simple average and the proposed ensemble scheme for all three time series.

Table 2. Forecasting results for the three time series

Error Measures

ARIMA

Sunspots

S & P 5002

Exchange Rate

MAE
MSE
ARV
MAE
MSE
ARV
MAE
MSE
ARV

17.63
483.5
0.216
12.68
28.27
0.344
0.255
0.105
0.188

ANN
15.58
494.9
0.308
12.33
21.58
0.378
0.140
0.032
0.070

EANN
14.71
492.7
0.280
13.27
24.61
0.397
0.137
0.030
0.064

Average

Ensemble

13.59
384.3
0.218
10.43
15.69
0.271
0.134
0.029
0.064

12.50
274.7
0.120
9.368
13.59
0.230
0.133
0.028
0.053

From Table 2, it can be seen that our ensemble technique has provided low-
est forecast errors among all methods. Moreover, the proposed technique has
also achieved considerably better forecasting accuracies than the simple average
combination method, for all three time series. However, we have empirically ob-
served that like the simple average, the performance of our ensemble method is
also quite sensitive to the extreme errors of the component models.

In this paper, we use the term Forecast Diagram to refer the graph which shows
the actual and forecasted observations of a time series. In each forecast diagram,
the solid and dotted line respectively represents the test and forecasted time
series. The forecast diagrams, obtained through our proposed ensemble method
for sunspots, S & P and exchange rate series are depicted in Fig. 3(b), Fig. 4(b)
and Fig. 5(b), respectively.

1 In RW-ARIMA, the preceding observation is the best guide for the next prediction.
2 Original MAE=Obtained MAE×10

−3; Original MSE=Obtained MSE×10

−5.

A Novel Weighted Ensemble Technique for Time Series Forecasting

47

(a)

200

150

100

50

0

1

(b)

200

150

100

50

0

1

19

37

55

67

51

101

151

201

251 287

Fig. 3. (a) The sunspots series, (b) Ensemble forecast diagram for the sunspot series

(a)

7.4

7.3

7.2

7.1

7

(b)

7.36

7.34

7.32

7.3

7.28

7.26

7.24

7.22

0

200

400

600

800

1000

1

42

83

124

165

206

Fig. 4. (a) The S & P series, (b) Ensemble forecast diagram for the S & P series

52

50

48

46

44

42

0

(a)

150

300

450

600

700

(b)

48

47

46

45

44

1

21

41

61

81

101 116

Fig. 5. (a) Exchange rate series, (b) Ensemble forecast diagram for exchange rate series

48

R. Adhikari and R.K. Agrawal

6 Conclusions

Improving the accuracy of time series forecasting is a major area of concern in
many practical applications. Although numerous forecasting methods have been
developed during the past few decades, but it is often quite diﬃcult to select
the best among them. It has been observed by many researchers that combining
multiple forecasts eﬀectively reduces the prediction errors and hence provides
considerably increased accuracy.

In this paper, we propose a novel nonlinear weighted ensemble technique
for forecasts combination. It is an extension of the common linear combina-
tion scheme in order to include possible correlation eﬀects between the partic-
ipating forecasts. An eﬃcient successive validation mechanism is suggested for
determining the appropriate combination weights. The empirical results with
three real-world time series and three forecasting methods demonstrate that our
proposed technique signiﬁcantly outperforms each individual method in terms
of obtained forecast accuracies. Moreover, it also provides considerably better
results than the classic simple average combining technique. In future works,
our ensemble mechanism can be further explored with other diverse forecasting
models as well as other varieties of time series data.

Acknowledgments. The ﬁrst author sincerely expresses his profound gratitude
to the Council of Scientiﬁc and Industrial Research (CSIR) for the obtained
ﬁnancial support, which helped a lot in performing this research work.

References

1. Gooijer, J.G., Hyndman, R.J.: 25 Years of time series forecasting. J. Forecast-

ing 22(3), 443–473 (2006)

2. Box, G.E.P., Jenkins, G.M.: Time Series Analysis: Forecasting and Control, 3rd

edn. Holden-Day, California (1970)

3. Armstrong, J.S.: Combining Forecasts. In: Armstrong, J.S. (ed.) Principles of Fore-
casting: A Handbook for Researchers and Practitioners. Kluwer Academic Publish-
ers, Norwell (2001)

4. Zhang, G.P.: Time series forecasting using a hybrid ARIMA and neural network

model. Neurocomputing 50, 159–175 (2003)

5. Bates, J.M., Granger, C.W.J.: Combination of forecasts. Operational Research

Quarterly 20(4), 451–468 (1969)

6. Clemen, R.T.: Combining forecasts: A review and annotated bibliography. J. Fore-

casting 5(4), 559–583 (1989)

7. Aksu, C., Gunter, S.: An empirical analysis of the accuracy of SA, OLS, ERLS and

NRLS combination forecasts. J. Forecasting 8(1), 27–43 (1992)

8. Zou, H., Yang, Y.: Combining time series models for forecasting. J. Forecast-

ing 20(1), 69–84 (2004)

9. Jose, V.R.R., Winkler, R.L.: Simple robust averages of forecasts: Some empirical

results. International Journal of Forecasting 24(1), 163–169 (2008)

10. Lemke, C., Gabrys, B.: Meta-learning for time series forecasting and forecast com-

bination. Neurocomputing 73, 2006–2016 (2010)

A Novel Weighted Ensemble Technique for Time Series Forecasting

49

11. Bunn, D.: A Bayesian approach to the linear combination of forecasts. Operational

Research Quarterly 26(2), 325–329 (1975)

12. Frietas, P.S., Rodrigues, A.J.: Model combination in neural-based forecasting. Eu-

ropean Journal of Operational Research 173(3), 801–814 (2006)

13. Zhang, G., Patuwo, B.E., Hu, M.Y.: Forecasting with articial neural networks: The

state of the art. J. Forecasting 14, 35–62 (1998)

14. Faraway, J., Chatﬁeld, C.: Time series forecasting with neural networks: a compar-

ative study using the airline data. J. Applied Statistics 47(2), 231–250 (1998)

15. Reidmiller, M., Braun, H.: A direct adaptive method for faster backpropagation
learning: The rprop algorithm. In: Proceedings of the IEEE Int. Conference on
Neural Networks (ICNN), San Francisco, pp. 586–591 (1993)

16. Demuth, M., Beale, M., Hagan, M.: Neural Network Toolbox User’s Guide. The

MathWorks, Natic (2010)

17. Lim, C.P., Goh, W.Y.: The application of an ensemble of boosted Elman networks
to time series prediction: A benchmark study. J. of Computational Intelligence 3,
119–126 (2005)

18. Time Series Data Library, http://robjhyndman.com/TSDL
19. Yahoo! Finance, http://finance.yahoo.com
20. Paciﬁc FX database, http://fx.sauder.ubc.ca/data.html


