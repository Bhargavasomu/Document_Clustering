 

Chinese-English OOV Term Translation   
with Web Mining, Multiple Feature Fusion   

and Supervised Learning 

Yun Zhao1, Qinen Zhu1, Cheng Jin1, Yuejie Zhang1, Xuanjing Huang1,   

and Tao Zhang2 

1 School of Computer Science 

Shanghai Key Laboratory of Intelligent Information Processing, 

Fudan University, Shanghai 200433, P.R. China 

2 School of Information Management & Engineering, 

Shanghai University of Finance & Economics, Shanghai 200433, P.R. China 

{12210240077,13210240131,jc,yjzhang,xjhuang}@fudan.edu.cn, 

taozhang@mail.shfeu.edu.cn 

Abstract.  This  paper  focuses  on  the  Web-based  Chinese-English  Out-of-
Vocabulary (OOV) term translation pattern, and emphasizes on the translation 
selection  based  on  multiple  feature  fusion  and  the  ranking  based  on  Ranking 
Support  Vector  Machine  (Ranking  SVM).  By  utilizing  the  SIGHAN2005 
corpus for the Chinese Named Entity Recognition (NER) task and selected new 
terms,  the  experiments  based  on  different  data  sources  show  the  consistent 
results. From the experimental results for combining our model with Chinese-
English  Cross-Language  Information  Retrieval  (CLIR)  on  the  data  sets  of 
TREC,  it  can  be  found  that  the  obvious  performance  improvements  for  both 
query translation and CLIR are obtained. 

Keywords:  Chinese-English  OOV  Term  Translation,  Web  mining,  multiple 
feature fusion, supervised learning, Ranking SVM. 

1 

Introduction 

In  Cross-Language  Information  Retrieval  (CLIR),  users’  queries  are  generally 
composed of short terms, in which there are many Out-of-Vocabulary (OOV) terms 
like  Named  Entities  (NEs),  new  words,  terminologies  [1][5][6][12].  The  translation 
quality  of  OOV  term  directly  influences  the  precision  of  querying  multilingual 
information  and  OOV  term  translation  has  become  a  challenging  issue  in  CLIR 
[9][15][17].  With  the  increasing  growth  of  Web  information  which  includes 
multilingual  hypertext  resources  with  abundant  topics,  it  appears  that  Web 
information can mitigate the problem of the restricted OOV term translation accuracy 
[11][13][18]. However, how to select the correct translations from Web and locate the 
appropriate  translation  resources  rapidly  is  still  the  main  goal  for  OOV  term 
translation  [14][16][19].  Hence,  finding  the  effective  feature  representation  and  the 
optimal ranking pattern for translation candidates is the core part for the Web-based 
OOV term translation. 

M. Sun et al. (Eds.): CCL and NLP-NABD 2014, LNAI 8801, pp. 234–246, 2014. 
© Springer International Publishing Switzerland 2014 

 

Chinese-English OOV Term Translation 

235 

Many researchers have utilized Web search engines to find translation candidates 
for Chinese-English OOV term translation [8][10][13]. Zhang et al. [25] extracted the 
translation candidates for OOV query terms from Web in Chinese-English CLIR, and 
improved the CLIR performance. Zhang et al. [24] searched the translation candidates 
by using cross-language query expansion and Web, and obtained the Top-1 accuracy 
of  81.0%  in  Chinese-English  OOV  word  translation.  Fang  et  al.  [4]  used  semantic 
prediction  and  query  expansion  to  get  the  translation  candidates,  and  acquired  the 
Top-3 accuracy of 82.9% in  Chinese-English OOV term translation. Chen et al. [3] 
used  the  combination  of  Web  statistics  and  the  vocabulary,  and  acquired  the  Top-1 
accuracy of 87.6% in Chinese-English OOV word translation. Yang et al. [21] utilized 
the combination of transliteration, Web mining and ranking based on AdaBoost, and 
got the Top-5 accuracy of 76.35% for Chinese-English backward transliteration. Yang 
et al. [22] utilized heuristic Web mining and asymmetric alignment, and got the Top-1 
accuracy  of  48.71%  in  Chinese-English  organization  name  translation.  Yang  et  al. 
[23]  combined  Web  mining  and  ranking  by  SVM  and  Ranking  SVM,  and  obtained 
the Top-1 accuracy of 65.75% in Chinese-English organization name translation. 

Unfortunately,  there  are  still  three  common  problems  in  Chinese-English  OOV 
term  translation  based  on  Web  mining.  (1)  The  noises  in  English  translation 
candidates  cannot  be  processed  appropriately.  Although  there  does  not  exist  the 
issue  of  word  segmentation  in  English  key  term  extraction,  many  noises  may  be 
introduced into the candidates extracted from Web documents. However, such noises 
are  often  simply  processed,  or  even  without  any  processing.  (2)  The  feature 
information  for  the  evaluation  of  translation  candidates  is  not  enough  and 
comprehensive.  Most  methods  implement  the  evaluation  for  candidates  through 
mining simple local and Boolean features. However, if only a certain Web document 
that an OOV term appears is explored, the global information contained in the whole 
Web  document  set  is  ignored,  and  the  inconsistency  and  polysemy  of  candidates 
cannot  be  considered.  (3)  The  relevance  measurement  for  translation  pairs  is 
simple,  or  the  computation  cost  is  too  high.  For  ranking  candidates,  most 
approaches  adopt  the  simple  combination  computation  of  feature  values,  or  get 
assessment  based  on  classification  models.  The  feature  weights  are  determined 
according  to  the  general  induction  and  suitable  for  specific  fields,  and  cannot 
guarantee the accuracy for ranking. The Ranking SVM model can effectively express 
multiple ranking constraints, and has better universality and applicability [2][20]. 

To  support  more  precise  Chinese-English  OOV  term  translation,  we  establish  a 
multiple-feature-based  translation  pattern  based  on  Web  mining  and  Ranking  SVM. 
An  English  key  term  extraction  mechanism  is  built  on  the  simplified  selection,  and 
then  the  emphasis  is  put  on  the  noise  filtering.  Heuristic  rules  summarized  from 
translation  candidates  are  used  to  remove  insignificant  noises,  and  Information 
Entropy  is  introduced  to  further  discard  meaningless  substrings.  On  the  other  hand, 
translation  candidates  are  chosen  by 
the  fusion  of  multiple  features.  The 
representation  forms  of  local,  global  and  Boolean  feature  are  constructed  under  the 
consideration  for  the  characteristics  of  Chinese/English  OOV  term  and  Web 
information. For the relevance measurement between an OOV term and its translation 
candidates,  the  supervised  learning  based  on  Ranking  SVM  is  utilized  to  rank 

 

236 

Y. Zhao et al. 

candidates accurately. By utilizing the SIGHAN2005 corpus for the Chinese Named 
Entity Recognition (NER) task and manually selected new terms in various fields, our 
model  can  “filter”  the  most  possible  translation  candidates  with  better  ability.  This 
paper also attempts to apply our model in Chinese-English CLIR. It can be observed 
from the experimental results on the data sets of TREC that the obvious improvement 
for query translation is obtained. 

2 

English Key Term Extraction 

In Web mining of OOV term translation, a crucial problem is to select the translation 
candidates  from  the  returned  Web  documents,  that  is,  the  key  term  extraction  task. 
The Initial Extraction mechanism is first established to extract the initial English key 
terms from the webpage snippets obtained by using the Chinese OOV term as a query 
for  the  search  engine.  The  English  fragments  segmented  by  the  non-English 
characters  in  each  snippet  are  selected.  Given  the  following  snippet,  “Naruto 
wallpapers”,  “Naruto”,  “Two  destinys  two  different  fates”  and  “Recognize  my 
existence” are chosen as the initial key terms. 

 

Obviously, there are a lot of noises among the initial key terms. Therefore, some 
noise  patterns  are  regarded  as  Heuristic  Filtering  Rules  (HFR)  and  utilized  to 
remove the noisy  strings. (1) If an initial key term appears in the stoplist, then it is 
removed as a noisy string. The stoplist contains the stopwords with high frequency in 
common  use,  which  are  usually  irrelevant  with  the  original  OOV  term,  such  as 
“Translate  this  page”  and  “Retrieved  from  Wikipedia”.  (2)  If  an  initial  key  term 
begins or ends with a preposition or conjunction, then it is removed as a noisy string. 
(3) If an initial key term satisfies some filtering patterns, then it is removed as a noisy 
string.  Such  patterns  are  used  to  select  some  frequent  and  obviously  incorrect  key 
terms.  For  example,  an  initial  key  term  for  the  OOV  term  “ 非 洲 统 一 组 织
[Organization  of  African  Unity]”  is  “Fei1  zhou1  Tong3  yi1  Zu3  zhi1”,  which  is  a 
unreasonable form composed of both letters and numbers. (4) If multiple initial key 
terms  are  same  by  ignoring  the  case  sensitivity,  then  the  form  with  the  highest 
frequency is reserved and the others are removed as the noisy strings. For example, 
for the OOV term “费利克斯[Felix]”, all the related information for three initial key 
terms,  “Felix”,  “FELIX”  and  “felix”,  must  be  considered  in  the  subsequent  feature 
selection and computation. (5) For initial key terms with a single word corresponding 
to the same original OOV term, if a term is a prefix/suffix substring of the other terms, 
then it is removed as a noisy string. 

In  the  key  terms  obtained  by  HFR-based  filtering,  there  are  still  some  redundant 
substrings,  thus  the  optimization  based  on  Information  Entropy  is  proposed  to 
further filter such noises. For a key term x, its entropy is expressed as: 

 

 

Chinese-English OOV Term Translation 

237 

XH

(

)

−=

N



=
1

i

(
xp

i

log)

2

(
xp

i

)

 

)1(

where p(xi) denotes the frequency of x in the ith snippet, and computed as ni/n, ni is the 
occurrence times of x in the ith snippet and n is the total occurrence times of x in the 
whole snippet set; N is the total snippet number. 

Information Entropy can not only represent the amount of information content for 
key terms, but also the distribution similarity between two key terms in the snippet set. 
Given two key terms kt1 and kt2, kt1  is a substring of kt2. If λH(kt1)<H(kt2) (the setting 
for λ is shown in Section 6.2), then kt1 is removed as a noisy string. However, if only 
using Information Entropy to filter substrings, the relations between an OOV term and 
its key terms cannot be considered. For key terms with low frequency, they often co-
occur  with  some  noisy  strings.  For  example,  for  the  OOV  term  “ 萨 马 兰 奇
[Samaranch]”,  its  correct  translation  “Samaranch”  always  occurs  in  the  key  term 
“Juan  Antonio  Samaranch”.  If  only  determined  by  using  Information  Entropy, 
“Samaranch” will be removed. Thus the special feature P&S_IF (defined in Section 
4),  which  describes  the  phonetic  and  sense  relations  between  an  OOV  term  and  its 
translation  candidates,  is  added  to  solve  this  problem.  If  (λH(kt1)<H(kt2))  && 
(P&S_IF(OOVTerm, kt1)<P&S_IF (OOVTerm, kt2)), then kt1 is deleted. 

3  Multiple Feature Representation 

Local Feature (LF) is constructed based on neighboring tokens and the token itself. 
There are two types of contextual information to be considered when extracting LFs, 
namely internal lexical and external contextual information. 

(1#) Term length (Len) – Aims to consider the length of the translation candidate. 
(2#) Phonetic  Value (PV) – Aims to investigate the phonetic similarity between an 
OOV  term  and  its  translation  candidates.  Because  the  associated  syllabification 
representations can often be found between Chinese and English syllables with fewer 
ambiguities,  the  syllabification  has  become  a  very  effective  way  in  the  phonetic 
feature expression. PV means that for measuring the edit distance similarity between 
the  syllabification  sequences  of  an  OOV  term  and  its  candidates,  the  corresponding 
processing is executed according to the specific linguistic rules. 
)
)'

EditDist
Len

T
OOV
(
T

,'
Len

−=

1

(2) 

(
S
)
'

(

S

OOV

+

(
S

)

PV

,

T

OOV

OOV

'

OOV

OOV

                           

where SOOV and TOOV denote the OOV term and its translation candidate respectively, 
SOOV’  and  TOOV’  are  the  character  strings  after  the  syllabification  and  removing  the 
vowels, EditDist( , ) indicates the edit distance between two strings. 
(3#) Length Ratio of OOV Term and Its Translation  Candidate (LR) – Aims to 
explore the composition possibility that the translation candidate can be regarded as 
the final correct translation for an OOV term. An OOV term and its translation should 
have the similar length, so the LR value is close to 1 as possible. A Chinese term is 
segmented into significant pieces first, and the number of pieces is taken as its length. 

 

238 

Y. Zhao et al. 

SP

_&

For  example, “非典型肺炎[SARS]” is segmented into “非[non]”, “典型[typical]” 
and “肺炎[pneumonia]”, and its length is 3. For an English term, the number of words 
is  counted  as  the  length.  If  there  is  only  one  word  composed  of  capital  letters,  its 
length is defined as the number of letters, e.g., “SARS” has the length of 4. Thus the 
LR value of “非典型肺炎[SARS]” and its candidate “SARS” is 3/4=0.75. 
(4#)  Phonetic  and  Sense  Integration  Feature  (P&S_IF)  –  Aims  to  consider  the 
phonetic information and senses of an OOV term and its candidates synthetically. It is 
set up for multi-word OOV terms. Each constituent can be translated by the phonetic 
information or senses. 
(
S

(3)
 
where LScore( , ) is the matching word number of non-transliteration words in SOOV 
and  TOOV,  while  SOOV’’  and  TOOV’’  are  the  remaining  strings  of  SOOV  and  TOOV  after 
computing  LScore.  For  example,  given  SOOV  “斯 堪的纳维亚半岛[Scandinavian 
Peninsula]” and its TOOV “Scandinavian Peninsula” , the non-transliteration words “
半岛[peninsula]”  and  “Peninsula”  are  matched,  then  LScore(SOOV,  TOOV)=1;  the 
PV  value  between  the  remaining  strings  “ 斯 堪 的 纳 维 亚 [Scandinavian]”  and 
“Scandinavian” is 0.928, so the final P&S_IF value is 1.928/2=0.964. 
(5#) Un-Covered Ratio (UCR) – Aims to explore the ratio of the overlap between an 
OOV  term  and  the  translations  of  its  candidates  acquired  from  Chinese  Basic 
Dictionary (Yang et al. 2009b). It is set up for multi-word OOV terms. 

(
T
S
OOV
(
LScore

(
SPV
,
T

                 

OOV

,''
) 1
+

LScore

OOV
S

)

+

)

=

)

''

OOV

OOV

OOV

OOV

T

OOV

,

IF

,

T

(

S

UCR

,

T

OOV

OOV

)

−=

1

)

Len

(
unTrans
)
(
Len

S

OOV

(4) 

                       

where unTrans is the part in SOOV uncovered by the translation of TOOV. For example, 
given  SOOV  “苏伊士运河[Suez  Canal]” and  its  TOOV  “Suez  Canal”,  the  part  in 
TOOV which can be translated by Basic Dictionary is “Canal” and its translation is “
运河[canal]”. Thus the unTrans part in SOOV is “苏伊士[Suez]”, then the final UCR 
value is 1-3/5=0.4. 

Global  Feature  (GF) is extracted from other occurrences of the same or similar 
tokens  in  the  Web  document  set.  The  common  case  in  the  Web-based  OOV  term 
translation is that the translation candidates in the previous parts of Web documents 
often  occur  with  the  same  or  similar  forms  in  the  latter  parts.  The  contextual 
information from the same and other Web documents may be beneficial to determine 
the  final  translation.  To  utilize  global  information,  GFs  are  built  based  on  the 
characteristics of Web documents. 
(1#) Global Term Frequency (G_Freq) – Aims to utilize the frequency information 
that an OOV term and its translation candidates appear in the Web document set. It is 
always the most important feature and includes four parameters. FreqSOOV denotes the 
frequency of SOOV in all the returned snippets. TFTOOV indicates the number of TOOVs 
in  all  the  snippets.  DFTOOV  represents  the  number  of  snippets  that  contain  TOOV. 
CO_Freq  means  the  number  of  snippets  that  contain  both  SOOV  and  TOOV,  i.e.,  co-
occurrence frequency. 

 

 

Chinese-English OOV Term Translation 

239 

(2#) Global Statistical Feature (G_SF) – Aims to explore the statistical measure for 
the  strength  of  the  interdependence  between  an  OOV  term  and  its  translation 
candidates to judge the possibility of a translation candidate being taken as the final 
correct translation [7]. 

Chi-Square (χ2) Feature  Value (CV) – Aims to evaluate the semantic similarity 

between SOOV and TOOV by their occurrence in Web documents. 

(

S

CV

χ
2

,

T
OOV

OOV

)

=

(
)
×−××
cbdaN
(
(
)
(
+×+×+
+×
ca
db
dc

(
ba

)

)

2

)

 

)5(

where  a  is  the  number  of  snippets  with both  SOOV  and  TOOV,  b  is  the  number  of 
snippets that contain SOOV but do not contain TOOV, c is the number of snippets that do 
not  contain  SOOV  but  contain  TOOV,  d  is  the  number  of  snippets  that  do  not  contain 
neither of SOOV and TOOV, and N=a+b+c+d. 

Information Gain (IG) – Aims to compute the probability that TOOV appears in the 
snippets  with  SOOV. The larger IG shows that TOOV is a more possible translation for 
SOOV. 

(
S

IG

,

T
OOV

OOV

)

×=

a

log

a
)
×+

(
ba

(
+
ca

)

×+
b

log

b

(
ba

+×+

(
db

)

)

 

)6(

Correlation  Coefficient  (CC)  –  Aims  to  measure  the  linear  association  degree 
between SOOV and TOOV. It’s a variant of  CV. The larger  CC value indicates that the 
relation between SOOV and TOOV is more correlative, and CC2=χ2. 

(
SCC

,

T
OOV

OOV

)

=

(
×−××
cbdaN
)
(
+×+
ca

(
db

)

)
(
+×
dc

(
ba

)
×+

)

 

)7(

Relevance  Score  (RS) – Aims to measure the direct relevance between SOOV and 
TOOV.  It’s  computed  as  the  ratio  between  the  occurrence  probability  of  TOOV  in  the 
snippets  with  SOOV  and  that  of  TOOV  in  the  snippets  without  SOOV.  The  larger  RS 
indicates that SOOV and TOOV are more relevant. 

a
+
c
+
dc
where m is used to smooth the RS and usually set as 1. 

(
SRS

ba

log

OOV

OOV

=

T

)

,

+

+

m

m

 

)8(

Odds  Ratio  (OR)  –  Aims  to  measure  the  indirect  relevance  between  SOOV  and 
TOOV.  The  distribution  of  features  on  relevant  candidates  is  different  from  that  on 
irrelevant candidates. The larger OR indicates that SOOV and TOOV are more relevant. 

a
+

b

−

a

a




1

×




a
+

1

b

−

c

×


(
SOR

,

T
OOV

OOV

)

=

 





 

)9(

c
+

d
c
+

c

d

240 

Y. Zhao et al. 

GSS Coefficient (GSS) – Aims to measure the relevance between SOOV and TOOV. It 
is  another  simplified  variant  of  CV.  The  larger  GSS  also  represents  the  stronger 
relevance. 

(
SGSS

,

T
OOV

OOV

)

×−×=
cb

da

)10(

 

(3#)  Pointwise  Mutual  Information  (PMI)  –  Aims  to  evaluate  the  co-occurrence 
relation between an OOV term and its candidates. If both appear with the higher co-
occurrence frequency in the same snippet, they are more relevant. 

(

S

PMI

,

T

OOV

OOV

)

=

×
×

aN
(
)
b
a

(
a

+

+

)

c

 

)11(

(4#) Co-Occurrence Distance (CO_Dist) – Aims to investigate the distance between 
an OOV term and its candidates in Web documents. This distance is often very closer. 
For each snippet that contains both SOOV and TOOV, three positions are considered, 
that is, the first position that SOOV and TOOV appear (p1), the second position (p2) and 
the last one (p3). For example, in the following snippet, SOOV is “亚洲开发银行[Asian 
Development Bank, ADB]” and TOOV is “Asian Development Bank”. 

 
p1SOOV=0, p2SOOV=29, p3SOOV=159;    p1TOOV=36, p2TOOV=101, p3TOOV=101 

The position is indexed from 0. Then the nearest position pair p2SOOV and p1TOOV 

can be found for this example. The distance Dist between SOOV and TOOV is: 

(
SDist

,

T
OOV

OOV

)

=





S

pi
pj
T

OOV

−
−

OOV

pj
T
OOV
pi

S

OOV

−
−

Len
Len

(
T
OOV
(
S

OOV

)
,
)
,

pi
pi

S

S

OOV

OOV

>
<

OOV

pj
T
pj
T
OOV

 

)12(

Given the example above, Dist=p2SOOV-p1TOOV-6=36-29-6 =1, SOOV and TOOV are a 

left bracket ‘(’ apart. Thus the average distance CO-Dist in the snippet set is: 

(
SDist

CO

_

,

T
OOV

OOV

)

=

AVG

_

(
SDist

,

T
OOV

OOV

)

=

CO

_

(
Sum
Dist
(
Freq
S

OOV

)
,

)

T
OOV

 

)13(

where Sum( ) is the sum of Dist in each snippet. 
(5#) Rank  Value (RV) – Aims to consider the rank for translation candidates in the 
Web document set. It includes six parameters. Top_Rank (T_Rank) is the rank of the 
snippet  that  first  contains  TOOV  and  given  by  the  search  engine.  Average_Rank 
(A_Rank) is the average position of TOOV in the returned snippets. 

A

_

Rank

(
T

OOV

)

=

)

(

Sum
DF

T

OOV

Rank
(
T

OOV

)

 

)14(

where  Sum(  )  denotes  the  rank  sum  of  each  snippet.  Simple_Rank  (S_Rank)  is 
computed as S_Rank(TOOV)=TFTOOV(TOOV)*Len(TOOV), for investigating the impact of 
the  frequency  and  length  of  TOOV  on  ranking.  R_Rank  is  utilized  as  a  comparison 
basis. 

TF
T
OOV
Freq

S

OOV

)

(
T
OOV
(
S

OOV

)

 

)15(

R

_

Rank

(
T
OOV

)

β
×=

T
OOV
_
MAX

WL

(
−+
1

)
β

×

 

 

Chinese-English OOV Term Translation 

241 

where β is set as 0.25 empirically, |TOOV| is the length of TOOV, and MAX_WL denotes 
the  maximum  length  of  candidates.  DF_Rank  (D_Rank)  is  similar  to  S_Rank,  and 
D_Rank(TOOV)=DFTOOV(TOOV)*Len(TOOV). TF_Rank is computed as TF_Rank(TOOV)= 
TFTOOV(TOOV), which aims at investigating the impact of the frequency of TOOV . 
(6#)  Similarity  of  Context  Vector  (SCV)  –  Aims  to  evaluate  the  distribution 
similarity between an OOV term and its candidates in the snippet set. The OOV term 
SOOV  and 
two  context  vectors, 
CVSOOV=(ts1,  …,  tsi,  …,  tsN)  and  CVTOOV=(tt1,  …,  tti,  …,  ttN),  tsi  and  tti  denote  the 
number  of  SOOVs  and  TOOVs  in  the  ith  snippet  respectively.  Thus  the  SCV  can  be 
computed as: 

its  candidate  TOOV  are  first  represented  as 

(

SCV

S
OOV

,

T
OOV

)

=

(
cos
CV
S

,

CV
T
OOV

OOV

)

=

N



=
1

i

N

i

=
1


(
)
ts
i

2

(
ts
i

)

×

tt
i

×

N



=
1

i

 

)16(

)

2

(
tt
i

Boolean  Feature  (BF)  is  a  binary  feature  and  equivalent  to  a  heuristic  rule 
designed  for  the  particular  relations  between  an  OOV  term  and  its  translation 
candidates.  BFs  are  used  to  explore  the  different  occurrence  forms  with  higher 
possibility for the candidates in Web documents. (1#) Position Distance with OOV 
Term (PD_SOOV) – If TOOV occurs close to SOOV (within 10 characters), this feature is 
set as 1. (2#) Neighbor Relation with OOV Term (NR_SOOV) – If TOOV occurs prior 
or next to SOOV, this feature is set as 1. (3#) Bracket Neighbor Relation with OOV 
Term  (BNR_SOOV)  –  If  TOOV  locates  prior or  next  to  SOOV and  occurs  with  the  form 
“TOOV  (SOOV)”  or  “SOOV  (TOOV)”,  this  feature  is  set  as  1.  (4#)  Special  Mark  Word 
(SMW) – Within a certain co-occurrence distance (less than 10 characters) between an 
OOV  term and its candidates, if there is such a term like “全称[full name]”, “叫[be 
named as]”, “译为[be translated as …]” or “(或/又)称为[(or/also) be called as …]”, 
or their English translation terms and so on, this feature is set as 1. (5#) Capitalized 
First Letter (CFL) – If TOOV begins with a capitalized letter, this feature is set as 1. 

4 

Ranking Based on Ranking SVM 

For  the  OOV  term  translation  based  on  Web  mining,  another  difficulty  is  how  to 
evaluate  the  relevance  between  an  OOV  term  and  its  translation  candidates,  that  is, 
how to rank all the translation candidates from “best” to “worst”. 

The candidate ranking can be regarded as a binary classification problem. However, 
usually only  highly related fragments of OOV terms can be found, rather than their 
correct translations. Instead of regarding the candidate ranking as binary classification, 
it is solved as an Ordinal Regression problem. Ranking SVM maps different objects 
into a certain  kind of order relation. The key is  modeling  the judgements  for user’s 
preferences, and then the constraint relations for ranking can be derived. 

 
 

 

242 

Y. Zhao et al. 

For a  SOOV, if there are two translation candidates  TOOVi and  TOOVj, the preference 
judgement  can  be  formulated  as  TOOVi>SOOVTOOVj.  Thus  more  training  samples  are 
constructed,  which  contain  multiple  constraint  features.  The  judgement  can  be 
transformed into the feature function as: 
)
>

)

 

(
Sf

(
Sf

,

T

OOV

OOV

i

,

w

S

OOV

,

T

OOV

OOV

j

,

w

)17(

where w is a parameter and represented as a vector {w1, …, wi, …, wn}. This function 
can also be expressed as: 

(
Sf

OOV

,

T
OOV

,

)
w

=

p



=
1

k

(
S
OOV

,

T
OOV

)

+

LFw
k

k

q

(
SGFw
+=
1
pl



l

l

OOV

)

+

,

T
OOV

n

(
SBFw
+=
1
qm



m

m

OOV

)

,

T
OOV

 

)18(

where  LFk(  ,  ),  GFl(  ,  )  and  BFm(  ,  )  are    the  local,  global  and  Boolean  feature 
representation  respectively.  These  three  kinds  of  feature  representation  can  be 
incorporated as a whole and represented as a feature function family with the multi-
dimensional feature vector in Formula (19). 

(
Sf

,

T
OOV

,

w

OOV

)

⋅=

(
Shw

,

T
OOV

OOV

)

)19(

 

Thus the relevance for each feature vector x (translation candidate) containing a group 
of features can be evaluated. 

5 

Experiment and Analysis 

4,170 NEs are selected from  the  Chinese  NER  corpus  in  SIGHAN2005. The test set 
contains  310  Person  Names  (PRNs),  324  Location  Names  (LCNs)  and  252 
Organization  Names  (OGNs),  and  the  remaining  is  taken  as  the  training  set.  300 
Chinese new terms chosen randomly from 9 categories (movie name, book title, brand 
name,  terminology,  idiom,  rare  animal  name  and  NE),  are  used  to  investigate  the 
generalization ability of our model. Top-N-Inclusion-Rate is defined as the percentage 
of the OOV terms whose correct translations could be found in the first N translation 
candidates. 

To  verify  the  effectiveness  for  multiple  feature  fusion,  the  test  on  the  feature 
combination for our model is implemented. As shown in Table 1, the highest Top-1-
Inclusion-Rate of 88.8889% can be acquired by using all the features. It can be seen 
from Table 1 that the most important features are P&S_IF, NR_SOOV, BNR_SOOV and 
UCR.  As  for  the  frequency  feature,  its  contribution  is  limited,  because  many 
candidates  with  higher  P&S_IF  values  are  the  terms  with  low  frequency.  However, 
when training based on only the features that are beneficial to the whole performance, 
the best translation accuracy is 85.8024%, which is worse than that by combining all 
the features. Multiple feature fusion can indeed improve the translation accuracy. 

 

 

 

Chinese-English OOV Term Translation 

243 

Table 1. Results for feature combination 

Top-1-Inclusion Rate  Reduction 

Feature 

All Features 

Local 

Numerical Feature 

Numerical Feature 

Global 

Numerical Feature 

Boolean Feature 

Global 

Frequency

-TFTOOV
-DFTOOV
-CO_Freq

-Len
-PV
-LR

-P&S_IF

-UCR

-CV
-IG
-CC
-RS
-OR
-GSS
-PMI

-CO_Dist

RV 

-T_Rank
-A_Rank

-SCV

-PD_SOOV
-NR_SOOV
-BNR_SOOV

-SMW
-CFL

88.8889%
88.8889%
84.8765%
88.8889%
81.1728%
84.2592%
88.8889%
90.1234%
89.1975%
88.8889%
84.5679%
88.8889%
85.1852%
89.8148%
88.8889%
89.8148%
87.0370%
88.2716%
89.8148%
89.5062%
88.2716%
83.6419%
83.9506%
88.8889%
89.1975%

— 

0.0%
-4.01234%
0.0%
-7.7160%
-4.6296%
0.0%
+1.2345%
+0.3086%
0.0%
-4.3210%
0.0%
-3.7037%
+0.9259%
0.0%
+0.9259%
-1.8518%
-0.6172%
+0.9259%
+0.6173%
-0.6173%
-5.2469%
-4.9383%
0.0%
+0.3086%

 
Yang et al. [23] is very similar to our approach, we accomplished this method on 
the same data set to make a contrast, as shown in Table 2. It can be concluded that the 
ranking  based  on  the  supervised  learning  outperforms  the  existing  conventional 
strategies, Ranking SVM is better than SVM for ranking, and our approach is superior 
to Yang et al.’s. Meanwhile, the best performance is obtained for PRNs. It shows that 
our model is sensitive to the category and the popularity of OOV term. 

Table 2. Performance comparison results 

Method 

Ranking Pattern

Our Model 

Yang et al. [23] 

based on SVM 

(Multiple Features) 

based on Ranking SVM 

(Multiple Features) 

based on SVM

(TFTOOV+LR+UCR+CFL)
based on Ranking SVM
(TFTOOV+ LR+UCR+CFL)

Category

PRN
LCN
OGN
All
PRN
LCN
OGN
All

OGN (Only) 

Top-1
Top-2 
88.70% 97.09% 
76.23% 93.82% 
76.58% 92.06% 
80.69% 94.46% 
92.58% 97.74% 
87.34% 95.37% 
84.52% 95.23% 
88.89% 96.16% 
53.96% 
76.98% 

Top-3 
99.35% 
96.91% 
96.42% 
97.62% 
99.03% 
98.14% 
97.22% 
98.19% 
88.49% 

OGN (Only) 

62.69% 

83.33% 

88.49% 

 
Another test for the other kinds of Chinese OOV term is performed on the selected 

new terms and the consistent results can be observed in Table 3. 

 

 

244 

Y. Zhao et al. 

Table 3. Results for Chinese OOV new terms 

Top-N-Inclusion-Rate 

Top-1
74.66%

Top-3
90.33%

Top-5
94.33%

Top-7
95.00%

Top-9 
96.00% 

Chinese OOV New Terms 
 
Four CLIR runs are carried out on the Chinese topic set and English corpus from 
TREC-9.  (1)  C-E_LongCLIR1  –  using  Long  Query  (LQ,  terms  in  both  title  and 
description fields) and the Dictionary-Based Translation (DBT); (2) C-E_LongCLIR2 
– using LQ, DBT and our model; (3) C-E_ShortCLIR1 – using Short Query (SQ, only 
terms  in  the  title  field)  and  DBT;  (4)  C-E_ShortCLIR2  –  using  SQ,  DBT  and  our 
model. The Precision-Recall curves and Median Average Precision (MAP) are shown 
in  Fig.  1.  It  can  be  seen  from  Fig.  1  that  the  best  run  is  C-E_LongCLIR2,  and  its 
results exceed those of C-E_LongCLIR1. By adopting both query translation based on 
bilingual dictionary and OOV term translation, Chinese-English CLIR for long query 
has  gained the  significant retrieval performance improvement. The same conclusion 
can be obtained for the other two runs C-E_ShortCLIR1 and C-E_ShortCLIR2. 

Fig. 1. Results for Chinese-English CLIR combining our model 

 

Through analyzing the results, it can be found that the translation quality is highly 
related to the following aspects. (1) The translation results are associated with the 
search engine  used, especially for some  specific OOV terms. For example, given 
an  OOV  term  “经 济 法 制化”,  the  mining result based on Google in China is  “to 
manage  economic  affairs  according  to  1aw”,  which  is  more  reasonable  than 
“Economic  law”  acquired  by  Bing.  (2)  Some  terms  are  idioms,  conventional  and 
political terminologies with Chinese characteristics, and cannot be translated literally. 
For example, “党群关系[party masses relationship]” should be translated into “party 
masses relationship”, rather than “ties between the party” given by Google Translate. 
(3) The proposed  model is sensitive to the notability degree of OOV term. This 
phenomenon  is  the  main  reason  why  there  is  an  obvious  difference  among  the 
translation performance  for  PRN,  LCN  and  OGN.  (4)  There  are  some  particular 
and  inherent  noises  in  the  extracted  translation  candidates.  For  example,  a 
candidate  for  the  Chinese  OOV  term    “ 广 东 人 民 出 版 社 [Guangdong  People’s 
Publishing  House]”  is  “Guangdong  ren  min  chu  ban  she”.  (5)  Word  Sense 

 

 

Chinese-English OOV Term Translation 

245 

Disambiguation (WSD) should be added to improve the translation performance. 
Although  most  of  OOV  terms  have  a  unique  sense  definition,  there  are  still  a  few 
OOV  terms  with  sense  ambiguity,  e.g.,  “ 东 北 大 学 [Northeastern  University  or 
Tohoku University]”. 

6 

Conclusions 

Traditional OOV term translation methods concern two aspects, that is, transliteration 
and  sense  translation.  However,  more  and  more  Chinese  OOV  terms  cannot  be 
measured  by  phonetic  or  meaning  information  separately.  Our  proposed  model 
improves the acquirement ability for Chinese-English OOV term translation through 
Web mining, and solves the translation pair selection and evaluation in a novel way 
by fusing multiple features and introducing the supervised learning based on Ranking 
SVM.  Our  future  research  will  focus  on  applying  the  key  techniques  on  statistical 
machine  learning,  alignment  of  sentence  and  phoneme,  and  WSD  into  Chinese-
English OOV term translation. 
 
Acknowledgments. This work is supported by National Science & Technology Pillar 
Program  of  China  (No.  2012BAH59F04),  National  Natural  Science  Fund  of  China 
(No.  61170095;  No.  71171126),  and  Shanghai  Municipal  R&D  Foundation  (No. 
12dz1500203, 12511505300). Cheng Jin is the corresponding author. 

References 

1.  Al-Onaizan, Y., Knight, K.: Translating Named Entities using Monolingual and Bilingual 

Resources. In: Proceedings of ACL 2002, pp. 400–408 (2002) 

2.  Cao, Y.B., Xu, J., Liu, T.Y., Li, H., Huang, Y.L., Hon, H.W.: Adapting Ranking-SVM to 

Document Retrieval. In: Proceedings of SIGIR 2006, pp. 186–193 (2006) 

3.  Chen,  C.,  Chen,  H.H.:  A  High-Accurate  Chinese-English  NE  Backward  Translation 
System  Combining  Both  Lexical  Information  and  Web  Statistics.  In:  Proceedings  of 
COLING-ACL 2006, pp. 81–88 (2006) 

4.  Fang,  G.L.,  Yu,  H.,  Nishino,  F.:  Chinese-English  Term  Translation  Mining  based  on 

Semantic Prediction. In: Proceedings of COLING-ACL 2006, pp. 199–206 (2006) 

5.  Ge,  Y.D.,  Hong,  Y.,  Yao,  J.M.,  Zhu,  Q.M.:  Improving  Web-Based  OOV  Translation 
Mining  for  Query  Translation.  In:  Cheng,  P.-J.,  Kan,  M.-Y.,  Lam,  W.,  Nakov,  P.  (eds.) 
AIRS 2010. LNCS, vol. 6458, pp. 576–587. Springer, Heidelberg (2010) 

6.  Hu, R., Chen, W., Bai, P., Lu, Y., Chen, Z., Yang, Q.: Web Query Translation via Web 

Log Mining. In: Proceedings of SIGIR 2008, pp. 749–750 (2008) 

7.  Huang,  S.,  Chen,  Z.,  Yu,  Y.,  Ma,  W.Y.:  Multitype  Features  Coselection  for  Web 
Document  Clustering.  IEEE  Transactions  on  Knowledge  and  Data  Engineering 18(4), 
448–459 (2006) 

8.  Jiang, L., Zhou, M., Chien, L.F., Niu, C.: Named Entity Translation with Web Mining and 

Transliteration. In: Proceedings of IJCAI 2007, pp. 1629–1634 (2007) 

9.  Joachimes,  T.:  Optimizing  Search  Engines  using  Click  through Data.  In: Proceedings  of 

SIGKDD 2002, pp. 133–142 (2002) 

 

246 

Y. Zhao et al. 

10.  Lee,  C.J.,  Chang,  J.S.,  Jang,  J.R.:  Alignment  of  Bilingual  Named  Entities  in  Parallel 
Corpora Using Statistical Models and Multiple Knowledge Sources. ACM Transactions on 
Asian Language Processing 5(2), 121–145 (2006) 

11.  Lu,  W.H.,  Chien,  L.F.:  Translation  of  Web  Queries  using  Anchor  Text  Mining.  ACM 

Transactions on Asian Language Information Processing 1(2), 159–172 (2002) 

12.  Lu, W.H., Chien, L.F.: Anchor Text Mining for Translation of Web Queries: A Transitive 
Translation Approach. ACM Transactions on Information Systems 22(2), 242–269 (2004) 
13.  Ren,  F.L.,  Zhu,  M.H.,  Wang,  H.Z.,  Zhu,  J.B.:  Chinese-English  Organization  Name 
Translation Based on Correlative Expansion. In: Proceedings of the 2009 Named Entities 
Workshop, ACL-IJCNLP 2009, pp. 143–151 (2009) 

14.  Shao,  L.,  Ng,  H.T.:  Mining  New  Word  Translations  from  Comparable  Corpora.  In: 

Proceedings of COLING 2004, pp. 618–624 (2004) 

15.  Shi, L.: Mining OOV Translations from Mixed-Language Web Pages for Cross Language 
Information  Retrieval.  In:  Gurrin,  C.,  He,  Y.,  Kazai,  G.,  Kruschwitz,  U.,  Little,  S., 
Roelleke, T., Rüger, S., van Rijsbergen, K. (eds.) ECIR 2010. LNCS, vol. 5993, pp. 471–
482. Springer, Heidelberg (2010) 

16.  Sproat, R., Tao, T., Zhai, C.X.: Named Entity Transliteration with Comparable Corpora. 

In: Proceedings of COLING-ACL, pp. 73–80 (2006) 

17.  Virga,  P.,  Khudanpur,  S.:  Transliteration  of  Proper  Names 

in  Cross-Language 

Applications. In: Proceedings of SIGIR 2003, pp. 365–366 (2003) 

18.  Wang, J.H., Teng, J.W., Cheng, P.J., Lu, W.H., Chien, L.F.: Translating Unknown Cross-
Lingual  Queries  in  Digital  Libraries  using  a  Web-based  Approach.  In:  Proceedings  of 
JCDL 2004, pp. 108–116 (2004) 

19.  Wu, J.C., Chang, J.S.: Learning to Find English to Chinese Transliterations on the Web. 

In: Proceedings of EMNLP-CoNLL 2007, pp. 996–1004 (2007) 

20.  Xu,  J.,  Cao,  Y.B.,  Li,  H.,  Zhao,  M.:  Ranking  Definitions  with  Supervised  Learning 

Methods. In: Proceedings of WWW 2005, pp. 811–819 (2005) 

21.  Yang,  F.,  Zhao,  J.,  Zou,  B.,  Liu,  K.:  Chinese-English  Backward  Transliteration  Assisted 

with Mining Monolingual Web Pages. In: Proceedings of ACL 2008, pp. 541–549 (2008) 

22.  Yang,  F.,  Zhao,  J.,  Liu,  K.:  A  Chinese-English  Organization  Name  Translation  System 
Using  Heuristic  Web  Mining  and  Asymmetric  Alignment.  In:  Proceedings  of  ACL-
AFNLP 2009, pp. 387–395 (2009a) 

23.  Yang,  M.,  Shi,  Z.,  Li,  S.,  Zhao,  T.,  Qi,  H.:  Ranking  vs.  Classification:  a  Case  Study  in 
Mining Organization Name Translation from Snippets. In: Proceedings of IALP 2009, pp. 
308–313 (2009b) 

24.  Zhang,  Y.,  Huang,  F.,  Vogel,  S.:  Mining  Translations  of  OOV  Terms  from  the  Web 
through  Cross-Lingual  Query  Expansion.  In:  Proceedings  of  SIGIR  2005,  pp.  669–670 
(2005) 

25.  Zhang,  Y.,  Vines,  P.:  Using  the  Web  for  Automated  Translation  Extraction  in  Cross-

Language Information Retrieval. In: Proceedings of SIGIR 2004, pp. 162–169 (2004) 

 


