Fuzzy Multi-Sphere Support Vector Data

Description

Trung Le1, Dat Tran2, and Wanli Ma2

1 HCM City University of Education, Vietnam
2 University of Canberra, ACT 2601, Australia

dat.tran@canberra.edu.au

Abstract. Current well-known data description methods such as Sup-
port Vector Data Description and Small Sphere Large Margin are con-
ducted with assumption that data samples of a class in feature space are
drawn from a single distribution. Based on this assumption, a single hy-
persphere is constructed to provide a good data description for the data.
However, real-world data samples may be drawn from some distinctive
distributions and hence it does not guarantee that a single hypersphere
can oﬀer the best data description. In this paper, we introduce a Fuzzy
Multi-sphere Support Vector Data Description approach to address this
issue. We propose to use a set of hyperspheres to provide a better data
description for a given data set. Calculations for determining optimal hy-
perspheres and experimental results for applying this proposed approach
to classiﬁcation problems are presented.

Keywords: Kernel Methods, Fuzzy Interference, Support Vector Data
Description, Multi-Sphere Support Vector Data Description.

1

Introduction

Support Vector Machine (SVM) [2], [4] has been proven a very eﬀective method
for binary classiﬁcation. However, it cannot render good performance for one-
class classiﬁcation problems where one of two classes is under-sampled, or only
data samples of one class are available for training [9] . One-class classiﬁcation in-
volves learning data description of normal class to build a model that can detect
any divergence from normality [11]. The samples of abnormal class if existed
contribute to reﬁning the data description. Support Vector Data Description
(SVDD) was introduced in [14], [13] as a kernel method for one-class classiﬁca-
tion. SVDD aims at constructing an optimal hypersphere in feature space which
includes only normal samples and excludes all abnormal samples with tolerances.
This optimal hypersphere is regarded as a data description since when mapped
back to input space it becomes a set of contours that tightly enclose the normal
data samples [1].

Variations of SVDD were proposed to enhance this approach. In [9], density-
induced information was incorporated to the samples so that the dependency
of data description on support vectors can be less imposed when these support

J. Pei et al. (Eds.): PAKDD 2013, Part II, LNAI 7819, pp. 570–581, 2013.
c(cid:2) Springer-Verlag Berlin Heidelberg 2013

Fuzzy Multi-Sphere Support Vector Data Description

571

vectors cannot characterise well the data. To reduce the impact of less important
dimensions, a single ellipse was learnt rather than a single hypersphere [10].
However, this work was not general since it was only proposed for the model in
input space. Other approaches introduced better margins for SVDD such as [15]
[7]. To reduce the chance of acceptance of outliers, in [15] a small sphere with
large margin was proposed. However, this can induce side-eﬀect which causes
the interference of decision boundary into normal data region. To overcome this
issue, an optimal sphere with two adjustable margins for reducing both true
positive (T P %) and true negative (T N %) error rates was proposed [7].

Fig. 1. Inside outliers would be improperly included if only one hypersphere is con-
structed [16]

SVDD assumes that all samples of the training set are drawn from a single
uniform distribution [13]. However, this hypothesis is not always true since real-
world data samples may be drawn from distinctive distributions [5]. Therefore,
a single hypersphere cannot be a good data description. For example, in Fig-
ure 1, data samples are scattered over some distinctive distributions and one
single hypersphere would improperly record the inside outliers. In [16], a multi-
sphere approach to SVDD was proposed for multi-distribution data. The domain
for each distribution was detected and for each domain an optimal sphere was
constructed to describe the corresponding distribution. However, the learning
process was heuristic and did not follow up learning with minimal volume prin-
ciple [12]. In [6], a method was proposed to link the input space to the feature
space. The dense regions (clusters) in the input space were identiﬁed and be-
came a single sphere in the feature space. Again, this method was heuristic and
did not abide by learning with minimum volume principle. To motivate learning
with this principle, a hard multi-sphere support vector data description (HMS-
SVDD) [8] was proposed. A set of hyperspheres was introduced to enclose all the

572

T. Le, D. Tran, and W. Ma

data samples. A data sample will belong to only one hypersphere. This restric-
tion should be relaxed to allow a data sample to belong to diﬀerent hyperspheres
if that sample has similar degrees of belonging to those hyperspheres. To address
this issue, we propose fuzzy multi-sphere support vector data description (FMS-
SVDD) in this paper. A fuzzy membership is assigned to each data sample to
denote the degree of belonging of that sample to a hypersphere. We prove that
classiﬁcation error will be reduced after each iteration in the learning process.
The set of hyperspheres will gradually converge to a stable conﬁguration. To
evaluate the proposed apprach, we performed classiﬁcation experiments on 23
data sets in UCI repository. The experimental results showed that FMS-SVDD
could provide better classiﬁcation rates in comparison to other one-class kernel
methods.

2 Fuzzy Multi-Sphere Support Vector Data Description

(FMS-SVDD)

2.1 Problem Formulation

Let xi, i = 1, . . . , p be normal data samples with label yi = +1 and xi, i =
p + 1, . . . , n be abnormal data samples with label yi = −1. Consider a set of
m hyperspheres Sj(cj, Rj) with center cj and radius Rj, j = 1, . . . , m. This
hypershere set is a good data description of the data set X = {x1, x2, . . . , xn}

if each of the hyperspheres describes a distribution in this data set and the sum

m(cid:2)

of all radii

R2

j should be minimised.

j=1

Let matrix U = [uij]p×m, uij ∈ [0, 1], i = 1, . . . , p, j = 1, . . . , m where uij is
the membership representing degree of belonging of sample xi to hypersphere
Sj. It is necessary to construct a set of hyperspheres so that these hyperspheres
can include all normal data and exclude all abnormal data with tolerances. The
optimisation problem of fuzzy multi-sphere SVDD can be formulated as follows

⎛

⎝

m(cid:5)

j=1

min
R,c,ξ,u

R2

j +

1
ν1p

p(cid:5)

i=1

ξi +

1
ν2q

n(cid:5)

m(cid:5)

i=p+1

j=1

⎞

⎠

ξij

subject to

m(cid:2)

ij(cid:3)φ(xi) − cj(cid:3)2 ≤ m(cid:2)
ud

j=1

(cid:3)φ(xi) − cj(cid:3)2 ≥ R2
m(cid:2)

uij = 1, i = 1, . . . , p

j=1

ud
ijR2

j + ξi, i = 1, . . . , p

j=1

j − ξij , i = p + 1, . . . , n, j = 1, . . . , m

(1)

(2)

where R = [Rj]j=1,...,m is vector of radii, ν1 and ν2 are constants, ξi and ξij are
slack variables, c = [cj]j=1,...,m is vector of centres, and q = n − p is the number

of abnormal (negative) samples.

Fuzzy Multi-Sphere Support Vector Data Description

573

In the FMS-SVDD model, we also introduce parameter d > 1 to adjust the

relative ratios among the memberships uij, i = 1, . . . , p, j = 1, . . . , m.

Minimising the function in (1) over variables R, c and ξ subject to (2) will
determine radii and centres of hyperspheres and slack variables if the matrix U
is given. On the other hand, the matrix U will be determined if radii and centres
of hyperspheres are given. Therefore an iterative algorithm will be applied to
ﬁnd a complete solution. The algorithm consists of two alternative steps: 1)
Calculate radii and centres of hyperspheres and slack variables, and 2) Calculate
membership U .

We present in the next sections the iterative algorithm and the proof of key
theorem which states that the classiﬁcation error in the current iteration will be
smaller than that in the previous iteration. It means that the model is gradually
reﬁned and converged to a stable conﬁguration.

For classifying a sample x, the following decision function is used:

(cid:8)

(cid:9)

j − ||φ(x) − cj||2
R2

(cid:10)(cid:11)

max
1≤j≤m

f (x) = sign

(3)
The unknown sample x is normal if f (x) = +1 or abnormal if f (x) = −1. This
decision function implies that the mapping of a normal sample has to be in one
of the hyperspheres and that the mapping of an abnormal sample has to be
outside all of those hyperspheres.

The following theorem is used to consider the relation of slack variables to

the classiﬁed samples:

Theorem 1. Assume that (R, c, ξi, ξij ) is a solution of the optimisation problem
(1), xi, i ∈ {1, 2, . . . , n} is the i-th sample. The slack variable ξi or ξij can be
computed as

(cid:12)

m(cid:2)

(cid:8)(cid:3)φ (xi) − cj(cid:3)2 − R2

j

(cid:13)

(cid:11)

, i = 1, . . . , p

ξi = max

ξij = max

0,
(cid:9)

ud
ij
j − (cid:3)φ(xi) − cj(cid:3)2

j=1
0, R2

(cid:10)

, i = p + 1, . . . , n, j = 1, . . . , m

Proof
For all i, from equation (2) we have

(cid:12)

m(cid:2)

ξi ≥ max
ξij ≥ max

0,
(cid:9)

ud
ij
j − (cid:3)φ(xi) − cj(cid:3)2

j=1
0, R2

(cid:8)(cid:3)φ (xi) − cj(cid:3)2 − R2

j

(cid:13)

(cid:11)

, i = 1, . . . , p

(cid:10)

,

i = p + 1, . . . , n, j = 1, . . . , m

(4)

(5)

Moreover, (R, c, ξ) is minimal solution of (1). Hence, the theorem 1 is proved.
It is natural to deﬁne error(i), i.e. the error at sample xi, 1 ≤ i ≤ n as follows

If xi is normal data sample then

error(i) =

(cid:12)

0

min
1≤j≤m

(cid:9)(cid:3)φ(xi) − cj(cid:3)2 − R2

j

(cid:10)

if xi is correctly classif ied

otherwise

(6)

574

T. Le, D. Tran, and W. Ma

Else

(cid:12)

0

(cid:9)

error(i) =

j − (cid:3)φ(xi) − cj(cid:3)2
R2
where J = {j : xi ∈ Sj and 1 ≤ j ≤ m}.

min
j∈J

(cid:10)

if xi is correctly classif ied

otherwise

(7)

We can prove that

is an upper bound of

p(cid:2)

i=1

n(cid:2)

i=p+1

ξi is an upper bound of

1

md−1

p(cid:2)

i=1

error(i), and

n(cid:2)

m(cid:2)

i=p+1

j=1

ξij

error(i). The second inequality is trivial, therefore we

primarily concentrate on the ﬁrst one.

p(cid:2)

i=1

Theorem 2.

ξi is an upper bound of

1

md−1

error(i).

Proof
Let us denote dij = (cid:3)φ(xi) − cj(cid:3)2 − R2
j , i = 1, . . . , p, j = 1, . . . , m. Given
1 ≤ i ≤ p, we will prove that ξi ≥ 1
md−1 error(i). This above inequality is trivial
if xi is correctly classiﬁed. We consider the case where xi is misclassiﬁed, i.e.,

p(cid:2)

i=1

m(cid:2)

ud
ijdij. From deﬁnition of error(i), we

j=1
ud
ij. To fulﬁll this proof, we will show that

dij > 0 for all j. It means that ξi =

ijdij ≥ error(i)
ud
(cid:14)

(cid:15)d

m(cid:2)

j=1

have: ξi =

m(cid:2)

j=1

m(cid:2)

ij ≥ 1
ud
md−1

j=1
follows

m(cid:2)

j=1

uij

= 1

md−1 . Indeed, this inequality can be rewritten as

⎛

⎜
⎜
⎝

m(cid:5)

j=1

muij
m(cid:2)
j(cid:2)=1

uij(cid:2)

⎞

d

⎟
⎟
⎠

≥ m

(8)

By referring to Bernoulli inequality which says (1 + x)r ≥ 1 + rx if r ≥ 1 and
x > −1, we have
⎛

⎛

⎛

⎞

⎛

⎞

⎞

⎞
d

d

⎜
⎜
⎝

muij
m(cid:2)
j(cid:2)=1

uij(cid:2)

⎟
⎟
⎠

⎜
⎜

⎝1 −

=

⎜
⎜

⎝1 − muij
m(cid:2)
uij(cid:2)
j(cid:2)=1

⎟
⎟
⎠

⎟
⎟
⎠

≥ 1 − d

⎜
⎜

⎝1 − muij
m(cid:2)
uij(cid:2)
j(cid:2)=1

⎟
⎟
⎠ f or all j

It follows that

⎛

⎜
⎜
⎝

m(cid:5)

j=1

muij
m(cid:2)
j(cid:2)=1

uij(cid:2)

⎞

d

⎟
⎟
⎠

⎛

⎛

≥ m(cid:5)

⎜
⎜

⎝1 − d

j=1

⎜
⎜

⎝1 − muij
m(cid:2)
uij(cid:2)
j(cid:2)=1

⎞

⎞

⎟
⎟
⎠

⎟
⎟
⎠ = m

(9)

(10)

Fuzzy Multi-Sphere Support Vector Data Description

575

2.2 Calculating Radii, Centres and Slack Variables

The Lagrange function for the optimisation problem (1) subject to (2) is as
follows

L (R, c, ξ, α, β) =

R2

j + C1

ξi + C2

p(cid:2)

i=1

n(cid:2)

m(cid:2)

ξij

(cid:14)

m(cid:2)

j=1

m(cid:2)

i=p+1

j=1

(cid:8)(cid:3)φ(xi) − cj(cid:3)2 − R2
ud
ij
(cid:8)(cid:3)φ(xi) − cj(cid:3)2 − R2

j

j + ξij

(cid:15)

(cid:11) − ξi

(cid:11)

(11)

m(cid:2)

j=1
p(cid:2)

i=1

αi
+
− n(cid:2)
− n(cid:2)

i=p+1

j=1
m(cid:2)

αij
βijξij − p(cid:2)

βiξi

i=1

i=p+1

j=1

where C1 = 1
samples.

ν1p , C2 = 1

ν2q and q = n − p, q is the number of abnormal data

Setting derivatives of L(R, c, ξ, α, β) with respect to primal variables to 0, we

obtain

∂L
∂cj

= 0 → p(cid:5)

∂L
∂Rj
= 0 → cj =

i=1
p(cid:5)

ijαi − n(cid:5)
ud

i=p+1

αij = 1, j = 1, . . . , m

(12)

ijαiφ(xi) − n(cid:5)
ud

αij φ(xi), j = 1, . . . , m

(13)

i=1

i=p+1

= 0 ⇒ αi + βi = C1, i = 1, . . . , p

∂L
∂ξi

= 0 → αij + βij = C2, i = p + 1, . . . , n j = 1, . . . , m

∂L
∂ξij

(14)

(15)

To get the dual form, we substitute equations (12)-(15) to the Lagrange function
in (11) and obtain the following

(cid:8)(cid:3)φ(xi) − cj(cid:3)2

(cid:11) − n(cid:2)

m(cid:2)

(cid:8)(cid:3)φ(xi) − cj(cid:3)2

(cid:11)

αij

L(R, c, ξ, α, β) =

p(cid:2)

m(cid:2)

i=1

j=1

αiud
ij
ijαiK(xi, xi) − m(cid:2)
ud
(cid:14)
ijαiφ(xi) − n(cid:2)
ud
ij − m(cid:2)
ud

j=1

i=1

p(cid:2)

m(cid:2)

αiK(xi, xi)

i=p+1

j=1

n(cid:2)

αijK(xi, xi)

i=p+1

(cid:15)

αij φ(xi)

+

m(cid:2)

(cid:3)cj(cid:3)2
αijK(xi, xi) − m(cid:2)

j=1

i=p+1

n(cid:2)

(cid:14)

p(cid:2)

i=1

(cid:3)cj(cid:3)2

j=1

j=1

i=p+1

j=1

(cid:15)

ijαi − n(cid:2)
ud

i=p+1

αij

αisiK(xi, xi) − (cid:2)
αisiK(xi, xi) − (cid:2)
αrjαr(cid:2)jK(xr, xr(cid:2) ) + 2

αrjK(xr, xr) − m(cid:2)
αrjK(xr, xr) − (cid:2)
i,i(cid:2)

(cid:2)

r,j

r,j

j=1

ud
ijK(xi, xr)αiαrj

i,rj

(cid:18)
(cid:18)
(cid:18)
(cid:18)
(cid:18)

p(cid:2)

i=1

ijαiφ(xi) − n(cid:2)
ud

i=p+1

uiui(cid:2)K(xi, xi(cid:2) )αiαi(cid:2)

(cid:18)
(cid:18)
2
(cid:18)
(cid:18)
(cid:18)

αijφ(xi)

(16)

p(cid:2)

m(cid:2)

i=1
m(cid:2)

=
−2
j=1
p(cid:2)

j=1

cj

=

=

i=1
p(cid:2)

i=1
p(cid:2)

i=1

=
− (cid:2)
rj,r(cid:2)j

576

T. Le, D. Tran, and W. Ma

where 1 ≤ i, i
(cid:5) ≤ p, p + 1 ≤ r, r
(cid:5) ≤ n, 1 ≤ j ≤ m, ui = [ud
ud
ud
ijud
uiui(cid:2) =
i(cid:2)j, and si =
ij.

m(cid:2)

m(cid:2)

j=1

j=1

i1, ud

i2, . . . , ud

im],

We come up with the following optimisation problem
αrjαr(cid:2)jK(xr, xr(cid:2))−2

uiui(cid:2) K(xi, xi(cid:2) )αiαi(cid:2)+

(cid:2)

(cid:2)

⎛

(cid:2)

i,rj

ud
ijK(xi, xr)αiαrj

⎞

⎟
⎠

min

α

⎜
⎝

i,i(cid:2)
− p(cid:2)

i=1

αiuiK(xi, xi) +

αrjK(xr, xr)

rj,r(cid:2)j

(cid:2)

r,j

subject to

p(cid:2)

ijαi − n(cid:2)
ud

αij = 1, j = 1, . . . , m

i=1

i=p+1

0 ≤ αi ≤ C1, i = 1, . . . , p
0 ≤ αij ≤ C2, i = p + 1, . . . , n, j = 1, . . . , m

(17)

(18)

Note that the number of variables in solution of the optimisation problem (17)
is p + (n − p) × m. In practice, we apply the Interior Point (IP) method [3]
to solve out the above optimisation problem. The complexity is dependent on
double logarithmic of tolerance , i.e. log(log(1/)).

2.3 Calculating Membership U

We are in position to describe how to evaluate matrix U after obtaining new
(R, c). Given 1 ≤ i ≤ p, let us denote

dij = (cid:3)φ(xi) − cj(cid:3)2 − R2
j0 = arg min
1≤j≤m

dij

j

and Dij =

(cid:11) 1
d−1

(cid:8)

1
dij

(19)

The membership matrix can be updated as follows

If dij0 ≤ 0 then uij0 = 1 and uij = 0, j (cid:8)= j0

Else

uij =

Dij

m(cid:2)

Dik

k=1

, j = 1, . . . , m

(20)

2.4 Iterative Learning Process

The proposed iterative learning process for FMS-SVDD will run two alternative
steps until a convergence is reached as follows

Initialise U by clustering the normal data set in the input space
Repeat the following

Calculate R, c and ξ using U
Calculate U using R and c
Until convergence is reached

Fuzzy Multi-Sphere Support Vector Data Description

577

2.5 Theoretical Background of FMS-SVDD

In the objective function

m(cid:2)

j=1

R2

j + 1
ν1p

p(cid:2)

i=1

ξi + 1
ν2q

n(cid:2)

m(cid:2)

i=p+1

j=1

ξij , the ﬁrst summand

can be regarded as regularisation quantity and the rest can be considered as

empirical risk (referred to Theorem 2). We will prove that structural risk

1
ν1p

p(cid:2)

i=1

ξi + 1
ν2q

n(cid:2)

m(cid:2)

i=p+1

j=1

ξij gradually becomes smaller in Theorem 4.

Theorem 3. Given a m-multivariate function f (x1, x2, ..., xm) =

d > 1. The following optimisation problem

subject to

yields the solution as follows

min

x

f (x)

m(cid:5)

i=1

xi = 1

m(cid:2)

i=1

dixd

i and

(21)

(22)

m(cid:2)

j=1

R2

j +

If di0 ≤ 0 then xi0 = 1 and xi = 0, i (cid:8)= i0

(cid:2)

(cid:3) 1
d−1
(cid:3) 1
d−1

1
(cid:2)
di

1
dk

Else xi =

m(cid:4)

k=1

,

i = 1, . . . , m

where i0 = arg min
1≤i≤m

di.

Proof
Case 1: di0 < 0

i ≥ di0

dixd

m(cid:5)

i=1

i ≥ di0
xd

m(cid:5)

i=1

m(cid:5)

i=1

xi = di0 = f (0, ..., 1i0, ..., 0)

(23)

since d > 1.
Case 2: di0 ≥ 0
The Lagrange function is of

L(x, λ) =

i − λ

dixd

(cid:14)

m(cid:5)

(cid:15)
xi − 1

i=1

m(cid:5)

i=1

where λ is Lagrange multiplier.

Setting derivatives to 0, we gain

∂L

∂xi = 0 ⇒ ddixd−1
⇒ xi =

(cid:11) 1
d−1

(cid:8)

λ
ddi

i − λ = 0

, i = 1, . . . , m

(24)

(25)

578

From

m(cid:2)

i=1

T. Le, D. Tran, and W. Ma
(cid:2)

xi = 1, we have xi =

m(cid:4)

k=1

(cid:3) 1
d−1
(cid:3) 1
d−1

1
(cid:2)
di

1
dk

,

i = 1, . . . , m.

, ξ(t+1)
Theorem 4. Let (R(t), c(t), ξ(t)
,
U (t+1)) be solutions at the previous iteration and current iteration, respectively.
The following inequality holds

, U (t)) and (R(t+1), c(t+1), ξ(t+1)

, ξ(t)

ij

ij

i

i

(cid:11)2

R(t+1)

j

(cid:8)

m(cid:2)

j=1

+ 1
ν1p

p(cid:2)

i=1

ξ(t+1)

i

+ 1
ν2q

n(cid:2)

m(cid:2)

i=p+1

j=1
+ 1
ν2q

i

ξ(t+1)
n(cid:2)

≤ m(cid:2)
j=1
m(cid:2)

i=p+1

j=1

ξ(t)

i

(cid:11)2

(cid:8)

R(t)

j

+ 1
ν1p

p(cid:2)

i=1

ξ(t)

i

Proof
By referring to Theorem 3, it is easy to see that u(t+1)
1, . . . , p is solution of the following optimisation problem

i

= (u(t+1)

i1

⎛

⎞

(26)

, ..., u(t+1)

im ), i =

⎝

min

m(cid:5)

j=1

d(t)
ij ud
ij

⎠

m(cid:5)

j=1

uij = 1

m(cid:5)

j=1

d(t)
ij (u(t+1)

ij

)d ≤ m(cid:5)

j=1

d(t)
ij (u(t)

ij )d

subject to

Therefore, we have

It means that

(cid:8)

m(cid:2)

(cid:11)d

(cid:11)d

(cid:19)(cid:18)
(cid:18)φ(xi) − c(t)
(cid:18)
(cid:19)(cid:18)
(cid:18)φ(xi) − c(t)
(cid:18)

j

j

(cid:18)
(cid:18)
(cid:18)
(cid:18)
(cid:18)
(cid:18)

2 − (cid:8)
2 − (cid:8)

R(t)

j

R(t)

j

(cid:20)

(cid:11)2

(cid:20)

(cid:11)2

≤ ξ(t)

i

u(t+1)
ij
(cid:8)
u(t)

ij

j=1

≤ m(cid:2)

j=1

(27)

(28)

(29)

(30)

(31)

(32)

or

m(cid:5)

(cid:8)

(cid:11)d(cid:18)
(cid:18)φ(xi) − c(t)
(cid:18)

j

(cid:18)
2 ≤ m(cid:5)
(cid:18)
(cid:18)

(cid:8)

u(t+1)

ij

(cid:11)d(cid:8)

R(t)

j

(cid:11)2

u(t+1)

ij

+ ξ(t)

i

j=1

j=1

It is certain that for i = p + 1, . . . , n and j = 1, . . . , m we have

(cid:18)
(cid:18)φ(xi) − c(t)
(cid:18)

j

(cid:18)
(cid:18)
(cid:18)

2 ≥ (cid:8)

R(t)

j

(cid:11)

2 − ξ(t)

ij

Hence, (R(t), c(t), ξ(t)
at time t + 1. Since (R(t+1), c(t+1), ξ(t+1)
, ξ(t+1)
this optimisation problem, Theorem 4 is proved.

, U (t)) is feasible solution of optimisation problem (1)
, U (t+1)) is minimal solution of

, ξ(t)

ij

ij

i

i

Fuzzy Multi-Sphere Support Vector Data Description

579

3 Experiments

To show the performance of the proposed method, we established an experiment
on 23 data sets in UCI repository as shown in Table 1. Most of them are two-class
data sets and others are multi-class data sets. For each data set, we randomly
selected one class and regarded its data samples as normal data samples. Data
samples from the remaining class(es) were randomly selected to form a set of
abnormal samples such that the ratio of normal samples and abnormal samples
was kept to 12 : 1. We run cross validation with ﬁve folds and ten times and
then take average of ten accuracies to obtain the ﬁnal cross validation accuracy.

Table 1. Details of the data sets: #normal,#abnormal and d are number of normal,
abnormal data and dimension of the input space, respectively

Datasets

#normal #abnormal #d

Astroparticle

Australian

Bioinformatics
Breast Cancer

Diabetes

Dna

DelfPump

Germany Number

Four class

Glass
Heart

Ionosphere

Letter

Liver Disorders

Sonar
Specf
Splice

SvmGuide1
SvmGuide3

Thyroid
Vehicle
Wine
USPS

2000
307
221
444
500
464
1124
300
307
70
164
225
594
145
97
254
517
2000
296
3679
212
59

1194

166
25
18
36
41
38
93
24
25
5
13
18
49
12
8
21
43
166
24
93
17
5
99

4
14
20
10
8
180
64
24
2
9
13
34
16
6
60
44
60
4
22
21
18
13
256

−γ(cid:6)x−x(cid:2)(cid:6)2

We compared the proposed method with SVDD [14] and HMS-SVDD [8]. The
(cid:5)
popular RBF Kernel K(x, x
was applied and the parameter γ was
varied in grid {2i : i = 2j + 1, j = −8, . . . , 2}. The parameter ν1 and ν2 were
selected in grid {0.1, 0.2, 0.3, 0.4}. For FMS-SVDD, the number of spheres was
chosen in grid {3, 5, 7, 9} and parameter d was set to 1.5. To evaluate the classi-
ﬁcation rate, we employed the accuracy metric given by acc = acc++acc−
where

) = e

2

580

T. Le, D. Tran, and W. Ma

−

acc+ and acc
classes, respectively.

are the accuracies on positive (normal) and negative (abnormal)

For most of the data sets, especially for the large data sets, the proposed

method outperforms other kernel methods.

Table 2. Experimental results on 23 data sets in UCI repository

Datasets

SVDD HMS-SVDD FMS-SVDD

Four class

Glass
Heart

Diabetes

Dna

DelfPump

Astroparticle

Australian

Bioinformatics
Breast Cancer

91%
82%
69%
95%
65%
81%
69%
Germany Number 68%
93%
82%
83%
88%
90%
62%
63%
70%
57%
92%
63%
88%
58%
97%
93%

Thyroid
Vehicle
Wine
USPS

SvmGuide1
SvmGuide3

Ionosphere

Letter

Liver Disorders

Sonar
Specf
Splice

94%
82%
82%
98%
71%
97%
74%
70%
96%
88%
86%
91%
95%
71%
69%
76%
63%
98%
68%
92%
59%
98%
95%

96%
82%
84%
99%
70%
97%
74%
72%
97%
90%
85%
94%
96%
72%
71%
76%
64%
98%
70%
94%
60%
98%
94%

4 Conclusion

In this paper, we have presented a fuzzy approach to Multi-sphere Support
Vector Data Description to provide a better description to data sets with mixture
of distinctive distributions. Each sample is assigned a fuzzy membership function
representing the degree of belonging of that sample to a hypersphere. We have
theoretically proved that structural risk becomes smaller across iterations in the
learning process. Experiments on 23 real data sets in UCI repository have showed
a better performance of the proposed method in comparison to HMS-SVDD and
SVDD.

Fuzzy Multi-Sphere Support Vector Data Description

581

References

[1] Ben-Hur, A., Horn, D., Siegelmann, H.T., Vapnik, V.: Support vector clustering.

Journal of Machine Learning Research 2, 125–137 (2001)

[2] Boser, B.E., Guyon, I.M., Vapnik, V.: A training algorithm for optimal margin
classiﬁers. In: Proceedings of the 5th Annual ACM Workshop on Computational
Learning Theory, pp. 144–152. ACM Press (1992)

[3] Boyd, S., Vandenberghe, L.: Convex Optimisation. Cambridge University Press

(2004)

[4] Burges, C.J.C.: A tutorial on support vector machines for pattern recognition.

Data Mining and Knowledge Discovery 2, 121–167 (1998)

[5] Chen, Y., Zhou, X., Huang, T.S.: One-class svm for learning in image retrieval.

In: ICIP (2001)

[6] Chiang, J.-H., Hao, P.-Y.: A new kernel-based fuzzy clustering approach:support
vector clustering with cell growing. IEEE Transactions on Fuzzy Systems 11(4),
518–527 (2003)

[7] Le, T., Tran, D., Ma, W., Sharma, D.: An optimal sphere and two large margins
approach for novelty detection. In: The 2010 International Joint Conference on
Neural Networks (IJCNN), pp. 1–6 (2010)

[8] Le, T., Tran, D., Ma, W., Sharma, D.: A theoretical framework for multi-sphere
support vector data description. In: Wong, K.W., Mendis, B.S.U., Bouzerdoum, A.
(eds.) ICONIP 2010, Part II. LNCS, vol. 6444, pp. 132–142. Springer, Heidelberg
(2010)

[9] Lee, K., Kim, W., Lee, K.H., Lee, D.: Density-induced support vector data de-

scription. IEEE Transactions on Neural Networks 18(1), 284–289 (2007)

[10] GhasemiGol, M., Monseﬁ, R., Yazdi, H.S.: Ellipse support vector data description.
In: Palmer-Brown, D., Draganova, C., Pimenidis, E., Mouratidis, H. (eds.) EANN
2009. CCIS, vol. 43, pp. 257–268. Springer, Heidelberg (2009)

[11] Moya, M.M., Koch, M.W., Hostetler, L.D.: One-class classiﬁer networks for target

recognition applications, pp. 797–801 (1991)

[12] Scott, C.D., Nowak, R.D.: Learning minimum volume sets. Journal of Machine

Learning Research 7, 665–704 (2006)

[13] Tax, D.M.J., Duin, R.P.W.: Support vector data description. Journal of Machine

Learning Research 54(1), 45–66 (2004)

[14] Tax, D.M.J., Duin, R.P.W.: Support vector domain description. Pattern Recogni-

tion Letters 20, 1191–1199 (1999)

[15] Wu, M., Ye, J.: A small sphere and large margin approach for novelty detection
using training data with outliers. IEEE Transactions on Pattern Analysis and
Machine Intelligence 31(11), 2088–2092 (2009)

[16] Xiao, Y., Liu, B., Cao, L., Wu, X., Zhang, C., Hao, Z., Yang, F., Cao, J.: Multi-
sphere support vector data description for outliers detection on multi-distribution
data. In: ICDM Workshops, pp. 82–87 (2009)


