Adaptive Matching Based Kernels for Labelled Graphs

Adam Wo´znica, Alexandros Kalousis, and Melanie Hilario

University of Geneva, Computer Science Department

7 Route de Drize, Battelle batiment A

{Adam.Woznica,Alexandros.Kalousis,Melanie.Hilario}@unige.ch

1227 Carouge, Switzerland

Abstract. Several kernels over labelled graphs have been proposed in the liter-
ature so far. Most of them are based on the Cross Product (CP) Kernel applied
on decompositions of graphs into sub-graphs of speciﬁc types. This approach has
two main limitations: (i) it is difﬁcult to choose a-priori the appropriate type of
sub-graphs for a given problem and (ii) all the sub-graphs of a decomposition
participate in the computation of the CP kernel even though many of them might
be poorly correlated with the class variable. To tackle these problems we pro-
pose a class of graph kernels constructed on the proximity space induced by the
graph distances. These graph distances address the aforementioned limitations by
learning combinations of different types of graph decompositions and by ﬂexible
matching the elements of the decompositions. Experiments performed on a num-
ber of graph classiﬁcation problems demonstrate the effectiveness of the proposed
approach.

1 Motivation and Related Work

Support Vector Machines (SVMs), and Kernel Methods in general, became popular
due to their very good predictive performance [1]. As most of the real-world data can
not be easily represented in an attribute-value format many kernels for various kinds
of structured data have been proposed [2]. In particular, graphs are a widely used tool
for modelling structured data in data mining / machine learning and many kernels over
these complex structures have been proposed so far. These kernels have been mainly
applied for predicting the activity of chemical molecules represented by sparse, undi-
rected, labelled and two-dimensional graphs.

However, due to the rich expressiveness of graphs it has been proved that kernels over
arbitrarily structured graphs, taking their full structure into account, can be neither com-
puted [3] nor even approximated efﬁciently [4]. The most popular approach to tackle
the above problem is based on decompositions of graphs into sub-graphs of speciﬁc
types which are compared via sub-kernels. The sub-graph types mainly considered are
walks [3, 5–8]. However, other researchers have experimented with shortest paths [9],
sub-trees [4, 10], cyclic and tree patterns [11] and more general sub-graphs [12, 13].

A common feature in all the above graph kernels is that the Cross Product (CP)
Kernel is used between the corresponding multi-sets of decompositions. More precisely,
for particular decompositions Gt
2 of the graphs G1 and G2 into sub-structures
of type t the above kernels can be written as K(G1, G2) =
k(g1, g2)

1 and Gt

(cid:2)

(g1,g2)∈Gt

1×Gt

2

M.J. Zaki et al. (Eds.): PAKDD 2010, Part II, LNAI 6119, pp. 374–385, 2010.
c(cid:2) Springer-Verlag Berlin Heidelberg 2010

Adaptive Matching Based Kernels for Labelled Graphs

375

where k is a kernel over a speciﬁc type t of graphs, and the summation over the elements
of the multisets takes into account their multiplicity.

One problem with the above kernel is that all the possible sub-graphs of a given type
are matched by means of a sub-kernel. This might adversely affect the generalization
of a large margin classiﬁer since, due to the combinatorial growth of the number of dis-
tinct sub-graphs, most of the features in the feature space will be poorly correlated with
the target variable [12, 14]. Possible solutions to this problem include down-weighting
the contribution of larger sub-graphs [3, 15], using prior knowledge to guide the se-
lection of relevant parts [16], or considering contextual information for limited-size
sub-graphs [12]. Yet another solution was proposed in [10] in which only speciﬁc el-
ements of the corresponding multi-sets are matched in such a manner that the sum of
similarities of the matched elements is maximum. The underlying idea in this kernel is
that the actual matching will focus on the most important structural elements, neglect-
ing the sub-structures which are likely to introduce noise to the representation. The idea
of using speciﬁc pairs of points in a set kernel is promising, however, it is easy to show
that this kernel is not positive semi-deﬁnite (PSD) in general [17, 18].

More importantly, to the best of our knowledge, all existing graph kernels are cur-
rently limited to a single type of decomposition which is then, most often, used in the
context of the CP kernel. However, in general it is difﬁcult to specify in advance the
appropriate type of sub-structures for a given problem. Although, it is in principle pos-
sible to simultaneously exploit kernels deﬁned over different representations, this is
usually not done because there is a trade-off between the expressivity gained by enlarg-
ing the kernel-induced feature space and the increased noise to signal ratio (introduced
by irrelevant features). A common intuition is that by decomposing into more complex
sub-graphs the expressiveness, and consequently the performance, of resulting kernels
increases. This is, however, in contrast with some experimental evidence [12] which
suggest that decompositions into rather simple sub-structures perform remarkably well
with respect to more complex decompositions on a number of different datasets. A
simpliﬁed solution to the problem of representation selection is to select the decompo-
sition by cross-validation. However, this approach is problematic since it requires the
use of extra data and only one representation can be selected which limits the expres-
siveness of the resulting method. We can also directly learn to combine graph kernels
using multi-kernel learning methods, [19]. Nevertheless, the problem with learning ker-
nel combinations is that the combined elements should, obviously, be valid kernels.
However, as we saw previously this type of kernels are based on the CP kernel that re-
quires the complete matching of the components, raising the problems that we described
above.

To tackle the above problems we propose a class of adaptive graph kernels built on
proximity spaces [20]. The proximity spaces are induced by learned weighted combi-
nations of a given set distance measure on a number of decompositions of different
sub-graph types, and constructed on the basis of a representation set, typically the full1
training set. The weighted combinations are learned from the data, exploiting the NCA
method [21] developed for learning Mahalanobis metrics for vectors. By learning the

1 It is also possible to reduce the size of the representation set relying on feature selection in the

proximity space. However, this approach is not discussed in this paper.

376

A. Wo´znica, A. Kalousis, and M. Hilario

weights of the different decompositions we hope to obtain a combination of graph rep-
resentations that is expressive enough for the task at hand and does not overﬁt. Using the
set distance measures on the graph decompositions allows for ﬂexible ways of mapping
the elements of these decompositions, namely it is not necessary to use all the elements
(sub-graphs) in the mapping. We originally explored the use of proximity spaces to de-
ﬁne a graph kernel in [22]; however, there we were limited to a ﬁxed decomposition
and did not consider learning the combinations of the different types of substructures.
In this paper we will experiment with, and learn combinations of, two types of de-
compositions: walks and unordered trees of various lengths and heights, respectively.
The former were shown to be very effective in chemical domains and achieved state-
off-the-art results [3, 5–8]. Decomposition kernels based on trees were ﬁrst proposed
in [4], however, experimental evaluation was not performed. Kernels based on trees
were examined in [10]. Walks and trees can be easily compared by a graded similarity
(e.g. standard Euclidean metric for walks). Nevertheless, most of the kernels on graphs
use the Kronecker Delta Kernel (i.e. kδ(x, y) = 1 if x = y, kδ(x, y) = 0 otherswise) on
sub-parts. As a result, the ability to ﬁnd partial similarities is lost and the expressivity of
these kernels is reduced [7]. A graded similarity on walks was considered in the graph
kernel presented in [7], however, it suffers from high computational complexity since
it requires taking powers of the adjacency matrix of the direct product graph, leading
to huge runtime and memory requirements [9]. Graded similarities on trees were also
used in [10]. Finally, we note that our framework is not limited only to walks and trees
and can be applied on decompositions of any type.

This paper is organized as follows. In Sect. 2 we deﬁne all the necessary notions of
graphs. In Sect. 3 we deﬁne the adaptive kernels on graphs in the proximity space in-
duced by the set distance measures. Experimental results are reported in Sect. 4. Finally,
Sect. 5 concludes the work.

2 Primer of Graph Theory
An undirected graph G = (V,E) is described by a ﬁnite set of vertices V = {v1, . . . , vk}
and a ﬁnite set of edges E, where E = {{vi, vj} : vi, vj ∈ V}; for directed graphs we
set E = {(vi, vj) : vi, vj ∈ V}. We shall also use the following notation for edges:
eij = {vi, vj} (or eij = (vi, vj)). For labelled graphs there is additionally a set of ver-
tex labels LV and edge labels LE together with functions lV : V → LV and lE : E → LE
that assign labels to vertices and edges, respectively. We will use lab(x) to denote, in
a more general form the label of x; whether lab(x) = lV(x) or lab(x) = lE(x) will
be clear from the type of the x argument, i.e. vertex or edge. In this work we assume
that the labels are vectors, i.e. LV ⊆ IRp and LE ⊆ IRn for some values of m and
n. By dim(lab(x)) we will denote the dimensionality of the label vector lab(x) which
obviously will be the dimensionality of either LV or LE, depending again on x.

Some special types of graphs that are relevant to our work are walks and trees.
A walk, W , in a graph G is a sequence of vertices and edges, W = [v1, e12, v2
, . . . , es,s+1, vs+1], such that vj ∈ V for 1 ≤ j ≤ s+1 and eij ∈ E for 1 ≤ i ≤ s. Walks
can also end to an edge, e.g. W = [v1, e12, v2, . . . , es,s+1]. The length l of a walk, W ,
is the number of vertices and edges in W . We denote the set of all walks of length l in a

Adaptive Matching Based Kernels for Labelled Graphs

377
graph G by W(G)l. A tree T with vertices V and edges E of a graph G is a sub-graph
of G which is connected and acyclic. The root node of a tree is denoted as root(T ). In
this work we will only consider directed trees where the order is given from the root
node to the leafs. The height h of a tree T is the length of the longest walk from the
root node to any of the leaf nodes (similar to walks we allow that the trees have edges
as leafs). We denote the set of all directed trees of height h in a graph G by T (G)h. We
deﬁne the neighborhood of a node, v, in a tree as δ(v) = {e : e = (v, u) ∈ E}, and
the neighborhood of an edge, e, as δ(e) = {u : e = (v, u) ∈ E}, note that in fact the
neighborhood of an edge is a one element set, containing a single node.

3 Kernels on Graphs

We denote the decomposition of a graph, G, into the multi-set of sub-graphs of type t
by Gt. We focus on decompositions into walks of various lengths l, i.e. Gt = W(G)l,
and (directed) trees of various heights h, i.e. Gt = T (G)h. More generally, a graph G
can be represented by a tuple of m different decompositions as G = (Gt1 , . . . ,Gtm)T .
In this section we will deﬁne a class of adaptive kernels for labelled graphs. The
construction of these kernels is based on set distance measures which will be used to
compute the distances on the components of G. Since it is difﬁcult to select a priori the
appropriate types of sub-structures, i.e. components of G, for a given problem, we pro-
pose a method which learns a weighted (quadratic) combination of a ﬁxed set distance
on the different components of G. Finally, we use the learned weighted combination of
the set distance to induce a proximity space on which we deﬁne the ﬁnal kernel.

Distances on Sets. The central idea in the set distance measures that we consider is
the deﬁnition of a mapping of the elements of one set to the elements of the other such
that the ﬁnal distance is determined on the basis of speciﬁc pairs of elements from the
two sets. More precisely, consider two nonempty and ﬁnite sets A = {a} ⊆ X and
B = {b} ⊆ X . Let d(·,·) be a distance measure deﬁned on X . The set distance mea-
sure dset is deﬁned on 2X
i.e. it is a normalized sum
of pairwise distances over speciﬁc pairs which are deﬁned by F ⊆ A × B; different
F correspond to different set distance measures. Within this framework we can deﬁne
Average Linkage (dAL), Single Linkage (dSL), Complete Linkage (dCL), Sum of Min-
imum Distances (dSMD), Hausdorff (dH), RIBL (dRIBL), Tanimoto (dT), Surjections
(dS), Fair Surjections (dFS), Linkings (dL) and Matchings (dM) distance measures. De-
tailed description of these distance measures can be found in [23].

as dset(A, B) =

(cid:2)

(a,b)∈F d(a,b)

|F|

Distances Between Sets of Decompositions. We will now show how to use the above
set distance measures to compute distances between sets of decompositions of graphs
to sub-graphs of speciﬁc types. As already mentioned, we will focus on decompositions
into walks and (directed) trees of various lengths and heights. In the latter, we consider
unordered trees where the order among the siblings at a given height is not important.
Both types of decompositions are obtained from a depth ﬁrst exploration emanating
from each node in a graph. We note that the walks and trees we consider in this work
are non-standard in the sense that a walk can end in an edge and a tree can have edges
as leafs.

378

A. Wo´znica, A. Kalousis, and M. Hilario

In our decompositions we do not allow repetitions of a node within a walk or a tree,
i.e. we do not allow cycles, in order to avoid the problem of tottering [6]. We exclude
from the graph decomposition walks of the form W = [v1, e12, v2, . . . , es,s+1,vs+1]
with vi = vi+2 for some i since these are likely to introduce redundancy. In the case of
trees we do not allow such walks in the trees’ descriptions. Even though tottering-free
graph representations do not seem to bring an improvement to the predictive perfor-
mance [6, 10], they have the advantage of inducing decompositions smaller in size.
This has a direct inﬂuence on the computational complexity of the set distance mea-
sures applied over this representation.

k=1 d

N

2
lab(lab(Wi[k]),lab(Wj [k]))

The main difﬁculty in applying the set distance measures from the previous section
is to deﬁne the distance, d, between the elements of the sets, i.e. walks or trees. To
deﬁne a distance measure over walks we exploit the (normalized) Euclidean metric.
More precisely, the distance of two walks Wi and Wj of equal length l is deﬁned as
W (Wi, Wj) =
the Euclidean metric between the labels of the elements of the walks d2
(cid:2) l
where W [k] is the k-th element of walk W , and dlab(·,·)
is the Euclidean metric between the labels of corresponding walks. Obviously, Wi[k]
and Wj[k] are of the same type and they will be either edges or vertices. The N in
the denominator is a normalization factor and corresponds to the sum of the dimen-
sionalities of the label vectors of the l elements that make up the paths, obviously
N =

k=1 dim(lab(Wj[k]). We have 0 ≤ d2

k=1 dim(lab(Wi[k]) =

W(·,·) ≤ 1.

Lets now deﬁne a distance, dT(Ti, Tj), between two trees, Ti, Tj; we should note
here that unlike walks, trees do not have to be of the same height. Let x, y, be two
elements of the two trees found at the same height h. These elements will be either
two vertices, u, v, or two edges ei, ej. Then the (squared) distance between x and y,
t (x, y), is recursively deﬁned as:
d2

(cid:2)

l

(cid:2)

l

⎧
⎪⎨

⎪⎩

d

d

d

2
lab(lab(x),lab(y))+d

N(cid:3)
2
lab(lab(x),lab(y))
2
lab(lab(x),lab(y))+1

N(cid:3)
N(cid:3)

2
M(δ(x),δ(y))

if δ(x) (cid:6)= ∅ ∧ δ(y) (cid:6)= ∅
if δ(x) = ∅ ∧ δ(y) = ∅
if δ(x) = ∅ ⊗ δ(y) = ∅

where ⊗ is the logical XOR; dlab(lab(x), lab(y)) is the Euclidean metric between the
labels of the x, y; δ(x) is the neighbourhood function, deﬁned in Sect. 2, that returns
either: the set of edges to which a vertex connects to as a starting vertex, if x is of
type vertex, or the vertex to which an edge arrives if x is of type edge; ﬁnally dM is
the matching set distance measure between the sets of elements that are found in the
in the denominator is also a normalization factor that
neighborhoods of x and y. The N
corresponds to the dimensionality of the label vectors of x and y plus one to account
(cid:3) = dim(lab(x)) + 1 = dim(lab(y)) + 1. The
for the set distance dimension, i.e. N
matching distance dM will establish the best mapping among the elements of δ(x), δ(y),
letting outside, and penalizing for, sub-structures that cannot be matched. In a discrete
case scenario it is equivalent to a frequency based distance, i.e. for each of the discrete
sub-structures in δ(x), δ(y), it will count how many times it appears and the ﬁnal dis-
tance will be the sum of differences of these frequencies. Note here that dt(x, y) is a
recursive distance requiring the computation of set distances between the elements of

(cid:3)

Adaptive Matching Based Kernels for Labelled Graphs

379

the trees that are associated to x, y, at the h + 1 height. The ﬁnal distance between two
trees, Ti, Tj, is given by dT(Ti, Tj) = dt(root(Ti), root(Tj)).

Combining Different Decompositions. Here we show how to combine different de-
compositions of labelled graphs in order to produce a ﬁnal weighted graph distance.
Recall that two graphs G1 and G2 can be represented by m different decompositions
as (Gt1
2 )T of types t = t1, . . . , tm. For a ﬁxed set dis-
tance measure dset and the given m decompositions we deﬁne the vector d(G1, G2) =
(dset(Gt1
2 ))T which consists of the distances over the differ-
ent decompositions. Then the weighted combination of these decompositions is deﬁned
as

1 )T and (Gt1
2 ), . . . , dset(Gtm

2 , . . . ,Gtm
1 , Gtm

1 , . . . ,Gtm
1 ,Gt1

A(G1, G2) = d(G1, G2)T AT A d(G1, G2)
d2

(1)
where A is a m×m matrix. It should be noted that for any matrix A the matrix AT A is
PSD, and hence dA is a pseudo-metric. Here we propose a method for learning the ma-
trix A of equation (1) directly from the data by casting the problem as an optimization
task.

To learn the matrix A we use the Neighborhood Component Analysis (NCA) method
from [21] which was originally developed for metric learning over vectorial data.2
This method attempts to directly optimize a continuous version of the leave-one-out
(LOO) error of the kNN algorithm on the training data. First, a conditional distri-
bution is introduced which for each example Gi selects another example Gj as its
neighbor with some probability pA(j|i), and inherits its class label from the point it
selects. The probability pA(j|i) is based on the softmax of the d2
A distance given by
pA(j|i) =
(Gi ,Gk) , p(i|i) = 0. We denote Ci as the set of points that share
the same class with Gi, i.e. Ci = {j | class(Gi) = class(Gj)}. The objective function
to be maximized, as described in [21], is FA =

A(Gi ,Gj )
−d2
A

−d2
e
k(cid:4)=i e

j∈Ci pA(j|i)).

i log(

(cid:2)

(cid:2)

(cid:2)

A =

It is clear that for full matrices A the number of parameters to estimate is m2. This
could be problematic in cases where m is large with respect to the number of instances
in the training database. One possible solution to overcome this problem is to add a soft
constraint to the above objective function which results in the following regularized
j∈Ci pA(j|i)) + λ(cid:10)A(cid:10)2F where (cid:10)A(cid:10)F denotes a
objective function F reg
Frobenious norm of matrix A and λ > 0 is a regularization parameter. The other solu-
tion is to restrict matrix A to be diagonal resulting in a weighted combination of dis-
tances for different decompositions (we denote NCA where optimization is performed
over a diagonal matrix by NCAdiag). The main advantage of the NCA algorithm is that
it is non-parametric, making no assumptions about the shape of the class conditional
distributions [21]. Its main problem is that the objective function is not convex, hence
some care should be taken to avoid local optima.

i log(

(cid:2)

(cid:2)

Adaptive Matching Based Kernels. Here we exploit the adaptive combinations of
decompositions presented in Sect. 3, and the notion of proximity spaces, to deﬁne a
class of adaptive matching based kernels over labelled graphs. The proximity space is

2 We also experimented with other metric learning methods; however, due to the lack of space,

these results are not reported in this paper.

380

A. Wo´znica, A. Kalousis, and M. Hilario

deﬁned by an instantiation of dA from equation (1) and a representation set (i.e. set of
prototypes) of learning instances. More precisely, consider a graph G which is decom-
posed in m different ways as G = (Gt1 , . . . ,Gtm)T . For a given representation set S =
{G1, . . . , Gp} (we assume that each graph is uniquely identiﬁed by its index) and a dis-
tance measure dA we deﬁne a mapping dA(G, S) = (dA(G, G1), . . . , dA(G, Gp))T .
Since distance measures dA are non-negative, all the data examples are projected as
points to a non-negative orthotope of that vector space. The dimensionality of this
space is controlled by the size of the set S. In this setting the adaptive graph ker-
nel in the proximity space between graphs G1 and G2 is deﬁned as kdA(G1, G2) =
k(dA(G1, S), dA(G2, S)), where k denotes an elementary kernel in the proximity
space; k can be any standard kernel on vectorial data.

We mention that the complexity of computing the adaptive matching kernel between
two graphs (if the weights of different decompositions are known) can be shown to be
at most O{|S|m2|G|3(α3l + α3h)} where |S| is the cardinality of the representation
set, |G| is the number of vertices in graph G, α is the branching factor which can
be upper bounded by a small constant (usually 4), and m = l + h is the number of
decompositions. The complexity of learning the weight is at most O{m2(n2|G|3+n3)}
where n is the number of graphs in the training set.

4 Experiments

We will experiment with two graph classiﬁcation problems: Mutagenesis and Carcino-
genicity. The application task in the Mutagenesis dataset is the prediction of mutagenic-
ity of a set of 188 aromatic and hetero aromatic nitro-compounds which constitute the
“regression friendly” version of this dataset. The other classiﬁcation problem comes
from the Predictive Toxicology Challenge and is deﬁned over carcinogenicity prop-
erties of chemical compounds. This dataset lists the bioassays of 417 chemical com-
pounds for four type of rodents: male rats (MR), male mice (MM), female rats (FR)
and female mice (FM) which give rise to four independent classiﬁcation problems. We
transformed the original dataset (with 8 classes) into a binary problem by ignoring
EE (equivocal evidence), E (equivocal) and IS (inadequate study) classes, grouping SE
(some evidence), CE (clear evidence) and P (positive) in the positive class and N (nega-
tive) and NE (no evidence) in the negative one. After this transformation the MR, MM,
FR and FM datasets had 344, 336, 351 and 349 instances, respectively. References to
these datasets can be found in [22].

In the experiments we want to examine a number of issues related to our graph ker-
nels. More precisely, ﬁrst, for different set distance measures we will explore the effect
of the length of walks (and the height of trees) on the performance of kdset, i.e. we ﬁx the
walk length to a speciﬁc l for l = 1, . . . , 11 (and the height of a tree is ﬁxed to a speciﬁc
h, h = 2, . . . , 5) and we use only the corresponding decomposition to construct the ﬁ-
nal kernel; we do not combine decompositions. We will compare the performance of
the above simpliﬁed kernels with kdA where we combine all the different m = 11 + 4
decompositions. As the kernel k on the proximity space we will be always using the
linear kernel in order to make a fair comparison between the algorithms and to avoid
the situation where an implicit mapping given by a nonlinear kernel will inﬂuence the

Adaptive Matching Based Kernels for Labelled Graphs

381

)

%

(
 
y
c
a
r
u
c
c
a

 

t

d
e
a
m
s
E

i

95

90

85

80

75

70

65

60

1

w

2

w

3

w

4

w

5

w

6

w

7

w

8

9

w

w

0
Decompositions

w

1

SL
CL
AL
SMD
H
RIBL
T
S
FS
L
M

1

1

w

t 2

t 3

t 4

t 5

A

C

N

Fig. 1. Estimated accuracy of different kernels in the proximity space (kdset) vs. different de-
compositions for different set distance measures in the Mutagenesis dataset. wl denotes walks of
length l whereas th denotes trees of height h. The last values in the plot denote the performance
of kdA where the different decompositions are combined.

results. Second, we will examine how the performance of the kernel kdA compares with
the following two set kernels based on averaging: (i) the direct sum kernel [1] based of
the cross product kernels applied on the different m = 11 + 4 decompositions with
the linear kernel (in case of walks) and the tree kernel3 (for trees) as kernels on the
sets’ elements (kΣ,CP), and (ii) the linear kernel in the proximity space induced by dA,
also over the combination of the different m = 11 + 4 decompositions, where dAL
set distance measure is applied over the decompositions. Finally, we will compare our
graph kernels with other graph kernels form the literature.
We use the SVM algorithm where the parameter C is optimized in an inner 10-fold
cross-validation loop over the set C = {0.1, 1, 10, 50}. In all the experiments, unless
stated otherwise, we use the regularized version of the NCA algorithm over full matri-
ces A, where the regularization parameter λ is internally 10-fold cross-validated over
λ = {0, 0.1, 1, 10}. In some experiments we also use the version of NCA, denoted as
NCAdiag, where A is limited to a diagonal matrix. Note that in the experiments we have
11 different instantiations of the kdA kernel, each one corresponding to one of the 11 set
distance measures. In all the experiments accuracy is estimated using stratiﬁed 10-fold
cross-validation and controlled for the statistical signiﬁcance of observed differences
using McNemar’s test, with signiﬁcance level of 0.05.

4.1 Results and Analysis

Performance of Individual vs. Combined Decompositions. We ﬁrst examine the in-
ﬂuence of the length of the walks and heights of the trees to the predictive performance
of SVM. The results for the Mutagenesis dataset are presented in Fig. 1. From the re-
sults it is clear that in Mutagenesis the optimal decomposition depends on the actual set
distance measure. For example, for dSMD the highest predictive accuracy is obtained

3 The deﬁnition of the tree kernel is based on recursive and alternating computations of the
linear and cross product kernels [1]; the way these kernels are combined to compute the ﬁnal
tree kernel is similar to the deﬁnition of the tree distance from Sect. 3.

382

A. Wo´znica, A. Kalousis, and M. Hilario

1

0.8

0.6

s
n
o

i
t
i
s
o
p
m
o
c
e
d

 
f

o

 
s
t

i

h
g
e
w
d
e
z

 

i
l

a
m
r
o
n

 

e
v
i
t

l

a
e
R

0.4

0.2

0

w1

w2

w3

w4

w5

w6

w8

w7
Decompositions

w9 w10 w11

(a) NCAdiag

s
n
o

i
t
i
s
o
p
m
o
c
e
D

w1

w2

w3

w4

w5

w6

w7

w8

w9

w10

w11

t2

t3

t4

t5

0.1

0.05

0

−0.05

−0.1

t2

t3

t4

t5

w1 w2 w3 w4 w5 w6 w7 w8 w9 w10 w11

t2

t3

t4

t5

Decompositions

(b) NCA, λ = 0

Fig. 2. Relative importance of different decompositions (FM dataset) for the dSMD set distance
measure both for diagonal and full matrices A. wl denotes walks of length l whereas th denotes
trees of height h. It is represented as normalized weights A.

for walks of length 6, whereas for dS the best decomposition is into walks of length 10.
Additionally, with the exception of dSL, decompositions in walks in general outperform
decompositions into trees. For the other examined datasets the optimal decompositions
are different. Thus we have no way to know a-priori which type of decomposition is
appropriate. By combining them using NCA (results also given in Fig. 1) we get a clas-
siﬁcation performance that is in most cases as good as that of the single decompositions.
Indeed, in Mutagenesis the performance of NCA was signiﬁcantly better in 68 cases,
in 92 cases the differences were not signiﬁcantly meaningful, and in 5 cases it was sig-
niﬁcantly worse.4 In FM the corresponding values are 7, 158 and 0; in FR: 5, 147 and
22; in MM: 12, 153 and 0; in MR: 81, 83 and 1. We should stress here that it is not
fair to compare NCA with the results of the best decomposition, since this estimate is
optimistically biased since we need to have the results of the cross-validation to deter-
mine which is the best decomposition. In a fair comparison NCA should be compared
to a model selection strategy in which the best decomposition is selected using an in-
ner cross-validation. However an inner cross-validation-based model selection strategy
does not make full use of the data, thus its estimates might not be reliable, and it is
computationally expensive.

The results of the optimization process that learns the optimal combination of de-
compositions can be graphically presented providing insight to the relative importance
of different decompositions. In Fig. 2 we give an example of such a visualization for
the FM dataset and the dSMD set distance measure. In the left graph of that ﬁgure the
optimization was performed for diagonal matrices using NCAdiag. The different ele-
ments in x-axis are the elements of the diagonal of the matrix A which correspond to a
decomposition into walks and trees whereas the y-axis represents the weights, normal-
ized by a Frobenious norm of A, returned by the optimization method. What we see
from the graph is that in FM the highest weights are assigned to walks of lengths longer
than 6 and all tress, independent of their heights. The right graph of Fig. 2 provides the
visualization of the optimization process for NCA, where now the matrix A is a full

4 The total number of comparisons is 165 = 11 walk decompositions x 11 set distances + 4 tree

decompositions x 11 set distances.

Adaptive Matching Based Kernels for Labelled Graphs

383

Table 1. Accuracy and signiﬁcance test results of SVM in the graph datasets (the + sign stands for
a signiﬁcant win of the ﬁrst algorithm in the pair, - for a signiﬁcant loss and = for no signiﬁcant
difference). The sign in the ﬁrst parenthesis corresponds to the comparison of SVM vs. SVM with
kdA with dAL, while the sign in the second parenthesis compares SVM vs. SVM with kΣ,CP.

FM

FR

MM

MR

81.91 (=)(=) 65.04 (=)(=) 67.23 (=)(=) 66.96 (=)(=) 63.95 (=)(+)
80.85 (=)(=) 60.46 (=)(=) 63.24 ( -)(=) 65.18 (=)(=) 67.73 (+)(+)
89.36 (=)(+) 62.17 (=)(=) 67.52 (=)(=) 66.37 (=)(=) 65.99 (=)(+)
85.11 (=)(=) 64.18 (=)(=) 66.09 (=)(=) 66.07 (=)(=) 65.11 (=)(+)
87.23 (=)(=) 64.76 (=)(=) 67.52 (=)(=) 66.66 (=)(=) 67.15 (+)(+)
82.45 (=)(=) 64.76 (=)(=) 67.81 (=)(=) 68.45 (=)(+) 68.02 (+)(+)
92.02 (+)(+) 64.18 (=)(=) 67.52 (=)(=) 66.96 (=)(=) 61.34 (=)(=)
86.70 (=)(=) 61.60 (=)(=) 64.10 (=)(=) 63.99 (=)(=) 61.34 (=)(=)
86.17 (=)(=) 64.18 (=)(=) 65.81 (=)(=) 64.88 (=)(=) 66.28 (+)(+)
84.57 (=)(=) 64.46 (=)(=) 66.66 (=)(=) 65.48 (=)(=) 67.73 (+)(+)

Set Distance Mutagenesis
dSL
dCL
dSMD
dH
dRIBL
dT
dS
dFS
dL
dM
dAL
kΣ,CP

85.64
84.04

62.46
62.46

66.38
64.96

66.07
63.39

60.46
57.85

matrix (λ = 0). One surprising observation is that for NCA the decompositions into
trees and long walks are assigned low weights. At the same time combinations of trees
with long walks and combinations of shorter walks are of high importance.

Performance of Matchings Strategies. The next dimension of comparison is the rel-
ative performance of the instantiations of the kdA kernel for different set distances and
the following two set kernels based on averaging: (i) the direct sum kernel [1] based on
the cross product kernels applied on the different 15 decompositions (kΣ,CP), and (ii)
the linear kernel in the proximity space induced by dA, also over the combination of
the different 15 decompositions, with the dAL set distance measure.

The results are presented in Table 1. The relative performance of kernels based on
speciﬁc pairs of elements and kernels based on averaging depends on the actual appli-
cation. For Mutagenesis and MR there is an advantage of the kernels based on speciﬁc
pairs of elements, while for the remaining datasets the performances are similar. Over-
all, the choice of the appropriate way of matching the elements of two sets depends on
the application and ideally should be guided by domain knowledge, if such exists. Nev-
ertheless, the relative performance of the different kernels provides valuable informa-
tion about the type of problem we are facing. For example, by examining Mutagenesis
and MR we see that averaging performs poorly, indicating that the local structure of the
molecules is important, while in the remaining datasets both global and local structures
seem to be equally informative.

Comparison with Other Graph Kernels. In Table 2 we provide the best results re-
ported in the literature on the same benchmark datasets.5 Additionally, we report the
performance of kernels corresponding to dSMD and dT, both achieving stable good

5 We only cite works which use similar features to describe atoms and bonds. In particular our

results for Carcinogenicity are not directly comparable with the ones reported in [10].

384

A. Wo´znica, A. Kalousis, and M. Hilario

Table 2. Comparison with other related graph kernels

Graph kernels
Best kernel from [5]
Best kernel from [8]
Based on dSMD
Based on dT
Our best kernel

Mutagenesis FM FR MR MM
63.4 66.1 58.4 64.3
64.5 66.9 65.7 66.4
62.2 67.5 66.4 66.0
64.8 67.8 68.4 68.0
65.0 67.8 68.5 68.0

85.1
91.5
89.4
82.4
92.0

results across the different datasets. The values in the ”Our best kernel” row correspond
to the best kernel for each dataset, selected over all the examined mappings. It is obvi-
ous that the latter results are optimistically biased since they are selected after extensive
experimentation with various set distance measures. However, the same could be ar-
gued for the results of all the other related kernels given in Table 2 since in all the cases
multiple results where available and we reported on the best. The corresponding results
show that if the mapping is carefully selected for a problem at hand, then the perfor-
mance of the corresponding kernel is better than the performance of the existing graph
kernels. At the same time kernels corresponding to dSMD and dT achieve stable results
which are close to the state-of-the-art for the considered datasets.

5 Conclusions and Future Work

It has been argued in [7] that a “good” kernel for graphs should fulﬁl at least the follow-
ing requirements: (i) should be a good similarity measure for graphs, (ii) its computa-
tional time should be possible in polynomial time, (iii) should be positive semi-deﬁnite
and (iv) should be applicable for various graphs. In this paper we proposed a class of
adaptive kernels for graphs which are based on walks and trees, that are computable
in polynomial time, that are PSD, and are applicable to a wide range of graphs. A
distinctive feature of our kernels is that they allow for (i) combinations of different
types of decompositions and (ii) speciﬁc types of mappings between sub-parts. The
effectiveness of the approach was demonstrated on problems of activity prediction of
chemical molecules. Finally, the ideas presented in this work are not limited to graphs
and can be directly exploited to deﬁne kernels over not only special kinds of graphs
like sequences and trees but also over instances described using ﬁrst- (or higher-)order
logic [24].

In the future work we would like to examine decompositions of graphs into sub-
structures other than walks and trees (e.g. the cyclic patterns from [11]). Moreover, we
will apply the methods developed in this work on larger datasets, e.g. the HIV dataset
described in [12].

Acknowledgments. This work was partially funded by the European Commission
through EU projects DebugIT (FP7-217139) and e-LICO (FP7-231519). The support
of the Swiss NSF (Grant 200021-122283/1) is also gratefully acknowledged.

Adaptive Matching Based Kernels for Labelled Graphs

385

References

1. Shawe-Taylor, J., Cristianini, N.: Kernel Methods for Pattern Analysis. Cambridge Univer-

sity Press, Cambridge (2004)

2. G¨artner, T.: A survey of kernels for structured data. SIGKDD Explor. Newsl. 5(1), 49–58

(2003)

3. G¨artner, T., Flach, P., Wrobel, S.: On graph kernels: Hardness results and efﬁcient alterna-

tives. In: Proceedings of COLT 16 and the 7th Kernel Workshop (2003)

4. Ramon, J., G¨artner, T.: Expressivity versus efﬁciency of graph kernels. In: First International

Workshop on Mining Graphs, Trees and Sequences (held with ECML/PKDD’03) (2003)

5. Kashima, H., Tsuda, K., Inokuchi, A.: Marginalized kernels between labeled graphs. In:

ICML, Washington, DC (2003)

6. Mah´e, P., Ueda, N., Akutsu, T., Perret, J.L., Vert, J.P.: Extensions of marginalized graph

kernels. In: ICML (2004)

7. Borgwardt, K.M., Ong, C.S., Sch¨onauer, S., Vishwanathan, S., Smola, A.J., Kriegel, H.P.:

Protein function prediction via graph kernels. Bioinformatics 21(1), i47–i56 (2005)

8. Ralaivola, L., Swamidass, S.J., Saigo, H., Baldi, P.: Graph kernels for chemical informatics.

Neural Networks, 1093–1110 (2005)

9. Borgwardt, K.M., Kriegel, H.P.: Shortest-path kernels on graphs. In: ICDM (2005)
10. Fr¨ohich, H., Wegner, J., Sieker, F., Zell, A.: Optimal assignment kernels for attributed molec-

ular graphs. In: ICML (2005)

11. Horv´ath, T., G¨artner, T., Wrobel, S.: Cyclic pattern kernels for predictive graph mining. In:

KDD (2004)

12. Menchetti, S., Costa, F., Frasconi, P.: Weighted decomposition kernels. In: ICML (2005)
13. Sherashidze, N., Vishwanathan, S., Petri, T., Mehlhorn, K., Borgwardt, K.: Efﬁcient graphlet

kernels for large graph comparison. In: 12th AISTATS (2009)

14. Ben-David, S., Eiron, N., Simon, H.: Limitations of learning via embeddings in euclidean

half spaces. Journal of Machine Learning Research 3, 441–461 (2002)

15. Collins, M., Duffy, N.: Convolution kernels for natural language. In: NIPS (2002)
16. Cumby, C., Roth, D.: On Kernel Methods for Relational Learning. In: ICML (2003)
17. Wo´znica, A., Kalousis, A., Hilario, M.: Distances and (indeﬁnite) kernels for sets of objects.

In: ICDM (2006)

18. Vert, J.P.: The optimal assignment kernel is not positive deﬁnite (2008)
19. Lanckriet, G., Cristianini, N., Bartlett, P.L., Ghaoui, L.E., Jordan, M.: Learning the kernel

matrix with semi-deﬁnite programming. Journal of Machine Learning Research 5 (2004)

20. Pekalska, E., Duin, R.: The Dissimilarity Representation for Pattern Recognition. In: Foun-

dations and Applications. World Scientiﬁc Publishing Company, Singapore (2005)

21. Goldberger, J., Roweis, S., Hinton, G., Salakhutdinov, R.: Neighbourhood component anal-

ysis. In: NIPS. MIT Press, Cambridge (2005)

22. Wo´znica, A., Kalousis, A., Hilario, M.: Matching based kernels for labeled graphs. In: Min-

ing and Learning with Graphs (MLG 2006), with ECML/PKDD, Berlin, Germany (2006)

23. Ramon, J., Bruynooghe, M.: A polynomial time computable metric between point sets. Acta

Informatica 37(10), 765–780 (2001)

24. G¨artner, T., Lloyd, J.W., Flach, P.A.: Kernels and distances for structured data. Mach.

Learn. 57(3), 205–232 (2004)


