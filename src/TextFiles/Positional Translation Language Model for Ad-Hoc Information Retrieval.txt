 

Positional Translation Language Model for Ad-Hoc 

Information Retrieval 

Xinhui Tu1,2, Jing Luo1,2, Bo Li3, Tingting He3, and Jinguang Gu1,4 

1 College of Computer Science and Technology,   

Wuhan University of Science and Technology, Wuhan, China 

2 Hubei Province Key Laboratory of Intelligent Information Processing and 

Real-time Industrial System, Wuhan, China 

3 Department of Computer Science, Central China Normal University, Wuhan, China 

4State Key Lab of Software Engineering, Wuhan University, Wuhan, China 

{tuxinhui,luoluocat,simongu}@gmail.com,  
liboccnu@126.com, tthe@mail.ccnu.edu.cn 

Abstract. Most existing language modeling approaches are based on the term 
independence  hypothesis.  To  go beyond  this  assumption,  two  main  directions 
were investigated. The first one considers the use of the proximity features that 
capture the degree to which search terms appear close to each other in a docu-
ment. Another one considers the use of semantic relationships between words. 
Previous  studies  have  proven  that  these  two  types  of  information,  including 
term  proximity  features  and  semantic  relationships  between  words,  are  both 
useful to improve retrieval performance. Intuitionally, we can use them in com-
bination to further improve retrieval performance. Based on this idea, this paper 
propose a positional translation language model to explicitly incorporate both of 
these two types of information under language modeling framework in a unified 
way. In the first step, we present a proximity-based method to estimate word-
word  translation  probabilities.  Then,  we  define  a  translation  document  model 
for  each  position  of  a  document  and  use  these  document  models  to  score  the 
document.  Experimental  results  on  standard  TREC  collections  show  that  the 
proposed  model  achieves  significant  improvements  over  the  state-of-the-art 
models, including positional language model, and translation language models. 

Keywords: Positional Language Model, Translation Language Model, Informa-
tion Retrieval. 

1 

Introduction 

Language modeling (LM) for Information Retrieval (IR) has been a promising area of 
research  over  the past decade and a half.  It  provides  an  elegant  mathematical  model 
for  ad-hoc  text  retrieval  with  excellent  empirical  results  reported  in  the  literature 
[12][20].  However,  language  models  suffer  from  one  problem:  term  independence 
assumption which is common for all retrieval models.   

To  address  this  problem,  two  main  directions  were  investigated.  The  first  one  is 
based on the use of the proximity features. These features capture the degree to which 

V.S. Tseng et al. (Eds.): PAKDD 2014, Part II, LNAI 8444, pp. 134–145, 2014. 
© Springer International Publishing Switzerland 2014   

 

Positional Translation Language Model for Ad-Hoc Information Retrieval 

135 

search  terms  appear  close  to  each  other  in  a  document. To  incorporate  the  cues  of 
term position and term proximity under language model framework, Lv and Zhai [10] 
proposed  a  positional  language  model  (PLM).  In  PLM,  a  language  model  for  each 
term position in a document is defined, and document is scored based on the scores of 
its PLMs.   

The second one considers the use of semantic relationships between words. In or-
der to reduce the semantic gap between documents and queries, statistical translation 
models (TLM) have been proposed for information retrieval to capture semantic word 
relations [2]. The basic idea of translation language models is to estimate the proba-
bilities of translating a word in a document to query words. Since a word in a docu-
ment  could  be  translated  into  different  words  in  the  query,  translation  language   
models can avoid exact matching of words between documents and queries.   

The previous studies have proven that term proximity features and semantic rela-
tionships between words are both useful information to improve retrieval performance 
(e.g.,  [2][7][10][11]).  Intuitionally,  we  can  use  them  in  combination  to  further  im-
prove  retrieval  performance.  Based  on  this  idea,  this  paper  proposes  a  positional 
translation language model to explicitly incorporate these two types of information in 
a united way. In the first step, we present a proximity-based method to estimate word-
word translation probabilities. Then, we define a translation document model for each 
position of a document and use these document models to score the document.   

The main contribution of this paper is as follows：First, we propose a new proxim-
ity-based method, in which the proximity of co-occurrences is taking into account,  to 
estimate word-word translation probabilities. Second, we propose a positional transla-
tion  language  model  (PTLM)  to  explicitly  incorporate  term  proximity  features  and 
semantic  relationships  between  words  in  a  unified  way.  Finally,  extensive  experi-
ments  on  standard  TREC  collections  have  been  conducted  to  evaluate  the  proposed 
model. Experimental results on standard TREC collections show that PTLM achieves 
significant  improvements  over  the  state-of-the-art  models,  including  positional  lan-
guage model, and translation language models. 

2 

Background: PLM and TLM   

2.1  Basic Language Modelling Approach 

The basic idea of language models is to view each document to have its own language 
model and model querying as a generative process. Documents are ranked based on 
the probability of their language model generating the given query. Different imple-
mentations were proposed [20]. The general ranking formula is defined as follows: 

(cid:1864)(cid:1867)(cid:1859)(cid:1868)(cid:4666)(cid:1875)|(cid:1830)(cid:4667)                                 (cid:4666)1(cid:4667) 
  means equivalence for the purpose of ranking documents, (cid:1855)(cid:4666)(cid:1875),(cid:1843)(cid:4667)  is the 
count of word w in query (cid:1843), and (cid:1848)  is the vocabulary set. The challenging part is to   

(cid:1864)(cid:1867)(cid:1859)(cid:1868)(cid:4666)(cid:1830)|(cid:1843)(cid:4667)  (cid:3533)(cid:1855)(cid:4666)(cid:1875),(cid:1843)(cid:4667)

(cid:3050)(cid:1488)(cid:3023)

where 

 

136 

X. Tu et al. 

2.2 

Positional Language Model   

(cid:1868)(cid:4666)(cid:1875)|(cid:1843)(cid:4667)(cid:3404) 

|(cid:1830)|
|(cid:1830)|(cid:3397)(cid:2020)(cid:1868)(cid:3040)(cid:3039)(cid:4666)(cid:1875)|(cid:1830)(cid:4667)(cid:3397) 

To  incorporate  the  cues  of  term  position  and  term  proximity  under  language  model 
framework, Lv and Zhai [10] proposed a positional language model (PLM). In PLM 

maximum  likelihood  estimator.  However,  this  method  is  suffering  from  the  data 
sparseness problem.    To address this problem, some effective smoothing approaches, 
which combine the document model with the background collection model, have been 
proposed.  One  commonly  used  method  is  Dirichlet  Prior  smoothing  methods  [18], 
which is defined as follows: 

estimate  a  document  model (cid:1868)(cid:4666)(cid:1875)|(cid:1830)(cid:4667).  The  simplest  way  to  estimate (cid:1868)(cid:4666)(cid:1875)|(cid:1830)(cid:4667)  is  the 
(cid:2020)|(cid:1830)|(cid:3397)(cid:2020)(cid:1868)(cid:3040)(cid:3039)(cid:4666)(cid:1875)|(cid:1829)(cid:4667)                             (cid:4666)2(cid:4667) 
model, for each document (cid:1830)(cid:4666)(cid:1875)(cid:2869);…;(cid:1875)(cid:3036);…;(cid:1875)(cid:3037);…;(cid:1875)(cid:3015)(cid:4667), where 1, i, j, and N are abso-
document, a virtual D(cid:2919)  document is estimated at each position. This model is repre-
sented  as  a  term  frequency  vector (cid:1830)(cid:1731)(cid:1855)(cid:4593)(cid:4666)(cid:1875)(cid:2869);(cid:1861)(cid:4667);…;(cid:1855)(cid:4593)(cid:4666)(cid:1875)(cid:3015);(cid:1861)(cid:4667)(cid:1732),  where (cid:1855)(cid:4593)(cid:4666)(cid:1875);(cid:1861)(cid:4667)  is  the 
positions.  That  is (cid:1855)(cid:4593)(cid:4666)(cid:1875),(cid:1861)(cid:4667)(cid:3404)∑
(cid:1855)(cid:4666)(cid:1875),(cid:1862)(cid:4667)(cid:1863)(cid:4666)(cid:1861),(cid:1862)(cid:4667)
  where c(cid:4666)(cid:1875),(cid:1862)(cid:4667)  is  the  count  of  term 
w at position i in document D. If w occurs at position i, it is 1, otherwise 0. (cid:1863)(cid:4666)(cid:1861),(cid:1862)(cid:4667)  is 
Circle  kernel,  Cosine  kernel).  Once  the  virtual  document (cid:1830)(cid:3036)  is  estimated,  the  lan-
(cid:1855)(cid:4593)(cid:4666)(cid:1875),(cid:1861)(cid:4667)
                                                    (cid:4666)3(cid:4667) 
(cid:1855)(cid:4593)(cid:4666)(cid:1875)(cid:4593),(cid:1861)(cid:4667)
(cid:3050)(cid:4594)(cid:1488)(cid:3023)
where V is the vocabulary, (cid:1868)(cid:4666)(cid:1875)|(cid:1830),(cid:1861)(cid:4667)  is noted as a positional language model at posi-
tion  i. To  compute  the  final  score  of  document (cid:1830),  they  used  the  position-specific 

the propagated count to position i from a term at position j. Several proximity-based 
density  functions are used to estimate this factor: (Gaussian kernel, Triangle kernel, 

lute positions of the corresponding terms in the document, and N is the length of the 

total  propagated  count  of  term  w  at  position  i  from  the  occurrences  of  w  in  all  the 

guage model of this virtual document can be estimated as follow 

(cid:1868)(cid:4666)(cid:1875)|(cid:1830),(cid:1861)(cid:4667)(cid:3404)

(cid:3015)(cid:3037)(cid:2880)(cid:2869)

∑

scores. Different strategies were used: Best Position Strategy, Multi-Position Strategy, 
Multi-σ Strategy. 

2.3 

Statistical Translation Language Model 

To  incorporate  the  semantic  relationship  between  terms  under  language  model 
framework,  Berger and Lafferty proposed translation language modelling approach to 

estimate (cid:1868)(cid:4666)(cid:1875)|(cid:1830)(cid:4667)  based  on  statistical  machine  translation  [2].  In  this  approach,  the 
document  model (cid:1868)(cid:4666)(cid:1875)|(cid:1830)(cid:4667)   can  be  calculated  by  using  the  following  “translation 
(cid:1868)(cid:4666)(cid:1873)|(cid:1830)(cid:4667)                                            (cid:4666)4(cid:4667) 

(cid:1868)(cid:3047)(cid:4666)(cid:1875)|(cid:1830)(cid:4667)(cid:3404)(cid:3533)(cid:1868)(cid:3047)(cid:4666)(cid:1875)|(cid:1873)(cid:4667)

document model”:   

(cid:3048)(cid:3106)(cid:3005)

 

Positional Translation Language Model for Ad-Hoc Information Retrieval 

where (cid:1868)(cid:4666)(cid:1873)|(cid:1830)(cid:4667)  is  the  probability  of  seeing  word (cid:1873)  in  document (cid:1856),  and (cid:1868)(cid:3047)(cid:4666)(cid:1875)|(cid:1873)(cid:4667)  is 
the probability of “translating” word (cid:1873)  into word (cid:1875). In this way, a word can be trans-

137 

lated into its semantically related words with non-zero probability, which allows us to 
score  a  document  by  counting  the  matches  between  a  query  word  and  semantically 
related words in document.   

The key part for translation language model is estimating translation probabilities. 
Berger  and  Lafferty  [2]  proposed  a  method  to  estimate  translation  probabilities  by 
generating synthetic query. This method is inefficient and does not have good cover-
age of query words. In order to overcome these limitations, Karimzadehgan and Zhai 
[7] proposed an effective estimation method based on mutual information.  Recently, 
Karimzadehgan  and  Zhai  [8]  defined  four  constraints  that  a  reasonable  translation 
language  model  should  satisfy,  and  proposed  a  new  estimation  method  which  is 
shown to be able to better satisfy the constraints. This new estimation method, namely 
conditional context analysis, is described in formula 5.   

3 

Positional Translation Language Model 

In this section, we will describe the PTLM in detail. In the first part, a proximity-based 
method  is  presented  to  estimate  word-word  translation  probabilities.  Then,  we  will 
introduce how to estimate the translation document model for each position within a 
document. Finally, these positional document models are used to score the document. 

3.1  Estimating Translation Probability   

In the conditional context analysis method proposed in [8], the probability of translat-

ing word (cid:1873)    into word (cid:1875)  can be estimated as follows: 
(cid:1855)(cid:4666)(cid:1875),(cid:1873)(cid:4667)(cid:3397)1
                                                     (cid:4666)5(cid:4667) 
∑ (cid:1855)(cid:4666)(cid:1875)(cid:4593),(cid:1873)(cid:4667)(cid:3397)|(cid:1848)|
(cid:3050)(cid:4594)
where (cid:1855)(cid:4666)(cid:1875),(cid:1873)(cid:4667)  is the co-occurrences of word (cid:1873)  with word  w, and |(cid:1848)|  is the size of 

(cid:1868)(cid:4666)(cid:1875)|(cid:1873)(cid:4667)(cid:3404)

the vocabulary. 

In this method, any co-occurrence within the document is treated in the same way, 
no matter how far they are from each other.    This  strategy  is  not  optimal  as  a  docu-
ment may cover several different topics and thus contain much irrelevant information. 
Intuitionally,  closer  words  usually  have  stronger  relationships,  thus  should  be  more 
relevant.  Therefore,  we  introduce  a  new  concept,  namely  proximity-based  word  co-

occurrence frequency (pcf) to model the proximity feature of co-occurrences. 
cases. The Gaussian-based pcf  can be calculated as follows: 

Recently, density functions based on proximity are proven to be effective to cha-
racterize  term  influence  propagation.  A  number  of  term  propagation  functions  (e.g. 
Gaussian, Triangle, Cosine and Circle) have been proposed [10][11]. In this section, 
we  adopted  Gaussian  functions  because  it  has  been  shown  to  be  effective  in  most 

138 

(cid:3433)

 
D = { 

1 
2 
w  u 

3 
c 

4 
5 
k  w 

6 
u 

7 
k 

8 
9 
e  w 

10 
g 

 
} 

(cid:3005)(cid:1488)(cid:3004)(cid:3042)(cid:3039)(cid:4666)(cid:3050),(cid:3048)(cid:4667)

calculated from the position vectors. 

explain how to calculate distance score in the three distance measures. 

In 

this  paper, 

three  commonly  used  distance  measures  are  adopted 

to 

Minimum  pair  distance:  It is deﬁned as the minimum distance between any oc-

Average pair distance: It is deﬁned as the average distance between w and u for 

X. Tu et al. (cid:1868)(cid:1855)(cid:1858)(cid:4666)(cid:1875),(cid:1873)(cid:4667)(cid:3404) (cid:3533) (cid:1857)(cid:1876)(cid:1868)(cid:3429)(cid:3398)(cid:3435)(cid:1856)(cid:1861)(cid:1871)(cid:1872)(cid:4666)(cid:1875),(cid:1873),(cid:1830)(cid:4667)(cid:3439)(cid:2870)
2(cid:2026)(cid:2870)

                              (cid:4666)6(cid:4667) 
where, (cid:2026)  is a parameter in Gaussian distribution, (cid:1829)(cid:1867)(cid:1864)(cid:4666)(cid:1875),(cid:1873)(cid:4667)  is the set  of documents 
which contain both (cid:1875)  and (cid:1873), and (cid:1856)(cid:1861)(cid:1871)(cid:1872)(cid:4666)(cid:1875),(cid:1873),(cid:1830)(cid:4667)  is the distance score of word (cid:1875)  and 
(cid:1873)  in document (cid:1830).   
late (cid:1856)(cid:1861)(cid:1871)(cid:1872)(cid:4666)(cid:1875),(cid:1873),(cid:1830)(cid:4667).  We  will  use  the  following  short  document  D  as  an  example  to 
currences of w and u in document D. In the example, (cid:1856)(cid:1861)(cid:1871)(cid:1872)(cid:4666)(cid:1875),(cid:1873),(cid:1830)(cid:4667)  is 1 and can be 
all position combinations in (cid:1830). In the example, the distances from the first occurrence 
of (cid:1875)  (in position 1) to all occurrences of (cid:1873)  are: {1 and 5}. This is computed for the 
next occurrence of (cid:1875)  (in position 5) and so on. (cid:1856)(cid:1861)(cid:1871)(cid:1872)(cid:4666)(cid:1875),(cid:1873),(cid:1830)(cid:4667)  for the example is (((2-
rence of the other word. In the example, u  is the least frequently occurring word so 
(cid:1856)(cid:1861)(cid:1871)(cid:1872)(cid:4666)(cid:1875),(cid:1873),(cid:1830)(cid:4667)  = ((2−1)+(6−5))/2 =1.   
(cid:1868)(cid:1855)(cid:1858)(cid:4666)(cid:1875),(cid:1873)(cid:4667)(cid:3397)(cid:2035)
(cid:1868)(cid:4666)(cid:1875)|(cid:1873)(cid:4667)(cid:3404)
                                          (cid:4666)7(cid:4667) 
∑ (cid:1868)(cid:1855)(cid:1858)(cid:4666)(cid:1875)(cid:4593),(cid:1873)(cid:4667)(cid:3397)|(cid:1848)|(cid:1499)(cid:2035)
(cid:3050)(cid:4594)
where (cid:2035)  is a smoothing parameter in order to account for unseen words in the context 
of (cid:1873). Here (cid:2035)  is set equals to the smallest of all (cid:1868)(cid:1855)(cid:1858)  values in collection. 
(cid:1868)(cid:3047)(cid:4666)(cid:1873)|(cid:1873)(cid:4667)(cid:3404)(cid:1871) (cid:4666)(cid:1871)(cid:3410)0.5(cid:4667)                                                          (cid:4666)8(cid:4667) 
(cid:1868)(cid:4666)(cid:1875)|(cid:1873)(cid:4667)
(cid:1868)(cid:3047)(cid:4666)(cid:1875)|(cid:1873)(cid:4667)(cid:3404)(cid:4666)1(cid:3398)(cid:1871)(cid:4667)(cid:1499)
                                        (cid:4666)9(cid:4667) 
(cid:1868)(cid:4666)(cid:1874)|(cid:1873)(cid:4667)
∑
(cid:3049)(cid:2999)(cid:3048)
where parameter s  is a constant value that could be set to 0.5 <= s <= 1.  Note that 
when (cid:1871)  = 1, the query likelihood model are gained. 

Average minimum pair distance: It is defined as the average of the shortest dis-
tance between each occurrence of the least frequently occurring word and any occur-

Then, the probability of translating word u into word w can be estimated as follows: 

In order to satisfy the constraints defined in [8], we adjust self-translation  proba-

1) + (6-1)) + ((5-2) + (6-5)) + ((9-2) + (9-6)))/(2 · 3) = 20/6 = 3.33.   

bilities as follows: 

3.2  Estimating Translation Document Model 

The state-of-art translation language models use an entire document as a unit to esti-
mate the generative probability of the query [7][8]. This strategy is not optimal as a 

 

Positional Translation Language Model for Ad-Hoc Information Retrieval 

139 

document may cover several different topics. Intuitionally, the words referring to the 
same topic may occur close to each other. Positional language model has been proven 
to  be  an  effective  way  to  incorporate  the  cues  of  term  position  and  term  proximity 
under language model framework [10]. In this section, we will introduce a positional 
translation language model to naturally incorporate two types of information, includ-
ing term proximity features and semantic relationships between terms, under language 
model framework in a united way.   

The key idea of our method is to extend the translation language model from doc-
ument level to positional level via the positional language model. The proposed model 
can capture the topic of the document at the position by giving more weight on words 
close  to  the  position  and  less  weight  on  words  far  away.  The  translation  language 
model at each position can be estimated based on all the propagated counts of all the 
words  to  the  position  as  if  all  the  words  had  appeared  actually  at  the  position  with 
discounted counts.     

of document D , and can be estimated as follows: 

Previous  studies  have  shown  that  translation  language  model  works  better  with   
Dirichlet prior smoothing [7][8]. Therefore, in the rest of the paper, we further focus 
on  PTLM  with  Dirichlet  prior  smoothing  only.  The  final  positional  translation  lan-

guage model for position (cid:1861)  in document (cid:1830)  can be defined as follows: 
(cid:1868)(cid:3047)(cid:4666)(cid:1875)|(cid:1830),(cid:1861)(cid:4667)(cid:3404) |(cid:1830)|
(cid:2020)|(cid:1830)|(cid:3397)(cid:2020)(cid:1868)(cid:4666)(cid:1875)|(cid:1829)(cid:4667)                  (cid:4666)10(cid:4667) 
|(cid:1830)|(cid:3397)(cid:2020)(cid:3429)(cid:3533)(cid:1868)(cid:3047)(cid:4666)(cid:1875)|(cid:1873)(cid:4667)
(cid:1868)(cid:4666)(cid:1873)|(cid:1830),(cid:1861)(cid:4667)(cid:3433)(cid:3397)
(cid:3048)(cid:3106)(cid:3005)
where (cid:1868)(cid:3047)(cid:4666)(cid:1875)|(cid:1873)(cid:4667)  is  the  translation  probability  from  word  u  to  word  w,  and  can  be 
estimated by formula 8 and 9; (cid:1868)(cid:4666)(cid:1873)|(cid:1830),(cid:1861)(cid:4667) is the positional document model at position i 
(cid:1855)(cid:4593)(cid:4666)(cid:1873),(cid:1861)(cid:4667)
(cid:1868)(cid:4666)(cid:1873)|(cid:1830),(cid:1861)(cid:4667)(cid:3404)
                                                          (cid:4666)11(cid:4667) 
∑
(cid:1855)(cid:4593)(cid:4666)(cid:1873)(cid:4593),(cid:1861)(cid:4667)
(cid:3048)(cid:4594)(cid:1488)(cid:3023)
where (cid:1855)(cid:4593)(cid:4666)(cid:1873),(cid:1861)(cid:4667)  is  the  total  propagated  count  of  term (cid:1873)    at  position (cid:1861)  from  the  oc-
currences  of (cid:1873)  in  all  the  positions. (cid:1855)(cid:4593)(cid:4666)(cid:1873),(cid:1861)(cid:4667)  can  be  estimated  using  the  Gaussian 
kernel function: (cid:1855)(cid:4593)(cid:4666)(cid:1873),(cid:1861)(cid:4667)(cid:3404)(cid:3533)(cid:1855)(cid:4666)(cid:1873),(cid:1862)(cid:4667)exp (cid:4680)(cid:3398)(cid:4666)(cid:1861)(cid:3398)(cid:1862)(cid:4667)(cid:2870)
|(cid:3005)|
                                                (cid:4666)12(cid:4667) 
2(cid:2026)(cid:2870)
(cid:3037)(cid:2880)(cid:2869)
where   (cid:1861)  and (cid:1862)  are absolute positions of the corresponding terms in document, and 
|(cid:1830)|  is the length of the document, (cid:1855)(cid:4666)(cid:1873),(cid:1862)(cid:4667)  is the real count of term (cid:1873)   at position (cid:1862) . 

(cid:4681)

3.3  Ranking Document   

In the section 3.2, we have obtained a translation language model for each position in 
a document. Intuitively, we can imagine that the PTLMs give us multiple representa-
tions of  D. Thus  given a query  Q,  we can adopt the KL-divergence retrieval  model 
[19] to score each PTLM as follows: 

140 

X. Tu et al. (cid:1845)(cid:4666)(cid:1843),(cid:1830),(cid:1861)(cid:4667)(cid:3404)(cid:3398)(cid:3533)(cid:1868)(cid:4666)(cid:1875)|(cid:1843)(cid:4667)(cid:1864)(cid:1867)(cid:1859) (cid:1868)(cid:4666)(cid:1875)|(cid:1843)(cid:4667)
(cid:1868)(cid:3047)(cid:4666)(cid:1875)|(cid:1830),(cid:1861)(cid:4667)

(cid:3050)(cid:1488)(cid:3023)

                                     (cid:4666)13(cid:4667)  

Then, the position-specific scores can be used to compute the final score of doc-
ument D. In this paper, we compute the final score of document D using the best posi-
tion strategy [10], which simply scores a document based on its best match position 
and can be defined as follows: 

(cid:1845)(cid:4666)(cid:1843),(cid:1830)(cid:4667)(cid:3404)(cid:1865)(cid:1853)(cid:1876)(cid:3036)(cid:1488)(cid:4670)(cid:2869),(cid:3015)(cid:4671)(cid:4668)(cid:1845)(cid:4666)(cid:1843),(cid:1830),(cid:1861)(cid:4667)(cid:4669)                                               (cid:4666)14(cid:4667) 

4 

Experiments 

4.1  Data Set 

We used six standard TREC data sets in our study. They represent different sizes and 
genre  of  text  collections.  Table  1  shows  some  basic  statistics  about  these  data  sets. 
Each document is processed in a standard way for indexing. Words are stemmed (using 
porter-stemmer), and stop words are removed. In the experiments, we only use title of 
the queries because semantic word matching is necessary for such short queries.   

Table 1. Document set characteristic 

 

TREC7 

DOE 

WSJ 

TREC8 

AP88-89 

FR 

queries 

351-400 

51-100 

51-100 

401-450 

51-100 

51-100 

#doc 

528,155 

226,087 

74,520 

528,155 

164,597 

45,820 

 

In each experiment, we use the KL-divergence model using Dirichlet prior smooth-
ing  (with  prior  parameter  μ=1000)  to  retrieve  2000  documents  for  each  query,  and 
then  use  the  PTLM  to  re-rank  them.  The  top-ranked  1000  documents  are  used  for 
comparison with other models. In order to evaluate our model and compare it to other 
models  we  use the MAP  measure,  which is  widely accepted measure for evaluating 
effectiveness of ranked retrieval systems. 

In the  section  3.1, three different proximity measures are adapted to measure the 
distance  score  of  two  words  in  a  document.  The  corresponding  models  based  on   
the  three  different  proximity measures  are evaluated on standard TREC collections. 
The methods used for the experiments are: 
•  QL: baseline, query likelihood model with Dirichlet prior smoothing [18]. 
•  KL: baseline, KL-divergence model with Dirichlet prior smoothing [19].   
•  TM-MI: translation language model with mutual information [7]. 
•  TM-CCON: translation language model with conditional context analysis [8]. 
•  PLM: positional language model with the best position strategy [10]. 
•  PTLM-1: PTLM with minimum pair distance. 

 

Positional Translation Language Model for Ad-Hoc Information Retrieval 

141 

•  PTLM-2: PTLM with average pair distance.   
•  PTLM-3: PTLM with average minimum pair distance. 

4.2  Comparing with Existing Retrieval Models 

As we can see from all the PTLM models used in our experiments, there are several 
controlling  parameters  to  tune.  In  order  to  make  the  comparison  fair,  we  evaluate 
PTLMs  and  PLM by a 5-fold cross-validation  on each collection.  For the two  base-

lines (QL and KL), parameter µ  in the Dirichlet smoothing is set to the optimal value 

for each collection. The results of TM-MI, TM-CCON are directly from [8].   

Table 2 shows the results for these models with Dirichlet prior smoothing. Com-
paring the rows in the table indicates that  the PTLM models  achieve  significant  im-
provements over the state-of-the-art models, including positional language model and 
translation  language  models. In addition, the results confirm our hypothesis that  the 
two  types  of  information  can  be  used  in  combination  to  improve  retrieval  perfor-
mance. Comparing the three variants of PTLM, PTLM-3 is more effective and robust 
than  PTLM-2  and  PTLM-1.  It  also  indicates  that  average-minimum-pair  distance 
measure can capture the proximity feature of co-occurrences better than the other two 
measures. The significance test results using Wilcoxon signed-rank test indicate that 
the differences between the PTLM models and the start-of-art models are statistically 
significant.   

Table 2. The comparison of experiment results 

(  *  and    +  mean  improvements  over  TM-CCON  and  PLM  are  statistically  significant  with 
Wilcoxon signed-rank test, respectively) 

 

QL 

KL 

TREC7 

DOE 

WSJ 

TREC8 

AP88-89 

FR 

  0.1852     

0.1740 

0.2600  

0.2518 

0.2154 

0.2817   

0.1847   

0.1742  

0.2584 

0.2509 

0.2196 

0.2697   

TM-MI 

0.1854   

0.1750  

0.2658 

TM-CON 

0.1920   

0.1844  

0.2780 

PLM 

0.1893   

0.1795  

0.2641 

PTLM-1 

0.2003*+ 

0.1952*+ 

PTLM-2 

PTLM-3 

0.2021*+ 

0.1967*+ 

0.2030*+ 

0.1975*+ 

0.2896*+ 

0.2913*+ 

0.2924*+

- 

- 

0.2548 
0.2672+ 

0.2685+ 

0.2692+ 

- 

- 

0.2196 
0.2246+ 

0.2259+ 

0.2276+ 

- 

- 

0.2842   
0.2885+ 

0.2891+ 

0.2920+ 

4.3 

Parameter Sensitivity Study 

ity of their parameters (cid:1871)  (in Equation 8, 9) and (cid:2026)  (in Equation 6,  12). The parameter 
(cid:1871)  controls  the  amount  of  self-translation  probabilities.  The  kernel  parameter (cid:2026)  in 

An important issue that may affect the robustness of the PTLM models is the sensitiv-

142 

X. Tu et al. 

values for these PTLM models are not the same. For example, on the TREC7  collec-

Equation  6  determines  the  distance  in  which  words  are  considered  to  be  related. 

tual  document.  In this section, we study how sensitive these parameters are to MAP 
measure.   

Another kernel parameter (cid:2026)    in Equation 12 restricts the propagation scope of a vir-
We  investigate  a  large  range  of (cid:2026)  (in  Equation  6)  from  10  to  1000.  Generally,   
the value of (cid:2026)  affects the performance of all PTLM models extensively. The experi-
mental results show that the influence of (cid:2026)  is collection-based. For the three PTLM 
models, their curves fluctuate similarly on the same collection. However, the best (cid:2026) 
tion, optimal (cid:2026)  value for PTLM-1 is 80, and the corresponding value for PTLM-2 is 
150. Thus, the optimal values of (cid:2026)  depend on the proximity measures and the collec-
with   (cid:2026)  values ranging from 10 to 1000 on TREC7. 

tions. Figure 1 plots the evaluation metrics MAP obtained by the three PTLM models 

from10 to 1000 

Fig.  1.  PTLM-1,  PTLM-2,  PTLM-3  over  TREC7  with (cid:2763)  (in  Equation  6)  values  ranging 
The experimental results also show that the influence of (cid:1871)  is collection-based. For 
one collection, the best (cid:1871)  values for all PTLM models are the same. Specifically, the 
best (cid:1871)  values are 0.7, 0.8, and 0.5 for TREC7, DOE, and WSJ, respectively.   
In order to see how the propagation scope parameter (cid:2026)  (in Equation 12) affects the 
It also seems that the performance of the PTLM models stabilizes after (cid:2026)  reach 175.   
To  investigate  how  the  Dirichlet  prior  parameter µ  (in  Equation  10)  affects  the 

performance of the PTLM models,  we  test  a  set  of  values  from  25  to  275  in  incre-
ments of 25. Overall, we see that a relatively large often brings the best performance. 

 

performance of the PTLM models, we also change the settings of the smoothing pa-
rameters for them. The results indicate that the optimal smoothing parameters are the 
same (equals to 500) for all the three PTLM models on all collections. 

 

5 

Positional Translation Language Model for Ad-Hoc Information Retrieval 

143 

Related Work 

Most  existing  information  retrieval  model  including  probabilistic  and  vector  space 
models  are  based  on  the  term  independence  hypothesis.  Given  common  knowledge 
about  language,  such  an  assumption  might  seem  unrealistic.  To  go  beyond  the   
term  independency  assumption  in  information  retrieval,  two  main  directions  were 
investigated. 

The first one considers the use of the proximity features that capture the degree to 
which search terms appear close to each other in a document. For example, it looks at 
the minimum span of the query terms appearing in the document. Term proximity, as 
an effective retrieval heuristic, has been studied extensively in the past few years. In 
these papers, various methods have been proposed to integrate proximity information 
into different retrieval models. Keen [9] firstly attempted to import term proximity in 
the  Boolean  retrieval  model  by  introducing  a  “NEAR”  operator.  Buttcher  et  al.  [3] 
proposed an integration of term proximity scoring into Okapi BM25 and obtain im-
provements on several collections. Tao et al. [14] systematically studied five proximi-
ty measures and compared their performance in various retrieval models. Zhao et al. 
[16] used a query term’s proximity centrality as a hyper parameter in Dirichlet lan-
guage model under the language modelling framework. Lv and Zhai [10] integrated 
the  position  and  proximity  information  into  the  language  model  by  defining  a  lan-
guage model for each position within a document. Zhao et al. [17] introduce a pseudo 
term,  namely  cross  term,  to  model  term  proximity  for  boosting  retrieval  model.   
Miao et al. [11] has attempted to incorporate proximity information into the Rocchio’s 
model.     

The second one considers the use of semantic relationships between words. Under 
this way, relevant words are used to enrich document or query representation.    Many 
studies have tried to bridge the vocabulary gap between documents and queries both 
based on term co-occurrences [1, 6, 13] and hand-crafted thesaurus [15]. Some other 
works have considered to combine both approaches [4]. Berger and Lafferty [2] firstly 
proposed  a  translation  language  model  to  corporate  semantic  relationship  between 
words under the language modeling framework. To train translation models, they used 
synthetically  generated  query-document  pairs.  An  alternation  way  of  estimating  the 
translation model is based on document titles [5]. Recent works have relied on docu-
ment-based word co-occurrences to estimate the translation model [7][8]. 

6 

Conclusion 

Term proximity features and semantic relationships between words have proven to  be 
two kinds  of  useful information to improve retrieval performance. In this paper, we 
proposed  a  positional  translation  language  model  to  incorporate  both  of  them  in  a 
unified way. In the first step, a new proximity-based method is presented to estimate 
the  translation  model.  Three proximity measures are then adopted for calculating the 
distance score of two words within a  document. The corresponding models based on 
these measures, PTLM-1, PTLM-2 and PTLM-3, are evaluated on six standard TREC 

144 

X. Tu et al. 

collections. Our experiment results indicate that the PTLM models are more effective 
than the state-of-art models, including positional language model and translation lan-
guage  models.  Comparing  the  three  variants  of  PTLM,  PTLM-3  is  more  effective 
than the other two. 

Since the number of positions is  much larger than the  number of documents, the 
cost of estimating PTLMs can be extremely high. For the sake of efficiency, we use 
PTLM to re-rank the top 2000 documents from initial search results. However, such a 
strategy does not fully take advantage of the capacity of PTLM to potentially retrieve 
relevant  documents  that  do  not  match  any  query  word.  In  the  future,  we  will  try  to 
study how to reduce the computational complexity of PTLM and to further improve 
retrieval performance. 
 
Acknowledgments.  This  work  was  partially  supported  by  the  National  Science 
Foundation of China under grants No. 60803160 and No. 61300144, the Key Projects 
of  National  Social  Science  Foundation  of  China  under  grant  number  11ZD&189.  It 
was partially supported by NSF of Hubei Prov. under grant number 2013CFB334, and 
the  State  Key  Lab  of  Software  Engineering  Open  Foundation  of  Wuhan  University 
under grant number SKLSE2012-09-07.  

References 

1.  Bai, J., Song, D., Bruza, P., Nie, J.Y., Cao, G.: Query expansion using term relationships 
in  language  models  for  information  retrieval.  In:  Proceedings  of  the  14th  ACM  Interna-
tional Conference on Information and Knowledge Management, pp. 688–695. ACM, New 
York (2005) 

2.  Berger,  A., Lafferty, J.: Information retrieval as statistical translation. In: Proceedings of 
the 22nd Annual International ACM SIGIR Conference on Research and Development in 
Information Retrieval, pp. 222–229. ACM, New York (1999) 

3.  Büttcher, S., Clarke, C.L.A., Lushman, B.: Term proximity scoring for ad-hoc retrieval on 
very large text collections. In: Proceedings of the 29th Annual International ACM SIGIR 
Conference on Research and Development in Information Retrieval, pp. 621–622. ACM, 
New York (2006) 

4.  Cao, G., Nie, J.Y., Bai, J.: Integrating word relationships into language  models. In: Pro-
ceedings of the 28th Annual International ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, pp. 298–305. ACM, New York (2005) 

5.  Jin, R., Hauptmann, A.G., Zhai, C.X.: Title language model for information retrieval. In: 
Proceedings  of  the  25th  Annual  International  ACM  SIGIR  Conference  on  Research  and 
Development in Information Retrieval, pp. 42–48. ACM, New York (2002) 

6.  Jing, Y., Croft, W.B.: An association thesaurus for information retrieval. In: Proceedings 

of RIAO, pp. 146–160 (1994) 

7.  Karimzadehgan, M., Zhai, C.X.: Estimation of statistical translation models based on mu-
tual information for ad hoc information retrieval. In: Proceedings of the 33rd International 
ACM  SIGIR  Conference  on  Research  and  Development  in  Information  Retrieval,  pp.   
323–330. ACM, New York (2010) 

8.  Karimzadehgan, M., Zhai, C.: Axiomatic analysis of translation language model for infor-
mation  retrieval.  In:  Baeza-Yates,  R.,  de  Vries,  A.P.,  Zaragoza,  H.,  Cambazoglu,  B.B., 
Murdock, V., Lempel, R., Silvestri, F. (eds.) ECIR 2012. LNCS, vol. 7224, pp. 268–280. 
Springer, Heidelberg (2012) 

 

Positional Translation Language Model for Ad-Hoc Information Retrieval 

145 

9.  Keen, E.M.: Some aspects of proximity searching in text retrieval systems. The Journal of 

Information Science 18(2), 89–98 (1992) 

10.  Lv, Y., Zhai, C.X.: Positional language models for information retrieval. In: Proceedings 
of  the  32nd  International  ACM  SIGIR  Conference  on  Research  and  Development  in  In-
formation Retrieval, pp. 299–306. ACM, New York (2009) 

11.  Miao, J., Huang, X.J., Ye, Z.: Proximity-based Rocchio’s model for pseudo relevance. In: 
Proceedings of the 35th International ACM SIGIR Conference on Research and Develop-
ment in Information Retrieval, pp. 535–544. ACM, New York (2012) 

12.  Ponte, J.M., Croft, W.B.: A language modeling approach to information retrieval. In: Pro-
ceedings of the 21st Annual International ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, pp. 275–281. ACM, New York (1998) 

13.  Schütze, H., Pedersen, J.O.: A co-occurrence based thesaurus and two applications to in-

formation retrieval. Information Processing and Management 33(3), 307–318 (1997) 

14.  Tao,  T.,  Zhai,  C.X.:  An  exploration  of  proximity  measures  in  information  retrieval.  In: 
Proceedings  of  the  30th  Annual  International  ACM  SIGIR  Conference  on  Research  and 
Development in Information Retrieval, pp. 295–302. ACM, New York (2007) 

15.  Voorhees, E.M.: Query expansion using lexical-semantic relations. In: Proceedings of the 
17th Annual International ACM SIGIR Conference on Research and Development in In-
formation Retrieval, pp. 61–69. ACM, New York (1994) 

16.  Zhao, J., Yun, Y.: A proximity language model for information retrieval. In: Proceedings 
of  the  32nd  International  ACM  SIGIR  Conference  on  Research  and  Development  in  In-
formation Retrieval, pp. 291–298. ACM, New York (2009) 

17.  Zhao, J., Huang, X., He, B.: CRTER: Using cross terms to enhance probabilistic informa-
tion  retrieval.  In:  Proceedings  of  the  34th  International  ACM  SIGIR  Conference  on  Re-
search and Development in Information Retrieval, pp. 155–164. ACM, New York (2011) 

18.  Zhai, C.X., Lafferty, J.: A study of smoothing methods for language models applied to Ad 
Hoc information retrieval. In: Proceedings of the 24th Annual International ACM SIGIR 
Conference on Research and Development in Information Retrieval, pp. 334–342. ACM, 
New York (2001) 

19.  Zhai, C.X., Lafferty, J.: Model-based feedback in the language modeling approach to in-
formation retrieval. In: Proceedings of the Tenth International Conference on Information 
and Knowledge Management, pp. 403–410. ACM, New York (2001) 

20.  Zhai,  C.X.:  Statistical  Language  Models  for  Information  Retrieval  A  Critical  Review. 

Foundations and Trends in Information Retrieval 2(3), 137–213 (2008) 


