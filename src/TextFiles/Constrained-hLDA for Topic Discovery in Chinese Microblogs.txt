Constrained-hLDA for Topic Discovery

in Chinese Microblogs

Wei Wang, Hua Xu, Weiwei Yang, and Xiaoqiu Huang

State Key Laboratory of Intelligent Technology and Systems,

Tsinghua National Laboratory for Information Science and Technology,
Department of Computer Science and Technology, Tsinghua University,

Beijing 100084, China

{ww880412,alexalexhxqhxq}@gmail.com,
xuhua@tsinghua.edu.cn, ywwbill@163.com

Abstract. Since microblog service became information provider on web scale,
research on microblog has begun to focus more on its content mining. Most
research on microblog context is often based on topic models, such as: Latent
Dirichlet Allocation(LDA) and its variations. However,there are some challenges
in previous research. On one hand, the number of topics is ﬁxed as a priori, but
in real world, it is input by the users. On the other hand, it ignores the hierar-
chical information of topics and cannot grow structurally as more data are ob-
served. In this paper, we propose a semi-supervised hierarchical topic model,
which aims to explore more reasonable topics in the data space by incorporating
some constraints into the modeling process that are extracted automatically. The
new method is denoted as constrained hierarchical Latent Dirichlet Allocation
(constrained-hLDA). We conduct experiments on Sina microblog, and evaluate
the performance in terms of clustering and empirical likelihood. The experimen-
tal results show that constrained-hLDA has a signiﬁcant improvement on the in-
terpretability, and its predictive ability is also better than that of hLDA.

Keywords: Hierarchical Topic Model, Constrained-hLDA, Topic Discovery.

1 Introduction

In the information explosion era, social network not only contains relationships, but
also much unstructured information such as context. Furthermore, how to effectively
dig out latent topics and internal semantic structures from social network is an im-
portant research issue. Early work on microblogs mainly focused on user relationship
and community structure. [1] studied the topological and geographical properties of
Twitter. Others work such as [2] studied user behaviors and geographic growth pat-
terns of Twitter. Only little research on content analysis of microblog was proposed
recently. [3] was mainly based on traditional text mining algorithms. [4] proposed MB-
LDA by overall considering contactor relevance relation and document relevance re-
lation of microblogs. In this paper, we propose a novel probabilistic generative model
based on hLDA, called constrained-hLDA, which focuses on both text content and topic
hierarchy.

V.S. Tseng et al. (Eds.): PAKDD 2014, Part II, LNAI 8444, pp. 608–619, 2014.
c(cid:2) Springer International Publishing Switzerland 2014

Constrained-hLDA for Topic Discovery in Chinese Microblogs

609

Previous work on microblog text was mainly based on LDA. To our best knowledge,
there was little research on the topic hierarchy on microblog text. However, hierarchical
topic modeling is able to obtain the relations between topics. [5] proposed an unsuper-
vised hierarchical topic model, called hierarchical Latent Dirichlet Allocation (hLDA),
to detect automatically new topics in the data space after ﬁxing the level. Based on the
stick-breaking process, [6] proposed the fully nonparametric hLDA without ﬁxing the
level. After that, some modiﬁcations of hLDA were proposed [7–9]. Given a parameter
L indicating the depth of the hierarchy, hLDA makes use of nested Chinese Restau-
rant Process(nCRP) to automatically ﬁnd useful sets of topics and learn to organize the
topics according to a hierarchy in which more abstract topics are near the root of the
hierarchy and more concrete topics are near the leaves. However, the traditional hLDA
is an unsupervised learning which does not incorporate any prior knowledge. In this pa-
per, we attempt to extract some prior knowledge and incorporate them to the sampling
process.

The rest of the paper is organized as follows. Section 2 introduces the previous work
related to this paper. Section 3 describes the hLDA brieﬂy. Section 4 introduces the
novel model constrained-hLDA. The experiment is introduced in Section 5, which is
followed by the conclusion in Section 6.

2 Related Work

There have been many variations of probabilistic topic models, which was ﬁrst in-
troduced by [10]. The probabilistic topic model is based on the idea that documents
are generated by mixtures of topics which is a multinomial distribution over words.
One limitation of Hofmann’s model is that it is not clear how the mixing proportions
for topics in a document are generated. To overcome this limitation, [11] propose La-
tent Dirichlet Allocation(LDA). In LDA, the topic proportion of every document is a
K-dimensional hidden variable randomly drawn from the same Dirichlet distribution,
where K is the number of topics. Thus, generative semantics of LDA are complete,
and LDA is regarded as the most popular approach for building topic models in recent
years[12–16].

LDA is a useful algorithm for topic modeling, but it fails to draw the relationship
between one topic and another and fails to indicate the level of abstract for a topic. To
address this problem, many models have been proposed to build the relations, such as
hierarchical LDA(hLDA) [5, 6], Hierarchical Dirichlet processes(HDP) [17], Pachinko
Allocation Model(PAM) [18] and Hierarchical PAM(HPAM) [19] etc. These models
extend the ”ﬂat” topic models into hierarchical versions for extracting hierarchies of
topics from text collections. [6] proposed the most up-to-date hLDA model, which is
a fully nonparametric model. It simultaneously learns the structure of a topic hierar-
chy and the topics that are contained within that hierarchy. Furthermore, it can also
learn the most appropriate levels and hyper-parameters although it is time-consuming.
In recent years, some modiﬁcations of hLDA has also been proposed. [7] proposed a
supervised hierarchical topic model, called hierarchical Labeled Latent Dirichlet Al-
location(hLLDA), which uses hierarchical labels to automatically build correspond-
ing topic for each label. [8] propose an unsupervised hierarchical topic model, called

610

W. Wang et al.

Semi-Supervised Hierarchical Latent Dirichlet Allocation (SSHLDA), which can not
only make use of the information from the hierarchy of observed labels, but also can
explore new latent topics in the data space. Although our work has some slight resem-
blance with their work, there still exist several important differences:

1. Our constrained-hLDA mainly focuses on the text of microblogs or reviews without

observed labels.

2. The prior knowledge is extracted automatically from the corpus instead of ﬁrst-

hand observation.

3. The constraints are alterable by different parameters.

3 Preliminaries

The nested Chinese restaurant process (nCRP) is a distribution over hierarchical
partitions[5, 6]. It generalizes the Chinese restaurant process (CRP), which is a sin-
gle parameter distribution over partitions of integers. It has been used to represent the
uncertainty over the number of components in a mixture model. The generative process
is as follow:

1. There are N customers entering the restaurant in sequence, which is labeled with

the integers {1, ..., N}.

2. First customer sits at the ﬁrst table.
3. The nth customer sit at:

(a) Table i with probability

ni

γ+n−1, where ni is the number of customers currently

sitting at table i, which has been occupied.

(b) A new table with probability

γ

γ+n−1 .

4. After N customers have sat down, their seating plan describes a partition of N

items.

In the nested CRP, suppose there are an inﬁnite number of inﬁnite-table Chinese restau-
rants in a city. One restaurant is identiﬁed as the root restaurant and its every table has
a card with the name that refers to another restaurant. This structure repeats inﬁnitely
many times, thus, the restaurants in the city are organized into an inﬁnitely branched,
inﬁnitely-deep tree. When a tourist arrives at the city, he selects a table, which is as-
sociated with a restaurant at next level, using the CRP distribution at each level. After
M tourists have visited in this city, the path collection, which they selected, describes a
random subtree of the inﬁnite tree.

Based on identifying documents with the paths generated by the nCRP, the hierar-
chical topic model, which consists of an inﬁnite tree, is deﬁned. Each node in the tree is
associated with a topic, which is a probability distribution across words. Each document
is assumed to be generated by a mixture of topics on a path from the root to a leaf. For
each token in the document, one picks a topic randomly according to the distribution,
and draws a word from the multinomial distribution of that topic. To infer the topic hi-
erarchy, the per-document paths cd and the per-word level allocation to topics in those
paths zd,n must be sampled. Then we will introduce the process brieﬂy.

Constrained-hLDA for Topic Discovery in Chinese Microblogs

611

For the path sampling, the path associated with each document conditioned on all
other paths and the observed words need to be sampled. Assume the depth is ﬁnite and
let T denotes it, the posterior distribution of path cd is as denote:

p(cd|w, c−d, z, η, γ) ∝ p(cd|c−d, γ)p(wd|c, w−d, z, η)

(1)

In Equation 1, two factors inﬂuence the probability that a document belongs to a path.
The ﬁrst factor is the prior on paths implied by the nested CRP. The second factor is
the probability of observing the words in the document given a particular choice of path
with equation organized as follows:
Γ (n(·)
(cid:3)
w Γ (n(w)

(cid:3)
w Γ (n(w)
Γ (n(·)

p(wd|c, w−d, z, η) =

cd,t,−d + V η)
cd,t,−d + η))

cd,t,d + η))
cd,t,d + V η)

T(cid:2)

t=1

cd,t,−d + n(w)

cd,t,−d + n(·)

(2)

where n(w)
cd,t,−d is the number of word w that have been allocated to the topic indexed
by cd,t, not including those in the current document, V denotes the total vocabulary
size, and Γ (·) is the standard gamma function. When c contains a previously unvisited
restaurant, n(w)

cd,t,−d is zero.

After selecting the current path assignments, the level allocation variable zd,n for
word n in document d conditioned on the current values of all other variables need to
be sampled as:

p(zd,n|z−(d,n), c, w, m, π, η) ∝ p(wd,n|z, c, w−(d,n), η)p(zd,n|zd,−n, m, π)

(3)

where z−(d,n) and w−(d,n) are the vectors of level allocations and observed words
leaving out zd,n and wd,n, zd,−n denotes the level allocations in document d , leaving
out zd,n.

4 Constrained-hLDA

In this section, we will introduce a constrained hierarchical topic model, i.e., the con-
strained herarchical Latent Dirichlet Allocation(constrained-hLDA).As we have known,
similar to LDA, the original hLDA is a purely unsupervised model without consider-
ing any pre-existing knowledge. However, in semi-supervised clustering framework, the
prior knowledge can help clustering algorithm produce more meaningful clusters. In our
algorithm, the extracted prior knowledge can help to pre-establish a part of the inﬁnite
tree structure. In this section, we will give an introduction to the constraint extraction
and the proposed constrained-hLDA which can use pre-existing knowledge expressed
as constraints.

4.1 Path Constraints Extraction

To construct constrained hierarchical topic model, we adopt hLDA and incorporate the
constraints from the pre-existing knowledge. Compared with hLDA, constrained-hLDA
has one more input for improving path sampling. The input is a set of constrained indi-
cators, which is in the form of {{w1,1, w1,2, . . .}, . . . ,{wN,1, wN,2, . . .}}. Each subset

W. Wang et al.

612
{wi,1, wi,2, . . .}, which corresponds to a node in constrained-hLDA, consists of several
high correlation words. In our work, these words, which can indicate the correlation of
a path and a document, are called constrained indicators. These corresponding nodes,
which are pre-allocated several constrained indicators, are called constrained nodes.

The intuition of above idea is very simple and easy to follow. In this paper, we just
attempt to solve it based on a correlation approach, more novel and efﬁcient method will
be further explored in the future. Algorithm 1 summarizes the main steps of constraints
extraction. First, the FP-tree algorithm is adopted to extract the one-dimension frequent
items according to the minimum support and maximum support(Line 1). The maximum
support is used to ﬁlter some common words in order to make sure that the occurrences
of each candidate are close, therefore, there will not be hierarchical relationship of these
frequent items. Next, for each f isi, it is added to an empty collection CSi ﬁrst, and then
the correlation of f isi with other items is computed. If the correlation of f isi and f isj
is greater than the given threshold, it is assumed that f isi and f isj should constitute a
must-link and f isj is appended to CSi (Line 2 - Line 9). In this work, the correlation
is calculated by overlap as follows:

overlap(A, B) =

PA&B

min(PA, PB)

(4)

where PA&B is the co-occurrence of word A and word B, PA is the occurrence of word
A, and PB is the occurrence of word B. The range of equation 4 is between [0, 1.0], so
the threshold can be easily given for different corpora. In the end, we delete the same
set only retaining one from CS (Line 10 - Line 14). Based on Algorithm 1, the prior set
CS, each of which contains several high correlation indicators, can be acquired. In this
paper, the threshold of overlap is set as 0.4, the maximum support is set as ﬁve times as
minimum support, all these parameters are estimated number. Additionally, we attempt
to utilize different minimum supports to obtain different set so that different experiment
results can be made for sure.

if correlation(f isi, f isj ) > threshold then

CSi ← f isi
for each f isj|j!=i in F IS do

Algorithm 1. Constraints extraction
1. Frequent Item Set F IS ← FP-tree (D, min sup, max sup)
2. for each f isi in F IS do
3.
4.
5.
6.
7.
8.
9. end for
10. for each CSi in Constraint Set CS do
11.
12.
end if
13.
14. end for

if there exists CSj == CSi then

delete CSi from CS

CSi ← f isj

end if
end for

Constrained-hLDA for Topic Discovery in Chinese Microblogs

613

4.2 Path Constraints Incorporation

To integrate constrained indicators into hLDA, we extend the nCRP to a more realistic
situation. Suppose the root restaurant has inﬁnite tables, some tables have a menu con-
taining some special dishes. Suppose N tourists arrive at the city, some of them have a
list of special dishes that they want to taste. When a tourist enters into the root restau-
rant, if he has a list, he will select a table whose menu contains the special dishes of his
list. Otherwise, according to his willingness to taste the special dishes, he will use CRP
equation to select a table among those tables without menus. To keep it simple in this
paper, we assume only the root restaurant has menus.

In constrained-hLDA model, each constrained set CSi corresponds to a menu and
each constrained indicator corresponds to a special dish. Then the documents in a cor-
pus are assumed drawn from the following generative process:
1. For each table k ∈ T in the inﬁnite tree
2. For each document,d ∈ {1, 2, . . . , D}

(a) Draw a topic βk ∼ Dir(η)

(a) Let c1 be the root node.
(b) For level l = 2:

i. If d contains the constrained indicators {id,1, id,2, . . .}, select a table c2
+γ
+γ) , where nid,i is the number of table which con-

(cid:2)

nid,i
(nid,i

with probability
tains id,i.

ii. Otherwise, draw a table c2 among Cnm,2 from restaurant using CRP, where

Cnm,2 is the set of tables which have no menus on root restaurant.

(c) For each level l ∈ {3, . . . , L}
(d) Draw a distribution over levels in the tree, θd|{m, π} ∼ GEM (m, π)
(e) For each word n ∈ {1, 2, . . . , N}

i. Draw a table cl from restaurant using CRP.

i. Choose level z|θd ∼ Discrete(θd).
ii. Choose word w|{z, c, β} ∼ Discrete(βcz ), which is parameterized by

the topic associated with restaurant cz.

Fig. 1. One illustration of constrained-hLDA. The tree has 3 levels. The shaded nodes are con-
strained topics, which is pre-deﬁned. The circled nodes are latent topics. After learning, each
node in this tree is a topic, which is a corresponding probability distribution over words.

614

W. Wang et al.

As the example shown in Figure 1, we assume that the height of the desired tree is L =
3, and the constrained-topics extracted are {A2, A3}. The constrained topics amount to
the tables containing menu, each of which is pre-deﬁned as the constrained indicators
coming from a CSi, and the constrained indicators amount to special dishes. In our
work, because microblogs are mainly short texts, the maximum level is truncated to 3.
Furthermore, it is notable that the constraints can be extended to the deeper level. For
example, the constrained set can be extracted again from the documents which pass by
the node A2, and then the constrained indicators set corresponding to A2 can be drawn
from these documents.

In constrained-hLDA, the idea of incorporating prior knowledge derives from [20],
and the most important process is incorporating the constraints to the path sampling
process according to the probabilities calculated using Equation 5:

p(cd|w, c−d, z, η, γ) ∝ (η(cid:2)δ(wd, cd) + 1 − η(cid:2))p(cd|cd, γ)p(wd|c, w−d, z, η)

(5)

where δ(wd, cd) is an indicator function, which indicates whether the nodes from cd
contain the same constrained indicator with that of wd: If wd contains such node,
δ(wd, cd) = 1, otherwise, δ(wd, cd) = 0. The hard constraint indicator can be re-
, Let 0 ≤ η(cid:2) ≤ 1 be the strength of our constraint, where η(cid:2) = 1 recovers
laxed by η(cid:2)
a hard constraint, η(cid:2) = 0 recovers unconstrained sampling and 0 < η(cid:2) < 1 recovers a
soft constraint sampling.

4.3 Level Constraints Extraction and Incorporation

After revising the path sampling process and selecting a particular path, some prior
knowledge can also integrate into level sampling process. As we have known, hLDA
can discover the function words in root topic, furthermore, these words have no effect
on the document interpretability and often appear in many documents. Therefore, we
hope to improve level sampling process by pre-discriminating some function words and
non-function words. In our work, the function words are discriminated according to the
Part-Of-Speech(POS) and the term frequency in each document. Algorithm 2 describes
our purpose, where RDw denotes the ratio of the documents containing the word w
in the current corpus. For each word, if its RDw is greater than the given threshold

if RDw > thresholdupper AND P OSw /∈ SP OS then
else if RDw < thresholdbelow AND P OSw ∈ SP OS then

Algorithm 2 Constraints extraction
1. for each wi in current document do
2.
3.
4.
5.
6.
7.
8.
9.
10. end for

samleLevelw ← 0
samleLevelw ← 1, . . . , K
samleLevelw ← 0, . . . , K

else

end if
sample the level according samleLevelw

Constrained-hLDA for Topic Discovery in Chinese Microblogs

615

thresholdupper and it does not belong to the pre-deﬁned POS set SP OS, it would be
likely to be a function word that is allocated to root node directly(Line 2 - Line 3). If its
RDw is less than the given threshold thresholdbelow and it belongs to the pre-deﬁned
POS set SP OS at the same time, it would be likely to be a non-function word without
being allocated to root node(Line 4 - Line 5). Finally, we sample the level according to
these prior knowledge (Line 9). In this paper, thresholdupper and thresholdbelow are
set to 0.02 and 0.005, and the pre-deﬁned POS set SP OS is set as noun, adjective and
verb.

5 Experiment

5.1 Data Sets

Due to the lack of standard data set for this kind of research yet, we collected the
experiment data from sina microblog1 by ourselves. It is generally known that Ya’an
Earthquake2 on 20th, April, 2013 was a catastrophe shocking everyone, which is exactly
an ideal hot issue for research. We crawled 19811 microblog users all coming from
Ya’an, and also crawled their posted microblogs from 8am 20th April 2013 to 8am
25th April 2013. There are 58476 original microblogs released by these users, each
of which contains several sentences. As time passed by, people’s concern level on this
issue would decline gradually, therefore, we use the data on a daily level for further
analysis. Table 1 depicts the data sets for evaluation. The designed experiments and
sampling results can also be referred in [6]. For hLDA and constrained-hLDA, there is
a restriction that documents can only follow a single path in the tree. In order to make
each sentence of a document can follow different paths, we split texts into sentences,
such a change can get a remarkable improvement for hLDA and constrained-hLDA
in the corpus of microblogs. In our experiment, hLDA algorithm is completed with
Java codes by ourselves according to [6]. In our constrained-hLDA, the stick-breaking
procedures are truncated at three levels to facilitate visualization of results. The topic
Dirichlet hyper-parameters are ﬁxed at η = {1.0, 1.0, 1.0} , The nested CRP parameter
γ is ﬁxed at 0.5, the GEM parameters are ﬁxed at π = 100 and m = 0.25.

Table 1. Experiment data

Time

Token Number of microblogs Number of sentences

20-21
21-22
22-23
23-24
24-25

T1
T2
T3
T4
T5

19709
11678
9779
9308
8002

50631
32311
28215
27213
23159

1 http://weibo.com/
2 http://en.wikipedia.org/wiki/2013_Lushan_earthquake

616

W. Wang et al.

5.2 Hierarchy Topic Discovery

Figure 2 depicts the hierarchical structure of cluster results. It is natural to conclude
that the constrained-hLDA can well discover the underlying hierarchical structure of the
content of micorblogs, and each topic and its child node mainly relate to pre-allocated
the constrained indicator, which is the underlined word. For example, there are three
sub-topic of Ya’an, the ﬁrst sub-topic relates to blessing, the second talks about the sit-
uations of Ya’an, the third talks about relief of Ya’an. Furthermore, as we can ﬁnd, the
latent topic of second level is a meaningless topic, which is hard to summarize the inter-
pretability of these topics. This phenomenon illustrates that the irrelevant information
in microblog context that can be ﬁltered well by our algorithm.

5.3 Comparison with hLDA

In this section, we compare the experimental results with hLDA, and the per-document
distribution over levels is truncated at three levels. In order to evaluate our model, we
use predictive held-out likelihood as a measure of performance to compare the two
approaches quantitatively. The procedure is to divide the corpus into D1 observed doc-
uments and D2 held-out documents, and approximate the conditional probability of the
held-out set given the training set:
p(wheld−out

, . . . , wheld−out

|wobs

1 , . . . , wobs
D1

)

1

D2

(6)

For this evaluation method, more details can be found in [6].

Figure 3 depicts the performance of constrained-HLDA on several data sets by dif-
ferent minimum support. Table 2 depicts the best performance of different constraints
on several data sets. According to these experimental results, we can conclude that:
(1) Both path sampling constraints and level sampling constraints can improve hLDA.
(2) The smaller minimum support can obtain more constrained indicators so that it can
achieve better log likelihood. (3) The likelihood of constrained-hLDA is better than the
likelihood of hLDA, but for different corpus, the degree of improvement is different.
When the topic of corpus is more concentrated, the improvement seems to be better.

Table 2. The Best Results of Different Prior Constraints(800 samplers)

Data set token

hLDA

hLDA + level constrains

hLDA + path constraints

constrained-hLDA

T1
T2
T3
T4
T5

-233776.355
-169646.036
-137735.633
-130254.675
-106223.172

-226566.45
-164147.048
-133976.967
-127269.889
-104269.033

-225814.798
-162871.358
-134931.846
-128493.374
-104670.728

-218583.987
-158950.632
-130895.533
-124552.455
-100796.114

In order to avoid interference from the values of hyperparameters, as with [6]’work,
we also interleave Metropolis-Hastings (MH) steps between iterations of the Gibbs
sampler to obtain new values of m,π,γ and η. Table 3 present the results by sampling
the hyperparameters in the same case, from which we can see that constrained-hLDA
still performs better than hLDA.

(cid:17814)(cid:17846)(cid:11)(cid:89)(cid:72)(cid:75)(cid:76)(cid:70)(cid:79)(cid:72)(cid:12)

(cid:17994)(cid:18051)(cid:11)(cid:83)(cid:68)(cid:86)(cid:86)(cid:68)(cid:74)(cid:72)(cid:90)(cid:68)(cid:92)(cid:12)

(cid:6216)(cid:3867)(cid:11)(cid:86)(cid:82)(cid:79)(cid:71)(cid:76)(cid:72)(cid:85)(cid:12)
(cid:6208)(cid:18221)(cid:11)(cid:38)(cid:75)(cid:72)(cid:81)(cid:74)(cid:71)(cid:88)(cid:12)

(cid:19767)(cid:1117)(cid:11)(cid:72)(cid:83)(cid:76)(cid:70)(cid:72)(cid:81)(cid:87)(cid:72)(cid:85)(cid:12)

(cid:7041)(cid:6692)(cid:11)(cid:85)(cid:72)(cid:86)(cid:70)(cid:88)(cid:72)(cid:12)
(cid:19701)(cid:4537)(cid:11)(cid:60)(cid:68)(cid:255)(cid:68)(cid:81)(cid:12)
(cid:9144)(cid:5791)(cid:11)(cid:81)(cid:72)(cid:90)(cid:86)(cid:12)

(cid:5482)(cid:9976)(cid:11)(cid:76)(cid:81)(cid:87)(cid:72)(cid:81)(cid:86)(cid:72)(cid:12)
(cid:1085)(cid:16305)(cid:11)(cid:71)(cid:82)(cid:81)(cid:255)(cid:87)(cid:12)
(cid:2561)(cid:11087)(cid:11)(cid:75)(cid:68)(cid:83)(cid:83)(cid:72)(cid:81)(cid:12)
(cid:8984)(cid:5951)(cid:11)(cid:81)(cid:82)(cid:87)(cid:76)(cid:70)(cid:72)(cid:12)

(cid:1117)(cid:7133)(cid:11)(cid:76)(cid:81)(cid:87)(cid:72)(cid:85)(cid:85)(cid:88)(cid:83)(cid:87)(cid:12)

(cid:17994)(cid:16863)

(cid:11)(cid:70)(cid:82)(cid:80)(cid:80)(cid:88)(cid:81)(cid:76)(cid:70)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:12)

(cid:1944)(cid:18200)(cid:11)(cid:87)(cid:82)(cid:87)(cid:68)(cid:79)(cid:12)

(cid:16864)(cid:13877)(cid:11)(cid:77)(cid:82)(cid:88)(cid:85)(cid:81)(cid:68)(cid:79)(cid:76)(cid:86)(cid:87)(cid:12)

(cid:11665)(cid:16377)(cid:11)(cid:86)(cid:79)(cid:72)(cid:72)(cid:83)(cid:12)

(cid:5877)(cid:2021)(cid:11)(cid:86)(cid:76)(cid:87)(cid:88)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:12)
(cid:1109)(cid:18429)(cid:11)(cid:86)(cid:72)(cid:85)(cid:76)(cid:82)(cid:88)(cid:86)(cid:12)
(cid:2543)(cid:3582)(cid:11)(cid:70)(cid:82)(cid:88)(cid:81)(cid:87)(cid:92)(cid:12)
(cid:1258)(cid:8769)(cid:11)(cid:83)(cid:72)(cid:82)(cid:83)(cid:79)(cid:72)(cid:12)

(cid:3424)(cid:19767)(cid:11)(cid:72)(cid:68)(cid:85)(cid:87)(cid:75)(cid:84)(cid:88)(cid:68)(cid:78)(cid:72)(cid:12)

(cid:1258)(cid:8769)(cid:11)(cid:83)(cid:72)(cid:82)(cid:83)(cid:79)(cid:72)(cid:12)
(cid:14550)(cid:4769)(cid:11)(cid:47)(cid:88)(cid:86)(cid:75)(cid:68)(cid:81)(cid:12)
(cid:3339)(cid:5133)(cid:11)(cid:54)(cid:76)(cid:70)(cid:75)(cid:88)(cid:68)(cid:81)(cid:12)

(cid:19701)(cid:4537)(cid:11)(cid:60)(cid:68)(cid:255)(cid:68)(cid:81)(cid:12)
(cid:3339)(cid:5133)(cid:11)(cid:54)(cid:76)(cid:70)(cid:75)(cid:88)(cid:68)(cid:81)(cid:12)
(cid:2561)(cid:11087)(cid:11)(cid:75)(cid:68)(cid:83)(cid:83)(cid:72)(cid:81)(cid:12)
(cid:14550)(cid:4769)(cid:11)(cid:47)(cid:88)(cid:86)(cid:75)(cid:68)(cid:81)(cid:12)

(cid:19701)(cid:4537)(cid:11)(cid:60)(cid:68)(cid:255)(cid:68)(cid:81)(cid:12)
(cid:9902)(cid:2410)(cid:11)(cid:71)(cid:76)(cid:86)(cid:68)(cid:86)(cid:87)(cid:72)(cid:85)(cid:3)

(cid:68)(cid:85)(cid:72)(cid:68)(cid:12)

(cid:1258)(cid:2696)(cid:11)(cid:86)(cid:87)(cid:68)(cid:73)(cid:73)(cid:12)
(cid:11087)(cid:2733)(cid:11)(cid:79)(cid:76)(cid:73)(cid:72)(cid:12)

(cid:5598)(cid:2442)(cid:11)(cid:80)(cid:76)(cid:70)(cid:85)(cid:82)(cid:69)(cid:79)(cid:82)(cid:74)(cid:12)

(cid:17929)(cid:18428)(cid:11)(cid:75)(cid:72)(cid:85)(cid:72)(cid:12)
(cid:17929)(cid:1114)(cid:11)(cid:87)(cid:75)(cid:76)(cid:86)(cid:12)

(cid:14362)(cid:5153)(cid:11)(cid:82)(cid:81)(cid:72)(cid:86)(cid:72)(cid:79)(cid:73)(cid:12)
(cid:1085)(cid:16305)(cid:11)(cid:71)(cid:82)(cid:81)(cid:255)(cid:87)(cid:12)

(cid:5180)(cid:7499)(cid:11)(cid:75)(cid:82)(cid:83)(cid:72)(cid:12)
(cid:19701)(cid:4537)(cid:11)(cid:60)(cid:68)(cid:255)(cid:68)(cid:81)(cid:12)
(cid:4582)(cid:1258)(cid:11)(cid:73)(cid:82)(cid:79)(cid:78)(cid:86)(cid:12)
(cid:12152)(cid:12199)(cid:11)(cid:83)(cid:85)(cid:68)(cid:92)(cid:12)

(cid:1109)(cid:18429)(cid:11)(cid:86)(cid:72)(cid:85)(cid:76)(cid:82)(cid:88)(cid:86)(cid:12)

(cid:3424)(cid:19767)(cid:11)(cid:72)(cid:68)(cid:85)(cid:87)(cid:75)(cid:84)(cid:88)(cid:68)(cid:78)(cid:72)(cid:12)

(cid:2543)(cid:3582)(cid:11)(cid:70)(cid:82)(cid:88)(cid:81)(cid:87)(cid:92)(cid:12)
(cid:19701)(cid:4537)(cid:11)(cid:60)(cid:68)(cid:255)(cid:68)(cid:81)(cid:12)

(cid:3424)(cid:19767)(cid:11)(cid:72)(cid:68)(cid:85)(cid:87)(cid:75)(cid:84)(cid:88)(cid:68)(cid:78)(cid:72)(cid:12)

(cid:2122)(cid:2122)(cid:11)(cid:77)(cid:88)(cid:86)(cid:87)(cid:12)
(cid:19701)(cid:4537)(cid:11)(cid:60)(cid:68)(cid:255)(cid:68)(cid:81)(cid:12)
(cid:7306)(cid:1082)(cid:11)(cid:72)(cid:89)(cid:72)(cid:81)(cid:76)(cid:81)(cid:74)(cid:12)

(cid:14550)(cid:4769)(cid:11)(cid:47)(cid:88)(cid:86)(cid:75)(cid:68)(cid:81)(cid:12)
(cid:9144)(cid:5791)(cid:11)(cid:81)(cid:72)(cid:90)(cid:86)(cid:12)
(cid:19701)(cid:4537)(cid:11)(cid:60)(cid:68)(cid:255)(cid:68)(cid:81)(cid:12)

(cid:1955)(cid:8984)(cid:11)(cid:68)(cid:87)(cid:87)(cid:72)(cid:81)(cid:87)(cid:76)(cid:82)(cid:81)(cid:12)

(cid:11444)(cid:11)(cid:82)(cid:73)(cid:12)
(cid:6209)(cid:11)(cid:44)(cid:15)(cid:3)(cid:80)(cid:72)(cid:12)

(cid:3416)(cid:11)(cid:68)(cid:87)(cid:12)

(cid:1258)(cid:11)(cid:83)(cid:72)(cid:85)(cid:86)(cid:82)(cid:81)(cid:12)

(cid:7263)(cid:11)(cid:76)(cid:86)(cid:12)

(cid:12152)(cid:12199)(cid:11)(cid:83)(cid:85)(cid:68)(cid:92)(cid:12)
(cid:16978)(cid:16978)(cid:11)(cid:87)(cid:75)(cid:68)(cid:81)(cid:78)(cid:86)(cid:12)
(cid:4582)(cid:1258)(cid:11)(cid:73)(cid:82)(cid:79)(cid:78)(cid:86)(cid:12)
(cid:7483)(cid:2555)(cid:11)(cid:73)(cid:85)(cid:76)(cid:72)(cid:81)(cid:71)(cid:86)(cid:12)

(cid:4557)(cid:1956)(cid:2543)(cid:11)(cid:37)(cid:68)(cid:82)(cid:91)(cid:76)(cid:81)(cid:74)(cid:3)

(cid:38)(cid:82)(cid:88)(cid:81)(cid:87)(cid:92)(cid:12)

(cid:19701)(cid:4537)(cid:11)(cid:60)(cid:68)(cid:255)(cid:68)(cid:81)(cid:12)
(cid:16978)(cid:16978)(cid:11)(cid:87)(cid:75)(cid:68)(cid:81)(cid:78)(cid:86)(cid:12)
(cid:6389)(cid:5619)(cid:11)(cid:90)(cid:82)(cid:85)(cid:85)(cid:92)(cid:12)

(cid:6208)(cid:18221)(cid:11)(cid:38)(cid:75)(cid:72)(cid:81)(cid:74)(cid:71)(cid:88)(cid:12)
(cid:1258)(cid:8769)(cid:11)(cid:83)(cid:72)(cid:82)(cid:83)(cid:79)(cid:72)(cid:12)
(cid:7041)(cid:6692)(cid:11)(cid:85)(cid:72)(cid:86)(cid:70)(cid:88)(cid:72)(cid:12)

(cid:19701)(cid:4537)(cid:11)(cid:60)(cid:68)(cid:255)(cid:68)(cid:81)(cid:12)
(cid:13956)(cid:13099)(cid:11)(cid:70)(cid:82)(cid:81)(cid:87)(cid:68)(cid:70)(cid:87)(cid:12)
(cid:3339)(cid:5133)(cid:11)(cid:54)(cid:76)(cid:70)(cid:75)(cid:88)(cid:68)(cid:81)(cid:12)

(cid:3424)(cid:19767)(cid:11)(cid:72)(cid:68)(cid:85)(cid:87)(cid:75)(cid:84)(cid:88)(cid:68)(cid:78)(cid:72)(cid:12)

(cid:19701)(cid:4537)(cid:11)(cid:60)(cid:68)(cid:255)(cid:68)(cid:81)(cid:12)
(cid:14362)(cid:5153)(cid:11)(cid:82)(cid:81)(cid:72)(cid:86)(cid:72)(cid:79)(cid:73)(cid:12)
(cid:3339)(cid:5133)(cid:11)(cid:54)(cid:76)(cid:70)(cid:75)(cid:88)(cid:68)(cid:81)(cid:12)
(cid:9902)(cid:2410)(cid:11)(cid:71)(cid:76)(cid:86)(cid:68)(cid:86)(cid:87)(cid:72)(cid:85)(cid:3)

(cid:68)(cid:85)(cid:72)(cid:68)(cid:12)

(cid:8619)(cid:1233)(cid:11)(cid:71)(cid:72)(cid:68)(cid:87)(cid:75)(cid:12)
(cid:7472)(cid:7136)(cid:11)(cid:79)(cid:68)(cid:87)(cid:72)(cid:86)(cid:87)(cid:12)
(cid:4557)(cid:1956)(cid:2543)(cid:11)(cid:37)(cid:68)(cid:82)(cid:91)(cid:76)(cid:81)(cid:74)(cid:3)

(cid:38)(cid:82)(cid:88)(cid:81)(cid:87)(cid:92)(cid:12)

(cid:14550)(cid:4769)(cid:2543)(cid:11)(cid:47)(cid:88)(cid:86)(cid:75)(cid:68)(cid:81)(cid:3)

(cid:38)(cid:82)(cid:88)(cid:81)(cid:87)(cid:92)(cid:12)

(cid:10393)(cid:17268)(cid:11)(cid:74)(cid:82)(cid:82)(cid:71)(cid:86)(cid:12)
(cid:10720)(cid:3434)(cid:11)(cid:86)(cid:70)(cid:72)(cid:81)(cid:72)(cid:86)(cid:12)
(cid:5184)(cid:12839)(cid:11)(cid:87)(cid:72)(cid:81)(cid:87)(cid:12)
(cid:19701)(cid:4537)(cid:11)(cid:60)(cid:68)(cid:10)(cid:68)(cid:81)(cid:12)

(cid:4557)(cid:1956)(cid:11)(cid:37)(cid:68)(cid:82)(cid:91)(cid:76)(cid:81)(cid:74)(cid:12)
(cid:7483)(cid:2555)(cid:11)(cid:73)(cid:85)(cid:76)(cid:72)(cid:81)(cid:71)(cid:86)(cid:12)

(cid:9144)(cid:5791)(cid:11)(cid:81)(cid:72)(cid:90)(cid:86)(cid:12)
(cid:4582)(cid:1258)(cid:11)(cid:73)(cid:82)(cid:79)(cid:78)(cid:86)(cid:12)

(cid:17820)(cid:2561)(cid:11)(cid:85)(cid:72)(cid:83)(cid:82)(cid:86)(cid:87)(cid:12)

(cid:2612)(cid:1405)(cid:11)(cid:72)(cid:89)(cid:72)(cid:85)(cid:92)(cid:69)(cid:82)(cid:71)(cid:92)(cid:12)

(cid:5214)(cid:2265)(cid:11)(cid:75)(cid:72)(cid:79)(cid:83)(cid:12)

(cid:1553)(cid:5791)

(cid:11)(cid:76)(cid:81)(cid:73)(cid:82)(cid:85)(cid:80)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:12)
(cid:5598)(cid:2442)(cid:11)(cid:80)(cid:76)(cid:70)(cid:85)(cid:82)(cid:69)(cid:79)(cid:82)(cid:74)(cid:12)

(cid:19701)(cid:4537)(cid:11)(cid:60)(cid:68)(cid:255)(cid:68)(cid:81)(cid:12)
(cid:9902)(cid:2410)(cid:11)(cid:71)(cid:76)(cid:86)(cid:68)(cid:86)(cid:87)(cid:72)(cid:85)(cid:3)

(cid:68)(cid:85)(cid:72)(cid:68)(cid:12)

(cid:8619)(cid:1233)(cid:11)(cid:71)(cid:72)(cid:68)(cid:87)(cid:75)(cid:12)

(cid:3424)(cid:19767)(cid:11)(cid:72)(cid:68)(cid:85)(cid:87)(cid:75)(cid:84)(cid:88)(cid:68)(cid:78)(cid:72)(cid:12)
(cid:5316)(cid:5717)(cid:11)(cid:72)(cid:80)(cid:72)(cid:85)(cid:74)(cid:72)(cid:81)(cid:70)(cid:92)(cid:12)

(cid:7041)(cid:9902)(cid:708)(cid:85)(cid:72)(cid:79)(cid:76)(cid:72)(cid:73)(cid:709)
(cid:19760)(cid:16305)(cid:708)(cid:81)(cid:72)(cid:72)(cid:71)(cid:709)

(cid:12152)(cid:12223)(cid:708)(cid:69)(cid:79)(cid:72)(cid:86)(cid:86)(cid:76)(cid:81)(cid:74)(cid:709)

(cid:2173)(cid:5552)(cid:708)(cid:74)(cid:82)(cid:3)(cid:87)(cid:82)(cid:709)

Constrained-hLDA for Topic Discovery in Chinese Microblogs

617

(cid:2411)(cid:19602)(cid:11)(cid:75)(cid:82)(cid:86)(cid:83)(cid:76)(cid:87)(cid:68)(cid:79)(cid:12)
(cid:4557)(cid:1956)(cid:2543)(cid:11)(cid:37)(cid:68)(cid:82)(cid:91)(cid:76)(cid:81)(cid:74)(cid:3)

(cid:38)(cid:82)(cid:88)(cid:81)(cid:87)(cid:92)(cid:12)

(cid:1109)(cid:18429)(cid:11)(cid:86)(cid:72)(cid:85)(cid:76)(cid:82)(cid:88)(cid:86)(cid:12)
(cid:20744)(cid:17999)(cid:11)(cid:75)(cid:76)(cid:74)(cid:75)(cid:90)(cid:68)(cid:92)(cid:12)
(cid:9902)(cid:5877)(cid:11)(cid:71)(cid:76)(cid:86)(cid:68)(cid:86)(cid:87)(cid:72)(cid:85)(cid:12)

(cid:5184)(cid:12839)(cid:11)(cid:87)(cid:72)(cid:81)(cid:87)(cid:12)
(cid:10393)(cid:17268)(cid:11)(cid:74)(cid:82)(cid:82)(cid:71)(cid:86)(cid:12)
(cid:1274)(cid:7306)(cid:11)(cid:87)(cid:82)(cid:81)(cid:76)(cid:74)(cid:75)(cid:87)(cid:12)
(cid:2567)(cid:9902)(cid:11)(cid:68)(cid:73)(cid:73)(cid:72)(cid:70)(cid:87)(cid:72)(cid:71)(cid:12)
(cid:7041)(cid:9902)(cid:11)(cid:85)(cid:72)(cid:79)(cid:76)(cid:72)(cid:73)(cid:12)

(cid:2256)(cid:8937)(cid:11)(cid:70)(cid:75)(cid:72)(cid:72)(cid:85)(cid:12)

(cid:12152)(cid:12223)(cid:11)(cid:69)(cid:79)(cid:72)(cid:86)(cid:86)(cid:76)(cid:81)(cid:74)(cid:12)
(cid:5283)(cid:4537)(cid:11)(cid:86)(cid:68)(cid:73)(cid:72)(cid:87)(cid:92)(cid:12)
(cid:1072)(cid:17319)(cid:11)(cid:87)(cid:82)(cid:74)(cid:72)(cid:87)(cid:75)(cid:72)(cid:85)(cid:12)

(cid:19701)(cid:4537)(cid:5170)(cid:11)(cid:60)(cid:68)(cid:10)(cid:68)(cid:81)(cid:3)(cid:38)(cid:76)(cid:87)(cid:92)(cid:12)

(cid:14550)(cid:4769)(cid:2543)(cid:11)(cid:47)(cid:88)(cid:86)(cid:75)(cid:68)(cid:81)(cid:3)

(cid:38)(cid:82)(cid:88)(cid:81)(cid:87)(cid:92)(cid:12)

(cid:3339)(cid:5133)(cid:11569)(cid:11)(cid:54)(cid:76)(cid:70)(cid:75)(cid:88)(cid:68)(cid:81)(cid:3)

(cid:51)(cid:85)(cid:82)(cid:89)(cid:76)(cid:81)(cid:70)(cid:72)(cid:12)

(cid:2561)(cid:11087)(cid:11)(cid:75)(cid:68)(cid:83)(cid:83)(cid:72)(cid:81)(cid:12)

(cid:26)(cid:17)(cid:19)(cid:11)(cid:26)(cid:17)(cid:19)(cid:16)(cid:80)(cid:68)(cid:74)(cid:81)(cid:76)(cid:87)(cid:88)(cid:71)(cid:72)(cid:12)

(cid:8870)(cid:5133)(cid:11)(cid:58)(cid:72)(cid:81)(cid:70)(cid:75)(cid:88)(cid:68)(cid:81)(cid:12)

(cid:7206)(cid:1609)(cid:11)(cid:87)(cid:76)(cid:80)(cid:72)(cid:12)
(cid:12636)(cid:1072)(cid:11)(cid:73)(cid:76)(cid:85)(cid:86)(cid:87)(cid:12)
(cid:1072)(cid:1114)(cid:11)(cid:82)(cid:81)(cid:72)(cid:12)

(cid:1072)(cid:1114)(cid:11)(cid:82)(cid:81)(cid:72)(cid:12)

(cid:16978)(cid:16978)(cid:11)(cid:87)(cid:75)(cid:68)(cid:81)(cid:78)(cid:86)(cid:12)
(cid:7306)(cid:1082)(cid:11)(cid:72)(cid:89)(cid:72)(cid:81)(cid:76)(cid:81)(cid:74)(cid:12)
(cid:5967)(cid:16978)(cid:11)(cid:87)(cid:75)(cid:68)(cid:81)(cid:78)(cid:12)
(cid:1955)(cid:5619)(cid:11)(cid:70)(cid:68)(cid:85)(cid:72)(cid:12)

(cid:11805)(cid:1553)(cid:11)(cid:80)(cid:72)(cid:86)(cid:86)(cid:68)(cid:74)(cid:72)(cid:12)
(cid:13956)(cid:13099)(cid:11)(cid:70)(cid:82)(cid:81)(cid:87)(cid:68)(cid:70)(cid:87)(cid:12)
(cid:6389)(cid:5619)(cid:11)(cid:90)(cid:82)(cid:85)(cid:85)(cid:92)(cid:12)

(cid:17994)(cid:16863)

(cid:11)(cid:70)(cid:82)(cid:80)(cid:80)(cid:88)(cid:81)(cid:76)(cid:70)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:12)

(cid:5283)(cid:4537)(cid:11)(cid:86)(cid:68)(cid:73)(cid:72)(cid:87)(cid:92)(cid:12)

(cid:10393)(cid:17268)(cid:11)(cid:74)(cid:82)(cid:82)(cid:71)(cid:86)(cid:12)
(cid:19701)(cid:4537)(cid:11)(cid:60)(cid:68)(cid:255)(cid:68)(cid:81)(cid:12)

(cid:6343)(cid:19767)(cid:7041)(cid:9902)

(cid:11)(cid:72)(cid:68)(cid:85)(cid:87)(cid:75)(cid:84)(cid:88)(cid:68)(cid:78)(cid:72)(cid:3)
(cid:85)(cid:72)(cid:79)(cid:76)(cid:72)(cid:73)(cid:3)(cid:90)(cid:82)(cid:85)(cid:78)(cid:12)

(cid:5316)(cid:5717)(cid:11)(cid:72)(cid:80)(cid:72)(cid:85)(cid:74)(cid:72)(cid:81)(cid:70)(cid:92)(cid:12)
(cid:2411)(cid:19602)(cid:11)(cid:75)(cid:82)(cid:86)(cid:83)(cid:76)(cid:87)(cid:68)(cid:79)(cid:12)

(cid:4537)(cid:4013)(cid:11)(cid:90)(cid:72)(cid:79)(cid:79)(cid:12)
(cid:14550)(cid:4769)(cid:11)(cid:47)(cid:88)(cid:86)(cid:75)(cid:68)(cid:81)(cid:12)
(cid:1286)(cid:1308)(cid:11)(cid:87)(cid:75)(cid:72)(cid:92)(cid:12)
(cid:2122)(cid:2122)(cid:11)(cid:77)(cid:88)(cid:86)(cid:87)(cid:12)

(cid:7472)(cid:7136)(cid:11)(cid:79)(cid:68)(cid:87)(cid:72)(cid:86)(cid:87)(cid:12)
(cid:4557)(cid:1956)(cid:2543)(cid:11)(cid:37)(cid:68)(cid:82)(cid:91)(cid:76)(cid:81)(cid:74)(cid:3)

(cid:38)(cid:82)(cid:88)(cid:81)(cid:87)(cid:92)(cid:12)

(cid:19701)(cid:4537)(cid:3424)(cid:19767)(cid:11)(cid:60)(cid:68)(cid:255)(cid:68)(cid:81)(cid:3)

(cid:72)(cid:68)(cid:85)(cid:87)(cid:75)(cid:84)(cid:88)(cid:68)(cid:78)(cid:72)(cid:12)
(cid:2543)(cid:3582)(cid:11)(cid:70)(cid:82)(cid:88)(cid:81)(cid:87)(cid:92)(cid:12)

Fig. 2. A portion of the hierarchy learned from T1 Data. The shaded nodes are constrained topics,
the bold and underlining words are the constrained indicators extracted by Algorithm 1.

Table 3. The Results by sampling the hyperparameters (800 samplers)

Data set token

hLDA

constrained-hLDA

T1
T2
T3
T4
T5

-210794.652
-151989.286
-123816.852
-117760.271
-94291.395

-195034.999
-140812.547
-115215.844
-110516.351
-90791.294

618

W. Wang et al.

Fig. 3. The Results of Constrained-hLDA

6 Conclusions

This paper improves the popular topic modeling method hLDA by considering existing
knowledge in the form of path sampling constraints and level sampling constraints. In
the experiment, the proposed constrained-hLDA outperforms hLDA by a large margin,
showing that constraints as prior knowledge can help unsupervised topic modeling.
Moreover, this paper also proposes the extraction method for two types of constraints
automatically. Experimental results show that their qualities are relatively higher than
that of unsupervised one.

Acknowledgments. This work is supported by National Natural Science Foundation
of China (Grant No: 61175110) and National Basic Research Program of China (973
Program, Grant No: 2012CB316305).

References

1. Java, A., Song, X., Finin, T., Tseng, B.: Why we twitter: understanding microblogging usage
and communities. In: Proceedings of the 9th WebKDD and 1st SNA-KDD 2007 Workshop
on Web Mining and Social Network Analysis, pp. 56–65. ACM (2007)

2. Krishnamurthy, B., Gill, P., Arlitt, M.: A few chirps about twitter. In: Proceedings of the First

Workshop on Online Social Networks, pp. 19–24. ACM (2008)

3. Ramage, D., Dumais, S., Liebling, D.: Characterizing microblogs with topic models. In:

International AAAI Conference on Weblogs and Social Media, vol. 5, pp. 130–137 (2010)

4. Zhang, C., Sun, J.: Large scale microblog mining using distributed mb-lda. In: Proceedings
of the 21st International Conference Companion on World Wide Web, pp. 1035–1042. ACM
(2012)

5. Blei, D.M., Grifﬁths, T.L., Jordan, M.I., Tenenbaum, J.B.: Hierarchical topic models and the

nested chinese restaurant process. In: NIPS (2003)

6. Blei, D.M., Grifﬁths, T.L., Jordan, M.I.: The nested chinese restaurant process and bayesian

nonparametric inference of topic hierarchies. Journal of the ACM 57(2), 7 (2010)

Constrained-hLDA for Topic Discovery in Chinese Microblogs

619

7. Petinot, Y., McKeown, K., Thadani, K.: A hierarchical model of web summaries. In: Proceed-
ings of the 49th Annual Meeting of the Association for Computational Linguistics: Human
Language Technologies, pp. 670–675 (2011)

8. Mao, X.L., Ming, Z.Y., Chua, T.S., Li, S., Yan, H., Li, X.: Sshlda: a semi-supervised hierar-
chical topic model. In: Proceedings of the 2012 Joint Conference on Empirical Methods in
Natural Language Processing and Computational Natural Language Learning, pp. 800–809.
Association for Computational Linguistics (2012)

9. Mao, X.L., He, J., Yan, H., Li, X.: Hierarchical topic integration through semi-supervised
hierarchical topic modeling. In: Proceedings of the 21st ACM International Conference on
Information and Knowledge Management, pp. 1612–1616. ACM (2012)

10. Hofmann, T.: Probabilistic latent semantic analysis. In: Proceedings of the Fifteenth Confer-
ence on Uncertainty in Artiﬁcial Intelligence, pp. 289–296. Morgan Kaufmann Publishers
Inc. (1999)

11. Blei, D.M., Ng, A.Y., Jordan, M.I.: Latent dirichlet allocation. The Journal of Machine

Learning Research 3, 993–1022 (2003)

12. Chemudugunta, C., Steyvers, P.S.M.: Modeling general and speciﬁc aspects of documents
with a probabilistic topic model. In: Advances in Neural Information Processing Systems
19: Proceedings of the 2006 Conference, vol. 19, p. 241. The MIT Press (2007)

13. Grifﬁths, T.L., Steyvers, M.: Finding scientiﬁc topics. Proceedings of the National Academy

of Sciences of the United States of America 101(suppl. 1), 5228–5235 (2004)

14. Boyd-Graber, J., Blei, D., Zhu, X.: A topic model for word sense disambiguation. In: Pro-
ceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learning (EMNLP-CoNLL), pp. 1024–1033
(2007)

15. Rosen-Zvi, M., Grifﬁths, T., Steyvers, M., Smyth, P.: The author-topic model for authors and
documents. In: Proceedings of the 20th Conference on Uncertainty in Artiﬁcial Intelligence,
pp. 487–494. AUAI Press (2004)

16. Mei, Q., Ling, X., Wondra, M., Su, H., Zhai, C.: Topic sentiment mixture: modeling facets
and opinions in weblogs. In: Proceedings of the 16th International Conference on World
Wide Web, pp. 171–180. ACM (2007)

17. Teh, Y.W., Jordan, M.I., Beal, M.J., Blei, D.M.: Hierarchical dirichlet processes. Journal of

the American Statistical Association 101(476) (2006)

18. Li, W., McCallum, A.: Pachinko allocation: Dag-structured mixture models of topic cor-
relations. In: Proceedings of the 23rd International Conference on Machine Learning, pp.
577–584. ACM (2006)

19. Mimno, D., Li, W., McCallum, A.: Mixtures of hierarchical topics with pachinko allocation.
In: Proceedings of the 24th International Conference on Machine Learning, pp. 633–640.
ACM (2007)

20. Andrzejewski, D., Zhu, X.: Latent dirichlet allocation with topic-in-set knowledge. In: Pro-
ceedings of the NAACL HLT 2009 Workshop on Semi-Supervised Learning for Natural
Language Processing, pp. 43–48. Association for Computational Linguistics (2009)


