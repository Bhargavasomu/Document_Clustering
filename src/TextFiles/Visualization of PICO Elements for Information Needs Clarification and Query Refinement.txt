 

Visualization of PICO Elements for Information Needs 

Clarification and Query Refinement 

Wan-Tze Vong and Patrick Hang Hui Then 

Faculty of Engineering, Computing and Science, Swinburne University of Technology,   

Sarawak Campus, Jalan Simpang Tiga, 93350 Kuching, Sarawak, Malaysia 

{wvong,pthen}@swinburne.edu.my 

Abstract.  The  UMLS  semantic  types  and  natural  language  processing  tech-
niques  were  collectively  utilized  to  extract  PICO  elements  from  the  titles  and 
abstracts of 114 MEDLINE articles. 24 sets of PICO elements were generated 
from the articles based on the derivation of, and the tokenization methods and 
weighting schemes applied to the elements. The similarity of the I and C ele-
ments (called jointly the “Interventions”) between pairs of documents was cal-
culated  using  42  similarity/distance  measures.  Similar  interventions  were 
grouped  together  using  complete-/average-/ward-link  hierarchical  clustering. 
The  similarity  measure,  Yule,  performed  significantly  better  than  other  meas-
ures  in  identifying  paired  interventions  derived  from  the  titles  and  which  had 
been  pre-processed  into  single  term  and  weighted  by  binary  term-occurrence. 
The clustering algorithm, complete-link, provides the most appropriate structure 
for the visualization of interventions. Similarity-based clustering gave a higher 
mean  average  precision  than  random-baseline  clustering  (MAP  =  0.4298  vs. 
0.2364) over the 25 queries evaluated.   

Keywords:  Hierarchical  Clustering, PICO  Element,  Query  Refinement,  Simi-
larity Measure, Distance Measure. 

1 

Introduction 

A focused, well-defined question warrants a high quality answer. The quality of an-
swers returned by a question-answering (QA) system depends on the quality of ques-
tions posed by users. Doctors have difficulty in generating high quality questions that 
unambiguously and comprehensively defined their information needs [1]. The use of 
PICO (an acronym for Problem/Population, Intervention, Comparison and Outcome) 
framework has been widely accepted for the formulation of answerable clinical ques-
tions.  However,  a  study  by  [2]  reported  that  not  all  clinical  questions  have  all  four 
PICO  elements  present.  Two  examples  of  questions  maintained  by  the  National  Li-
brary  of  Medicine  (NLM)  [3]  are  “What  is  the  best  treatment  for  external  otitis?” 
(Question 1) and “I have a lady with graves’ disease (33 years old). She was trying to 
get pregnant when she was diagnosed with graves. So the question is, what is the best 
treatment for graves in someone who is trying to get pregnant, and if we use radioac-
tive iodine, how long does she need to wait?” (Question 2). Both of the questions are 

V.S. Tseng et al. (Eds.): PAKDD 2014, Part II, LNAI 8444, pp. 360–372, 2014. 
© Springer International Publishing Switzerland 2014 

 

Visualization of PICO Elements for Information Needs Clarification 

361 

categorized  under  “Treatment  and  Prevention”.  Question  1  represents  a  definitional 
question that contains only the P element (“external otitis”). Question 2 is described 
in  paragraph  format  and  contains  both  the  P  (“graves’  disease”,  “lady”)  and  I  (“ra-
dioactive iodine”) elements. An alternative intervention to the clinical condition and 
the expected treatment outcome, which denote the C and O elements respectively, are 
not stated in both Questions 1 and 2. As reported in [4], questions with the I/C and O 
elements  are  unlikely  to  go  unanswered.  Therefore,  the  visualization  of  PICO  ele-
ments in documents relevant to a user’s input query has the potential to assist the user 
in refining his/her information needs.   

The paper presents a case study of utilizing  similarity-based clustering to aid the 
visualization  and  exploration  of  interventions  (i.e.  the  I  and  C  elements)  for  the  re-
finement of questions relating to treatments and drugs. The proposed user interface is 
illustrated in Fig. 1. As shown in the figure, the natural language (NL) question en-
tered by the user contains only the P element (“breast cancer”). To assist the user in 
refining  or  clarifying  his/her  information  needs,  the  user  is  allowed  to  explore  a   
particular subject domain by browsing through the interventions which have been pre-
clustered into a hierarchical structure. Simultaneously, the user can identify the inter-
ventions  encompassed  in  each  cluster  and  discover  the  relationships  between  the   
interventions.  The  most  potent  sets  of  PICO  queries  are  produced  by  returning  the 
interventions  selected  by  the  user  (circled  by  black  line  in  Fig.  1),  accompanied  by   
the P and O elements identified from the titles or abstracts. It is expected that through 
this process, a user can understand his/her information needs and obtain a more com-
prehensive knowledge about the domain of interest. An ambiguous query can also be 
refined by selecting the PICO query that best described the information needs. 

Fig. 1. The proposed user interface 

 

2  Methodology 

2.1  Collection of MEDLINE Documents 

The  processing  of  the  NL  question  in  Fig.  1  as  described  in  Section  2.3  returns  the 
medical concepts: “breast cancer” and “breast neoplasms”. The concepts were used as 
the main search terms and the following filters were activated to retrieve relevant doc-
uments from the MEDLINE database: randomized controlled trial, abstract available, 

 

362 

W.-T. Vong and P.H. Hui Then 

publication date from 2002/10/01 to 2012/10/04, humans and English. The documents 
were  limited  to those published in  7  core journals:  N  Engl  J  Med,  JAMA,  Ann  Inter 
Med, Lancet, Br Med J, BMJ and BMJ (Clin Res Ed). The titles and abstracts of the 
documents were collected for the extraction of PICO elements.   

2.2  Generation of PICO Sentences   

Based on previous studies, the position of a sentence  within an abstract is useful in 
determining the PICO elements that the sentence carries [5,6]. Two types of abstracts 
were  identified:  structured  abstracts  with 
internal  section  headings  such  as 
METHODS  and  RESULTS,  and  unstructured  abstracts  written  in  paragraph  format 
without the headings. Both structured and unstructured abstracts  were cut into three 
segments respectively based on the headings and the position of the sentences in the 
abstracts (Table 1). The extracted sentences were called in the remainder of this paper 
the “PICO sentences”. 

Table 1. Derivation of PICO sentences 

Representation 
P 
I/C 

Internal Section Heading 
Introduction, Background, Objective 
Method 

O 

Result, Conclusion 

Position of Sentence 
First 3 sentences 
Sentences in between the first and the   
last 3 sentences 
Last 3 sentences 

2.3  Generation of PICO Elements 

NL  questions,  titles  and  PICO  sentences  were  processed  by  the  MetaMap  Transfer 
(MMTx)  program  [7]  to  semantically  identify  medical  concepts  as  PICO  elements. 
The program tokenizes the questions, titles and sentences into phrases, and returns a 
list of best matching concept candidates together with their associated semantic types 
from  the  Unified  Medical  Language  System  (UMLS)  Metathesaurus.  Each  of  the 
candidates was labeled with a concept unique identifier (CUI) number.   

The concept candidates were post-processed using Rapidminer 5.2 [8] to identify 
the best matching candidates. Candidates with semantic types listed in Table 2 were 
recognized as PICO elements whereas those with other semantic types were deleted. 
Duplicate  terms,  synonyms  and  stopwords  were  removed  by  identifying  their  CUI 
numbers.  For  instance,  “blood  sugar”  and  “blood  glucose”  are  synonyms  with  the 
same  CUI  number  (i.e.  C0005802).  Examples  of  stopwords  are  “find”,  “release”, 
“peer  support”,  “still”,  “little”  and  “inform”.  If  candidate  terms  of  different  lengths 
were identified at the same location in a document, candidates with the highest num-
ber of words were selected. For example, the processing of the phrase “management 
of orbital cellulitis” returns the concept candidates “orbital”, “cellulitis” and “orbital 
cellulitis”.  “Orbital  cellulitis”  is  selected  and  the  rest  are  removed.  For  each  docu-
ment,  a  list  of  best  matching  medical  concepts  was  collected  respectively  from  the 
titles and the abstracts as PICO elements.   

 

 

Visualization of PICO Elements for Information Needs Clarification 

363 

Table 2. Identification of PICO elements by semantic types (adapted from [6]) 

Representation 
P/O 

I/C 

Semantic Type 
Age  group,  Family  group,  Group,  Human,  Patient  or  disabled  group,  Population 
group, Acquired abnormality, Anatomical abnormality, Cell or molecular dysfunction, 
Congenital abnormality, Disease or syndrome, Experimental model of disease, Find-
ing, Injury or poisoning, Mental or behavioral dysfunction, Neoplastic process, Patho-
logic function, Sign or symptom. 
Daily  or  recreational  activity,  Amino  acid,  peptide,  or  protein,  Antibiotic,  Clinical 
drug,  Eicosanoid,  Enzyme,  Hormone,  Inorganic  chemical,  Lipid,  Neuroreactive 
substance or biogenic amine, Nucleic acid, nucleoside, or nucleotide, Organic chemi-
cal,  Organophosphorus  compound,  Pharmacologic  substance,  Receptor,  Steroid, 
Vitamin, Diagnostic procedure, Therapeutic or preventive procedure.   

2.4 

Preprocessing of PICO Elements 

The preprocessing  involves three steps: (1)  the I and C elements, i.e. the  “interven-
tions”,  were  collected  from  titles,  abstracts  or  a  combination  from  both  sections   
(“Tile  +  Abstract”),  (2)  the  interventions  were  tokenized  using  “Loose”  (LO)  or 
“Strict” (ST) method, and (3) the interventions were weighted using normalized term 
frequency (TF), binary term occurrence (BI), term occurrence (TO) or term frequen-
cy-inverse  document  frequency  (TF-IDF).  The  tokenization  methods  and  weighting 
schemes are detailed as follow: 
─  LO:  The  interventions  were  tokenized  into  single  term.  For  instance,  the  phrase 
“ascorbic acid” is tokenized into “ascorbic” and “acid”; ST: The interventions were 
not tokenized. For example, the phrase “breast radiotherapy” remains unchanged. 

─  TF: The ratio of the frequency of a term to the  maximum  term frequency of any 
term in a document, producing a numerical value between 0 and 1; BI: The occur-
rence of a term in a document with a binary value of 0 or 1; TO: A nominal value 
obtained by calculating the number of times a term occurs in a document; TF-IDF: 
A numerical value calculated by multiplying the frequency of a term in a document 
to the inverse of the number of documents in a collection that contains the term.   

The three steps described above were achieved using Rapidminer 5.2 [8]. 24 sets of 
baseline data were generated based on the derivation of, and the tokenization methods 
and weighting schemes applied to the interventions.   

2.5 

Inter-document Similarity Tests 

The baseline data were assembled into pairs of interventions. The similarity between 
each pair of interventions was computed using the “dist” and “simil” functions avail-
able  in  the  R  package  “proxy”  [9].  A  total  of  42  similarity/distance  measures   
(Table  3)  were  utilized  to  compute  the  similarity  or  distance  between  the  pairs  of 
interventions.  A  distance  measure  was  converted  to  a  similarity  measure  using  (1). 
The similarity values were normalized to a scale of 0 to 1. The normalized similarity 

value of each pair of interventions Si was calculated using (2).   (cid:1845)(cid:3040)(cid:3036)(cid:3041) is the minimum 
similarity value and (cid:1845)(cid:3040)(cid:3028)(cid:3051) is the maximum similarity value among all pairs of inter-

ventions. 

 

364 

W.-T. Vong and P.H

H. Hui Then 

Tab

ble 3. Similarity and distance measures 

Data Type 
Numerical 

Similarity Meas
Correlation, Cos

ure 
sine, eJaccard, fJaccard 

Binary 

Nominal 

 (cid:1845)(cid:1861)(cid:1865)(cid:1861)(cid:1864)(cid:1853)(cid:1870)(cid:1861)(cid:1872)(cid:1877)(cid:3404)

Braun-blanquet, 
man,  Jaccard,  K
Michael,  Mount
Russel,  Simple  M
Tanimoto, Yule, 
Chi-squared,  Cra
Tschuprow 

1
(cid:1830)(cid:1861)(cid:1871)(cid:1872)(cid:1853)(cid:1866)(cid:1855)(cid:1857)(cid:3397)1  

Dice,  Fager,  Faith,  Ham-
Kulczynski1,  Kulczynski2, 
tford,  Mozley,  Ochiai,  Phi, 
Matching,  Simpson,  Stiles, 
Yule2 
amer,  Pearson,  Phi-square, 

                  (1) 

Distance Measure 
Bhattacharyya,  Bray,  Canberra,  Cho
Divergence, 
Geodes
Hellinger,  Manhattan,  Soergel,  Sup
mum, Wave, Whittaker 
- 

Euclidean, 

ord, 
sic,   
pre-

- 

(2)

A  retrospective  analysis
pairs of interventions. Inter
interventions whereas those
tions. Histograms and boxp
larity/distance  measures  in
tions. A one-way ANOVA 
between paired and unpaire
to compare the performanc

(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)  is the mean of 
(3). (cid:1845)(cid:3043)(cid:3028)(cid:3114)(cid:3045)(cid:3032)(cid:3031)
(cid:1839)(cid:1857)(cid:1853)(cid:1866) (cid:1830)(cid:1861)(cid:1858)(cid:1858)(cid:1857)

the mean of similarity value

s  was  conducted  manually  to  judge  the  similarity  of 
ventions which are highly similar were identified as pai
e with low similarity were identified as unpaired interv
plots  were created to assess the  effectiveness of the si
n  separating  paired  interventions  from  unpaired  interv
was performed to compare the means of similarity val
ed interventions. The mean difference (MD) was calcula
ce of the 42 measures on the 24 sets of baseline data us

(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)
(cid:3031)(cid:3364)(cid:3364)  is 
similarity values of paired interventions and   (cid:1845)(cid:3048)(cid:3041)(cid:3043)(cid:3028)(cid:3114)(cid:3045)(cid:3032)(cid:3031)
(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)                                           
 (3) 
(cid:1857)(cid:1870)(cid:1857)(cid:1866)(cid:1855)(cid:1857) ((cid:1839)(cid:1830))(cid:3404) (cid:1845)(cid:3043)(cid:3028)(cid:3114)(cid:3045)(cid:3032)(cid:3031)

(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3398) (cid:1845)(cid:3048)(cid:3041)(cid:3043)(cid:3028)(cid:3114)(cid:3045)(cid:3032)(cid:3031)

the 
ired 
ven-
imi-
ven-
lues 
ated 
sing 

es of unpaired interventions.   

2.6  Cluster Structure A

Analysis 

Similar  interventions  were 
tering  methods:  average-li
three types of clusterings w
R package “stats” [9].   

clustered  together  using  agglomerative  hierarchical  cl
ink  (AL),  complete-link  (CL)  and  ward-link  (WL).  T
were generated using the “hclust” function available in 

lus-
The 
the 

A sample of clustering w
tions (e.g. “Tamoxifen Bev
levels and the number of d
relevant  documents  for  a  t
need to explore two levels 
tion Tamoxifen”.   

The precision (P), recall 
the ratio of relevant docum
ber of relevant and irreleva
of relevant documents retrie
vant  documents  retrieved 
mean of P and R (6). 

was shown in Fig. 2a. Based on the figure, each interv
vacizumab”) represents a single document. The number
documents that a user will need to explore to obtain all 
topic  of  interest  were  identified.  For  instance,  a  user  w
to discover two documents representing the topic “Irrad

ments retrieved for a given topic ((cid:1840)(cid:3019)(cid:3032)(cid:3039)) over the total nu
ant documents retrieved ((cid:1840)(cid:3019)(cid:3032)(cid:3039)(cid:3397) (cid:1840)(cid:3010)(cid:3045)(cid:3045)(cid:3032)(cid:3039)) (4). R is the r
eved for a given topic ((cid:1840)(cid:3019)(cid:3032)(cid:3039)) over the total number of re
and  not  retrieved  ((cid:1840)(cid:3019)(cid:3032)(cid:3039) (cid:3397) (cid:1839)(cid:3019)(cid:3032)(cid:3039) )  (5).  F  is  the  harmo

(R) and F-measure (F) of each cluster were calculated. 

P is 
um-
atio 
ele-
onic 

ven-
r of 
the 
will 
dia-

 

 

 

 

365 

Visualization of PICO Elements for Information Needs Clarification 

(cid:1840)(cid:3019)(cid:3032)(cid:3039)
(cid:1840)(cid:3019)(cid:3032)(cid:3039)
(cid:1840)(cid:3019)(cid:3032)(cid:3039)(cid:3397) (cid:1840)(cid:3010)(cid:3045)(cid:3045)(cid:3032)(cid:3039)             (4) 
 (cid:1842)(cid:1870)(cid:1857)(cid:1855)(cid:1861)(cid:1871)(cid:1861)(cid:1867)(cid:1866) ((cid:1842))(cid:3404) 
(cid:1840)(cid:3019)(cid:3032)(cid:3039)(cid:3397) (cid:1839)(cid:3019)(cid:3032)(cid:3039)                      (5) 
(cid:1832)(cid:3398)(cid:1865)(cid:1857)(cid:1853)(cid:1871)(cid:1873)(cid:1870)(cid:1857) ((cid:1832))(cid:3404)2(cid:3400)(cid:1842) (cid:3400)(cid:1844)
(cid:1842) (cid:3397)(cid:1844)                                                                                                           (6) 

(cid:1844)(cid:1857)(cid:1855)(cid:1853)(cid:1864)(cid:1864) ((cid:1844))(cid:3404)

 

Random-baseline clusterings were constructed to evaluate the information retrieval 
performance  of  similarity-based  clusterings.  A  random-baseline  clustering  was 
created  by  randomly  assigning  the  interventions  into  a  clustering  that  has  the  same 
number of clusters and the same number of documents in each cluster of a similarity-
based clustering. An example is given in Fig. 2. The P, R and mean average precision 
(MAP) over 25 topics were computed using the TREC_EVAL program [10]. 

 
Fig.  2.  (a)  Similarity-based  clustering  and  (b)  random-baseline  clustering.  Doc  =  number  of 
documents 

3 

Results 

3.1  The Inter-document Similarity Tests 

42  types  of  similarity/distance  measures  were  employed  to  calculate  the  similarities 
between pairs of interventions. A value close to 1 indicates strong similarity whereas 
a value close to 0 means low similarity. The MD was calculated to indicate the differ-
ence between the mean of paired and the mean of unpaired similarities. The larger the 
MD, the greater the differentiation and the less overlap between the distributions of 
paired and unpaired similarities. Table 4 summarizes the measures that produced the 
highest MD over the 24 sets of baseline data. The table revealed that: (1) BI is better 
than other weighting schemes, (2) the tokenization method, LO, is superior to ST, (3) 
interventions derived from title are better than those derived from abstract or “title + 
abstract”, and (4) Yule gives the highest MD compared to other measures. 

One of the most popular distance measures between two document vectors is the 
Cosine  similarity.  Fig.  3a  compares  the  MD  of  Yule  to  the  MD  of  Cosine  with  an 

 

366 

W.-T. Vong and P.H. Hui Then 

increase  in  number  of  pairs  of  interventions.  The  figure  shows  that  the  number  of 
pairs  has  little  influence  on  the  performance  of  Yule  and  Cosine.  The  MD  of  Yule 
(average MD = 0.86 ± 0.04) is evidently higher than the MD of Cosine (average MD 
= 0.50 ± 0.02 and 0.40 ± 0.02 respectively when tokenized by TF and TF-IDF). For 
both  measures,  a  significant  difference  in  similarity  values  between  paired  and  un-
paired interventions was found (p < 0.005). 

Table 4. Similarity/distance measures that produced the highest mean difference (MD) 

Weighting 
Scheme 

Tokenization 
Method 

Derivation of Interventions 
Title (MD) 

Abstract (MD) 

TF 

TF-IDF 

BI 

TO 

LO 
ST 
LO 
ST 
LO 
ST 
LO 
ST 

eJaccard (0.53) 
eJaccard (0.44) 
eJaccard (0.43) 
eJaccard (0.36) 
Yule (0.86) 
Yule (0.66) 
Pearson (0.57) 
Pearson (0.49) 

Cosine (0.34) 
Cosine (0.28) 
Cosine (0.24) 
Cosine (0.20) 
Yule (0.67) 
Yule (0.53) 
Pearson (0.34) 
Pearson (0.31) 

Title + Abstract   
(MD) 
Cosine (0.45) 
Cosine (0.36) 
Cosine (0.31) 
Cosine (0.27) 
Yule (0.74) 
Yule (0.63) 
Pearson (0.53) 
Pearson (0.43) 

Histograms and boxplots (Fig. 3b) were plotted to investigate the frequency distri-
bution of similarity values of 450 paired and 450 unpaired interventions. As shown in 
the  figure,  the  less  overlap  between  the  two  histograms,  the  greater  the  separation 
between the two distributions. The paired histogram for BI-Yule combination skewed 
significantly to the right, showing that most of the paired interventions has a similari-
ty  value  close  or  equal  to  1.  In  contrast,  the  paired  histograms  for  the  two  Cosine 
combinations are relatively flat with similarity values range between 0 and 1 (as indi-
cated  also  by  the  whiskers  and  outliers  in  boxplots).  The  degree  of  overlap  for  the 
three combinations looks apparently the same. In terms of classifiability, Yule gave a 
more clear-cut separation of paired and unpaired similarities in histograms and box-
plots than Cosine. 

In summary, the similarity measure, Yule, performed better than other measures at 
identifying  paired  interventions  or  at  differentiating  between  paired  and  unpaired 
interventions derived from titles and which had been weighted and tokenized respec-
tively using BI and the LO method. 

3.2  The Clustering Tests 

The  similarities  between  the  interventions  that  occurred  in  a  collection  of  114   
documents  were  calculated  using  the  BI-LO-Title-Yule  combination.  Hierarchical 
clusterings were computed using AL, CL and WL algorithms. As shown in Fig. 4a (1st 
column), the structure of AL and CL clusterings are wide with many branches at the 
top of the hierarchies, whereas for WL clustering, branches are located mainly at the 
bottom of the hierarchy. The CL algorithm produced a structure with lower number of 
levels (the highest number of levels = 5 compared to 10 for WL and 15 for AL). A 
comparison of the structures obtained by calculating the similarities using the TF-LO-
Title-Cosine combination (Fig. 4a, 2nd column) revealed a higher number of levels in 
the clusterings (the highest number of levels = 7 for CL, compared to 13 for WL and 

 

 

Visualization of PICO Elements for Information Needs Clarification 

367 

18 for AL). The higher the number of levels, the longer it takes for a user to browse 
and  search  for  a  topic  in  a  hierarchy.  The  Yule-based  CL  clustering,  compared  to 
other clusterings, provides a  better structure in terms of the number of levels that a 
user needs to explore to reach a topic of interest. 

 

Fig. 3. (a) Mean difference against number of pairs of interventions; (b) Histograms and box-
plots of  the  distribution of  paired  and  unpaired  similarities.  Derivation  of  interventions:  title, 
tokenization method: LO 

The clusters that best represent 33 topics that covered in the 114 documents were 
identified by computing the P, R and F of each cluster in a hierarchy. Table 5 shows 
the level of the clustering hierarchy (Lev), the number of documents (Doc), the num-
ber of relevant documents (Rel) and the P, R and F values of a cluster in a Yule-based 
CL clustering. A good cluster is supposed to contain as many relevant documents as 
possible with high P and high R. The F-measure quantifies the balance between P and 
R.  The  higher  the  F  value,  the  higher  the  quality  of  a  cluster.  It  can  be  seen  from   
Table  5  that:  (1)  relevant  documents  are  grouped  in  one  (e.g.  Level  1  of  Topic  1,   
R = 1.0) or two clusters (e.g. Level 1 of Topic 2, R = 0.5 and 0.5 respectively), (2) the 
best clusters appear at the top of the structure with high P, R and F for Topics 1, 2 and 
4, (3) the best clusters occur at the bottom of the structure with high P, R and F for 
Topic 3, (4) Topic 4 can be identified without exploring the structure (Level = 0), and 
(5) some of the relevant documents are grouped in different clusters  with irrelevant 
documents (e.g. Level 1 of Topic 3, P = 0.6 and 0.3 respectively). The results indicate 
that the best clusters located at different levels of the structure.   

Table 6 shows the average number of levels that a user needs to explore to discover 
the  best  clusters  for  the  33  topics.  The  Yule-based  CL  clustering  provides  the   
best  hierarchical  structure  for  the  exploration  of  different  topics,  followed  by  the   

 

368 

W.-T. Vong and P.H. Hui Then 

Cosine-based  CL  clustering  (Average  No.  of  Level  =  1.70  ±  1.10  and  2.33  ±  1.95   
respectively). The findings suggest that the best clusters appear on average at the top 
two levels of a CL clustering. This was evaluated by identifying the clusters with the 
highest F-measure (FMax) from the top two levels. The percentages of relevant docu-
ments  covered  by  the  clusters  were  then  calculated  and  are  shown  in  Table  7.  On 
average over the 33 topics, the clusters from the top two levels contain approximately 
81%  and  79%  of  the  relevant  documents  respectively  for  Yule-based  and  Cosine-
based CL clusterings. This suggests that only a small number of clusters that will need 
to be further explored to obtain all the relevant documents from the clusterings. 

 

Fig. 4. (a) A comparison of clusterings by similarity measures and clustering methods; (b) The 
precision-recall performance of similarity-based and random-baseline CL clusterings 

The effectiveness of similarity-based clusterings in grouping similar interventions 
to the same or small number of clusters were evaluated by comparing with random-
baseline  clusterings.  Interventions  were  grouped  into  different  clusters  without   
similarity  constraint  to  produce  a  random-baseline  clustering.  A  total  of  25  topics 
were created for the evaluation. Each topic  was treated as a query. Similarity-based 
clusterings  outperform  random-baseline  clusterings  in  terms  of  mean  average  preci-
sion (MAP = 0.43 vs. 0.24 and 0.48 vs. 0.25 respectively for Yule-based and Cosine-
based CL clusterings). This is further indicated in the P-R curves shown in Fig. 4b.   

 

 

Visualization of PICO Elements for Information Needs Clarification 

369 

The overall clustering results indicate that the top two levels of a Yule-based CL 
clustering provide the most appropriate hierarchical clustering for the exploration and 
visualization of interventions.   

 

Table 5. Examples of the distribution of the 
best cluster in a Yule-based CL clustering 

Table 6. Location of the best cluster by the 
average number of level over 33 topics 

Topic  Lev  Doc  Rel  P 
1 

R 
1.0 
0.2 
0.8 
0.7 
0.5 
0.5 

1.0 
1.0 
1.0 
1.0 
0.3 
1.0 

F 
1.0 
1 
0.3 
2 
0.9 
2 
0.8 
3 
0.4 
1 
1 
0.7 
… … … … … …
0.2 
4 
0.2 
1 
0.4 
1 
0.3 
2 
0.4 
2 
3 
0.9 
0.5 
0 
0 
0.5

0.3 
0.6 
0.3 
0.5 
0.3 
1.0 
1.0 
1.0 

0.1 
0.3 
0.8 
0.3 
0.8 
0.8 
0.5 
0.5

10 
2 
2 
3 
15 
4 

3 
6 
11 
2 
10 
3 
1 
1

10 
2 
8 
7 
4 
4 

1 
1 
3 
1 
3 
3 
1 
1 

2 
 
 

 
3 
 
 
 
 
4 
 

Similarity 
Measure 
Cosine 
Yule 
Cosine 
Yule 
Cosine 
Yule 

Average ± SD 
No. of Level 
2.33 ± 1.95 
1.70 ± 1.10 
10.12 ± 4.84 
8.21 ± 3.56 
7.67 ± 3.91 
5.97 ± 2.49 

Clustering 
Method 
CL 
CL 
AL 
AL 
WL 
WL 
 
 
 
 
 

Table 7. Percentages of relevant documents in top clusters in CL clusterings 

Topic 
Q1 
Q2 
…
Q33 
Mean ± SD 

Cosine_CL 
FMax 
0.6 
0.1 
…
0.6 
 

Doc 
8 
1 
…
2 
 

Rel 
11 
2 
…
2 
 

% 
73 
50 
…
100 
79 ± 23 

Yule_CL 
FMax 
0.6 
0.2 
…
0.6 
 

Doc 
8 
1 
…
2 
 

Rel 
11 
2 
…
2 
 

% 
73 
50 
…
100 
81 ± 23 

4 

Discussion and Conclusions 

What types of clinical information do doctors need? Where do they search for informa-
tion? An early study by [11] reported that approximately 33% of information needs re-
lated  to  treatment  of  specific  conditions,  25%  to  diagnosis  and  14%  to  drugs.  Similar 
findings  were  reported  in  [12]  that  the  top  categories of  information  needs  were  treat-
ment/therapy (38%), diagnosis (24%) and drug therapy/ information (11%). Studies by 
[13,14] further supported that one of the doctors’ greatest information needs is for infor-
mation about treatments and drugs. The primary electronic resource used by doctors for 
evidence-based clinical decision making is MEDLINE [15,16]. Junior doctors accessed 
MEDLINE (44%), UpToDate (42%), internet search engines (5%), MDCONSULT (3%) 
and the Cochrane Library (2%) for clinical information [17]. The findings support the use 
of MEDLINE in this study as the preferred information source for PICO elements. 

The  Use  of  PICO  Framework  for  Query  Refinement.  One  of  the  obstacles  that  pre-
vents  physicians  from  answering  patient-care  question  is  the  tendency  to  formulate 

 

370 

W.-T. Vong and P.H. Hui Then 

unanswerable question [1]. To formulate an answerable question, physicians are rec-
ommended to change their search strategies by rephrasing their questions [17]. Other 
studies recommended the use of question frameworks such as PICO, PICOT, PICOS 
and PESICO for the formulation of clinical question [18,19,20,21]. A study evaluat-
ing  the  use  of  PICO  as  a  knowledge  representation  reported  that  the  framework  is 
primarily  centered  on  therapy  question  [2].  This  supports  the  focus  of  the  present 
study on refining questions relating to treatments and drugs. An earlier study by [4] 
found that questions that contain a proposed intervention and a relevant outcome were 
unlikely to go unanswered. It is recommended by [22] that at least 3 of the PICO ele-
ments are needed to formulate an answerable question. In summary, the completeness 
of PICO elements determines whether a clinical question is likely to be answered.     

 

Visualization of Interventions Using Similarity-Based Clustering. Current medical QA 
(MedQA)  systems  focus  on  providing  direct  and  precise  answers  to  doctors’   
questions. A recent review by [23] concluded that current MedQA systems have limi-
tations in terms of the types and formats of questions that they can process. The Info-
Bot [24] and the EpoCare [25] systems can only  handle  structured queries in PICO 
format but not in NL. An example of PICO query is “Atrial Fibrillation AND Warfa-
rin AND Aspirin AND Secondary Stroke”. The use of the system may be limited by 
the ability of users to apply Boolean operators (e.g. AND and OR) and by the lack of 
vocabulary due to limited knowledge of a particular domain. The AskHermes system 
[26,27], on the other hand, accepts both well-structured and ill-formed NL questions. 
For example, “What is the best treatment for a needle stick injury after a human im-
munodeficiency  virus  exposure?”  A  poorly  formulated  question  cannot  be  refined. 
This  can  in  turn  lead  to  the  discovery  of  irrelevant  documents.  Current  MedQA   
systems  assume  that  users  are  aware  of  their  knowledge  deficit.  Little  research  has 
focused on assisting users in formulating high quality questions, supporting them in 
exploring a problem domain and clarifying their information needs.   

The present study adopted the concept of system-mediated information access, intro-
duced by [28], to assist users in refining an ill-defined question. It is expected that users 
can  clarify  or refine  their information  needs  through  browsing  and  searching  interven-
tions  which  have  been  pre-clustered  into  a  hierarchical  structure.  The  inter-document 
similarity  and  cluster  structure  analysis  revealed  that  the  combination  of  BI-LO-Title-
Yule-CL  produced  the  most  appropriate  hierarchical  clustering  for  the  visualization  of 
interventions. The Yule measure appeared to be slightly better than the Cosine measure 
at contributing to the identification of similar interventions. The Cosine similarity, which 
measures the cosine of the angle between two vectors, has been applied to both document 
clustering [29] and short text clustering [30]. The Yule similarity calculates the strength 
of association between binary variables. Though not as well studied as the Cosine simi-
larity, [31] reported an improvement in clustering performance using the Yule measure. 
The  cluster  structure  analysis  revealed  that  documents  with  similar  interventions  are 
likely to be grouped into the same cluster. The top two levels of a CL clustering provide 
the most appropriate structure for the exploration of different topics. Previous work by 
[32]  reported  that  AL  produces  a  more  effective  clustering  than  CL  for  information   
retrieval.  However,  in  the  present  study,  the  AL  clustering  requires  users  to  explore  a 
higher  number  of  levels  to  discover  a  problem  domain.  Doctors  often  have  very  tight   
schedules. When seeking information for patient care, they are more likely to look for 

 

 

Visualization of PICO Elements for Information Needs Clarification 

371 

information that can be accessed quickly with minimal effort [11]. Therefore, it is argued 
that CL provides a quicker and more appropriate clustering than AL for the visualization 
and exploration of interventions. 

 

Limitations. The  study  was  conducted only  on  MEDLINE  articles relevant  to  a  single 
question. The single source of documents  may restrict the applicability of the findings 
from  this  study  to  documents  from  other  resources  such  as  the  Cochrane  Library  and 
UpToDate.  The  question  tested  was  posed  with  only  the  P  element.  Further  analysis 
should be undertaken with higher number of questions addressed with different combina-
tions of PICO elements. Compared to the titles, a higher number of interventions were 
collected from the abstracts. The case study however shows that the title-based approach 
superior  to  the  abstract-based  approach.  The  study  can  be  improved  by  evaluating  the 
effectiveness of the methodologies used for PICO extraction and the effects of different 
numbers  of  interventions  between  two  documents  on  the  measurement  of  similarity. 
Despite of the limitations, the experimental results show that the similarity-based cluster-
ing approach has the potential to aid the visualization and exploration of interventions for 
the applications of clinical information needs clarification and query refinement.   

References 

1.  Ely,  J.W.,  Osheroff,  J.A.,  Chambliss,  M.L.,  Ebell,  M.H.,  Rosenbaum,  M.E.:  Answering 
physicians’  clinical  questions:  obstacles  and  potential  solutions.  J.  Am.  Med.  Inform.   
Assoc. 12(2), 217–224 (2005) 

2.  Huang, X., Lin, J., Demner-Fushman, D.: Evaluation of PICO as a knowledge representation 
for clinical questions. In: AMIA Annual Symposium Proceedings 2006, pp. 359–363 (2006) 

3.  Cao,  Y.,  Liu,  F.,  Simpson,  P.,  Antineau,  L.,  Bennet,  A.,  Cimino,  J.J.,  Ely,  J.,  Yu,  H.:   
AskHERMES:  an  online  question  answering  system  for  complex  clinical  questions.  J. 
Biomed. Inform. 44(2), 227–288 (2011) 

4.  Bergus, G.R., Randall, C.S., Sinift, S.D., Rosenthal, D.M.: Does the structure of clinical 
questions  affect  the  outcome  of  curbside  consultations  with  specialty  colleagues?  Arch. 
Fam. Med. 9(6), 541–547 (2000) 

5.  Demner-Fushman,  D.,  Lin,  J.:  Answering  clinical  questions  with  knowledge-based  and   

statistical techniques. Association for Computational Linguistics 33(1), 63–103 (2007) 

6.  Boudin, F., Nie, J.Y., Bartlett, J.C., Grad, R., Pluye, P., Dawes, M.: Combining classifiers 

for robust PICO element detection. BMC Med. Inform. Decis. Mak. 10(1), 29–36 (2010) 

7.  Aronson,  A.R.:  Effective  mapping  of  biomedical  text  to  the  UMLS  Metathesaurus:  the 
MetaMap program. In: Proceedings of AMIA Annual Symposium 2001, pp. 17–21 (2001) 

8.  RapidMiner: Report the future, http://rapid-

i.com/content/blogcategory/38/69/ (Assessed: August 2013) 

9.  R: The R project for statistical computing, http://www.r-project.org (Assessed: 

August 2013) 

10.  Trec_eval, http://trec.nist.gov/trec_eval/ (Assessed: August 2013) 
11.  Smith, R.: What clinical information do doctors need? BMJ 313(7064), 1062–1068 (1996) 
12.  Davies,  K.,  Harrison,  J.:  The  information-seeking  behavior  of  doctors:  a  review  of  the   

evidence. Health Info. Libr. J. 24(2), 78–94 (2007) 

13.  Schwartz, K., Northrup, J., Crowell, K., Lauder, N., Neale, A.V.: Use of on-line evidence-

based resources at the point of care. Family Medicine 35(4), 251–256 (2003) 

 

372 

W.-T. Vong and P.H. Hui Then 

14.  Yu, H., Cao, Y.G.: Automatically extracting information needs from ad hoc clinical ques-

tions. In: AMIA Annual Symposium Proceedings 2008, pp. 96–100 (2008) 

15.  Davies, K.: UK doctors awareness and use of specified electronic evidence-based medicine 

resources. Inform. Health Soc. Care 36(1), 1–19 (2011) 

16.  Schilling,  L.M.,  Steiner,  J.F.,  Lundahl,  K.,  Anderson,  R.J.:  Residents’  patient-specific   
clinical  questions:  opportunities  for  evidence-based  learning.  Academic  Medicine 80(1), 
51–56 (2005) 

17.  Ely,  J.W.,  Osheroff,  J.A.,  Maviglia,  S.M.,  Rosenbaum,  M.E.:  Patient-care  questions  that 

physicians are unable to answer. J. Am. Med. Inform. Assoc. 14(4), 407–414 (2007) 

18.  Schardt,  C.,  Adams,  M.B.,  Owens,  T.,  Keitz,  S.,  Fontelo,  P.:  Utilization  of  the  PICO 
framework to improve searching PubMed for clinical questions. BMC Med. Inform. Decis. 
Mak. 7(1), 16 (2007) 

19.  Rios, L.P., Ye, C., Thabane, L.: Association between framing of the research question us-
ing  the PICOT  format  and  reporting  quality  of  randomized  controlled  trials.  BMC Med. 
Res. Methodol. 10(1), 11–18 (2010) 

20.  Robinson, K.A., Saldanha, I.J., Mckoy, N.A.: Frameworks for determining research gaps 
during systematic reviews. In: Methods Future Research Needs Reports, vol. 2. Agency for 
Healthcare Research and Quality, Rockville (MD) (2011) 

21.  Schlosser,  R.W.,  Koul,  R.,  Costello,  J.:  Asking  well-built  questions  for  evidence-based 
practice in augmentative and alternative communication. Journal of Communication Dis-
orders 40(3), 225–238 (2007) 

22.  Staunton, M.: Evidence-based radiology: steps 1 and 2 – asking answerable questions and 

searching for evidence. Radiology 242(1), 23–31 (2007) 

23.  Athenikos,  S.J.,  Han,  H.:  Biomedical  question  answering:  a  survey.  Comput.  Methods 

Programs Biomed. 99(1), 1–24 (2010) 

24.  Demner-Fushman,  D.,  Seckman,  C.,  Fisher,  C.,  Hauser,  S.E.,  Clayton,  J.,  Thoma,  G.R.:   
A  prototype  system  to  support  evidence-based  practice.  In:  AMIA  Annual  Symposium 
Proceedings 2008, pp. 151–155 (2008) 

25.  Niu,  Y.,  Hirst,  G., McArthur,  G., Rodriguez-Gianolli, P.:  Answering  clinical questions  with 
role identification. In: Proceedings, Workshop on Natural Language Processing in Biomedi-
cine, 41st Annual Meeting of the Association for Computational Linguistics, pp. 73–80 (2003) 
26.  Yu, H., Kaufman, D.: A cognitive evaluation of four online search engines for answering 
definitional questions posed by physicians. In: Pacific Symposium on Biocomputing 2007, 
pp. 328–339 (2007) 

27.  Cao, Y.G., Cimino, J.J., Ely, J., Yu, H.: Automatically extracting information needs from 

complex clinical questions. J. Biomed. Inform. 43(6), 962–971 (2010) 

28.  Muresan,  G.,  Harper,  D.J.:  Topic  modelling  for  mediated  access  to  very  large  document 

collections. J. Am. Soc. Inf. Sci. Technol. 55(10), 892–910 (2004) 

29.  Subhashini,  R.,  Kumar,  V.:  Evaluating  the  performance  of  similarity  measures  used  in 
document  clustering  and  information  retrieval.  In:  Proceedings  of  the  First  International 
Conference on Integrated Intelligent Computing 2010, pp. 27–31 (2010) 

30.  Rangrej, A., Kulkarni, S., Tendulkar, A.: Comparative study of clustering techniques for 
short text documents. In: Proceedings of the 20th International Conference Comparison on 
World Wide Web 2011, pp. 111–112 (2011) 

31.  Malik, H.H., Kender, J.R.: High quality, efficient hierarchical document clustering using 
closed interesting itemsets. In: Proceedings of the Sixth International Conference on Data 
Mining 2006, pp. 991–996 (2006) 

32.  Aljaber,  B.,  Stokes,  N.,  Bailey,  J.,  Pei,  J.:  Document  clustering  of  scientific  texts  using   

citation contexts. Information Retrieval 13(2), 101–131 (2010) 

 


