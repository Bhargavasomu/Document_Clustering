Supervised Nonlinear Factorizations
Excel In Semi-supervised Regression

Josif Grabocka1, Erind Bedalli2, and Lars Schmidt-Thieme1

1ISML Lab, University of Hildesheim

Samelsonplatz 22, 31141 Hildesheim, Germany

{josif,schmidt-thieme}@ismll.uni-hildesheim.de

2Department of Mathematics and Informatics, University of Elbasan

Rruga Rinia, Elbasan, Albania
erind.bedalli@uniel.edu.al

Abstract. Semi-supervised learning is an eminent domain of machine
learning focusing on real-life problems where the labeled data instances
are scarce. This paper innovatively extends existing factorization mod-
els into a supervised nonlinear factorization. The current state of the art
methods for semi-supervised regression are based on supervised manifold
regularization. In contrast, the latent data constructed by the proposed
method jointly reconstructs both the observed predictors and target vari-
ables via generative-style nonlinear functions. Dual-form solutions of the
nonlinear functions and a stochastic gradient descent technique which
learns the low dimensionality data are introduced. The validity of our
method is demonstrated in a series of experiments against ﬁve state-of-
art baselines, clearly improving the prediction accuracy in eleven real-life
data sets.

Keywords: Supervised Matrix Factorization, Nonlinear Dimensionality
Reduction, Feature Exctraction

1

Introduction

Regression is a core task of machine learning, aiming at identifying the relation-
ship between a series of predictor variables and a special target variable (labeled
instances) of interest [1]. Practitioners often face budget constraints in record-
ing/measuring instances of the target variable, in particular due to the need for
domain expertise [2]. On the other hand, the instances composed of predictor
variables alone (unlabeled instances) appear in abundant amounts because they
typically originate from less expensive automatic processes. Eventually the re-
search community realized the potential of unlabeled instances as an important
guidance in the learning process, establishing the rich domain of semi-supervised
learning [2]. Semi-supervised learning is expressed on two ﬂavors: regression and
classiﬁcation, depending on the metric used to evaluate the prediction of the
target variable.

The principle of incorporating unlabeled instances relies heavily on exploring
the geometric structure of unlabeled data, addressing the synchronization of the

2

Josif Grabocka, Erind Bedalli, Lars Schmidt-Thieme

detected structural regularities against the positioning of the labeled instances.
A stream of research focuses on the notion of clusters, where the predicted tar-
get values were inﬂuenced by connections to labeled instances through dense
data regions [3, 4]. The other prominent stream elaborates on the idea that data
is closely encapsulated in a reduced dimensionality space, known as the mani-
fold principle. Subsequently, the method of Manifold Regularization restricted
the learning algorithm by imposing the manifold geometry via the addition of
structural regularization penalty terms [5]. Discretized versions of the manifold
regularization highlighted the structural understanding of data through elabo-
rating the graph Laplacian regularization [6]. The extrapolating power of man-
ifold regularization have been extended to involve second-order Hessian energy
regularization [7], and parallel vector ﬁeld regularization [8].

Throughout this study we introduce a semi-supervised regression model. The
underlying foundation of our approach considers the observed data variables to
be dependent on a smaller set of hidden/latent variables. The proposed method
builds a low-rank representation of the data which can reconstruct both the
predictor variables and the target variable via nonlinear functions. The target
variable is utilized in guiding the reduction process, which in comparison to
unsupervised methods, help ﬁltering only those features which boosts the tar-
get prediction accuracy [9, 10]. The proposed method operates by constructing
latent nonlinear projections, opposing techniques guiding the reconstruction lin-
early [9]. Therefore we extend supervised matrix factorization into non-linear
capabilities. The nonlinear matrix factorization belongs to the family of mod-
els known as Gaussian Process Latent Variable Modeling [11]. Our stance on
nonlinear projections is further elaborated in Section 3.

The modus operandi of our paper is deﬁned as a joint nonlinear reconstruction
of the predictors and target variable by optimizing the regression quality over
the training data. The nonlinear functions are deﬁned as regression weights in a
mapped data space, which are expressed and learned in the dual-form using the
kernel theory. In addition, a stochastic gradient descent algorithm is introduced
for updating the latent data based on the learned dual regression weights. In
the context of semi-supervision our model can operate with very few labeled
instances. Detailed explanation of the method and all necessary derivations are
described in Section 4.

No previous paper has attempted to compare factorization approaches against
the state of the art in manifold regularization, regarding semi-supervised regres-
sion problems. In order to demonstrate the superiority of the presented method
we implemented and compared against ﬁve strong state-of-art methods. A bat-
tery of experiments over eleven real life datasets at varying number of labeled
instances is conducted. Our method clearly outperforms ﬁve state-of-art base-
lines in the vast majority of the experiments as discussed in Section 5. The main
contributions of this study are:

– Formulated a supervised nonlinear factorizations model
– Developed a learning algorithm in the dual formulation

Supervised Nonlinear Factorizations Excel In Semi-supervised Regression

3

– Conducted a throughout empirical analysis against the state of the art (man-

ifold regularization)

2 Related Work

Even though a plethora of regression models have been proposed, yet Sup-
port Vector Machines (SVMs) are among the strongest general purpose learning
models. A particular implementation of SVMs tailored for approximating square
error loss is called Least Square SVMs (LS-SVM) [12], and is shown to perform
equivalently to the epsilon loss regression SVMs [13]. This study empirically
compares against LS-SVM, in order to demonstrate the additive gain of incor-
porating unlabeled information.

The semi-supervised regression research was boosted by the elaboration
of the unlabeled instances’ structure into the regression models. A major stream
explored the cluster notion in utilizing high density unlabeled instances’ regions
for predicting the target values [3, 4]. The other stream, called Manifold Reg-
ularization, assumes the data lie on a low-dimensional manifold and that the
structure of the manifold should be respected in regressing target values of the
unlabeled instances [5]. A discretized variant of the regularization was proposed
to include the graph Laplacian representation of the unlabeled data as a penalty
term [6]. The regularization of the manifold surfaces have been extended to in-
volve second-order Hessian energy regularization [7], while a formalization of the
vector ﬁeld theory was employed in the so-called Parallel Field Regularization
(PFR) [8]. In addition, a recent elaboration of surface smoothing included en-
ergy minimizations called total variation and Euler’s elastica [14]. Another study
attempts to discover eigenfunctions of the integral operator derived from both
labeled and unlabeled instances [15], while eﬀorts have extended to incorporate
kernel theory to manifold regularization [16]. In this study we compare against
three of the strongest baselines, the Laplacian regularization, the Hessian Regu-
larization and the PFR regularization. In contrast to these existing approaches,
our novel method explores hidden data structures via latent nonlinear recon-
structions of both predictors and target variable.

Supervised Dimensionality Reduction involves label information as a
guidance for dimensionality reduction. The Linear Discriminant Analysis is the
pioneer of supervised decomposition [17]. SVMs were adjusted to high dimen-
sional data through reducing the dimensionality via kernel matrix decompo-
sition [18]. Generalized linear models [19] and Bayesian mixture modeling [10]
have also been combined with supervised dimensionality reduction. Furthermore
convolutional and sampling layers of convolutional networks are functioning as
supervised decomposition [20]. The ﬁeld of Gaussian Process Latent Variable
Models (GPLVM) aims at detecting latent variables through a set of functions
having a joint Gaussian distribution [21]. A similar model to ours has utilized
GPLVM for pose estimation in images [22].

Due to its empirical success, matrix factorization has been employed in de-
tecting latent features, while supervised matrix factorization is engineered to

4

Josif Grabocka, Erind Bedalli, Lars Schmidt-Thieme

emphasize the target variable [9]. For the sake of clarity, methods that reduce
the dimensionality in a nonlinear fashion such as the kernel PCA [23], or kernel
non-negative matrix factorization [24], should not be confused with the proposed
method, because such methods are unsupervised in terms of target variable. Our
method oﬀers novelty compared to state-of-art techniques in proposing joint non-
linear reconstruction of both predictors and target variables, in a semi-supervised
fashion, from a minimalistic latent decomposition through dual-form nonlinear-
ity.

3 Elaborated Principle

The majority of machine learning methods expect a target variable to be a con-
sequence of, or directly related to, the predictor variables. This study operates
over the hypothesis that both the predictors and the target variables are observed
eﬀects of other hidden original factors/variables which are not recorded/known.
Our method extracts original variables which can jointly approximate both pre-
dictors and target variables in a nonlinear fashion. The current study claims
that original variables contain less noise and therefore better predict the target
variable, while empirical results of Section 5 demonstrate its validity.
Let us assume the unknown original data to be composed of D-many hidden
variables in N training and N(cid:48) testing instances and denoted as Z ∈ R(N +N(cid:48))×D.
Assume we could observe M -many predictor variables X ∈ R(N +N(cid:48))×M and one
target variable Y ∈ RN , with the aim of accurately predicting the test targets
Yt ∈ RN(cid:48)
. Semi-supervised scenarios where N(cid:48) > N are taken into consideration.
Our method learns the original variables Z and nonlinear functions gj, h ∈ RD →
R which can jointly approximate X, Y . Equation 1 describes the idea, while
we included natural Gaussian noise with variance σX , σY in the process. We
introduce a syntactic notation Nb

a = {a, a + 1, . . . , b − 1, b}.

Xi,j = gj(Zi,:) + N (0, σX ) ; Yi = h(Zi,:) + N (0, σY )

(1)

i ∈ NN +N(cid:48)

1

, j ∈ NM

1

4 Supervised Nonlinear Factorizations (SNF)

As aforementioned, our novelty relies on learning a latent low-rank representa-
tion Z from observed data X, Y , such that the predictor variables and the target
variable are jointly reconstructible from the low-rank data via nonlinear func-
tions. Nonlinearity is achieved by expanding the low-rank data Z to a (probably
much) higher-dimensional space RF space via a mapping ψ : RD → RF . Lin-
ear hyperplanes V ∈ RF×M , W ∈ RF with bias terms V 0 ∈ RM , W 0 ∈ R can
therefore approximate X, Y in the mapped space as described in Equation 2.

ˆXi,j = (cid:104)ψ(Zi,:), V:,j(cid:105) + V 0
i ∈ NN +N(cid:48)

1

, j ∈ NM

1

j ; ˆYi = (cid:104)ψ(Zi,:), W(cid:105) + W 0

(2)

Supervised Nonlinear Factorizations Excel In Semi-supervised Regression

5

4.1 Maximum Aposteriori Optimization

Consecutively the objective is to maximize the joint likelihood of the predic-
tors X, target Y and the maximum aposteriori estimators V, V 0, W, W 0 as
shown in Equation 3. The hyperplanes parameters incorporate normal priors
V ∼ N (0, λ−1
W ). The distribution of the observed variables is
also assumed normal X ∼ N ((cid:104)ψ(Z), V (cid:105), σX ) and Y ∼ N ((cid:104)ψ(Z), W(cid:105), σY ) and
independently distributed. The logarithmic likelihood, depicted in Equation 4
converts the objective to a summation of terms.

V ), W ∼ N (0, λ−1

argmax

ψ(Z),V,V 0,W,W 0

argmax

ψ(Z),V,V 0,W,W 0

i=1

p(Xi,:|ψ(Zi,:), V, V 0) p(V )

N +N(cid:48)(cid:89)
N(cid:89)
N +N(cid:48)(cid:88)
N(cid:88)
log(cid:0)p(Xi,:|ψ(Zi,:), V, V 0)(cid:1) +
M(cid:88)

log (p(V:,j)) + log (p(W ))

i=1

l=1

l=1

+

j=1

p(Yl|ψ(Zl,:), W, W 0) p(W )

log(cid:0)p(Yl|ψ(Zl,:), W, W 0)(cid:1)

(3)

(4)

Inserting the normal probability into Equation 4 converts logarithmic likeli-
hoods into L2 norms with Tikhonov regularization terms as shown in Equation 5
and the variance terms σX , σY drop out as constants. An additional biased reg-
ularization term λZ(cid:104)Z, Z(cid:105) is included in order to help the latent data avoid
over-ﬁtting.

 M(cid:88)

j=1

 + (cid:104)φ, φ(cid:105) + λW(cid:104)W, W(cid:105) + λZ(cid:104)Z, Z(cid:105)

argmin

Z,V,V 0,W,W 0

(cid:104)ξ:,j, ξ:,j(cid:105) + λV (cid:104)V:,j, V:,j(cid:105)

subject to: ξi,j = Xi,j − (cid:104)ψ(Zi,:), V:,j(cid:105) − V 0

j , i ∈ NN +N(cid:48)

1

, j ∈ NM

1

(5)

φl = Yl − (cid:104)ψ(Zl,:), W(cid:105) − W 0,

l ∈ NN

1

Computing the ψ(Z) directly is intractable, therefore we will derive the dual-
form representation in the next Section 4.2, where the kernel trick will be utilized
to compute Z in the original space RD.

4.2 Dual-Form Solution - Learning the Nonlinear Regression

Weights

The optimization of Equation 5 is carried on in an alternated fashion. Hyper-
plane weights V, V 0, W, W 0 are converted to dual variables and then solved by
keeping Z ﬁxed, while in a second step Z is solved keeping the dual weights
ﬁxed. This section is dedicated to learning the nonlinear weights in the dual-
form. Each of the M -many predictors loss terms from Equation 5, (one per each

6

Josif Grabocka, Erind Bedalli, Lars Schmidt-Thieme

predictor variable X:,j) can be learned isolated as described in the sub-objective
function Jj of Equation 6. To facilitate forthcoming derivations we multiplied
the objective function by 1
2λV

.

argmin
V:,j ,V 0
j

Jj =

1
2λV

(cid:104)ξ:,j, ξ:,j(cid:105) +

(cid:104)V:,j, V:,j(cid:105)
ξi,j = Xi,j − (cid:104)ψ(Zi,:), V:,j(cid:105) − V 0

1
2

j

(6)

In order to optimize Equation 6, the equality conditions are added to the
objective function through Lagrange multipliers αi,j. The inner minimization
objective is solved by computing stationary solution points V:,j, V 0
j , ξ:,j and
eliminating out the ﬁrst derivatives ( ∂Lj
= 0) as shown
∂V:,j
in Equation 7.

= 0, ∂Lj
∂V 0
j

= 0, ∂Lj
∂ξ:,j

argmax
α:,j ,V 0
j

argmin
V:,j ,V 0
j ,ξ:,j

Lj =

+

1
2λV

N +N(cid:48)(cid:88)

1
2

(cid:104)ξ:,j, ξ:,j(cid:105) +

(cid:104)V:,j, V:,j(cid:105)

(cid:0)Xi,j − (cid:104)ψ(Zi,:), V:,j(cid:105) − V 0
N +N(cid:48)(cid:88)

αi,j

αi,jψ(Zi,:)

i=1

→ V:,j =

(cid:1)

(7)

j − ξi,j

i=1

ξ:,j = λV α:,j

N +N(cid:48)(cid:88)

i=1

αi,j = 0

Replacing the stationary point solution of V:,, ξ:, j, V 0

j back into the objective

function 7, we get rid of the variables V:,j, ξ:,j, yielding Equation 8.

(8)

i=1

The solution of the dual maximization is given through eliminating the
= 0 as presented in Equation 9. The kernel notation is in-

derivative of
troduced as Ki,l = (cid:104)ψ(Zi,:), ψ(Zl,:)(cid:105).

∂L
∂α:,j

j (cid:105) = X:,j (9)
2λV α:,j + 2K · α:,j + 2(cid:104)1, V 0
Combining Equation 9 and the constraint of Equation 8, the ﬁnal nonlinear
reconstruction solution is given through the closed-form formulation of α:,j, V 0
j
as depicted in Equation 10.

j (cid:105) = 2X:,j → (K + λV I) α:,j + (cid:104)1, V 0

argmax
α:,j ,V 0
j

− N +N t(cid:88)
N +N t(cid:88)

i=1,l=1

+ 2

(cid:0)Xi,j − V 0

j

(cid:1)

αi,j

αi,jαl,j(cid:104)ψ(Zi,:), ψ(Zl,:)(cid:105) − λV (cid:104)α:,j, α:,j(cid:105)

Supervised Nonlinear Factorizations Excel In Semi-supervised Regression

7

(cid:21)

(cid:20) V 0

j
α:,j

=

(cid:20)0

(cid:21)−1(cid:20) 0

(cid:21)

1T

1 K + λV I

X:,j

(10)

Symmetrical to the predictors case, a dual-form maximization objective func-
tion is created and a mot-a-mot procedure like Section 4.2 can be trivially
adopted in solving the nonlinear regression for the target variable. The derived
solution is shown in Equation 11. Instead of using the symbol α we denote the
dual-weights of the target regression dual problem using the symbol ω.

(cid:20)W 0

(cid:21)

ω

=

(cid:20)0

(cid:21)−1(cid:20) 0

(cid:21)

1T

1 K Y + λW I

Y

where K Y

i,l = (cid:104)ψ(Zi,:), ψ(Zl,:)(cid:105);

i, l ∈ NN

1

;

(11)

A prediction of the target value of a test instance t ∈ NN +N(cid:48)

N +1

is conducted

using the learned dual weights as shown in Equation 12.

ˆYt =

ωi K Y (Zi,:, Zt,:) + W 0

(12)

N(cid:88)

i=1

1

,∀l ∈ NN +N t

Stochastic Gradient Descent - Learning the Low Dimensionality Rep-
resentation A novel algorithm is applied to learn Z for optimizing Equa-
tion 8. Sub-losses composing only of αi,j, αl,j are deﬁned for all combinations
∀i ∈ NN +N t
and Z is updated in order to optimize each sub-loss
in a stochastic gradient descent fashion as presented in Equation 13. The ad-
dition of penalty terms controlled by the hyper-parameter λZ which controls
the regularization of Z as described in Equation 5. Our model called Supervised
Nonlinear Factorizations (SNF) utilizes polynomial kernels with the derivatives
needed for gradient descent represented in Equation 13.

1

(cid:18)
(cid:18)

Zi,k ← Zi,k + η

Zl,k ← Zl,k + η

αi,jαl,j

αi,jαl,j

∂Ki,l
∂Zi,k
∂Ki,l
∂Zl,k

− λZZi,k

− λZZl,k

(cid:19)
(cid:19)

(13)

Zl,k

Zi,k
0

if r = i
if r = l
else

Ki,l = ((cid:104)Zi,:, Zl,:(cid:105) + 1)d → ∂Ki,l
∂Zr,k

= d ((cid:104)Zi,:, Zl,:(cid:105) + 1)d−1 ×

Algorithm 1 combines all the steps of the proposed method. During each
epoch all predictors’ non-linear weights are solved and the latent data Z is
updated. The target model is updated multiple times after each predictor model
to boost convergence. The learning algorithm makes use of two diﬀerent learning
rates in updating Z, one for the predictors’ loss (ηX ) and one for the target loss
(ηY ).

8

Josif Grabocka, Erind Bedalli, Lars Schmidt-Thieme

ηX , ηY , Number of Iterations: NumIter, Regularization parameters: λZ , λV , λW

Algorithm 1 Learn SNF
Require: Data X ∈ R(N +N t)×M , Y ∈ RN l
1: Randomly set: Z ∈ R(N +N(cid:48))×D, V 0 ∈ RM , W 0 ∈ R, α ∈ R(N +N(cid:48))×M , ω ∈ RN l
2: for 1 . . . NumIter do
3:
4:

Compute Ki,l = K(Zi,:, Zl,:), i, l ∈ NN +N(cid:48)

for j ∈ {1 . . . M} do

, Latent Dimension: D, Learn Rates:

,

(cid:21)−1(cid:20) 0

(cid:21)

1

1T

(cid:20) V 0

(cid:21)

(cid:20)0

j
α:,j

Solve
=
for i ∈ NN +N(cid:48)

1

Zi,k ← Zi,k + ηX
Zl,k ← Zl,k + ηX

1 K + λV I
, l ∈ NN +N(cid:48)
1
αi,jαl,j

X:,j
, k ∈ ND

1 do
− λZ Zi,k
− λZ Zl,k

∂Ki,l
∂Zi,k
∂Ki,l
∂Zl,k

αi,jαl,j

(cid:17)
(cid:17)

end for
Compute K Y

(cid:20)W 0

(cid:21)
(cid:21)
i,l = K(Zi,:, Zl,:), i, l ∈ NN

(cid:21)−1(cid:20) 0

(cid:20)0

1T

1

Solve
=
for i ∈ NN +N(cid:48)

ω

1

1 K Y + λW I
, l ∈ NN +N(cid:48)
ωiωl

1

Y
, k ∈ ND
1 do
− λZ Zi,k
− λZ Zl,k

∂KY
i,l
∂Zi,k

∂KY
i,l
∂Zl,k

(cid:19)
(cid:19)

ωiωl

Zi,k ← Zi,k + ηY
Zl,k ← Zl,k + ηY

5:

6:

7:

8:
9:
10:

11:

12:

13:

14:

(cid:16)
(cid:16)

(cid:18)
(cid:18)

end for

end for

15:
16:
17: end for
18: return Z, α, V 0, ω, W 0

5 Empirical Results

Five strong state of the art baselines and empirical evidence over eleven datasets
are mainly the outline of our experiments, which will be detailed in this section,
together with the results and their interpretation.

5.1 Baselines

The proposed method Supervised Nonlinear Factorizatoins (SNF) is compared
against the following ﬁve baselines:

– Least Square Support Vector Machines (LS-SVM) [12] is a strong
general purpose regression model and the comparisons against it will show
the gain of incorporating unlabeled instances.

– Laplacian Manifold Regularization (Laplacian) [5], Hessian En-
ergy Regularization (Hessian) [7], Parallel Field Regularization
(PFR)[8] are strong state-of-art baselines belonging to the popular ﬁeld
of manifold regularization. Comparing against them gives an insight into
the state-of-art quality of our results.

Supervised Nonlinear Factorizations Excel In Semi-supervised Regression

9

– Linear Latent Reconstructions (LLR) [9] oﬀers the possibility to un-
derstand the addiditive beneﬁts of exploring nonlinear projections compared
to plain linear ones.

5.2 Reproducibility

All our experiments were run in a three fold cross-validation mode and the hyper-
parameters of our model were tuned using only train and validation data. The
evaluation metric used in all experiments is the Mean Square Error (MSE).

SNF requires the tuning of seven hyper-parameters: the regularization weights
λZ, λV , λW , the learning rates ηX , ηY , the number of latent dimensions D and
the degree of the polynomial kernel d. The search ranges of hyper-parameters are:
λZ, λV , λW ∈ {10−6, 10−5 . . . 1, 10}; ηX , ηY ∈ {10−5, 10−4 . . . 0.1}; d ∈ {1, 2, 3, 4}
while the latent dimensionality was set to one of 50%, 75% of the original dimen-
sions. The maximum number of epochs was set to 1000. A grid search method-
ology was followed in ﬁnding the best combination of hyper-parameters. Please
note that we followed exactly the same fair principle in computing the hyper-
parameters of all baselines.

We selected eleven popular regression datasets in a random fashion from
dataset repository websites. The selected datasets are AutoPrice, ForestFires,
BostonHousing, MachineCPU, Mpg, Pyrimidines, Triazines, WisconsinBreast-
Cancer from UCI1 ; Baseball, BodyFat from StatLib2; Bears3. All the datasets
were normalized between [-1,1] before usage.

5.3 Results

The experiments comparing the accuracy of our method SNF against the ﬁve
strong baselines were conducted in scenarios with few labeled instances, as typ-
ically encountered in semi-supervised learning situations.

In the ﬁrst experiment 5% labeled instances were selected randomly, while all
other instances left unlabeled. Therefore, the competing methods had 5% target
visibility and all methods, except LS-SVM, utilized the predictor variables of all
the unlabeled instances. The results of the experiments are shown in Table 1.
The metric of evaluation is the Mean Square Error (MSE), while both the mean
MSE and the standard deviation are shown in each dataset-method cell. The
winning method of each baseline is highlighted in bold. As it is distinguishable
from the sum of wins, SNF outperforms the baselines in the majority of datasets
(six in total), while the closest competing baseline wins in only two datasets.
Furthermore in datasets such as AutoPrice, BodyFat and Mpg the improvement
is signiﬁcant. Even when SNF is not the winning method, the margin to the
ﬁrst method is not signiﬁcant, as it occurs in the Baseball, ForestFires and
WisconsinBreastCancer datasets.

1 archive.ics.uci.edu
2 lib.stat.cmu.edu
3 people.sc.fsu.edu/∼jburkardt/datasets/triola/

10

Josif Grabocka, Erind Bedalli, Lars Schmidt-Thieme

Table 1. Results - MSE - Real-life Datasets (5 % Labeled Instances)

PFR

Hessian

LS-SVM

Laplacian

Dataset
0.090 ± 0.026 0.049 ± 0.026
0.075 ± 0.012
0.102 ± 0.036
A.Price
0.086 ± 0.022
0.093 ± 0.019
0.073 ± 0.017
0.072 ± 0.015
B.ball
0.205 ± 0.046
0.292 ± 0.146 0.130 ± 0.032
0.399 ± 0.218
Bears
0.017 ± 0.006 0.008 ± 0.002
0.013 ± 0.007
0.039 ± 0.009
B.Fat
0.020 ± 0.004 0.0137 ± 0.015 0.0141 ± 0.015 0.0137 ± 0.015 0.018 ± 0.017 0.0139 ± 0.014
F.Fires
0.087 ± 0.027
0.110 ± 0.031 0.069 ± 0.024
0.104 ± 0.042
B.Hous.
0.061 ± 0.041
0.030 ± 0.021 0.015 ± 0.007
0.027 ± 0.016
M.Cpu
0.062 ± 0.015 0.041 ± 0.006
0.055 ± 0.007
0.044 ± 0.006
Mpg
0.097 ± 0.050
Pyrim 0.131 ± 0.020
0.152 ± 0.065
0.102 ± 0.053
0.191 ± 0.014
0.196 ± 0.026
0.175 ± 0.039
0.165 ± 0.031
Triaz.
WiscBC. 0.608 ± 0.074
0.356 ± 0.042 0.3499 ± 0.035 0.429 ± 0.074 0.3504 ± 0.028

0.156 ± 0.059
0.131 ± 0.020
0.407 ± 0.212
0.081 ± 0.002
0.123 ± 0.027
0.053 ± 0.043
0.074 ± 0.009
0.106 ± 0.043
0.160 ± 0.026
0.356 ± 0.039

0.067 ± 0.024
0.082 ± 0.029
0.380 ± 0.220
0.019 ± 0.007
0.071 ± 0.027
0.018 ± 0.012
0.052 ± 0.008
0.101 ± 0.055
0.166 ± 0.035

SNF

LLR

Wins

0

1.5

2

1.5

0

6

Table 2. Results - MSE - Real-life Datasets (10 % Labeled Instances)

PFR

Hessian

LS-SVM

Laplacian

Dataset
0.115 ± 0.054
0.048 ± 0.018
0.062 ± 0.005
0.051 ± 0.007
0.037 ± 0.014
A.Price
0.092 ± 0.021
0.072 ± 0.010
0.084 ± 0.014
0.080 ± 0.022
0.127 ± 0.035
B.ball
0.182 ± 0.067
0.160 ± 0.053
0.156 ± 0.051 0.067 ± 0.021
0.071 ± 0.020
Bears
0.058 ± 0.006
0.011 ± 0.004
0.008 ± 0.004
0.006 ± 0.001
0.003 ± 0.002
B.Fat
0.018 ± 0.001 0.0146 ± 0.014 0.0140 ± 0.015 0.0142 ± 0.014
0.016 ± 0.015 0.0139 ± 0.015
F.Fires
0.115 ± 0.026
0.061 ± 0.015
0.067 ± 0.004
0.067 ± 0.015
0.071 ± 0.015
B.Hous.
0.012 ± 0.003
0.020 ± 0.011
0.039 ± 0.025
0.030 ± 0.007
0.024 ± 0.015
M.Cpu
0.040 ± 0.004
0.038 ± 0.008 0.066 ± 0.016
0.062 ± 0.009
0.071 ± 0.047
Mpg
0.076 ± 0.030
0.069 ± 0.021 0.107 ± 0.060
Pyrim 0.076 ± 0.005
0.086 ± 0.042
0.173 ± 0.041
0.265 ± 0.068
0.163 ± 0.038
0.169 ± 0.012
0.175 ± 0.009
Triaz.
WiscBC. 0.624 ± 0.044 0.283 ± 0.027
0.284 ± 0.024
0.344 ± 0.079
0.307 ± 0.020

0.084 ± 0.042
0.068 ± 0.011
0.202 ± 0.018
0.015 ± 0.007
0.095 ± 0.024
0.041 ± 0.030
0.048 ± 0.011
0.076 ± 0.041
0.162 ± 0.033
0.287 ± 0.017

SNF

LLR

Wins

0

1

2

2

1

5

For the sake of completeness we repeated the experiments with another de-
gree of randomly re-drawn labeled instances (10 % labeled instances). Table 2
presents the details of experiments over the selected eleven real-life datasets. The
accuracy of SNF is prolonged even in this experiment. The sum of the winning
methods (depicted in bold) shows that SNF wins in ﬁve of the datasets against
the only two wins of the closest baseline. In particular the cases of BodyFat and
MachineCpu demonstrate signiﬁcant improvements in terms of MSE. As shown
by the results, even in cases where our method is not the ﬁrst, still it is close to
the winner.

Fig. 1. Scale-up Experiments

Supervised Nonlinear Factorizations Excel In Semi-supervised Regression

11

In addition to the aforementioned results we extend our empirical analysis
by conducting more ﬁne-grained scale-up experiments with varying degree of
labeled training instances. Figure 1 demonstrates the performance of all com-
peting methods on a subset of datasets with a range of present labels varying
from 5% up to 20%. SNF is seen to win in the earliest labeled percentages of the
BostonHousing dataset (up to 10 %) while following in the later stages. On the
contrary, we observe that our method dominates in all levels of label presence
in the scaled-up experiments involving the BodyFat and AutoPrice datasets.

The accuracy of our method is grounded on a couple of reasons/observations.
First of all, we would like to emphasize that each mentioned method is based
on a diﬀerent principle and modus operandi. Consequently, the dominance of a
method compared to baselines depends on whether (or not) the datasets follow
the principle of that particular method. Arguably the domination of SNF over
manifold regularization baselines is due to the fact that our principle of mining
hidden latent variables is likely (as results show) more present in general real-life
datasets, therefore SNF is suited to the detection of those relations.

6 Conclusions

Throughout the present paper, a novel method that addresses the task of semi-
supervised regression was proposed. The proposed method constructs a low-rank
representation which jointly approximates the observed data via nonlinear func-
tions which are learned in their dual formulation. A novel stochastic gradient
descent technique is applied to learn the low-rank data using the obtained dual
weights. Detailed experiments are conducted in order to compare the perfor-
mance of the proposed method against ﬁve strong baselines over eleven real-
life datasets. Empirical evidence over experiments in varying degrees of labeled
instances demonstrate the eﬃciency of our method. The supervised nonlinear
factorizations outperformed the manifold regularization state-of-art methods in
the majority of experiments.

Acknowledgement

This research has been co-funded by the EU in FP7 via REDUCTION (#288254)
and iTalk2Learn (#318051) projects.

References

1. Hastie, T., Tibshirani, R., Friedman, J.H.: The Elements of Statistical Learning.

Corrected edn. Springer (July 2003)

2. Zhu, X.: Semi-supervised learning literature survey. Technical Report 1530, Com-

puter Sciences, University of Wisconsin-Madison (2008)

3. Singh, A., Nowak, R.D., Zhu, X.: Unlabeled data: Now it helps, now it doesn’t. In
Koller, D., Schuurmans, D., Bengio, Y., Bottou, L., eds.: NIPS, Curran Associates,
Inc. (2008) 1513–1520

12

Josif Grabocka, Erind Bedalli, Lars Schmidt-Thieme

4. Sinha, K., Belkin, M.: Semi-supervised learning using sparse eigenfunction bases.

In: NIPS. (2009) 1687–1695

5. Belkin, M., Niyogi, P., Sindhwani, V.: Manifold regularization: A geometric frame-
work for learning from labeled and unlabeled examples. Journal of Machine Learn-
ing Research 7 (2006) 2399–2434

6. Melacci, S., Belkin, M.: Laplacian support vector machines trained in the primal.

Journal of Machine Learning Research 12 (2011) 1149–1184

7. Kim, K.I., Steinke, F., Hein, M.: Semi-supervised regression using hessian energy
with an application to semi-supervised dimensionality reduction. In: NIPS. (2009)
979–987

8. Lin, B., Zhang, C., He, X.: Semi-supervised regression via parallel ﬁeld regulariza-

tion. In: NIPS. (2011) 433–441

9. Menon, A.K., Elkan, C.: Predicting labels for dyadic data. Data Min. Knowl.

Discov. 21(2) (2010) 327–343

10. Mao, K., Liang, F., Mukherjee, S.: Supervised dimension reduction using bayesian
mixture modeling. Journal of Machine Learning Research - Proceedings Track 9
(2010) 501–508

11. Lawrence, N.: Probabilistic non-linear principal component analysis with gaussian
process latent variable models. The Journal of Machine Learning Research 6 (2005)
1783–1816

12. Suykens, J.A.K., Vandewalle, J.: Least squares support vector machine classiﬁers.

Neural Processing Letters 9(3) (1999) 293–300

13. Ye, J., Xiong, T.: Svm versus least squares svm. Journal of Machine Learning

Research - Proceedings Track 2 (2007) 644–651

14. Lin, T., Xue, H., Wang, L., Zha, H.: Total variation and euler’s elastica for super-

vised learning. In: ICML. (2012)

15. Ji, M., Yang, T., Lin, B., Jin, R., Han, J.: A simple algorithm for semi-supervised

learning with improved generalization error bound. In: ICML. (2012)

16. Nilsson, J., Sha, F., Jordan, M.I.: Regression on manifolds using kernel dimension

reduction. In: ICML. (2007) 697–704

17. Ye, J.: Least squares linear discriminant analysis. In: ICML. (2007) 1087–1093
18. Pereira, F., Gordon, G.: The support vector decomposition machine.

In: Pro-
ceedings of the 23rd international conference on Machine learning. ICML ’06, New
York, NY, USA, ACM (2006) 689–696

19. Rish, I., Grabarnik, G., Cecchi, G., Pereira, F., Gordon, G.J.: Closed-form super-
vised dimensionality reduction with generalized linear models. In: Proceedings of
the 25th international conference on Machine learning. ICML ’08, New York, NY,
USA, ACM (2008) 832–839

20. Ciresan, D.C., Meier, U., Gambardella, L.M., Schmidhuber, J.: Convolutional
neural network committees for handwritten character classiﬁcation. In: ICDAR,
IEEE (2011) 1135–1139

21. Urtasun, R., Darrell, T.: Discriminative gaussian process latent variable models

for classiﬁcation. In: In International Conference in Machine Learning. (2007)

22. Navaratnam, R., Fitzgibbon, A., Cipolla, R.: The joint manifold model for semi-
supervised multi-valued regression. In: Computer Vision, 2007. ICCV 2007. IEEE
11th International Conference on. (2007) 1–8

23. Hoﬀmann, H.: Kernel pca for novelty detection. Pattern Recogn. 40(3) (March

2007) 863–874

24. Lee, H., Cichocki, A., Choi, S.: Kernel nonnegative matrix factorization for spectral

eeg feature extraction. Neurocomputing 72(13-15) (2009) 3182–3190


