ClaSP: An Eﬃcient Algorithm for Mining

Frequent Closed Sequences

Antonio Gomariz1,(cid:2), Manuel Campos2, Roque Marin1, and Bart Goethals3

1 Information and Communication Engineering Dept., University of Murcia, Spain

2 Languages and Systems Dept., University of Murcia, Spain

3 Mathematics and Computer Science Dept., University of Antwerp, Belgium

Abstract. In this paper, we propose a new algorithm, called ClaSP
for mining frequent closed sequential patterns in temporal transaction
data. Our algorithm uses several eﬃcient search space pruning methods
together with a vertical database layout. Experiments on both synthetic
and real datasets show that ClaSP outperforms currently well known
state of the art methods, such as CloSpan.

1

Introduction

Sequence Data Mining (SDM) is a well-extended ﬁeld of research in Temporal
Data Mining that consist of looking for a set of patterns frequently enough
occurring across time among a large number of objects in a given input database.
The threshold to decide if a pattern is meaningful is called minimum support.
SDM has been widely studied [6,9,3,5,2], with broad applications, such as the
discovery of motifs in DNA sequences, analysis of customer purchase sequences,
web click streams, and so forth.

The task of discovering the set of all frequent sequences in large databases
is challenging as the search space is extremely large. Diﬀerent strategies have
been proposed so far, among which SPADE, exploiting a vertical database for-
mat [9], FreeSpan and PreﬁxSpan, based on projected pattern growth [3,5] are
the most popular ones. These strategies show good performances in databases
containing short frequent sequences or when the support threshold is not very
low. Unfortunately, when long sequences are mined, or when a very low support
threshold is used, the performance of such algorithms decreases dramatically
and the number of frequent patterns increases sharply, resulting in too many
meaningless and redundant patterns. Even worse, sometimes it is impossible to
complete the algorithm execution due to a memory overﬂow.

One of the most interesting proposals to solve both problems are so called
closed sequences [8], based on the same notion for regular frequent closed item-
sets, as introduced by Pasquier et al. [4]. A frequent sequence is said to be closed,

(cid:2) Corresponding author: agomariz@um.es. This work is partially ﬁnanced by a PhD
grant from the Seneca Foundation (Regional Agency for Science and Technology of
the Region de Murcia), and by the Spanish Oﬃce for Science and Innovation through
project TIN2009-14372-C03-01 and PlanE, and the European Union by means of the
European Regional Development Fund (ERDF, FEDER).

J. Pei et al. (Eds.): PAKDD 2013, Part I, LNAI 7818, pp. 50–61, 2013.
© Springer-Verlag Berlin Heidelberg 2013

ClaSP: An Eﬃcient Algorithm for Mining Frequent Closed Sequences

51

if there no exists a supersequence with the same support in the database. The
ﬁnal collection of closed sequences provides a much more simpliﬁed output, still
keeping all the information about the frequency of each of the sequences. Some
algorithms have been developed to ﬁnd the complete set of closed sequences,
where most of them are based on the Pattern Growh strategy [8,7].

In this paper, we propose a new algorithm, called ClaSP (Closed Sequential
Patterns algorithm) which exploits several eﬃcient search space pruning meth-
ods. Depending on the properties of the database, we argue about the desirability
of using the vertical database format as compared to pattern growth techniques.
We also show the suitability of the vertical database format in obtaining the
frequent closed sequence set, and how, under some database conﬁgurations, a
standard vertical database format algorithm can already be faster than Pattern
Growth algorithms for closed sequences, by only adding a simple post-processing
step. Experiments on both synthetic and real datasets show that ClaSP gener-
ates the same complete closed sequences as CloSpan [8] but has much better
performance ﬁgures.

The remaining of the paper is organized as follows. Section 2 introduces the
preliminary concepts of frequent closed sequential pattern mining and the nota-
tion used in the paper. In Section 3, we present the most relevant related works.
In Section 4, the pruning methods and ClaSP algorithm are presented. The per-
formance study is presented in section 5 and, ﬁnally, we state our conclusions in
section 6.

2 Problem Setting
Let I be a set of items. A set X = {e1, e2, . . . , ek} ⊆ I is called an itemset or k-

itemsets if it contains k items. For simplicity, from now on we denote an itemset
I as a concatenation of items between brackets. So, I1 = (ab) and I2 = (bc) are
both two 2-itemsets. Also, without loss of generality, we assume the items in
every itemset are represented in a lexicographic order.
A sequence s is a tuple s = (cid:3)I1I2 . . . In(cid:4) with Ii ∈ I, and ∀i : 1 ≤ i ≤ n. We
denote the size of a sequence |s| as the number of itemsets in that sequence. We
|Ii|) as the number of items in it, and
denote the length of a sequence (l =
every sequence with k items is called a k-sequence. For instance, the sequence
α = (cid:3)(ab)(bc)(cid:4) is a 4-sequence with a size of 2 itemsets.
We say α = (cid:3)Ia1 Ia2 . . . Ian(cid:4) is a subsequence of another sequence β = (cid:3)Ib1 Ib2
. . . Ibm(cid:4) (or β is a supersequence of α), denoted as α (cid:8) β, if there exist integers
1 ≤ j1 < j2 < . . . < jn ≤ m such that Ia1 ⊆ Ibj1
, . . . , Ian ⊆ Ibjn . For
instance, (cid:3)(b)(c)(cid:4) is a subsequence of (cid:3)(ab)(bc)(cid:4), since (b) ⊆ (ab) and (c) ⊆ (bc)
and the order in the itemsets is preserved. Furthermore, the sequence (cid:3)(b)(c)(cid:4) is
not a subsequence of (cid:3)(abc)(cid:4).

, Ia2 ⊆ Ibj2

(cid:2)n

i=1

In the rest of the work, we use the terms pattern and sequence interchangeably.
An input sequence is is a tuple is = (cid:3)id, s(cid:4) with id ∈ N and s is a sequence.
We call id the identiﬁer of the input sequence. We say that an input sequence
is contains another sequence α, if α (cid:8) s.

52

A. Gomariz et al.

Table 1. A Sample Sequence Database

Sequence Id.

1
2
3
4

Sequence
(cid:2)(a)(ab)(bc)(cid:3)
(cid:2)(a)(abc)(cid:3)
(cid:2)(d)(a)(ab)(bc)(cid:3)
(cid:2)(d)(ad)(cid:3)

A sequence database D is collection of input sequences D = (cid:3)s1s2 . . . sn(cid:4),

incrementally ordered by the identiﬁer of the contained sequences. In table 1 we
show a sample input database D with four input sequences.

Deﬁnition 1. The support (or frequency) of a sequence, denoted as σ(α, D), is
the total number of input sequences in the input database D that contain α. A
pattern or sequence is called frequent if it occurs at least a given user speciﬁed
threshold min sup, called the minimum support. FS is the whole collection of
frequent sequences. The problem of frequent sequence mining is now to ﬁnd FS
in a given input database, for a given minimum support threshold.

n of α(cid:2)

= (cid:3)I1I2 . . . I(cid:2)

Given a sequence α = (cid:3)I1I2 . . . In(cid:4) and an item ei, we deﬁne the s-extension α(cid:2)
as the super-sequence of α, extending it with a new itemset containing a single
= (cid:3)I1I2 . . . InIn+1(cid:4), In+1 = (ei). We deﬁne the i-extension of α if the
item ei, α(cid:2)
n = In ∪ ei). That is, the item
last itemset I(cid:2)
ei is added to In. For instance, given the sequence α = (cid:3)(a)(b)(cid:4) and an item
c ∈ I, the sequence β = (cid:3)(a)(b)(c)(cid:4) is an s-extension and γ = (cid:3)(a)(bc)(cid:4) is an
i-extension.

n(cid:4) satisﬁes (I(cid:2)

Given two sequences β and γ such that both are s-extensions (or i-extensions)
of a common preﬁx α, with items ei and ej respectively, we say β precedes γ,
β < γ, if ei <lex ej in a lexicographic order. If, on the contrary, one of them is
an s-extension and the other one is i-extension, the s-extension always precedes
the i-extension.

Deﬁnition 2. If a frequent sequence α does not have another supersequence
with the same support, we say that α is a closed sequence. Otherwise, if a
frequent sequence β has a super-sequence γ with exactly the same support, we
say that β is a non-closed sequence and γ absorbs β. The whole set of frequent
closed sequences is denoted by FCS. More formally, α ∈ FCS if ∀β ∈ FS, α (cid:8)
β, σ(α, D) (cid:10)= σ(β, D). The problem of closed sequence mining is now to ﬁnd FCS
in a given input database, for a given minimum support threshold.

Clearly, the collection of frequent closed sequences is smaller than the collection
of all frequent sequences.

Example 1. In our sample database, shown in table 1, for a support min sup =
2, we ﬁnd |FCS| = 5 frequent closed sequences, FCS = {(cid:3)(a)(cid:4), (cid:3)(d)(a)(cid:4), (cid:3)(a)(ab)(cid:4),
(cid:3)(a)(bc)(cid:4), (cid:3)(a)(ab)(bc)(cid:4)}, while the corresponding FS has 27 frequent sequences.

ClaSP: An Eﬃcient Algorithm for Mining Frequent Closed Sequences

53

3 Related works

Looking for frequent sequences in sequence databases was ﬁrst proposed by
Agrawal and Srikant [1,6]. Their algorithms (apriori-based) consist of execut-
ing a continuous loop of a candidate generation phase followed by a support
checking phase. Two main drawbacks appear in those algorithms: 1) they need
to do several scans of the database to check the support of the candidates; and
2) a breath-ﬁrst search is needed for the candidate generation, leading to high
memory consumption.

Later, two other strategies were proposed: 1) depth-ﬁrst search based on a
vertical database format [9] and 2) projected pattern growth [3,5]. The vertical
database format strategy was created by Zaki in the Spade algorithm [9] which is
capable of obtaining the frequent sequences without making several scans of the
input database. His algorithm allows the decomposing of the original search tree
in independent problems that can be solved in parallel in a depth-ﬁrst search
(DFS), thus enabling the processing of big databases.

The Pattern growth strategy was introduced by Han et al. [3] and it consists
in algorithms that obtain the whole frequent sequence set by mean of techniques
based on the so called projected pattern growth. The most representative algo-
rithm in this strategy is PreﬁxSpan [5]. PreﬁxSpan deﬁnes a projected database
as the set of suﬃxes with respect to a given preﬁx sequence. After projecting
by a sequence, new frequent items are identiﬁed. This process is applied in a
recursive manner by means of DFS, identifying the new frequent sequences as
the concatenation of the preﬁx sequence with the frequent items that are found.
Preﬁxspan shows good performance and scales well in memory, especially with
sparse databases or when databases mainly consist of small itemsets. However,
when we deal with large dense databases that have large itemsets, the perfor-
mance of Preﬁxpan is worse than that of Spade. In order to show this issue, we
have conducted several tests. In a sequential database, several important prop-
erties have an inﬂuence on the algorithms execution, some of which are shown
in Table 2.

Table 2. Parameters for IBM Quest Data Generator

Abbr.

Meaning

Number of sequences (in 000s)
Average itemset in a sequence

D
C
T
N
S Average itemsets in maximal sequences
I

Average items in maximal sequences

Average items in a itemset

Number of diﬀerent items (in 000s)

We deﬁne the database density as the quotient δ = T

N . We have used the
well-known data generator provided by IBM to run Spade and PreﬁxSpan un-
der diﬀerent conﬁgurations. In Figures 1 and 2 we can observe the behaviour of
both Spade and Preﬁxspan when we vary the density and the number of itemsets.

54

A. Gomariz et al.

In ﬁgure 1 we show the running time of the algorithms with a diﬀerent number
of items (100, 500 and 2500 items) with a constant T = 20 value. Since for a
database, the density grows either if the numerator increases or the denominator
decreases, the ﬁgures have been obtained just varying the denominator. Besides,
ﬁgure 2 depicts the behaviour of the algorithms when the number of itemsets is
changed between values of C ∈ {10, 40, 80} while we keep the density constant
(δ = 20
2500 ). The higher δ the more dense the database. We can see that PreﬁxS-
pan shows good results when both density and the number of itemsets are low,
but when a database is denser and parameter C grows, we notice how Spade
outperforms Preﬁxspan.

Fig. 1. Behaviour of Spade and PreﬁxSpan when density changes (in the number of
items)

Fig. 2. Behaviour of Spade and PreﬁxSpan when the number of itemsets changes

For mining closed sequences, there exist two approaches: 1) run any algorithm
for mining all frequent sequences and execute a post-processing step to ﬁlter out
the set of closed sequences, or 2) obtain the set of closed sequences by gradually
discarding the non-closed ones. Some algorithms have been developed to ﬁnd the
complete set of closed sequences. The most important algorithms developed so
far, are CloSpan [8] and Bide [7], both derived from Preﬁxspan. While CloSpan
uses a preﬁx tree to store the sequences and uses two methods to prune non-
frequent sequences, Bide executes some checking steps in the original database
that allows it to avoid maintaining the sequence tree in memory. However, to
the best of our knowledge, there exist no algorithms for closed sequence mining
based on the vertical database format as is presented here.

ClaSP: An Eﬃcient Algorithm for Mining Frequent Closed Sequences

55

4 ClaSP: Algorithm and Implementation

In this section, we formulate and explain every step of our ClaSP algorithm.
ClaSP has two main phases: The ﬁrst one generates a subset of FS (and superset
of FCS) called Frequent Closed Candidates (FCC), that is kept in main memory;
and the second step executes a post-pruning phase to eliminate from FCC all
non-closed sequences to ﬁnally obtain exactly FCS.

Algorithm 1. ClaSP
1: F1 = {frequent 1-sequences}
2: FCC = ∅, FCS = ∅
i ∈ F1 do
3: for all
4:
5:
6:
7: end for
8: FCS = N-ClosedStep(FCC)
Ensure: The ﬁnal closed frequent pattern set FCS

Fie = {frequent 1-sequences greater than i}
FCCi=DFS-PRUNING(i,F1,Fie)
FCC = FCC ∪ FCCi

Algorithm 1, ClaSP, shows the pseudocode corresponding to the two main
steps. It ﬁrst ﬁnds every frequent 1-sequence, and after that, for all of frequent
1-sequences, the method DFS-Pruning is called recursively to explore the cor-
responding subtree (by doing a depth-ﬁrst search). FCC is obtained when this
process is done for all of the frequent 1-sequences and, ﬁnally, the algorithm ends
removing the non-closed sequences that appear in FCC.

Algorithm 2, DFS-Pruning, executes recursively both the candidate genera-
tion (by means of i-extensions and s-extensions) and the support checking, re-
turning a part of FCC relative to the pattern p taken as parameter. The method
takes as parameters two sets with the candidate items to do s-extensions and i-
extensions respectively (Sn and In sets). The algorithm ﬁrst checks if the current
pattern p can be discarded, by using the method checkAvoidable (this algorithm
is explained later in algorithm 5). Lines 4-9 perform all the s-extensions for the
pattern p and keep in Stemp the items which make frequent extensions. In line
10, the method ExpSiblings (algorithm 4) is called, and there, DFS-Pruning is
executed for each new frequent s-extensions. Lines 11-16 and 17 perform the
same steps, with i-extensions. Finally, in line 19, the complete frequent patterns
set (with a preﬁx p) is returned.

To store the patterns in memory, we use a lexicographic sequence tree. The
elements in the tree are sorted by a lexicographic order according to extension
comparisons (see section 2). In ﬁgure 3 we show the associated sequence tree for
FS in our example and we denote an s-extension with a line, and an i-extension
with a dotted line. This tree is traversed by algorithms 1, 2 and 4, using a
depth-ﬁrst traversal.

There are two main diﬀerent changes added in ClaSP with respect to SPADE:
(1) the step to check if the subtree of a pattern can be skipped (line 3 of algorithm
2), and (2) the step where the remaining non-closed patterns are eliminated (line
6 of algorithm 1).

56

A. Gomariz et al.

6:
7:
8:
9:
10:

11:
12:

13:
14:
15:
16:
17:

Algorithm 2. DFS-Pruning(p, Sn, In)
Require: Current

frequent pattern p =
(s1, s2, . . . , sn), set of items for s-extension
Sn, set of items for i-extension In

1: Stemp = ∅, Itemp = ∅
2: Fi = ∅, Ps = ∅, Pi = ∅
3: if (not checkAvoidable( p, I(Dp) )) then
i ∈ Sn do
4:
= (s1, s2, . . . , sn, {i}) is fre-
5:

for all
if (p(cid:2)
quent) then

Stemp = Stemp ∪ {i}
Ps = Ps ∪ {p(cid:2)}

end if

∪

end for
Fi
ExpSiblings(Ps,Stemp,Stemp)
for all
if (p(cid:2)
quent) then

Fi
=
i ∈ In do
= (s1, s2, . . . , sn ∪ {i}) is fre-

Ps

∪

Itemp = Itemp ∪ {i}
Pi = Pi ∪ {p(cid:2)}

end if

end for
Fi
ExpSiblings(Ps,Stemp,Itemp)

Fi

∪

=

Pi

∪

18: end if
19: return Fi
Ensure: Frequent pattern set Fi of this node

and its siblings

FCC

Algorithm 3. N-ClosedStep(FCC)
Require: A frequent closed candidates set
1: FCS = ∅
2: A hash table H is created
3: for all p ∈ FCC do
4:
5: end for
6: for all entry e ∈ H do
7:
8:
9:

Add a new entry (cid:6)T (Dp), p(cid:7) in H

for all pj ∈ e, j > i do

for all pi ∈ e do

if (pi.support() = pj .support())
then

if (pi (cid:8) pj ) then

else

end if

Remove pj from e

Remove pi from e
if (pj (cid:8) pi) then

10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22: end for
23: return FCS
Ensure: The ﬁnal closed frequent pattern set

end for
FCe= all patterns p ∈ e
FCS = FCS ∪ FCe

end for

end if

end if

FCS

Algorithm 4. ExpSiblings(P, Ss, Si)
Require: The pattern set P which contains all
the patterns whose children are going to be
explore, the set of valid items Ss which gen-
erate the P set by means of s-extensions,
the set of valid items Si which generate the
P set by means of i-extensions

1: Fs = ∅
2: for all p ∈ P do
3:

I = Elements in Si greater than the last
item ei in p
Fs = Fs ∪ DFS-Pruning(p,Ss,I)

4:
5: end for
6: return Fs
Ensure: Frequent pattern set Fs for all of the

patterns’ siblings

hash table H

Algorithm 5. CheckAvoidable(p, k)
Require: a frequent pattern p, its hash-key k,
1: Mk = Entries in the hash table with key k
2: if (Mk = ∅) then
3:
4: else
5:
6:
7:
8:
9:
10:

Insert a new entry (cid:6)k,p(cid:7) in H
for all pair m ∈ Mk do

p(cid:2)
if (p.support()=p(cid:2).support()) then

//Backward sub-pattern
if (p (cid:8) p(cid:2)

p has the same descendants as
p(cid:2)
, so p points to p(cid:2)
descendants

= m.value()

) then

11:
12:
13:
14:
15:

16:

17:
18:
19:
20:
21:
22:

return true

else

//Backward super-pattern
if (p(cid:2) (cid:8) p) then

, so p points to p(cid:2)

p has the same descendants
as p(cid:2)
de-
scendants
Remove the current entry
(cid:6)k, p(cid:2)(cid:7) from H

end if

end if

end if

end for
//Backward super-pattern executed?
if (Pruning method 2 has been accom-
plished) then

end if

Add a new entry (cid:6)k, p(cid:7) in H
return true

23:
24:
25:
26: end if
27: //k does not exist in the hash table or it
does exist but the present patterns are not
related with p

28: Add a new entry (cid:6)k, p(cid:7) in H
29: return false
Ensure: It answers if the generation of p can

be avoided

ClaSP: An Eﬃcient Algorithm for Mining Frequent Closed Sequences

57

To prune the space search, ClaSP used the method CheckAvoidable that is
inspired on the pruning methods used in CloSpan. This method tries to ﬁnd
those patterns p = (cid:3)α ej(cid:4) and p(cid:2)
= (cid:3)α ei ej(cid:4), such that, all of the appearances of
p are in those of p(cid:2)
, i.e., if every time we ﬁnd a sequence α followed by an item ej,
there exists an item ei between them, then we can safely avoid the exploration
of the subtree associated to the pattern p. In order to ﬁnd this kind of patterns,
we deﬁne two numbers: 1) l(s, p), is the size of all the suﬃxes with respect to p
in sequence s, and 2) I(Dp) =
l(si, p), the total number of remaining items
with respect to p for the database D, i.e. the addition of all of l(s, p) for every
sequence in the database. Using I(D) and the subsequence checking operation,
in algorithm 5, ClaSP checks the equivalence between the I(D) values for two
, if I(Ds) = I(Ds(cid:2) ), we
patterns: Given two sequences, s and s(cid:2)
can deduce that the support for all of their descendants is just the same.

, such that s (cid:8) s(cid:2)

(cid:2)n

i=1

Fig. 3. Whole lexicographic sequence
tree for our thorough example

Fig. 4. Whole lexicographic sequence
tree after processing ClaSP algorithm

In algorithm 5, the pruning phase is implemented by two methods: 1) Back-
ward sub-pattern checking and 2) Backward super-pattern checking. The ﬁrst
one (lines 8-10) occurs when we ﬁnd a pattern which is a subsequence of a pat-
tern previously found with the same I(D) value. In that case, we can avoid
exploring this new branch in the tree for this new pattern. The second method
(lines 12-16 and 20-24) is the opposite situation and it occurs when we ﬁnd a
pattern that is a super-sequence of another pattern previously found with the
same I(D) value. In this case we can transplant the descendants of the previous
pattern to the node of this new pattern.

In ﬁgure 4 we show the ClaSP search tree w.r.t. our example without all
pruned nodes. In our implementation, to store the relevant branches, we deﬁne
a hash function with I(D) value as key and the pattern (i.e. the node in the tree
for that pattern) as value ((cid:3)I(Dp), p(cid:4)). We use a global hash table and, when
we ﬁnd a sequence p, if the backward sub-pattern condition is accomplished, we
do not put the pair (cid:3)I(Dp), p(cid:4), whereas, if the backward super-pattern condition
is true, we replace all the previous pairs (cid:3)I(Dp(cid:2) ), p(cid:2)(cid:4) (s.t. p(cid:2) (cid:8) p) with the new
one (cid:3)I(Dp), p(cid:4). If instead we do not ﬁnd any pattern with the same I(D) value
of the pattern p, or those patterns with the same value are not related with p
by means of the subsequence operation, we put the pair (cid:3)I(Dp), p(cid:4) to the global
hash table (line 28). Note that when one of the two pruning conditions is true,

A. Gomariz et al.

58
is the same since two I(Dp) and
we also need to check if the support for s and s(cid:2)
I(Dp(cid:2) ) values can be equal but they do not necessarily have the same support.
We also need to consider all of the I(D)s for every appearance in a sequence.
For instance, in our example shown in table 1, regarding the three ﬁrst sequences,
if we consider just the ﬁrst I(Ds) for the ﬁrst appearance, if we have the pattern
(cid:3)(a)(b)(cid:4) in our example, we deduce that I(D(cid:6)(a)(ab)(cid:7)) = I(D(cid:6)(a)(b)(cid:7)) (both with
value I(D) = 5), so we can avoid generating the descendants of (cid:3)(a)(b)(cid:4) because
the are the same as in (cid:3)(a)(ab)(cid:4). However, we can check that (cid:3)(a)(bc)(cid:4) is frequent
(with support 3), whereas (cid:3)(a)(abc)(cid:4) is not (support 1). This forces us to count
in I(Ds), all the number of items after every appearance.

Finally, regarding the non-closed pattern elimination (algorithm 3), the pro-
cess consists of using a hash function with the support of a pattern as key and
the pattern itself as value. If two patterns have the same support we check if
one contains the other, and if this condition is satisﬁed, we remove the shorter
pattern. Since the support value as key provoke a high number of collisions in the
hash table (implemented with closed addressing), we use a T (Dp) =
id(si)
value, deﬁned as the sum of all sequence ids where a pattern p appears. How-
ever, as the equivalence of T (Dp) does not imply the equivalence of support, after
checking that two patterns have the same T (Dp) value, those patterns have to
have the same support to remove one of them.

(cid:2)n

i=1

5 Performance Study

We exhaustively experimented on both synthetic and real world datasets. To
generate the synthetic data, we have used the IBM data generator mentioned
above (see section 3). In all our experiments we compare the performance of
three algorithms: CloSpan, ClaSP and Spade. For the last algorithm we add the
same non-closed candidate elimination phase which is used in ClaSP to obtain
FCS.

All experiments are done on a 4-cores of 2.4GHZ Intel Xeon, running Linux
Ubuntu 10.04 Server edition. All the three algorithms are implemented in Java
6 SE with a Java Virtual Machine of 16GB of main memory.

Figure 5 shows the number of patterns and performance for the dataset
D5C10T5N5S6I4 (-rept 1 -seq.npats 2000 -lit.npats 5000). Figure 5(a) shows the
number of frequent patterns, the patterns processed by ClaSP, and the number
of closed patterns. We can see how there is approximately an order of diﬀerence
between these numbers, i.e. for every 100 frequent patterns we process around
10 patterns by ClaSP, and approximately only 1 of these 10 patterns is closed.
Figure 5(b) shows the running time. ClaSP clearly outperforms both Spade and
Clospan. For very low support (below 0.013), we have problems with the execu-
tion of Spade due to the space in memory taken for the algorithm.

Figure 6 shows a dataset with larger parameters of C, T and a lower N. This
database is denser than the database above and in ﬁgure 6(a) we can observe
that the diﬀerence between frequent patterns and processed patterns is not so
big. Therefore, the pruning method checkAvoidable is not so eﬀective and ClaSP

ClaSP: An Eﬃcient Algorithm for Mining Frequent Closed Sequences

59

Fig. 5. Varying Support for Dataset D5C10T5N5S6I4(-seq.npats 2000 -lit.npats 5000)

and CloSpan are closer to the normal behavior of Spade and PreﬁxSpan, as is
shown in ﬁgure 6(b). The results show that both ClaSP and Spade are much
faster than Clospan.

Fig. 6. Varying Support for Dataset D0.5C20T10N2.5S6I4(-seq.npats 2000 -lit.npats
5000)

Finally we test our algorithm with the gazelle dataset. This dataset comes
from click-stream data from gazelle.com, which no longer exists. The dataset
was once used in KDDCup-2000 competition and, basically, it includes a set of
page views (each page contains a speciﬁc product information) in a legwear and
legcare website. Each session contains page views done by a customer over a short
period. Product pages viewed in one session are considered as an itemset, and
diﬀerent sessions for one user is considered as a sequence. The database contains
1423 diﬀerent products and assortments which are viewed by 29369 diﬀerent
users. There are 29369 sequences, 35722 sessions (itemsets), and 87546 page

60

A. Gomariz et al.

views (items). The average number of sessions in a sequence is around 1. The
average number of pageviews in a session is 2. The largest session contains 342
views, the longest sequence has 140 sessions, and the largest sequence contains
651 page views. Figure 7 shows the runtime with several support (from 0.03%
to 0.015%). We compare the runtime behavior for both ClaSP and CloSpan and
we can see how ClaSP outperforms CloSpan.

Fig. 7. Varying Support for Dataset Gazelle click stream

All the experiments show that ClaSP outperforms CloSpan, even if databases
are sparse. This is because of, in very sparse databases, a high number of patterns
are found only with extremely low support. Therefore, the lower the support is,
the more patterns are found and the more items are chosen to create patterns.
In this point, CloSpan, as PreﬁxSpan, is penalized since the algorithm has to
projects several times the same item in the same sequence, having a worse time
with respect to ClaSP. Besides, in those algorithms where Spade is better than
Preﬁxspan, ClaSP is also faster than CloSpan.

6 Conclusions

In this paper, we study the principles for mining closed frequent sequential pat-
terns and we compare the two main existing strategies to ﬁnd frequent patterns.
We show the beneﬁts of using the vertical database format strategy against
pattern growth algorithms, especially when facing dense datasets. Then, we in-
troduced a new algorithm called ClaSP to mine frequent closed sequences. This
algorithm is inspired on the Spade algorithm using a vertical database format
strategy and uses a heuristic to prune non-closed sequences inspired by the
CloSpan algorithm. To the best of our knowledge, this is the ﬁrst work based
on the vertical database strategy to solve the closed sequential pattern mining
problem. In all our tests ClaSP outperforms CloSpan, and even, under certain
datasets conﬁguration, Spade also outperforms CloSpan when the non-closed
elimination phase is executed after it.

ClaSP: An Eﬃcient Algorithm for Mining Frequent Closed Sequences

61

References

1. Agrawal, R., Srikant, R.: Mining sequential patterns. In: Proceedings of the Eleventh

International Conference on Data Engineering, pp. 3–14. IEEE (1995)

2. Ayres, J., Flannick, J., Gehrke, J., Yiu, T.: Sequential pattern mining using a bitmap
representation. In: Proceedings of the Eighth ACM SIGKDD International Confer-
ence on Knowledge Discovery and Data Mining, pp. 429–435. ACM (2002)

3. Han, J., Pei, J., Mortazavi-Asl, B., Chen, Q.: FreeSpan: frequent pattern-projected
sequential pattern mining. In: Proceedings of the Sixth ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, pp. 355–359 (2000)

4. Pasquier, N., Bastide, Y., Taouil, R., Lakhal, L.: Discovering Frequent Closed Item-
sets for Association Rules. In: Beeri, C., Bruneman, P. (eds.) ICDT 1999. LNCS,
vol. 1540, pp. 398–416. Springer, Heidelberg (1998)

5. Pei, J., Han, J., Mortazavi-Asl, B., Wang, J., Pinto, H., Chen, Q., Dayal, U., Hsu,
M.: Mining sequential patterns by pattern-growth: the PreﬁxSpan approach. IEEE
Transactions on Knowledge and Data Engineering 16(11), 1424–1440 (2004)

6. Srikant, R., Agrawal, R.: Mining Sequential Patterns: Generalizations and Per-
formance Improvements. In: Apers, P.M.G., Bouzeghoub, M., Gardarin, G. (eds.)
EDBT 1996. LNCS, vol. 1057, pp. 3–17. Springer, Heidelberg (1996)

7. Wang, J., Han, J., Li, C.: Frequent closed sequence mining without candidate main-
tenance. IEEE Transactions on Knowledge and Data Engineering 19(8), 1042–1056
(2007)

8. Yan, X., Han, J., Afshar, R.: CloSpan: Mining closed sequential patterns in large
datasets. In: Proceedings of SIAM International Conference on Data Mining, pp.
166–177 (2003)

9. Zaki, M.J.: SPADE: An eﬃcient algorithm for mining frequent sequences. Machine

Learning 42(1), 31–60 (2001)


