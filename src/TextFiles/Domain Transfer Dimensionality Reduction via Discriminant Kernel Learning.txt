Domain Transfer Dimensionality Reduction

via Discriminant Kernel Learning

Ming Zeng and Jiangtao Ren

Sun Yat-Sen University, Guangzhou, 510006, China

mingtsang.zm@gmail.com,
issrjt@mail.sysu.edu.cn

Abstract. Kernel discriminant analysis (KDA) is a popular technique
for discriminative dimensionality reduction in data analysis. But, when a
limited number of labeled data is available, it is often hard to extract the
required low dimensional representation from a high dimensional feature
space. Thus, one expects to improve the performance with the labeled
data in other domains. In this paper, we propose a method, referred to as
the domain transfer discriminant kernel learning (DTDKL), to ﬁnd the
optimal kernel by using the other labeled data from out-of-domain distri-
bution to carry out discriminant dimensionality reduction. Our method
learns a kernel function and discriminative projection by maximizing the
Fisher discriminant distance and minimizing the mismatch between the
in-domain and out-of-domain distributions simultaneously, by which we
may get a better feature space for discriminative dimensionality reduc-
tion with cross-domain.

Keywords: Discriminant Kernel Learning, Dimensionality Reduction,
Transfer Learning.

1

Introduction

In many real-world applications, such as image processing, computational biol-
ogy and natural language processing, the dimensionality of data is usually very
high. Due to the complexity and noise of high-dimensional data, the eﬀectiveness
of regression or classiﬁcation is limited. This can be improved via dimensionality
reduction which ﬁnds a compact representation of the data for classiﬁcation.

A more popular technique for dimensionality reduction is discriminant anal-
ysis. To handle nonlinear problems, the kernel discriminant analysis (KDA) is
proposed in [1], which computes the discriminative projection from the data set
that is mapped nonlinearly into the reproducing kernel Hilbert space (RKHS).
We observe that the kernel is chosen before learning in the KDA method. How-
ever, the kernel-based learning methods are desirable when integrating the tuning
of kernel into the learning space.

In addition, the discriminant multiple kernel learning methods require a plenty
of labeled samples to discriminate the unlabeled data from each class. In real-
world applications, it is usually costly or even impossible to get such a huge

P.-N. Tan et al. (Eds.): PAKDD 2012, Part II, LNAI 7302, pp. 280–291, 2012.
c(cid:2) Springer-Verlag Berlin Heidelberg 2012

Domain Transfer Dimensionality Reduction

281

number of labeled samples from the same distribution. When this situation oc-
curs, the performance of discriminant kernel learning methods is poor. Then one
expects to carry out discriminant analysis with the help of other related labeled
data from other domains. This brings out the cross-domain problem since the ex-
isting discriminant kernel learning makes the assumption that the training data
and the test data are independent and identical. To resolve this problem, cross
transfer learning is proposed, whose aim is to improve learning in the in-domain
by porting the labeled sample from out-of-domain to that from in-domain to
carry out dimensionality reduction. Several works have been done by combining
unsupervised dimensionality with clustering, such as transferred dimensionality
analysis(TDA) [2], which intends to select the most discriminative subspace and
clustering at the same time. Maximum mean discrepancy embedding(MMDE)[3]
tries to ﬁnd a subspace where training and test samples distribute similarly to
solve the sample selection bias problem in an unsupervised way. S.Si et al.[4]
proposed using evolutionary cross-domain discriminative Hessian eigenmaps by
minimizing the quadratic distance between the distribution of the training set
and that of the test set. However, it could not solve non-linear problems.

In this paper, we develop a new dimensionality reduction method, called do-
main transfer discriminant kernel learning method (DTDKL), which transfers
the knowledge from labeled data in out-of-domain to the in-domain by explicitly
carrying out kernel discriminant learning and transfer leaning in a coherent way.
More speciﬁcally, DTDKL tries to ﬁnd a projection to maximize the Fisher dis-
criminant ratio in the optimal feature space and minimize the maximum mean
discrepancy (MMD) of the diﬀerent distributions simultaneously. In fact, DT-
DKL provides a method to learn an optimal kernel function and discriminant
projection at the same time.
The key contributions of the paper can be highlighted as follows:

– To the best of our knowledge, DTDKL is the ﬁrst semi-supervised
cross-domain discriminant kernel learning method. In contrast to the prior
discriminant kernel learning method, DTDKL does not assume that the
training and test data are drawn from the same distribution. Moreover, a
novel dimensionality reduction method with cross-domain is proposed, whose
objects are to maximize the Fisher discriminant ratio while minimizing the
maximum mean discrepancy of diﬀerent distributions.

– By comparing the state-of-the-art dimensionality reduction methods, DT-
DKL performs better in the dataset of SyskillWebert, Reuters-21578 and
20-Newsgroup ensuring promising performance in real applications.

The rest of this paper is organized as follows: Section 2 presents the related
works and preliminaries of DTDKL; Section 3 proposes DTDKL method by
embedding maximum mean discrepancy (MMD) into discriminant analysis to
tackle the cross-domain problem; Section 4 presents our experimental results to
demonstrate its applications. Finally, we conclude the study in Section 5.

282

M. Zeng and J. Ren

2 Brief Review of Prior Work

2.1 Discriminant Multiple Kernel Learning

Dimensionality reduction has always attracted amount of attention. Various meth-
ods have been proposed in a recent survey [5] to solve this problem. The canoni-
cal dimensionality reduction algorithm is linear discriminant analysis (LDA) [6],
which is ﬁnding the most discriminative subspace for diﬀerent classes in the origi-
nal space. And with the development of kernel-based methods, kernel discriminant
analysis has received a lot of interest for nonlinear problems. The KDA algorithm
ﬁnds the direction in a feature space, deﬁned by a kernel function, onto which
the projections of diﬀerent classes are well separated [1,7]. Note that the kernel
function plays a crucial role in kernel methods, and Lanckriet et al. [8] pioneered
the work of multiple kernel learning (MKL) in which the optimal kernel is ob-
tained as a linear combination of pre-determined kernel matrices. Based on ideas
of MKL, the kernel-based learning method for discriminant analysis was reformu-
lated as semi-deﬁnite programming (SDP) in Kim et al. [9]. Ye et al. [10] improved
the eﬃciency of the problem and extended naturally to the multi-class setting by
casting the SDP formulation in quadratically constrained quadratic programming
(QCQP) and semi-inﬁnite linear programming (SILP).

2.2 Transfer Learning and Maximum Mean Discrepancy

Formulation

Semi-supervised learning aims to make use of unlabeled data in the process
of supervised learning and it has also been widely used in many areas related
to transfer learning. One of the typical branches is to ﬁnd criteria to estimate
the distance between diﬀerent distributions. A well-known example is Kullback-
Leibler (K-L) divergence. Many criteria are parametric for the reason that an
intermediate density estimate is usually required. To avoid parametric estima-
tion, some nonparametric methods are proposed to evaluate the distance between
the diﬀerent distributions of data sets. Maximum Mean Discrepancy (MMD) is
a eﬀective nonparametric criterion for comparing distributions based on RKHS

[11]. Suppose F be a class of functions f : X → R, and X = (x1, . . . , xn1 ),
Y = (y1, . . . , yn2) be random variable sets drawn from distributions P and Q,
respectively. The maximum mean discrepancy and its empirical estimate is as
follows:

(cid:2)

(cid:4)

MMD[F, X, Y ] := sup
f∈F

1
n1

f (xi) − 1
n2

f (yi)

(1)

The function space F could be replaced by H which is a universal RKHS. By
the fact that in RKHS, f (x) can be expressed as an inner product via f (x) =
(cid:3)ϕ(x), f(cid:4)H, where ϕ(x) : X → H, then one may rewrite MMD as follows:

n1(cid:3)

i=1

n2(cid:3)

i=1

n2(cid:3)

i=1

(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)

H

(cid:5)(cid:5)(cid:5)(cid:5)(cid:5) 1

n1

n1(cid:3)

i=1

MMD =

ϕ(xi) − 1
n2

ϕ(yi)

= (cid:5)μ1 − μ2(cid:5)H

Domain Transfer Dimensionality Reduction

283

In terms of the MMD theory [11], the distance between distributions of two
samples is equivalent to the distance between the means of the two samples
mapped into a RKHS.

i

, yout

i

l = |Din

3 Semi-supervised Discriminant Analysis in Cross-Domain
Let X ⊆ IRd and Y = {−1, +1} denote the input space and the output space,
respectively. Let Dout = {(xout
) ∈ X × Y : 1 ≤ i ≤ nout} be the set of
out-of-domain data samples with nout = |Dout|, and Din = Din
u be the
i ) ∈ X × Y : 1 ≤ i ≤ nin
l }
set of in-domain data samples where Din
u = |Din
u |.
with nin
Typically, nin
u . Let P and Q be the marginal distribution of Din and
Dout, respectively. We assume that the nout out-of-domain samples and the
nin in-domain samples are drawn independently and identically from a ﬁxed but
unknown underlying probability distribution P and Q, respectively. Our task is to
predict the labels yin
, . . . , xin
n
in the in-domain data set.

l ∪ Din
i , yin
≤ i ≤ nin} with nin

n , which corresponds to the inputs xin

i ∈ X : nin

l = {(xin

|, and Din

u = {xin

l

l (cid:10) nin

, . . . , yin

nl+1

nl+1

l+1

3.1 Standard Discriminant Kernel Learning Analysis

The standard kernel discriminant analysis learns the kernel and the direction
from the labeled samples in Din
in order to project the unlabeled samples in Din
u .
Let K : X×X → IR be a kernel function. Then, Mercer’s Theorem [12] tells us the
l
kernel function implicitly maps the input space X to a high-dimensional (possibly
inﬁnite) Hilbert space H equipped with the inner product (cid:3)·,·(cid:4)H through a map
ϕ : K → H:

K(x, z) = (cid:3)ϕ(x), ϕ(z)(cid:4)H,

∀x, z ∈ X.

This space is called the feature space, and the mapping is called the feature
mapping. They depend on the kernel function K and will be denoted as ϕK and
HK.

Let x+ and x− denote the collection of data points from positive and negative
classes, respectively. Then the total number of data points in the training set
is nl = n+ + n−. The standard kernel discriminant analysis [9] learns the
Din
kernel K and direction w ∈ Hk via the optimization problem
l

n+(cid:3)

i=1

μ+

K =

1
n+

ϕK (xi), μ−

K =

1
n−

ϕK(xi),

i=n++1

nl(cid:3)

max
w,K

F (w, K) =

K − μ−

K))2

(wT (μ+

wT (Σ+

K + Σ−

K + λI)w

s.t. K =

θiKi, 1T θ = 1, θ (cid:12) 0,

(2)

p(cid:3)

i=1

where K1, . . . , Kp be the given p based kernels, θ (cid:12) 0 means its elements θi are
nonnegative, λ > 0 is a regularization parameter, I is the identity operator in
HK, μ+

K are the sample means

K and μ−

284

M. Zeng and J. Ren

and Σ+

K and Σ−

K are the sample covariances

n+(cid:3)
(ϕK(xi) − μ+
nl(cid:3)

i=1

i=n++1

Σ+

K =

Σ−
K =

1
n+

1
n−

K)(ϕK (xi) − μ+

K)T ,

(ϕK (xi) − μ−

K)(ϕK (xi) − μ−

K)T .

Note that (2) is a supervised learning model which neglects the knowledge of the
unlabeled data, and hence can not yield the favorable classiﬁcation. In addition,
this model requires all samples to come from the identical distribution, which
means that it can not deal with the cross-domain problem. Motivated by this,
we propose a domain transfer kernel learning method in the next subsection.

3.2 Domain Transfer Kernel Learning for Discriminant Analysis

Note that if the distributions P(x) and Q(x) are completely independent, then
the out-of-domain data Dout is useless; if P(x) and Q(x) are identical, then the
cross-domain problem becomes the standard classiﬁcation problem. However, in
most cases P(x) and Q(x) are neither independent nor identical, for which we
may use the cross-domain projection vector that is learned from out-of-domain
data set Dout and the in-domain data set Din with MMD formulation. Then,
the optimization problem of DTDKL can be formulated as:

KLDAK,w(Dl) − βMMD2

K (Dout, Din),

max
w,K

(3)
where β ≥ 0 is a parameter to balance the diﬀerence of data distributions of two
domains and the Fisher discriminant ratio of KLDA for labeled samples. This
optimization problem involves two classes of variables. One is the kernel matrix
K which represents the adaptive feature space, and the other is the projection
direction w for the dimensionality reduction.
K(Dout, Din) as F (w, K) and (cid:5)μin−
By specializing KLDAK,w(Dl) and MMD2
μout(cid:5), respectively, (3) becomes

max
w,K

Fλ,β(w, K)

s.t. K =

p(cid:3)

i=1

θiKi, 1T θ = 1, θ (cid:12) 0
(cid:5)(cid:5)μin
K − μout

− β

K))2

K − μ−

K

(cid:5)(cid:5)2 .

(4)

(5)

(wT (μ+

wT (Σ+

K + Σ−

K + λI)w

where

Fλ,β(w, K) =
K, Σ−

K, μ−

K, Σ+

and μ+
K denote the training samples’ (in-domain and out-of-domain)
means and covariances, respectively. Comparing with the model (2), we see that
a new term −(cid:5)μin− μout(cid:5)2 is introduced into the objective of (4). This term is

Domain Transfer Dimensionality Reduction

285

a concave function that will bring a concaviﬁcation eﬀect on the original non-
concave objective of (2). So, the globally optimal solution of the maximization
problem (4) can be easier found than that of the problem (2) proposed in [9] .
Note that the last term in Fλ,β(w, K) is independent of w. Hence, the maxi-

mization problem (4) can be rewritten as

(cid:6)
p(cid:3)

max

K

max

w

s.t. K =

(wT (μ+

K − μ−

K))2

K + Σ−

wT (Σ+
K + λI)w
θiKi, 1T θ = 1, θ (cid:12) 0.

(cid:7)

(cid:5)(cid:5)μin
K − μout

K

(cid:5)(cid:5)2

− β

(6)

i=1

Using the same arguments as in [9], we know that the globally optimal solution
of the inner maximization problem

w∗

= (Σ+

K + Σ−

K + λI)

−1(μ+

K − μ−
K).

Substituting this into the objective of (8), we obtain that

max

K

F ∗
λ,β(K)

p(cid:3)

s.t. K =

θiKi, 1T θ = 1, θ (cid:12) 0

i=1

where

(cid:5)(cid:5)μin
K − μ−
F ∗
K)T (Σ+
λ,β(K) = (μ+
K − μout
−β

(cid:5)(cid:5)2 .
K + Σ−

K

K + λI)

−1(μ+

K − μ−
K)

(7)

(8)

(9)

On the other hand, from the Representer Theory [12], the optimal discriminative
projection in DTDKL is the span of the images of the training points in the
feature space. Note that in this method the training set includes both labeled
data and unlabeled data due to the MMD formulation. Hence, there exists a
vector α ∈ IRn such that

n(cid:3)

w∗

=

α∗
i ϕK (xi) = UKα∗

i=1

where

UK = [ϕK(x1) ··· ϕK(xn)] .
In fact, we can ﬁnd a closed-form expression of α:

α∗

=

1

λ [I − G(λI + GKG)

−1GK]a

where a is an n-dimensional vector given by

a = [1/n+, . . . , 1/n+,−1/n−, . . . ,−1/n−, 0, . . . 0]T ∈ IRn,

(10)

(11)

286

M. Zeng and J. Ren

and the matrix G is deﬁned as
(I − 1

1√
n+

n+ 1n+1T

n+ )

⎛
⎜⎜⎜⎜⎝

G =

0

0

⎞
⎟⎟⎟⎟⎠ .

0

0

n− (I − 1
1√

n−) 0

n− 1n−1T
0

0

Because the unlabel data is introduced in this model, the representation of the
variables are diﬀerent from those of [9]. By equations (7), (10) and (11), F ∗
λ,β (K)
can be written as

F ∗
λ,β(K) = (μ+

K)T (Σ+

K + Σ−

K − μ−
−βbT Kb
K − μ−
KUKα∗ − βbT Kb

K)T w∗ − βbT Kb

= (μ+
= aT U T

K + λI)

−1(μ+

K − μ−
K)

aT K(I −G(λI +GKG)

−1GK)a −βbT Kb

=

1
λ

where b = (b1, . . . , bn) with

bi =

(cid:6)

Then, the optimization problem (8) can be reformulated as

1

nout

if xi ∈ Dout;
− 1
nin if xi ∈ Din.
p(cid:3)

− 1
λ

min
θ,t

θi(aT Kia − λβbT Kib) + t
−1GKa ≤ t,

i=1

s.t. aT KG(λI +GKG)
1T θ = 1, θ (cid:12) 0.
(cid:14)

By the Schur Complement Theorem, we know that

aT KG(λI +GKG)

λI + GKG GKa

aT KG

The last two equations show that (13) is equivalent to

−1GKa ≤ t ⇔
(cid:2)

t − p(cid:3)

θi(a +

(cid:16)

βb)T Ki(a +

βb)

(cid:15)

(cid:12) 0.
(cid:4)

t

(cid:16)

1
λ

i=1

min

t∈IR,θ∈IRp
s.t. S(t, θ) (cid:12) 0
(cid:14)(cid:17)p
(cid:17)p

S(t, θ) =

i=1

1T θ = 1, θ (cid:12) 0,

(cid:17)p

i=1

(cid:15)

.

θiGT Kia
t

θiJ T KiG + λI
i=1

θiaT KiG

where

(12)

(13)

(14)

Domain Transfer Dimensionality Reduction

287

Algorithm 1. Kernel Discriminant Learning in Cross-domain Problem
input : A labeled out-of-domain data set Dout= {xout

i }, an unlabeled
, yout

in-domain data set Din = {xin

i } and positive parameters λ, β.

i

output: Labels Y in of the unlabeled data X in in the in-domain.
1. Solve SDP problem in (14) to obtain a kernel matrix K
2. Compute the coeﬃcient vector α through (11), then the direction w can be
3. Use the direction w to get new representations {xout(cid:2)
4. Train a classiﬁer or regressor: f : xout(cid:2)
5. Use the trained classiﬁer or regressor to predict the labels

obtained from (10)
data {xout

i }, respectively.

i } and {xin

i → yout(cid:2)

i

i

} and {xin(cid:2)

i } of the original

Thus, we convert the nonconvex optimization problem (4) into a convex semidef-
inite programming problem. Similar to the one obtained by [9], it can be solved
by interior-point method softwares such as SeDuMi or SDPT3.

The cost of constructing the basic kernel matrices is O(n2d), the combining the
p basic kernel matrices costs O(n2p), and computing the gradient and Hessian
of the objective is O(n3). so the total cost per Newton step of interior-point
methods which can solve SDP is O(p3 + n2d + n2p + n3). In the case of p, d (cid:10) n,
the total cost grows like O(n3), which is the same as that of SVMs. The SDP
guarantees the convergence of the algorithm.

4 Experiment

In this work, we carried out experiments on three real-world data collections from
two diﬀerent domains to evaluate the described algorithms. The performance is
compared with MKDL-DA [9], and Semi-supervised kernel discriminant analysis
SKDA [13] as well as other transferred dimensionality reduction method, TKDR
[2] and MMDE [3].

4.1 Data Sets and Experiment Setup

As shown in Table 1, the data collections consist of Reuters-21578 [14], 20-
Newsgroups [15] and SyskillWebert [14]. Amoung them, Reuters-21578 and 20
Newsgroups is the standard used to test web page ratings. The important statis-
tics and pre-processing procedures of these collections are presented below.

Data Sets Description. With a hierarchical structure, SyskillWebert database
consists of the HTML source of web pages plus the ratings of a user on those
web pages. Four separate subjects are contained in the web pages. Associated
with each web page are the HTML source and a user’s rating in terms of ”hot”,
”medium” or ”cold” [16]. As demonstrated in Table 1, all of the four subjects
are involved in our study. ”Goat” is reserved as the set of in-domain and the
other are used as the out-of-domain data. Compared to the ”cold” pages, the

288

M. Zeng and J. Ren

total number of pages rated as ”medium” or ”hot” is fewer. Hence, we combine
the ”medium” and ”hot” pages together, and change the labels of those pages as
”non-cold” to form a binary classiﬁcation problem. The learning task is to pre-
dict the user’s preferences for the given web pages. the Rueters-21578 is another
text repository which consists of Reuters news wire articles organized into ﬁve
top categories, and each category contains various sub-categories. Three cate-
gories, ”orgs”, ”people” and ”places”, we remove all the documents of ”USA”
in order to make the size of these three categories nearly even [16]. For each
category, all of the sub-categories are then organized into two parts, and each
part has diﬀerent distribution and approximately equal size. Therefore, one part
can be used for the in-domain and the other is treated as the out-of-domain
purpose. According to the method described in [17], three cross-domain learning
tasks are generated as listed in Table 1, and the learning objective aims to clas-
sify articles into top categories. Similar to Reuters-21578 data, 20-Newsgroups
corpus contains 7 top categories and these top categories contain 20 subcate-
gories which have approximately 20,000 newsgroup documents. We select four
top categories ”com”, ”rec”, ”talk” and ”sci” in this experiment. Thus, three
other cross-domain tasks are formed as listed in Table 1.

Table 1. Summary of Datasets

Data Set

In-domain Out-of-domain

SyskillWebert

Goat

Bands
Sheep

Biomedical

Orgs vs People Documents of Documents of

Reuters Orgs vs Places
People vs Places

some sub
categories

other sub
categories

20

News-
group

Com vs Rec Documents of Documents of
Rec vs Sci
Rec vs Talk

other sub
categories

some sub
categories

Experiment Setup. On one hand, for each in-domain data set employed in the
experiment, we further split it into two parts: in-domain data with labels(Dl) and
the in-domain data without labels(Du). We randomly select 50% data from out-
of-domain and in-domain, respectively. The ratio between |Dl| and |Du| is 1:9.
All of the in-domain data without labels(Du) are used as the test sets while the
training sets consist of the data points with labels from both the in-domain Dl
and out-of-domain (Dout). On the other hand, the kernel is a convex combination
of 10 Gaussian kernels [10]:

10(cid:3)

K(x, z) =

θie(cid:5)x−z(cid:5)2/σ2

i

where θi are the weights of the kernels to be determined. The values of σi were
−1, 102] on the logarithmic scale. The
chosen uniformly over the interval [10

i=1

Domain Transfer Dimensionality Reduction

289

regularization parameter λ in DTDKL and the MMD parameter β was ﬁxed
−6 and 1, respectively. As a matter of fact, the algorithm is not sensitive
to 10
to the parameter β for a wide range.

Any ordinary classiﬁer, such as Na¨ıve Bayes, K-nearest, can be used in the di-
mensionality reduction method. In our experiments, we simply choose the nearest
centroid method.

4.2 Experimental Results

For performance evaluation, we use accuracy, which has been widely used as a
evaluation metric, we systematically compare the proposed algorithms to some
classiﬁers, including discriminant MKL-DA [9], SKDA [13], as well as TKDR [2],
MMDE[3]. All of the results reported below are mean of that running 10 times.

Table 2. Comparison of Performance (mean ± std %)

Data Set

MKL-DA

SKDA

TKDR

MMDE

DTDKL

Webert Goat-Sheep
Orgs-People
Reuters Orgs-Places
People-Places

50.47 (11.31) 55.56 (1.63) 54.81 (1.24) 57.98 (4.52) 59.03 (1.13)
Goat-Biomedical 58.38 (8.40) 60.54 (8.60) 61.14 (1.13) 60.32 (2.21) 61.38 (1.30)
80.52 (2.25) 67.38 (9.58) 71.05 (3.62) 80.04 (1.33) 81.75 (1.65)
67.08 (5.96) 66.05 (6.02) 68.80 (4.81) 71.22 (3.14) 73.65 (5.05)
54.90 (8.56) 55.60 (4.91) 58.31 (4.98) 69.19 (3.31) 70.90 (1.98)
50.54 (8.98) 54.94 (4.32) 53.60 (4.80) 64.86 (4.01) 65.60 (4.98)
54.49 (9.05) 72.02 (7.42) 72.69 (7.29) 73.05 (3.98) 73.46 (3.32)
70.05 (8.78) 73.90 (4.06) 76.90 (5.53) 77.63 (3.29) 77.68 (4.85)
70.80 (2.96) 77.35 (5.77) 78.03 (6.64) 78.90 (2.18) 79.08 (1.52)

Com-Rec
Rec-Sci
Rec-Talk

Syskill

-

Goat-Bands

20

News-
group

In this section, we use accuracy as the evaluation metric, and compare the
proposed algorithms to MKL-DA, SKDA and TKDR. The results show clearly
that DTDKL is able to alleviate the inﬂuence of diﬀerent distributions.

Table 2 summarizes the accuracies of MKL-DA, SKDA, TKDR, MMDE, and
DTDKL on the three databases with the best results highlighted in bold font. It
can be seen that the MAP of the DTDKL methods is consistently higher than
the other methods on all of the data sets. Moreover, it is general trend that those
problems with higher precision, generally, have a smaller error.

Overall Performance
Our proposed method DTDKL outperforms all the other algorithms in terms
of accuracy, demonstrating that DTDKL learns a robust target classiﬁer.And
it is also easy to notice that the MMDE and DTDKL performs much better
than the other three methods, even TKDR. Moreover, the standard deviation
of DTDKL is much smaller, means that it is more stable. For the SyskillWebert
collection, compared to DTDKL’s rivals, on average it achieves at least 1.23%,
0.24% and 1.05% higher accuracy on ”GoatVsBands”, ”GoatsVsBiomedical” and
”GoatVsSheep”, respectively. The better performance can be ascribed to trans-
ferring the in-domain and out-domain data to a features whose the discriminant

290

M. Zeng and J. Ren

y
c
a
r
u
c
c
A

0.7

0.65

0.6

0.55

0.5

0.45

0.4

0.35

 
2

0.8

0.75

0.7

0.65

0.6

0.55

y
c
a
r
u
c
c
A

0.5

0.45

0.4

0.35

 
2

 Goat vs Bands

 

MKL−DA
SKDA
TKDR
MMDE
DTDKL

4

6

8

10

Percentage of labeled in−domain data

(a)

Orgs vs People

 

MKL−DA
SKDA
TKDR
MMDE
DTDKL

4

6

8

10

Percentage of labeled in−domain data

y
c
a
r
u
c
c
A

0.75

0.7

0.65

0.6

0.55

0.5

0.45

0.4
 
2

0.75

0.7

0.65

y
c
a
r
u
c
c
A

0.6

0.55

 Goat vs Biomedical

MKL−DA
SKDA
TKDR
MMDE
DTDKL

 

4

6

8

10

Percentage of labeled in−domain data

(b)

Orgs vs Place

 

MKL−DA
SKDA
TKDR
MMDE
DTDKL

Goat vs Sheep

 

MKL−DA
SKDA
TKDR
MMDE
DTDKL

4

6

8

10

Percentage of labeled in−domain data

(c)

Place vs People
MKL−DA
SKDA
TKDR
MMDE
DTDKL

 

y
c
a
r
u
c
c
A

1

0.9

0.8

0.7

0.6

0.5

0.4

 
2

0.8

0.75

0.7

0.65

y
c
a
r
u
c
c
A

0.6

0.55

0.5

0.5

0.45

0.4
 
2

4

6

8

10

Percentage of labeled in−domain data

0.45

0.4

0.35

 
2

4

6

8

10

Percentage of labeled in−domain data

(d)

(e)

(f)

Fig. 1. Accuracy vs. diﬀerent size of Din
l

distance of the data is maximum and the maximum mean discrepancy comes
out to be minimum. For the Reuters-21578 data set, the accuracy of DTDKL on
average achieve at least 1.6%, 1.7% and 0.7% higher that other approaches on
”OrgsVsPeople”, ”OrgsVsPlaces” and ”PeopleVsPlaces” respectively.The simi-
lar performance explanation provided to DTDKL method on Reuters-21578 can
also applied here. On the 20 News-group data set, the DTDKL methods per-
form best among the total tasks. Compare DTDKL and MKL-DA, we can see
that the MAP of DTDKL is at least 6.6% higher, even nearly 10% higher than
MKL-DA, which conﬁrm the positive eﬀect of MMD.

Sensitivity
This study evaluates the sensitivity of varied sizes of labeled in-domain data
and conducted on the three collection. The results are demonstrated in Fig.1.
It is evident that, as the size of the labeled in-domain data increases, DTDKL
performs better than or as equal as its competitors at most case. For example,
as shown in Figure 1(c), DTDKL achieves at least 5% higher accuracy than
other methods on each size of labeled in-domain data. As a general trend, the
accuracy of DTDKL steadily improves when the number of labeled in-domain
data increase from 1% to 10%. Consequently, we infer that, better performances
can be obtained if more labeled in-domain data are provided.

5 Conclusion

We have proposed a uniﬁed dimensionality reduction in cross-domain problems
to simultaneously learn a kernel function as well as Fisher discriminant direction
by maximizing the Fisher discriminant distance and minimizing the distance of

Domain Transfer Dimensionality Reduction

291

out-of-domain and in-domain. Moreover, we assume that the kernel function in
optimal kernel discriminant analysis is a linear combination of multiple base
kernels; Thus, it can be eﬃciently solve by SDP. Experimental result show that
DTDKL method outperforms existing dimensionality reduction in cross-domain
in three text data sets.

References

1. Mika, S., Ratsch, G., Weston, J., Scholkopf, B., Mullers, K.: Fisher discriminant

analysis with kernels. In: NNSP Workshop, pp. 41–48 (1999)

2. Wang, Z., Song, Y., Zhang, C.: Transferred Dimensionality Reduction. In: Daele-
mans, W., Goethals, B., Morik, K. (eds.) ECML PKDD 2008, Part II. LNCS
(LNAI), vol. 5212, pp. 550–565. Springer, Heidelberg (2008)

3. Pan, S., Kwok, J., Yang, Q.: Transfer learning via dimensionality reduction. In:

AI, vol. 2, pp. 677–682 (2008)

4. Si, S., Tao, D., Chan, K.: Evolutionary cross-domain discriminative hessian eigen-

maps. IEEE Transactions on Image Processing 19(4), 1075–1086 (2010)

5. He, X., Yan, S., Hu, Y., Niyogi, P., Zhang, H.: Face recognition using laplacianfaces.
IEEE Transactions on Pattern Analysis and Machine Intelligence 27(3), 328–340
(2005)

6. Fukunaga, K.: Introduction to statistical pattern recognition. Academic Pr. (1990)
7. Baudat, G., Anouar, F.: Generalized discriminant analysis using a kernel approach.

Neural Computation 12(10), 2385–2404 (2000)

8. Lanckriet, G., Cristianini, N., Bartlett, P., Ghaoui, L., Jordan, M.: Learning the
kernel matrix with semideﬁnite programming. The Journal of Machine Learning
Research 5, 27–72 (2004)

9. Kim, S.J., Magnani, A., Boyd, S.: Optimal kernel selection in kernel ﬁsher discrim-

inant analysis. In: ICML, pp. 465–472 (2006)

10. Ye, J., Ji, S., Chen, J.: Multi-class discriminant kernel learning via convex pro-

gramming. The Journal of Machine Learning Research 9, 719–758 (2008)

11. Borgwardt, K., Gretton, A., Rasch, M., Kriegel, H., Sch¨olkopf, B., Smola, A.: In-
tegrating structured biological data by kernel maximum mean discrepancy. Bioin-
formatics 22(14), e49–e57 (2006)

12. Cristianini, N., Shawe-Taylor, J.: Kernel methods for pattern analysis. Cambridge

University Press, Cambridge (2004)

13. Cai, D., He, X., Han, J.: Semi-supervised discriminant analysis. In: ICCV, pp. 1–7

(2007)

14. Asuncioin, A., Newman, D.: Uci machine learning repository (2007),

http://www.ics.uci.edu/mlearn/MLRepository.html

15. Davidov, D., Gabrilovich, E., Markovitch, S.: Parameterized generation of labeled
datasets for text categorization based on a hierarchical directory. In: SIGIR, pp.
250–257 (2004)

16. Zhong, E., Fan, W., Peng, J., Zhang, K., Ren, J., Turaga, D., Verscheure, O.: Cross
domain distribution adaptation via kernel mapping. In: SIGKDD, pp. 1027–1036
(2009)

17. Dai, W., Yang, Q., Xue, G., Yu, Y.: Boosting for transfer learning. In: ICML, pp.

193–200 (2007)


