Topic Decomposition and Summarization

Wei Chen(cid:2), Can Wang, Chun Chen, Lijun Zhang, and Jiajun Bu

College of Computer Science, Zhejiang University
{chenw,wcan,chenc,zljzju,bjj}@zju.edu.cn

Hangzhou, 310027, China

Abstract. In this paper, we study topic decomposition and summarization for a
temporal-sequenced text corpus of a speciﬁc topic. The task is to discover dif-
ferent topic aspects (i.e., sub-topics) and incidents related to each sub-topic of
the text corpus, and generate summaries for them. We present a solution with
the following steps: (1) deriving sub-topics by applying Non-negative Matrix
Factorization (NMF) to terms-by-sentences matrix of the text corpus; (2) detect-
ing incidents of each sub-topic and generating summaries for both sub-topic and
its incidents by examining the constitution of its encoding vector generated by
NMF; (3) ranking each sentences based on the encoding matrix and selecting top
ranked sentences of each sub-topic as the text corpus’ summary. Experimental
results show that the proposed topic decomposition method can eﬀectively detect
various aspects of original documents. Besides, the topic summarization method
achieves better results than some well-studied methods.

Keywords: Non-negative Matrix Factorization, Topic Decomposition, Topic
Summarization, Singular Value Decomposition.

1 Introduction

Users nowadays are overwhelmed by the vast amount of information on the Web. Al-
though they can ﬁnd information for a speciﬁc topic easily using search engines, they
still have diﬃculty in ﬁnding more detailed aspects of a topic before reading dozens of
Web documents returned. For example, it is a non-trivial task to make a comprehensive
survey of a topic such as “9/11 attacks”. Related reports may cover various aspects (i.e.,
sub-topics) including “attackers and their motivation”, “the rescue attempts”, “9/11 in-
vestigations”, etc. Each sub-topic may further contain a set of related incidents, e.g.,
“9/11 investigations” has a series of related incidents along the timeline, such as “the
NSA intercepted communications that pointed to bin Laden on Sep.11, 2001”, “FBI
released photos of the 19 hijackers on Sep.27, 2001”, etc. Thus, discovering sub-topics
and related incidents for a speciﬁc topic in a text corpus and summarizing them will
greatly facilitate user’s navigation in the corpus space.

The above problems can be partially solved by topic decomposition and text summa-
rization, which was ﬁrst proposed systematically by Chen and Chen[1]. Their solution
is called TSCAN (Topic Summarization and Content ANatomy). TSCAN equals to la-
tent semantic analysis (LSA) based on the singular value decomposition (SVD)[2]. We

(cid:2) This work is supported by China National Key Technology R&D Program (2008BAH26B00).

M.J. Zaki et al. (Eds.): PAKDD 2010, Part I, LNAI 6118, pp. 440–448, 2010.
c(cid:2) Springer-Verlag Berlin Heidelberg 2010

Topic Decomposition and Summarization

441

also study this problem in this paper. However, our solution is based on Non-negative
Matrix Factorization (NMF)[3]. NMF has been demonstrated advantages over SVD in
latent semantic analysis, document clustering [4]. In our work, we model the documents
of a speciﬁc topic as a terms-by-sentences matrix. NMF is used to factorize the matrix
into a non-negative sub-topic matrix and a non-negative encoding matrix. Each row of
the encoding matrix is examined to extract incidents and their summaries. Summary for
each sub-topic is generated by composing its incidents’ summaries. We rank sentences
by analyzing the encoding matrix, and the top ranked sentences of each sub-topic are
selected as the summary for the text corpus.

2 Related Work

For a given temporal documents of a speciﬁc topic, TSCAN has following steps: Firstly,
the documents are decomposed into a set of blocks. Then, a m × n terms-by-blocks
matrix A is constructed. Ai, j is the weight of term i in block j, which is computed by
TF-IDF weighting scheme. The block association matrix B = AT A, it is factorized as
follow:
(1)
where Dr is a r × r diagonal matrix where the diagonal entries are the top r eigenvalues
of B. And Tr is a n × r matrix in which each of the r columns represents a sub-topic.
By examining the constitution of each columns of Tr, the signiﬁcant incidents of each
topic aspect are detected and their summaries are generated. Then, the summary of the
topic documents is obtained by combining all detected incident’s summary.

Tr ∈ Rn×r, Dr ∈ Rr×r,

B ≈ TrDrTT

r

We assume the SVD of the terms-by-blocks matrix A as follow:
U ∈ Rm×m, Σ ∈ Rm×n, V ∈ Rn×n,

A = UΣVT

(2)

where both U and V are orthogonal matrices, and Σ is a diagonal matrix. The diagonal
entries of Σ are the singular values of the matrix A. Each column of matrices U and V
are called left-singular and right-singular vectors. Then,

B = AT A = (UΣVT )T (UΣVT ) = VΣT UT UΣVT = V(ΣT Σ)VT .

(3)

The squares of singular values of the matrix A (i.e., ΣT Σ) are equal to the eigenvalues
of the matrix AT A (i.e., B) [5]. In LSA, the r largest singular values with corresponding
singular vectors from U and V are used to approximation the matrix A[2], i.e.,

A ≈ UrΣrVT

r

Ur ∈ Rm×r, Σr ∈ Rr×r, VT

r

∈ Rr×n,

Then, B can be approximated as follow:

B = AT A ≈ Vr(ΣT

Σr)VT
r

.

r

Because ΣT
Σr are the top r eigenvalues of the matrix B, the matrix Vr is equal to the
r
sub-topic matrix Tr derived by TSCAN. That is, the sub-topics derived by TSCAN
corresponds to the right singular vectors with most signiﬁcant singular values of A.
In this paper, we focus on extractive multi-document summarization which are widely
studied[6,7,8,9]. It extracts top signiﬁcant sentences calculated by a set of ranking meth-
ods from the documents set.

(4)

(5)

442

W. Chen et al.

3 The Proposed Solution Based on NMF
Given a pre-speciﬁed topic t, it is represented as D = {d1, d2, . . . , di, . . .}, where di
is a document at time point i. We call various topic aspects as sub-topics and deﬁne
them as S T = {st1, st2, . . . , stk, . . . , str}. The series of incidents corresponding to sub-
topic stk is deﬁned as S T Ik = {stik,1, stik,2, . . . , stik,i, . . . , stik,l}. Our task of topic de-
composition is to ﬁnd out sub-topics S T , the incidents S T Ik related to sub-topic stk.
Besides, we generate summaries for the text corpus D, sub-topics S T and incidents
S T I. Each document in D is decomposed into a sequence of sentences using sentence
separation software provided by DUC[10]. 425 Rijsbergen’s stopwords are removed
and stemming is performed. The documents in D is represented by a m × n terms-by-
sentences matrix M, where m is the number of terms and n is the number of sentences
respectively. Mi, j is computed by TF-IDF weighting scheme. The terms set is deﬁned
as T = {t1, t2, . . . , ti, . . . , tm} while the sentences set is S = {s1, s2, . . . , s j, . . . , sn}.

3.1 Topic Decomposition Based on NMF

Non-negative Matrix Factorization (NMF) is a matrix factorization method that gener-
ates positive factorization of a given positive matrix[3]. It represents object as a non-
negative linear combination of part information extracted from plenty of objects and is
able to learn parts of semantic features from text. Given the matrix M, NMF decom-
poses M into a non-negative matrix B and a non-negative matrix E so that

M ≈ BE

B ∈ Rm×r, E ∈ Rr×n.

(6)

We can ﬁnd out B and E by minimizing the following cost function:

(cid:5)M − BE(cid:5)2

arg min
B,E

(7)
where (cid:5) · (cid:5)F denotes the Frobenius norm. The above constrained optimization problem
can be solved by continuously updating B and E until the cost function converges under
the predeﬁned threshold or exceeds the number of repetitions[3,4]. The update rules are
as follows (1 ≤ i ≤ m, 1 ≤ j ≤ n and 1 ≤ k ≤ r):

,

F

Bi,k ← Bi,k

(MET )i,k
(BEET )i,k

Ek, j ← Ek, j

(BT M)k, j
(BT BE)k, j

.

(8)

The r columns of B embed the so called sub-topics and each column of E is the en-
coding. We refer B as the sub-topic matrix and E as the encoding matrix. Each sentence
s j can be represented by a linear combination of sub-topics. i.e.,

m j = Be j,

(9)

where m j is j-th sentence (i.e., j-th column of M) and e j represents the j-th column of
matrix E. The entry Bi,k indicates that the degree of term ti belongs to sub-topic k, while
Ek, j represents that the degree of sentence s j associates with sub-topic k.
Because the sentences set S = {s1, s2, . . . , s j, . . . , sn} is indexed chronologically, the
row k of encoding matrix E (i.e., ek, j, 1 ≤ j ≤ n, we refer it as sub-topic encoding

Topic Decomposition and Summarization

443

15

10

5

l

e
u
a
v
 

i

g
n
d
o
c
n
e

0

0

20

40

60

80

100

120

140

160

180

sentence index

Fig. 1. A sub-topic’s encoding vector of the document cluster ‘d30001t’ in DUC 2004

vector) also denotes the relative strength of sub-topic stk along the timeline. Herein, a
list of continuous elements of ek, j(1 ≤ j ≤ n) with high bursty values can be regarded as
an incident related to sub-topic k. Fig. 1 shows a sub-topic encoding vector of document
cluster ‘d30001t’ in DUC 2004[10] after applying NMF with r = 10. In Fig. 1, the
encoding value is bursty around the sentence 170. It means that the sub-topic has a
signiﬁcant development around the sentence 170 (i.e., an incident breaks out).

The bursty detection problem is well studied in stream mining community [11]. For-
mally, given an aggregate function G(here is sum), a sliding window of size w and
corresponding thresholds γ, the problem is to discover all these sub-sequences such
that the function G applied to ek, j: j+w−1(1 ≤ j ≤ n − w + 1) exceeds threshold γ, i.e.,
check if

G(ek, j: j+w−1) =

(10)
The threshold is set as γ = mean(G(ek, j: j+w−1))+×std(G(ek, j: j+w−1)), (1 ≤ j ≤ n−w+1),
where mean() and std() are the average and standard deviation function respectively.
We set  as 3 and w as 7 in our experiments. Finally, the detected bursty sequences are
recognized as incidents.

i=0

i=w−1(cid:2)

ek, j+i ≥ γ.

3.2 Topic, Sub-topics and Incidents Summarization

An interesting by-product of topic decomposition is that the produced information can
also be used to generate summary. Lee et al. [9] also use NMF to do generic document
summarization. In their work, the rank of each sentence is calculated as follows:

rank(s j) =

r(cid:2)

k=1

(Ek, j × weight(ek,1:n)),

weight(ek,1:n) =

(cid:3)

(cid:3)

n
y=1 Ek,y
(cid:3)

r
x=1

n
y=1 Ex,y

.

(11)

(12)

The weight(ek,1:n) is the relative relevance of k-th sub-topic among all sub-topics. Fi-
nally, the top-x sentences with highest rank are chosen as summaries. We refer this
method as NMF in our experiments. As point out by [8], a good summary should con-
tain as few redundant sentences as possible while contain every important aspects of the
documents. However, the solution of Lee et al. [9] doesn’t satisfy above requirements

444

W. Chen et al.

sometimes. The top-x sentences with highest rank may belong to the same sub-topics
and contain some overlapping information. In fact, most of the traditional summariza-
tion methods select sentences from diﬀerent sub-topics [6,7]. We design a generic multi-
document summarization method based on NMF (We refer it as INMF). Before going
on, we give the deﬁnition of a sentence’s main topic:

main topic(s j) = arg max

(Ek, j),

k

(13)

That is, the main topic of sentence s j is the sub-topic with the maximum encoding
value in column j of encoding matrix E [4]. The function topic() returns the union of
each sentence’s main topic of a sentences set, e.g.,

topic(S ) = main topic(s1) ∪ main topic(s2) ∪ . . . ∪ main topic(sn).

(14)

The proposed multi-document summarization method INMF is described in Algo-
rithm 1. Most of the summarization evaluations require the generated summaries in
limited size or limited sentences number. In Algorithm 1, we limit the number of sen-
tences of the summary. It can be easily revised to control the size of ﬁnal summary.
Diﬀerent from [9], the INMF algorithm selects sentences with most signiﬁcant ranking
scores from diﬀerent sub-topics in order to ensure the coverage and diversity of the
summary. For each incident stik,i, we can straightforwardly choose the sentences with
the largest or top-x encoding values of stik,i as the summary. Then, the summary for
sub-topic stk can be generated by composing all the summaries of S T Ik.

Algorithm 1. The proposed multi-document summarization method based on NMF
Input: S = {s1, s2, . . . , sn}; ns, the limitation of sentences number in summary
Output: a string array S S = {ss1, ss2, . . . , ssns}, sentences set of summary
1: Sort sentences in S using equation 11: rank(s1) ≥ rank(s2) ≥ . . . ≥ rank(sn)
2: k = 1; T S = ∅; // TS is the sub-topics set
3: for k ≤ ns do
i = 1;
4:
for i ≤ size(S ) do // size() returns the number of elements in set S
5:
6:
7:
8:
9:
10:
11:
12:
13:
14: end for

ssk ← si; S = S − si; T S = T S ∪ main top(si); k + +; break;

end for
if size(T S ) == r then // r is total sub-topics number

if main topic(si) (cid:2) T S then

end if
i + +;

T S = ∅;

end if

4 Experimental Studies

In the following, we ﬁrst evaluate the proposed topic summarization method. And then,
we give a case study of topic decomposition. The dataset of multi-document summa-
rization task in DUC 2004[10] is used to evaluate the proposed methods.

Topic Decomposition and Summarization

445

4.1 Summarization Evaluations

We implement four baseline summarization systems: FORWARD(extracts the initial
sentences of all documents of a topic); BACKWARD(generates summaries by selecting
the end sentences of a topic); TSCAN; NMF(method of Lee et al., [9]). The number of
sentences of summary generated by TSCAN is indeterminate. To ensure the comparison
is fair, the evaluation procedure is as follows [1]: For each r, we ﬁrstly apply TSCAN
to each document cluster to select a set of sentences as summary. Then, we use other
methods to extract the same number of sentences for each r and document cluster. Both
ROUGE-1 [12] and summary-to-document content coverage[1] metrics are used.

Table 1. Overall performance comparison of ROUGE-1 on DUC 2004

r
2
3
4
5
6
7
8
9
10

INMF
0.31161
0.33475
0.36529
0.39042
0.39739
0.43594
0.46620
0.47680
0.48975

NMF
0.29707
0.32156
0.35522
0.38238
0.40410
0.42632
0.45518
0.47117
0.48382

TSCAN
0.23983
0.25342
0.27096
0.29288
0.30370
0.31636
0.33862
0.35014
0.36947

FORWARD

BACKWARD

0.23875
0.25383
0.27092
0.29061
0.31152
0.32451
0.34409
0.35653
0.36348

0.18211
0.19766
0.22081
0.24342
0.25867
0.28100
0.29974
0.31159
0.32110

The overall performance comparison of ROUGE-1 on DUC 2004 is listed in Ta-
ble 1. It shows that the two NMF based summarization methods get much better results
than other methods for all r. This is because both the two NMF based methods try to
cover all content as much as possible. However, TSCAN may not consider sub-topics
successfully, FORWARD extracts beginning sentences and BACKWARD takes the end
sentences. As r increase, TSCAN extracts more sentences as the summary. Because
ROUGE is recall-oriented, the ROUGE-1 scores of all methods increase with the in-
creasing of summary size as showed in Table 1. The proposed INMF method increase
summary coverage by selecting sentences from diﬀerent sub-topics explicitly. As a re-
sult, INMF outperforms NMF in most cases except r = 6.

A good summary should contain important aspects of original topic documents as
much as possible [8]. Herein, we apply summary-to-document content similarity to
measure the coverage of summary according to [1]. That is, given a document cluster
of a speciﬁc topic and its summary which are represented by TF-IDF term vectors. It
computes the average cosine similarity between each of the document clusters and its
summary. The higher the similarity, the better the summary represents document cluster.
We show the summary-to-documents similarity corresponding to table 1 in table 2.

In table 2, both INMF and NMF achieve much better results than TSCAN, FOR-
WARD, BACKWARD for all r. It is easy to understand that all the three latter methods
lose some information and the coverage is poor. However, Non-negative Matrix Factor-
ization decomposes all of topic’s information into r sub-topics, and the two NMF based
summarization method extract the sentences with as much information as possible.

446

W. Chen et al.

Table 2. Summary-to-document content similarity corresponding to table 1

r
2
3
4
5
6
7
8
9
10

INMF

0.126703
0.128219
0.126229
0.127125
0.127987
0.128505
0.130945
0.127965
0.128130

NMF

0.123421
0.122003
0.121232
0.122199
0.122781
0.124848
0.126131
0.123313
0.124492

TSCAN
0.105427
0.107548
0.105550
0.108460
0.103569
0.102935
0.108045
0.106552
0.111100

FORWARD
0.103786
0.103958
0.104454
0.107260
0.104365
0.101614
0.105535
0.105038
0.109034

BACKWARD

0.102524
0.104369
0.100763
0.102478
0.101695
0.102715
0.101850
0.099187
0.107870

Table 3. Sub-topic’s description and the sentence id of each sub-topic’s summary

S T id

sub-topic description

sentence id

1 Hun Sen and Ranariddh often clashed over power-sharing and the integration

74,119

of guerrilla ﬁghters from the crumbling Khmer Rouge.

2 King Norodom Sihanouk called Ranariddh and Sam Rainsy to return to Cam-
bodia and wanted to preside over a summit meeting of the three party leaders.
3 Norodom Ranariddh and Sam Rainsy, citing Hun Sen’s threats to arrest op-

20,46,56,57

3,30,141,163

position ﬁgures, said they could not negotiate freely in Cambodia.

4 In July election, Hun Sen’s party collected 64 of the 122 parliamentary seats,
but was short of the two-thirds majority needed to set up a new government.
5 The violent crackdown in Cambodia, at least four demonstrators were killed.
6 Hun Sen and Ranariddh agreed to form a coalition that leave Hun Sen as sole

8,25,44,85,111

40,64,69

83,123,152,175

prime minister and make Ranariddh president of the National Assembly.

7 People’s Party criticized the resolution passed earlier this month by the U.S.

House of Representatives.

8 King Norodom Sihanouk praised agreements by Cambodia’s top two politi-

cal parties previously bitter rivals to form a coalition government.

9 Sam Rainsy wanted to attend the ﬁrst session of the new National Assembly

on Nov. 25, but complained that his party members’ safety.

10 The Cambodian People’s Party gave a statement about supporting the police

action of violent crackdown to protesters.

59

172

160

70

Besides, the proposed INMF summarization method explicitly tries to select sentences
belong to diﬀerent topic aspects. That’s why INMF outperforms NMF in all cases.

4.2 Topic Decomposition

The documents set ‘d30001t’ of DUC 2004 is used as a case study for topic decompo-
sition. It includes 10 documents and 179 sentences about “political crisis in Cambodia
in October 1998”. The detailed description about each sub-topic and sentence id of its
summary is showed in table 3. We manually compared each sub-topics’ summaries with
reference summaries(‘D30001.M.100.T.A’, ‘D30001.M.100.T.B’,‘D30001.M.100.T.C’
and ‘D30001.M.100.T.D’ with size 658, 661, 647 and 656 bytes respectively) created

Topic Decomposition and Summarization

447

by DUC assessors. For ‘D30001.M.100.T.C’ and ‘D30001.M.100.T.D’, the information
coverage is 100%. The text “the opposition tried to cut oﬀ his access to loans” (total
51 bytes) in ‘D30001.M.100.T.A’ and “Opposition parties ask the Asian Development
Bank to stop loans to Hun Sen’s government”(total 87 bytes) in ‘D30001.M.100.T.B’
are lost in the generated summaries. Then, the average information coverage of the
generated sub-topics’ summary to the reference summaries is ((658− 51)/658 + (661−
87)/661 + 100 + 100)/4 = 94.75%.

Some sub-topics contain a series of incidents while others contain only one. For ex-
ample, sub-topic 6 is about the process of forming a coalition government which con-
tains several incidents. Sub-topic 8 has only one incident, which is about King Norodom
Sihanouk’s praise about the agreements by Cambodia’s top two political parties. We
also compare our results with TSCAN for topic decomposition with r = 10. TSCAN
detects total 23 incidents. 5 incidents are the same (some incidents duplicate more than
2 times), with only 15 sentences left as the incidents’ summary. The 15 sentences are
from 7 documents while the incidents’ summaries of our method are from all 10 doc-
uments. Besides, our method covers more aspects than TSCAN, e.g. sub-topic 5 and
sub-topic 10 are not included in the result of TSCAN.

5 Conclusion

In this paper, we study the problem of topic decomposition and summarization for a
temporal-sequenced text corpus of a speciﬁc topic. We represent the text corpus as a
terms-by-sentences matrix and derive sub-topics by factorize the matrix using
Non-negative Matrix Factorization. By analyzing the encoding matrix, we can detect
incidents of each sub-topic and generate summaries for both sub-topics and their re-
lated incidents. The summary for the text corpus is generated by ﬁrstly ranking each
sentences based on the encoding matrix, and then selecting most signiﬁcant sentences
from each sub-topics. Experimental results show that our method can eﬀectively
ﬁnd out diﬀerent topic aspects of a documents set and generate promising results in
summarization.

References

1. Chen, C.C., Chen, M.C.: TSCAN: A Novel Method for Topic Summarization and Content

Anatomy. In: Proc. of the 31st ACM SIGIR conference, pp. 579–586. ACM, USA (2008)

2. Deerwester, S., Dumais, S.T., Harshman, R.: Indexing by latent semantic analysis. Journal of

the American Society for Information Science 41(6), 391–407 (1990)

3. Lee, D.D., Seung, H.S.: Learning the parts of objects by non-negative matrix factorization.

Nature 401, 788–791 (1999)

4. Xu, W., Liu, X., Gong, Y.H.: Document Clustering Based on Non-negative Matrix Factor-

ization. In: Proc. of the 26th ACM SIGIR conference, pp. 267–273. ACM, USA (2003)

5. Strang, G.: Introduction to Linear Algebra. Wellesley Cambridge Press, Wellesley (2003)
6. Gong, Y.H., Liu, X.: Generic Text Summarization Using Relevance Measure and Latent Se-
mantic Analysis. In: Proc. of the 24th ACM SIGIR conference, pp. 19–25. ACM, USA (2001)

448

W. Chen et al.

7. Zha, H.Y.: Generic Summarization and Keyphrase Extraction Using Mutual Reinforcement

Principle and Sentence Clustering. In: Proc. of 25th ACM SIGIR, pp. 113–120 (2002)

8. Wan, X.J., Yang, J.W., Xiao, J.G.: Manifold-Ranking Based Topic-Focused Multi-Document

Summarization. In: Proc. of IJCAI, pp. 2903–2908. ACM, USA (2007)

9. Lee, J.H., Park, S., Ahn, C.M., Kim, D.: Automatic generic document summarization based

on non-negative matrix factorization. Info. Processing and Management 45, 20–34 (2009)

10. Document Understanding Conferences (2004),

http://www-nlpir.nist.gov/projects/duc/index.html

11. Vlachos, M., Meek, C., Vagena, Z., Gunopulos, D.: Identifying Similarities, Periodicities and

Bursts for Search Queries. In: Proc. of ACM SIGMOD, pp. 131–142. ACM, USA (2004)

12. Lin, C.Y.: ROUGE: a Package for Automatic Evaluation of Summaries. In: Proc. of the

Workshop on Text Summarization Branches Out, Barcelona, Spain, pp. 74–81 (2004)


