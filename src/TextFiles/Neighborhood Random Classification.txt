Neighborhood Random Classiﬁcation

Djamel A. Zighed1, Diala Ezzeddine2, Fabien Rico2

1

abdelkader.zighed@ish-lyon.cnrs.fr

Institut des Sciences de l’Homme (ISH - USR 3385)

Universit´e de Lyon,

14, avenue Berthelot, 69007 Lyon
diala.ezzeddine@univ-lyon2.fr

2

fabien.rico@univ-lyon1.fr

Laboratoire Eric,

Universit´e de Lyon,

5, avenue Pierre Mend`es France, 69676 Bron Cedex, France

Abstract. Ensemble methods (EMs) have proved their eﬃciency in data
mining, especially in supervised machine learning (ML). An EM gener-
ates a set of classiﬁers using one or several machine learning algorithms
(MLAs) and aggregates them into a single classiﬁer (Meta-Classiﬁer,
MC) using, for example, a majority rule vote. Instance Based (IB) MLAs
, such as k-Nearest Neighbors (kNN), are very popular because of their
straightforwardness. To implement these, it is simply necessary to deﬁne
a dissimilarity measure based on the set of observations and ﬁx the value
of k. Thus, use of the kNN principle, as an EM algorithm is immediate.
However, handling the parameter k might be diﬃcult for some users.
To simplify this problem, we can use approaches based on neighbor-
hood graphs as alternatives. For example, relative neighborhood graphs
(RNGs) or Gabriel graphs (GGs) are good candidates. Like kNN, for an
unlabeled observation, the classiﬁer assigns a label based on neighbor-
hood graphs according to the labels in the neighborhood. For example,
we can simply use the majority rule vote in the neighborhood of the
unlabeled observation. While many studies using kNN in the context of
EMs have been done, we found no studies that assess the interestingness
of neighborhood graphs in EM approaches. In this paper, we provide
comparisons with many EM approaches based either on IB learning or
on other methods such as kSVM, decision tree (random forest) etc.

Keywords: Ensemble methods, neighborhood graphs, relative neigh-
borhood Graphs, Gabriel graphs, k-Nearest Neighbors

Neighborhood Random Classiﬁcation

No Author Given

No Institute Given

Abstract. Ensemble methods (EMs) have proved their eﬃciency in data
mining, especially in supervised machine learning (ML). An EM gener-
ates a set of classiﬁers using one or several machine learning algorithms
(MLAs) and aggregates them into a single classiﬁer (Meta-Classiﬁer,
MC) using, for example, a majority rule vote. Instance Based (IB) MLAs
, such as k-Nearest Neighbors (kNN), are very popular because of their
straightforwardness. To implement these, it is simply necessary to deﬁne
a dissimilarity measure based on the set of observations and ﬁx the value
of k. Thus, use of the kNN principle, as an EM algorithm is immediate.
However, handling the parameter k might be diﬃcult for some users.
To simplify this problem, we can use approaches based on neighbor-
hood graphs as alternatives. For example, relative neighborhood graphs
(RNGs) or Gabriel graphs (GGs) are good candidates. Like kNN, for an
unlabeled observation, the classiﬁer assigns a label based on neighbor-
hood graphs according to the labels in the neighborhood. For example,
we can simply use the majority rule vote in the neighborhood of the
unlabeled observation. While many studies using kNN in the context of
EMs have been done, we found no studies that assess the interestingness
of neighborhood graphs in EM approaches. In this paper, we provide
comparisons with many EM approaches based either on IB learning or
on other methods such as kSVM, decision tree (random forest) etc.

Keywords: Ensemble methods, neighborhood graphs, relative neigh-
borhood Graphs, Gabriel graphs, k-Nearest Neighbors

1 Introduction

Ensemble methods (EMs) have proved their eﬃciency in data mining, especially
in supervised machine learning (ML). An EM generates a set of classiﬁers using
one or several machine learning algorithms (MLA) and aggregates them into a
single classiﬁer (meta-classiﬁer, MC) using, for example, a majority rule vote.
Many papers [3,18,2,14] have shown that a set of classiﬁers produces a better
prediction than the best among them, regardless of the MLA. Theoretical and
experimental results have encouraged the implementation of EM techniques in
many application ﬁelds such as physics [6], face recognition [17], ecology [12],
recommender systems [9] and many others too numerous to mention here. The
eﬃciency of EMs lies in the fact that aggregating diﬀerent and independent
classiﬁers reduces the bias and the variance of the MC [8,1,5,3], which are two
key concepts for eﬀective classiﬁers.

Instance based (IB) MLAs such as k-Nearest Neighbors (kNN) are very popu-
lar because of their straightforwardness. Indeed, to implement these, it is simply
necessary to deﬁne a dissimilarity measure on the set of observations and ﬁx the
value of k. Thus, use of the kN N principle, as an EM algorithm is immediate.
However, handling the parameter k might be diﬃcult for some users. To simplify
this problem, we can use approaches based on neighborhood graphs as alterna-
tives. For example, Relative Neighborhood Graphs (RNG) or Gabriel Graphs
(GG) are “good” candidates. Like kN N , for an unlabeled observation, the clas-
siﬁer, based on neighborhood graphs, assigns a label according to the labels in
the neighborhood. For example, we can simply use the majority rule vote in the
neighborhood of the unlabeled observation. While many studies using kN N in
the context of EM have been carried out, we did not ﬁnd any study that as-
sesses the interestingness of such neighborhood graphs, based more particularly
on RNGs, in EM approaches. In this paper, we propose an EM approach based
on neighborhood graphs. We provide comparisons with many EM approaches
based on kSVM, Decision Tree (Random Forest), kN N etc. We have performed
our experiments on an R platform.

This paper is organized as follows. In section 2, we introduce and recall
some notations and deﬁnitions. In section 3, we introduce the EMs based on
neighborhoods. Besides the classic kN N neighborhood, we will present the RNG
and GG neighborhoods. Section 4 is devoted to evaluations and comparisons.
Section 5 gives the main conclusions of this study.

2 Basic concepts

2.1 Notations

Let us consider a training sample El of n individuals (ωi)1...n, described by p
attributes X j, j = 1, . . . , p and a membership class Y ∈ {y1, . . . , yk, . . .}. For
an individual ωi, we denote by Xi the vector (X j(ωi); j = 1, . . . , p and Yi its
membership class by Y (ωi). Below, we will refer to an individual ω or to a point
X(ω) with the same meaning, indistinguishably.

For the sake of illustration, we will use the toy example of Table 1. This is
a two-class data set of 17 individuals mapped into a two-dimensional space IR2.
In the ﬁgure, the class y1 = 1 is indicated by a bold dot and the class y2 = 2 by
an empty dot.

The goal of any machine learning algorithm is to lead to a classiﬁer capable
of predicting, with high accuracy, the membership class Y (ω) for any individual
ω knowing its attribute values in X(ω). Basically, the prediction is based on the
knowledge we can get about the probability distribution:

P (Y /X) = (p(Y = yk/X); k = 1, . . . , K).

Generally, a classiﬁer φ helps, somehow, for all individuals ω, to estimate ˆP
which is the membership probability vector for all classes. Thanks to the learning

El X 1 X 2 Y El X 1 X 2 Y
X1 2.13 2.33 2 X10
0 2.33 2
X2 2.13 4.11 2 X11 5.64 5.17 2
X3 2.22 1.76 2 X12 7.87 2.33 1
X4 3.37 6.88 1 X13 5.64 7.5 1
X5 6.77 0.67 1 X14 4.53 8.1 1
X6 4.53 1.16 1 X15 3.37 4.31 1
X7 3.37 0
1 X16 5.64 4.11 2
X8 1.8 6.47 2 X17 7.87 4.11 2
X9

0 5.77 2

7

6

5

4

3

2

1

X8

X4

X9

X14

X13

X11

X15

X16

X17

X2

X1

X10

X12

X3

X6

X7

X5

1

2

3

4

5

6

7

Table 1. set of points in IR2 with two classes

sample El, the predicted membership class is ˆyk the most likely, determined as
follows:

ˆyk is such that ˆp(Y = ˆyk/X) = max

k=1,...,K

ˆp(Y = yk/X)

By thresholding at the maximum value of this vector, the membership class
can be represented by a zero vector except for the most likely class by a value
1 at the corresponding rank: ˆP = (0, . . . , 0, 1, 0, . . . , 0). If the classiﬁer φ is con-
sidered as being quite reliable, then the prediction of the membership class for
an individual ω is φ(X(ω)) ∈ {y1, . . . , yk, . . .}.

2.2 Neighborhood structure

There are many types of neighborhood that could be used to build a classiﬁer.
Among the most well known are:

– The well-known k-nearest neighbors;
– The ε-neighbors, which are deﬁned by the subset of El that are in the ball

of radius ε, centered on the individual, i.e. a point in Euclidean space;

– The neighborhood regions brought about by a decision tree where each leaf
deﬁnes a subregion of the space. An individual that falls in a speciﬁc leaf
has, as neighbors, those of the learning sample located in the same leaf.

– Parzens window neighbors;
– The neighbors in random spaces. For example, we can cite the weak models
approach [7] where neighbors are obtained after a random projection along
axes.

– The neighbors in the sense of a speciﬁc property. For example, the Gabriel
Graphs (GG) neighbors are given by the subset of individuals of the learning
sample that fulﬁll a certain condition. Likewise, we can deﬁne the relative
neighbors (RN), the minimum spanning tree’s (MST) neighbors or the De-
launay’s polyhedron neighbors and so forth [13];

2.3 Neighborhood classiﬁers

The neighborhood classiﬁers depend on three components :

1. Neighbourhood set P : the set of all subsets of El . This is the set of all

possible neighbors to which each individual will be connected.

2. The neighborhood function V : this deﬁnes the way in which an individ-

ual is linked to an element of the neighborhood set: V : ℜ −→ P

This function links any point X to a subset of El which contains its neigh-
bors.

3. The decision rule C: this leads to the probability distribution of the classes

X 7−→ V(X)

C : ℜ × P −→ SK

X, V(X) 7−→ yp(X) = (p1, p2, . . . , pK)

Hence, we can deﬁne a neighborhood classiﬁer φ as based on a combination of
the triplet (P, V, C) :

φ(X) = yV(X)(X)

2.4 Partition by neighborhood graphs

Here we are focusing on geometrical graphs, therefore we build P using neigh-
borhood graphs, such as Voronoi diagrams [13] or their dual of the Delaunay
polyhedral, Gabriel graphs [10], relative neighbors graphs [16] or the mini-
mum spanning tree [11]. In such graphs, points are linked according to a speciﬁc
property. Below we give the properties that deﬁne RNGs and GGs: For a given
distance measure d, a learning sample El and a set of individuals x, y, z, . . ., any
two points x and y are linked by the following rules :

– Gabriel graph (GG) :

y ∈ VGG(x) ⇐⇒ ∀z ∈ El − {x, y} d(x, y) ≤ pd2(x, z) + d2(z, y);

– Relative neighbours graph (RNG) :

y ∈ VRN G(x) ⇐⇒ ∀z ∈ El − {x, y} d(x, y) ≤ max (d(x, z), d(y, z));

All these geometric structures induce a related neighborhood graph with a sym-
metric neighborhood relationship. Figures 1 and 2 show the neighbor structures
of the relative neighbors graph and the Gabriel graph, using the dataset intro-
duced above (cf 2.1).

3 Ensemble method classiﬁer based on neighborhood

We call this framework “Random Neighborhood Classiﬁer (RNC)”. The principle
of EMs is to generate M classiﬁers and then aggregate them into one. To do so,
M randomized iterations are performed. At iteration m, RNC:

X14

X14

7

6

5

4

3

2

1

X8

X4

X9

Lunula

X13

X11

X15

X16

X17

X2

X1

X10

X12

X3

X6

X7

X5

7

6

5

4

3

2

1

X9

X10

X8

X4

X13

X11

X2

X15

X16

X17

X1

X3

X12

X6

X7

X5

1

2

3

4

5

6

7

1

2

3

4

5

6

7

Fig. 1. Graph of relative neighbours

Fig. 2. Gabriel graph

1. generates a new learning set Em
2. generates a new classiﬁer φm = (P m, V m, Cm);
3. uses the generated classiﬁer to determine membership class of unclassiﬁed

l with a given size;

individual X.

Following these steps, RNC then aggregates the M predicted values related
to an unclassiﬁed individual to determine its ﬁnal membership class. The two
key points of this procedure are the sampling procedure for generating the M
classiﬁers and the procedure for combining the M predictions. Below, we give
some details about the two key points:

1. Sampling procedures : From the training data set El which is an n×p table
of values, we carry out M random samples. The sampling can be achieved
in diﬀerent ways:

– Sampling on rows with or without replacement;
– Sampling on columns;
– Building new columns by a linear combination of some existing ones

(oblique projection);

– Generating new individuals by a linear combination of columns;
– Adding randomly x% of rows and/or columns.
Each sample produced leads to a speciﬁc classiﬁer.

2. Aggregating function : Generally, the aggregating function is based on
the majority rule vote. However, many other possibilities could be used [15].
Among these, we may cite:

– Vote of classiﬁers, which aggregate the responses of each classiﬁer and
normalize them. The majority rule vote is a particular case of this one.
– Average vector where the score for each class is the mean of the answers

for all the classiﬁers.

– Weighted version (majority or mean)

– Maximum Likelihood calculated as the product of the answers for all the
classiﬁers, for each class. The winner class is the one that has the highest
value.

– Naive Bayes [15].
– Decision Templates [15]. This method is based on the concept of a deci-
sion template, which is the average vector over the individual of a test
sample belonging to each class, and a decision proﬁle, which is the set of
responses of all classiﬁers. The membership class is determined according
to the Euclidean distance between the decision proﬁle and the decision
template. The winner class is the one that minimizes this distance.

– Linear regression. In this method, we assume that the probability of
a class is the linear combination of the probabilities of class for each
classiﬁer.

4 Evaluation

To assess the performance of RNC, we carried out many experiments on dif-
ferent data sets taken from the UCI Irvine repository. For this, we made some
distinctions according to the type of neighborhood used. Indeed, as our work
was motivated by the absence of studies on EMs based on geometrical graphs
such as RNGs, we designed two separate experiments for RNC. One was based
on RNGs and the other on kN N with k = 1, 2, 3. The comparison was also
extended to random forests (RFs), K support vector machines (KSVMs), Ad-
aboost, discriminant analysis (DA), logistic regression (RegLog) and C4.5. All
was done with R software.

The implementation of RF is the randomForest library using 500 trees. We
used the R kernlab library to apply the KSVM algorithm with classiﬁcation
type C-svc. For AD and RegLog, we used lda and glm functions of the R MASS
library and for C4.5, the J48 function of the R RWeka library using the control
of the Weka learner.

For RNC based on RNGs, for building the graph, we have used the Maha-
lanobis distance between individuals. For aggregating the classiﬁers we used
the Decision Templates (DT). 100 iterations were carried that provided 100
classiﬁers. For kNN, we just replaced the neighborhood graph by the k-nearest
neighbors one using the same distances and the same number of classiﬁers. The
chosen aggregation method was majority voting.

We used 14 quantitative data sets. We ran the same protocol over all the
methods mentioned above. For each experiment, we applied 10-Cross Valida-
tions to obtain an estimation of the error rates. For each dataset, we used the
Wilcoxon test [4] to evaluate the results.

The results are reported in Table 2. For each dataset, we computed the
average error rate, the rank of each method among the others and the p-values
detected by Wilcoxon test.

As can be seen in Table 2, RNC based on RNGs performs well in comparison
to kN N and also in comparison to the other methods. Indeed, from the 14 data

Glass

Image

Ionosphere

Iris

Letter (RvsB)

% Err. Rank Wilcox % Err. Rank Wilcox % Err. Rank Wilcox % Err. Rank Wilcox % Err. Rank Wilcox

RNC
Random Forest
KSVM
adaBoost
AD
Log. Reg.
C4.5
kNN1
kNN2
kNN3

21.4
18.6
31.4

-

38.5

-

29.5
25.7
27.6
29.5

2
1
7
na
8
na
6
3
5
4

2.52
1.83
< 5% 5.57

=

-

< 5% 8.34

-

< 5% 3.17
2.17
3.34
< 5% 3.13

=
=

3
1
7
na
8
na
5
2
6
4

4

< 5% 6.29
< 1% 5.71
7.14
< 1% 12.3
12
8.86
6.58
< 5% 5.43
< 5% 8.86

=
=

=
=
=

4.66
5.33
5.33

1
4
3
6
10 < 1%
9
< 1%
7.5 < 5% 5.99
< 5% 5.99
5
2
< 5%
7.5 < 1%

-
2
-

8
4

Musk

Diabete (Pima)

Ringnorm

=
=
=
=
=
=
=
=
=
=

1.05
1.78
1.38
2.03
6.84
6.25

5

0.92
3.48
1.32

3
4.5
4.5
na
1
na
6.5
6.5
8
2

Sat

=
=
=

2
5
4
6
< 5%
10 < 1%
< 1%
9
8
< 1%
1
7
3

< 5%

=

=

Sonar

% Err. Rank Wilcox % Err. Rank Wilcox % Err. Rank Wilcox % Err. Rank Wilcox % Err. Rank Wilcox

RNC
Random Forest
Ksvm
adaBoostv
AD
Log. Reg.
C4.5
kNN1
kNN2
kNN3

3.02

2

3.46

2

5.58
4.84
3.13
3.61
5.24
4.49

=

24.9
3
23.
1.5 < 1%
5
24.1
1.5 < 1% 23.8
10 < 1% 22.2
8
< 1% 23.7
25.7
4
< 5%
6
25
< 1% 30.1
9
7
< 1% 25.4

=

6
2
5
4
1
3
9
7
10
8

=
=
=
=
=
=
=
=
=
=

2.26
5.24
1.56
3.4
38.1
34.8
13.9
6.37
24.2
8.82

=

10.8
< 1% 7.79
9.49

2
4
1
3
< 1%
10 < 1%
9
< 1%
< 1% 13.7
7
< 1% 10.6
5
< 1% 12.3
8
6
< 1% 11.9

-
16.
-

4
1
2
na
8
na
7
3
6
5

< 1%

12.5
< 1% 14.5
17
< 5%
16
26
28
31
11
< 5% 14.5
< 5% 12.5

< 1%

=

=
=
=

2
4.5
7
6
8
< 5%
9
< 5%
10 < 1%
1
4.5
3

=
=
=

Threenorm

Wisc. Breast Cancer
% Err. Rank Wilcox % Err. Rank Wilcox % Err. Rank Wilcox % Err. Rank Wilcox

Waveform

Twonorm

RNC
Random Forest
Ksvm
adaBoostv
AD
Log. Reg.
C4.5
kNN1
kNN2
kNN3

13.3
13.6
12.4
15.1
16.6
16.6
28.5
14.6
25.7
14.4

=
=
=
=

2.64
2
3.4
3
2.69
1
6
4.48
7.5 < 1% 2.56
7.5 < 1% 2.83
10 < 1% 16.7
2.64
5
9
10
2.4
4

< 1%

=

=

=

< 1%

14.8
< 5% 14.4
13.3

3.5
7
5
8
2
6
10 < 1% 24.1
3.5
16.9
< 5% 29.6
9
1
15.9

13.8

=
=

=

=

-

-

4
3
1
na
2
na
7
6
8
5

=

2.65
2.5
< 5% 4.12
3.09
< 5% 3.82
3.24
< 1% 4.41
< 1% 3.08
< 1% 2.65
< 1% 3.24

=

< 5%

=
=
=

2.5
1
9
5
8
6.5
10 < 5%
4
2.5
6.5

=
=
=

Table 2. Comparison of RNC with several classiﬁcation algorithms

sets, RNC is ﬁrst once, second 6 times and third 4 times. Thus, mostly RNC is
one of the 3 ﬁrst methods.

We also computed the mean rank. This is done 2 times, using or not the
classiﬁers adaBoost and Logistic Regression (which could not give an answer for
more than 2 classes). The result is shown in Table 3 where RNC comes out ﬁrst.
To see if the diﬀerence is signiﬁcant, we applied the simple Friedman test [4],
which showed a diﬀerence with a p-value of 6.849 × 10−6 and the post-hoc test
(comparing each classiﬁer by pair) gave the result shown in Table 4.

RNC Random Forest Ksvm adaBoost AD Log. Reg. C4.5 KNN1 KNN2 KNN3
2.88
Without adaBoost & LogReg 2.64

7,46 4,15
6.64 3.86

All methods

7,04
6.00

4,58
4.25

6,58
5.79

5,06

7,56

3.19
2.86

4,04
3.96

Table 3. Mean rank of the methods

RNC Random Forest Ksvm AD C4.5 KNN1 KNN2 KNN3

RNC

Random Forest

1

Ksvm

AD
C4.5
KNN1
KNN2
KNN3

0.84 0.015 0.0004 0.89 0.0067 0.66
0.93 0.032 0.0011 0.96
0.80

0.50 0.072
0.98

1

0.42
0.052

0.015
0.35

1
1

0.28

1

0.71
0.16

1

0.55

Table 4. p-value for the diﬀerence

These results are very encouraging, because we think that they could be

improved by varying some parameters such as:

– The choice of neighborhood structure, especially since we know that the
neighborhood graphs are particularly sensitive to the dimension of the rep-
resentation space.

– The type of base classiﬁer. Should we use the closest connected homogeneous
component? How to deﬁne precisely this notion of proximity ? Should we
take into consideration the size of the database or other characteristics of
the neighborhood such as density, etc...?

– The selection methods to improve the quality of the data sets or the classi-

ﬁers.

All these issues are under study and should produce signiﬁcant improvements
for RNCs based on geometrical graphs.

5 Conclusion and further work

Here we have provided a new approach for using neighborhood structures in
ensemble methods. The results obtained show that they are challenging the
most powerful techniques such as random forests and kSVM. Methods based on
geometrical neighborhood graphs outperform the classic methods such as kN N .
Many possibilities are underway in order to improve the RNC based on RNG.
A library containing all the functionalities that have been achieved is available
by emailing the authors.

References

1. Breiman, L.: Bias, variance, and arcing classiﬁers. Statistics (1996)
2. Breiman, L.: Random forests. Machine learning 45(1), 5–32 (2001)
3. Brown, G., Wyatt, J., Harris, R., Yao, X.: Diversity creation methods: a survey

and categorisation. Information Fusion 6(1), 5–20 (2005)

4. Demsar, J.: Statistical comparisons of classiﬁers over multiple data sets (2006)
5. Domingos, P.: A uniﬁed bias-variance decomposition and its applications. In:

ICML. pp. 231–238. Citeseer (2000)

6. Ham, J., Chen, Y., Crawford, M., Ghosh, J.: Investigation of the random forest
framework for classiﬁcation of hyperspectral data. IEEE Transactions on Geo-
science and Remote Sensing 43(3) (2005)

7. Ho, T., Kleinberg, E.: Building projectable classiﬁers of arbitrary complexity. In:

International Conference on Pattern Recognition. vol. 13, pp. 880–885 (1996)

8. Kohavi, R., Wolpert, D.: Bias plus variance decomposition for zero-one loss func-
tions. In: Machine Learning-International Workshop. pp. 275–283. Citeseer (1996)
9. O’Mahony, M.P., Cunningham, P., Smyth, B.: An assessment of machine learn-
ing techniques for review recommendation. In: Proceedings of the 20th Irish con-
ference on Artiﬁcial
intelligence and cognitive science. pp. 241–250. AICS’09,
Springer-Verlag, Berlin, Heidelberg (2010), http://portal.acm.org/citation.
cfm?id=1939047.1939075

10. Park, J., Shin, H., Choi, B.: Elliptic Gabriel graph for ﬁnding neighbors in a point
set and its application to normal vector estimation. Computer-Aided Design 38(6),
619–626 (2006)

11. Planeta, D.S.: Linear time algorithms based on multilevel preﬁx tree for ﬁnding
shortest path with positive weights and minimum spanning tree in a networks.
CoRR

12. Prasad, A., Iverson, L., Liaw, A.: Newer classiﬁcation and regression tree tech-
niques: bagging and random forests for ecological prediction. Ecosystems 9(2),
181–199 (2006)

13. Preparata, F., Shamos, M.: Computational geometry: an introduction. Springer

(1985)

14. Schapire, R.: The boosting approach to machine learning: An overview. Lecture

Notes In Statistics-Springer Verlag pp. 149–172 (2003)

15. Shipp, C., Kuncheva, L.: Relationships between combination methods and mea-
sures of diversity in combining classiﬁers. Information Fusion 3(2), 135–148 (2002)
16. Toussaint, G.: The relative neighbourhood graph of a ﬁnite planar set. Pattern

recognition 12(4), 261–268 (1980)

17. Wang, X., Tang, X.: Random sampling lda for face recognition pp. 259–267 (2004),

http://portal.acm.org/citation.cfm?id=1896300.1896337

18. Zhou, Z., Wu, J., Tang, W.: Ensembling neural networks: Many could be better

than all* 1. Artiﬁcial intelligence 137(1-2), 239–263 (2002)


