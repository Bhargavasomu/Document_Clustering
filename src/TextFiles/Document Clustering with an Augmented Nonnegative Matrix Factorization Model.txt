 

Document Clustering with an Augmented Nonnegative 

Matrix Factorization Model 

Zunyan Xiong, Yizhou Zang, Xingpeng Jiang, and Xiaohua Hu 

College of Computing and Informatics, Drexel University, Philadelphia, USA 

{zunyan.xiong,yizhou.zang,xingpeng.jiang, 

xiaohua.hu}@drexel.edu 

Abstract. In this paper, we propose an augmented NMF model to investigate 
the latent features of documents. The augmented NMF model incorporates the 
original  nonnegative  matrix  factorization  and  the  local  invariance  assumption 
on the document clustering. In our experiment, first we compare our model to 
baseline algorithms with several benchmark datasets. Then the effectiveness of 
the proposed model is evaluated using datasets from CiteULike. The clustering 
results are compared against the subject categories from Web of Science for the 
CiteULike dataset. Experiments of clustering on both benchmark data sets and 
CiteULike datasets outperforms many state of the art clustering methods. 

Keywords:  nonnegative  matrix  factorization,  graph  Laplacian,  regularization, 
clustering, social tagging. 

1 

Introduction 

Document clustering is an unsupervised machine learning technique aims to discover 
the classification of documents according to their similarities. So far many document 
clustering methods have been proposed, such as k-means [1], spectral clustering [2], 
non-negative  matrix  factorization  (NMF)  [3][4],  and  Probabilistic  Latent  Semantic 
Analysis (PLSA) [5] etc.   

Besides, traditional clustering methods mostly apply linear dimensional reduction 
to extract features of the data set. However, recent research has shown that most data 
structures are nonlinear. To deal with this problem, researchers referred to the idea of 
manifold  learning.  Manifold  learning  [6]  is  a  nonlinear  matrix  dimensionality 
reduction approach that tries to discover the low dimensional structure for the data in 
the  high  dimension.  There  are  several  popular  manifold  learning  methods,  such  as 
Isomap [7][8][9], Locally Linear Embedding [10], and Laplacian Eigenmaps [11] etc. 
In  [12],  Cai  et  al.  proposed  a  graph  regularized  non-negative  matrix  factorization 
(GNMF). GNMF embedded manifold learning into nonnegative matrix factorization 
by  means  of  local  invariance  assumption.  Experiments  proved  that  the  manifold 
learning can significantly improve the clustering results. 

One defect of the aforementioned clustering methods is that they only focus on one 
dimension of the data. However, to better study the cluster dataset, it’s important to 
explore the data structure from two dimensions, because the geometrical structures of 

V.S. Tseng et al. (Eds.): PAKDD 2014, Part II, LNAI 8444, pp. 348–359, 2014. 
© Springer International Publishing Switzerland 2014 

  Document Clustering with an Augmented Nonnegative Matrix Factorization Model 

349 

vectors in two dimensions of the matrix are independent to each other. For example, 
in a document-term matrix, the similarities of the document vectors are independent 
of the term vectors.   

Motivated  by  addressing  this  problem,  we  propose  a  novel  model  named 
augmented nonnegative matrix factorization (ANMF), which incorporates both matrix 
factorization and manifold learning on both dimensions of the data matrix. Then, we 
applied  the  method  in  a  social  tagging  system,  CiteULike.  One  of  the  biggest 
challenges here is how to establish a reliable clustering evaluation method. Since most 
contents of the social tagging systems are created by users, rarely any criterion exists 
to classify the contents, not to mention a gold standard for evaluation. In this paper, 
the dataset applied for the experiment is from CiteULike. In this paper, we solve the 
problem  by  using  the  subject  classification  from  Web  of Science  for  the  CiteULike 
dataset.  The  classification  provides  an  objective  and  reliable  standard  to  test  the 
effectiveness of algorithms.   

2 

Related Work 

2.1  NMF 

Nonnegative Matrix Factorization (NMF)[3][4][13] is widely applied in recent years 
and  proved  to  be  efficient  and  robust  in  various  situations.  NMF  decomposes  the 
original  matrix  to  the  product  of  two  nonnegative  matrices  consisting  of  latent 
vectors.  The  factorized  matrices  consist  of  nonnegative  components,  which  is 
convenient for data analysis. The relationship between the data points can be regarded 
as their distance of similarity on the graph.       

Given  a  data  matrix (cid:2180)(cid:3404)(cid:4670)(cid:1850)(cid:2869),…,(cid:1850)(cid:3015)(cid:4671)(cid:1488)(cid:1337)(cid:3014)(cid:3400)(cid:3015) ,  the  goal  of  nonnegative  matrix 
factorization  is  to  find  two  low-rank  nonnegative  matrices (cid:1847)  and (cid:1848)  whose  product 
can best approximate the original matrix (cid:1850): 
(cid:1850)(cid:3406)(cid:1847)(cid:1848)(cid:3021) 

Xu  et  al.  apply  NMF  method  in  document  clustering,  and  the  experiment  results 
indicate that NMF method outperforms the latent semantic indexing and the spectral 
clustering  methods  in  document  clustering  accuracies  [14].  However,  one  defect  of 
NMF  is  that  it  does  not  preserve  the  relational  structure  of  the  data  during  the 
factorization process. Two documents that are originally similar in their learned latent 
represents maybe dissimilar after factorization. 

2.2  GNMF 

In[12],  Cai  et  al.  introduced  Graph  Regularized  Nonnegative  Matrix  Factorization 
(GNMF)  model,  which  is  an  extension  of  Nonnegative  Matrix  Factorization  (NMF). 
The two nonnegative matrices represent the latent factors of the original matrix from 
the two dimensions respectively. GNMF model applies the local invariance assumption 
on the one of the two nonnegative matrices, which results in a regularization term to   
 
 

 

350 

Z. Xiong et al. 

(cid:2281)(cid:3404)(cid:1313)(cid:1850)(cid:3398)(cid:1847)(cid:1848)(cid:3021)(cid:1313)(cid:2870)(cid:3397)(cid:2019)(cid:1846)(cid:1870)(cid:4666)(cid:1848)(cid:3021)(cid:1838)(cid:1848)(cid:4667) 

the NMF objective function. The main idea of the local invariance assumption is that if 
two data points are  close to each other  in  the original  geometry,  they  should  still be 
close in the new representation after factorized [12]. This is formulated as follows: 

where (cid:1846)(cid:1870)(cid:4666)·(cid:4667)  denotes  the  trace  of  a  matrix. (cid:1838)  is  a  Laplacian  matrix,  which  is 
defined by (cid:1838)(cid:3404)(cid:1830)(cid:3398)(cid:1849)  [15]. (cid:1830)  is a diagonal matrix and its diagonal entries are the 
sum of columns or rows of the weight matrix (cid:1849)  (for (cid:1849)  is a symmetric matrix), i.e., 
(cid:1856)(cid:3036)(cid:3036)(cid:3404)∑ (cid:1875)(cid:3036)(cid:3037).
(cid:3014)(cid:3037)(cid:2880)(cid:2869)

   

In the experiment, two image data sets and one document corpus were selected for 
clustering and showed GNMF outperforms NMF model, k-means and SVD methods. 
A flaw of this model is that it only considers the local invariance assumption from one 
dimension of the data.   

2.3  Document Clustering on the Web 

Many document clustering methods including the aforementioned ones are developed 
on  the  relation  between  documents  and  terms  [14][5][12].  However,  for  web 
applications  other  than  textual  resources,  it’s  difficult  to  get  direct  content 
information. Sometimes even the textual documents are no longer available due to the 
instability of webpages. Therefore, it is necessary to exploit other information sources 
to improve the clustering effectiveness. Moreover, studies in [1] and [2] proved that 
compared  to  textual  contents  or  keywords  of  Web  pages,  social  tag  information  is 
more  reliable  for  clustering.  Besides,  most  traditional  content-based  clustering 
algorithms such as k-means and NMF ignore the semantic relations among terms. As 
a result, two documents  with no common terms  will be regarded as dissimilar even 
though  they  have  many  synonymic  or  semantically  related  terms.  Therefore,  it’s 
natural  to  incorporate  other  useful  information  to  benefit  the  document  clustering 
research. 

Ramage and Heymann proposed two methods in Web clustering that include both 
term  and  tag  information  [16].  One  applied  k-means  in  an  extended  vector  that 
includes both term and tag information; the other used the term and tag information in 
a  generative  clustering  algorithm  based  on  latent  Dirichlet  Allocation.  Their  study 
shows that including tagging data can significantly improve the clustering quality.   

Lu  et  al.  exploited  the  tripartite  information,  i.e.  resources,  users  and  tags,  for 
webpage clustering [17]. The authors integrated the tripartite information together for 
better clustering performance. Three different methods are proposed in the paper. The 
first  method  applies  the  structure  of  the  tripartite  network  to  cluster  Web  pages;   
the  second  method  uses  the  tripartite  information  in  k-means  by  combining  two   
or  three  different  vectors  together;  the  third  method  utilizes  the  Link  k-means 
algorithm  in  the  tripartite  information.  Results  indicated  that  all  clustering  methods 
incorporating 
the  content-based 
clustering.  Furthermore,  compared  to  the  other  two  methods,  the  tripartite  network 
has better performance.   

information  significantly  outperform 

tagging 

 

  Document Clustering with an Augmented Nonnegative Matrix Factorization Model 

351 

3 

Augmented Nonnegative Matrix Factorization Model 

In  this  section,  we  first  propose  the  augmented  nonnegative  matrix  factorization 
model,  which  simultaneously  incorporates  the  geometric  structures  of  both  the  data 
manifold  and  the  feature  manifold.  Then,  we  introduce  the  model  and  its  iterative 
algorithm in detail. 

(cid:2281)(cid:3404)(cid:1313)(cid:1850)(cid:3398)(cid:1847)(cid:1848)(cid:3021)(cid:1313)(cid:2870)(cid:3397)(cid:2019)(cid:1846)(cid:1870)(cid:4666)(cid:1848)(cid:3021)(cid:1838)(cid:1848)(cid:4667)(cid:3397)(cid:2020)(cid:1846)(cid:1870)(cid:4666)(cid:1847)(cid:3021)(cid:1838)(cid:3560)(cid:1847)(cid:4667) 

The objective function of the ANMF model is: 

Table 1. Important notation used in this paper 

Table 1. 

Notations 

Notations 

Description 
data feature set 

Description 

data set 

number of data points 

3.1  Notations and Problem Formalization 

For  convenience,  the  meaning  of  notations  used  in  the  paper  is  summarized  in 

Before describing the model, some useful definitions are introduced. Given a data set, 

data matrix of size (cid:1839)(cid:3400)(cid:1840) 
data partition of size (cid:1839)(cid:3400)(cid:1837) 
(cid:1863)th column of (cid:1847) 

(cid:2270)(cid:3404)(cid:4668)(cid:1856)(cid:2869),(cid:1856)(cid:2870),…,(cid:1856)(cid:3014)(cid:4669) ,  and (cid:2272)(cid:3404)(cid:4668)(cid:1858)(cid:2869),(cid:1858)(cid:2870),…,(cid:1858)(cid:3015)(cid:4669)   be  the  set  of  data  features.  Their 
relational  matrix  is  denoted  by (cid:1850)(cid:3404)(cid:4666)(cid:1876)(cid:3036)(cid:3037)(cid:4667)(cid:3014)(cid:3400)(cid:3015),  where (cid:1876)(cid:3036)(cid:3037)  denotes  the  weighting  of 
the data feature (cid:1858)(cid:3037)  for the data point (cid:1856)(cid:3036).   
(cid:2270) 
(cid:1839) 
(cid:1850) 
(cid:1847) 
(cid:2203)(cid:3038) 
(cid:1838) 
(cid:1849) 
(cid:1875)(cid:3037)(cid:3037)(cid:4594) 
(cid:1837) 
the nonnegative matrix factorization can be applied to decompose the matrix (cid:1850) 
such that the matrix U  and V  consists of the latent vectors associated to data points 
and features, i.e. the row vectors of U  and V  represent the latent vectors for the data (cid:1856)(cid:3036) 
and feature (cid:1858)(cid:3037)  respectively. In this approach, the latent vectors are supposed to represent 

number of features 

data point in the matrix 
feature partition of size 

(cid:1840)(cid:3400)(cid:1837) 
(cid:1863)th column of (cid:1848) 

To  analyze  the  similarities  between  the  data  points  and  the  features  respectively, 

a cell in the feature 
adjacency matrix 

the number of latent 

(cid:2272)
(cid:1840)
(cid:1876)(cid:3036)(cid:3037) 
(cid:1848) 
(cid:2204)(cid:3038)
(cid:1830)
(cid:1849)(cid:3561)
(cid:1875)(cid:3557)(cid:3036)(cid:3036)(cid:4594) 

(cid:1850)(cid:3406)(cid:1847)(cid:1848)(cid:3021), 

data graph Laplacian 

feature adjacency matrix 

data degree matrix 

data adjacency matrix 

a cell in the data 
adjacency matrix 

component 

 

 

 

the factorized meaningful parts (or topic) of the data set and their features. In order to 
quantize  the  similarities  of  data  and  features,  we  next  use  the  idea  of  local  invariance 
assumption to obtain two regularization terms, Regularizer I and Regularizer II.   

 

352 

Z. Xiong et al. 

3.2  Local Invariance Assumption 

nearest neighbors are considered. 

be easily compared by Euclidean distance.   

standard basis. Under the matrix decomposition, 

as 0-1 weighting, heat kernel weighting and dot-product weighting, the definitions of 

The local invariance assumption is a general principle that can be interpreted in this 
context  as  following.  If  the  data  are  close  in  some  sense,  after  the  NMF 
decomposition,  they  should  still  be  close  in  the  latent  space.  To  measure  the 

similarities between points of the original data, we construct two adjacency matrix (cid:1849) 
and (cid:1849)(cid:3561)   from (cid:1850)  for  both  feature  and  data  vectors.  The  metric  of  the  adjacency 
(closeness) between the vectors (cid:1875)(cid:3037)(cid:3037)(cid:4594)  (or (cid:1875)(cid:3557)(cid:3036)(cid:3036)(cid:4594)) can be defined in different ways, such 
the three weighting modes can be referenced in [18]. For each data point, only the (cid:1868) 
In  the  NMF  decomposition,  let (cid:1837)   be  the  number  of  latent  component,  and 
(cid:1837)(cid:1575) M, (cid:1837)(cid:1575)(cid:1840) .  The  data  and  features  are  mapped  to  points  in  a  lower 
(cid:1837) dimensional Euclidean space. From a geometric point of view, their similarity can 
Consider  the  matrix (cid:1850)(cid:3404)(cid:4666)(cid:1876)(cid:3036)(cid:3037)(cid:4667)(cid:3014)(cid:3400)(cid:3015) ,  let (cid:2206)(cid:3037)   be  the (cid:1862) th  column  vector,  i.e. 
(cid:1850) (cid:3404) (cid:4670)(cid:2206)(cid:2869),(cid:2206)(cid:2870),…(cid:2206)(cid:3015)(cid:4671). Then (cid:2206)(cid:3037)  can be regarded as the coordinates of feature (cid:2188)(cid:3037)  in the 
Let  the (cid:1863)th  column  vector  of  U  be (cid:2203)(cid:3038)(cid:4666)(cid:3012)(cid:4667)(cid:3404)(cid:1731)(cid:1873)(cid:3036)(cid:3038)(cid:4666)(cid:3012)(cid:4667)|(cid:1861)(cid:3404)1,2,…,|(cid:1839)|(cid:1732).  Then  the 
original vector (cid:2206)(cid:3037)  is approximated by the linear combination of vectors (cid:2203)’s: 
In  this  expression,  now (cid:2203)(cid:3038)'s  can  be  regarded  as  new  basis  vectors  for  the  latent 
space,  and  the  new  coordinates  for  feature (cid:2188)(cid:3037)   hence  are (cid:2204)(cid:3037)(cid:4666)(cid:3012)(cid:4667)(cid:3404)(cid:1731)(cid:1874)(cid:3037)(cid:3038)(cid:4666)(cid:3012)(cid:4667)|(cid:1863)(cid:3404)
1,2,…,|(cid:1837)|(cid:1732). According to the local invariance assumption, if feature vector (cid:2188)(cid:3037)  and 
(cid:2188)(cid:3037)(cid:4594)   are  close  in  the  original  coordinates,  they  should  still  be  close  in  the  new 
space (cid:3630)(cid:2188)(cid:3037)(cid:3398)(cid:2188)(cid:3037)(cid:4594)(cid:3630),  weighted  by  their  original  closeness (cid:1875)(cid:3037)(cid:3037)(cid:4594).  As  stated  previously, 
(cid:1875)(cid:3037)(cid:3037)(cid:4594)   can  be  calculated  by  0-1  weighting,  heat  kernel  weighting  or  dot-product 
It  can  be  seen  heuristically  that  for (cid:2284)(cid:2869)   bounded,  if (cid:1875)(cid:3037)(cid:3037)(cid:4594)   is  large,  meaning 
features (cid:1862), (cid:1862)(cid:4593)  are close in the adjacency, the Euclidean distance is forced to be small, 
which implies the factored feature (cid:2204)(cid:3037), (cid:2204)(cid:3037)(cid:4594)  are close. For computational convenience, 

(cid:1850)(cid:3406)(cid:1847)(cid:1848)(cid:3021), 
(cid:2206)(cid:3037)(cid:3406)∑
(cid:1874)(cid:3037)(cid:3038) 
(cid:2203)(cid:3038)
(cid:3012)(cid:3038)(cid:2880)(cid:2869)

coordinates. To quantize this information, we use the Euclidean distance in the latent 

weighting. Also as in [12], we define the Regularizer I as 

(cid:2284)(cid:2869)(cid:3404)(cid:2869)(cid:2870)∑
(cid:2284)(cid:2869)(cid:3404)(cid:2869)(cid:2870)∑

(cid:3630)(cid:2204)(cid:3037)(cid:3398)(cid:2204)(cid:3037)(cid:4594)(cid:3630)(cid:2870)(cid:1875)(cid:3037)(cid:3037)(cid:4594)
(cid:3630)(cid:2204)(cid:3037)(cid:3398)(cid:2204)(cid:3037)(cid:4594)(cid:3630)(cid:2870)(cid:1875)(cid:3037)(cid:3037)(cid:4594)

 

 

(cid:3015)(cid:3037),(cid:3037)(cid:4594)(cid:2880)(cid:2869)

(cid:3015)(cid:3037),(cid:3037)(cid:4594)(cid:2880)(cid:2869)

(1) 

 

 

 

 

 

we simplify the regularizer as following. 

 

353 

(cid:3015)
(cid:3037),(cid:3037)(cid:4594)(cid:2880)(cid:2869)

  Document Clustering with an Augmented Nonnegative Matrix Factorization Model 

(cid:3404)(cid:3533)(cid:2204)(cid:3037)(cid:3021)(cid:2204)(cid:3037)(cid:1875)(cid:3037)(cid:3037)(cid:3398) (cid:3533)(cid:2204)(cid:3037)(cid:3021)(cid:2204)(cid:3037)(cid:4594)(cid:1875)(cid:3037)(cid:3037)(cid:4594)
(cid:3015)
(cid:3037)(cid:2880)(cid:2869)
(cid:3404)(cid:1846)(cid:1870)(cid:4666)(cid:1848)(cid:3021)(cid:1830)(cid:1848)(cid:4667)(cid:3398)(cid:1846)(cid:1870)(cid:4666)(cid:1848)(cid:3021)(cid:1849)(cid:1848)(cid:4667)(cid:3404)(cid:1846)(cid:1870)(cid:4666)(cid:1848)(cid:3021)(cid:1838)(cid:1848)(cid:4667), 
  The Laplacian matrix (cid:1838)  is defined by (cid:1838)(cid:3404)(cid:1830)(cid:3398)(cid:1849)  [15]. 

where (cid:1846)(cid:1870)(cid:4666)·(cid:4667)  denotes the trace of a matrix. (cid:1830)  is a diagonal matrix and its diagonal 
entries  are  the  sum  of  columns  or  rows  of (cid:1849)  (for (cid:1849)  is  a  symmetric  matrix),  i.e., 
(cid:1856)(cid:3036)(cid:3036)(cid:3404)∑ (cid:1875)(cid:3036)(cid:3037).
(cid:3015)(cid:3037)(cid:2880)(cid:2869)
Here (cid:2019)  is a regularization parameter that balances the effects of local invariance. This 

(cid:2281)(cid:3404)(cid:1313)(cid:1850)(cid:3398)(cid:1847)(cid:1848)(cid:3021)(cid:1313)(cid:2870)(cid:3397)(cid:2019)(cid:1846)(cid:1870)(cid:4666)(cid:1848)(cid:3021)(cid:1838)(cid:1848)(cid:4667) 

Incorporating  this  information  to  the  NMF  model,  the  objective  function  now 

becomes 
 

(2) 

is the model considered in [1], called the graph regularized NMF method (GNMF). 

At  this  point,  it  is  important  to  notice  that  for  our  problem,  the  local  invariance 

assumption applies to the other piece of data, the features.   

To reflect the local invariance of the data, a second regularization term is added: 

 

(cid:2284)(cid:2870)(cid:3404)(cid:2869)(cid:2870)∑
(cid:1313)(cid:2203)(cid:3036)(cid:3398)(cid:2203)(cid:3036)(cid:4594)(cid:1313)(cid:2870)(cid:1875)(cid:3557)(cid:3036)(cid:3036)(cid:4594)
(cid:3015)(cid:3036),(cid:3036)(cid:4594)(cid:2880)(cid:2869)
(cid:3404)(cid:3533)(cid:2203)(cid:3036)(cid:3021)(cid:2203)(cid:3036)(cid:1856)(cid:3036)(cid:3036)(cid:3398)(cid:3533)(cid:2203)(cid:3036)(cid:3021)(cid:2204)(cid:2203)(cid:3036)(cid:4594)(cid:1875)(cid:3557)(cid:3036)(cid:3036)(cid:4594)
(cid:3015)
(cid:3015)
(cid:3036)(cid:2880)(cid:2869)
(cid:3036),(cid:3036)(cid:4594)(cid:2880)(cid:2869)
(cid:3404)(cid:1846)(cid:1870)(cid:3435)(cid:1847)(cid:3021)(cid:1830)(cid:3561)(cid:1847)(cid:3439)(cid:3398)(cid:1846)(cid:1870)(cid:3435)(cid:1847)(cid:3021)(cid:1849)(cid:3561)(cid:1847)(cid:3439) 
 (cid:3404)(cid:1846)(cid:1870)(cid:3435)(cid:1847)(cid:3021)(cid:1838)(cid:3560)(cid:1847)(cid:3439) 

 

 

function can be defined as 
 

We call this new model the augmented NMF (ANMF). Here the two regularization 

(cid:2281)(cid:3404)(cid:1313)(cid:1850)(cid:3398)(cid:1847)(cid:1848)(cid:3021)(cid:1313)(cid:2870)(cid:3397)(cid:2019)(cid:1846)(cid:1870)(cid:4666)(cid:1848)(cid:3021)(cid:1838)(cid:1848)(cid:4667)(cid:3397)(cid:2020)(cid:1846)(cid:1870)(cid:4666)(cid:1847)(cid:3021)(cid:1838)(cid:3560)(cid:1847)(cid:4667) 

where (cid:1849)(cid:3561)   is the adjacency matrix for the data and (cid:1838)(cid:3560)(cid:3404)(cid:1830)(cid:3561)(cid:3398)(cid:1849)(cid:3561) . Now the final cost 
parameters (cid:2019)  and (cid:2020)  are  positive  numbers  to  be  chosen  later.  They  balance  the 
optimal solution is obtained by minimizing (cid:2281)  over all non-negative matrices (cid:1847)  and 
(cid:1848).  We will discuss the algorithms in next section. 
function (cid:2281)  is  not  convex  in (cid:1849)  and (cid:1834)  jointly.  Thus  it  is  not  possible  to  find  global 
minima. However, it is convex in (cid:1849)  for fixed (cid:1834)  and vice versa. In fact, the Lagrange 

effects  of  local  invariance  and  the  original  NMF.  Heuristically,  the  larger  the 
parameters,  the  stronger  will  the  local  invariance  be  reflected  in  the  results.    The 

As  in  the  original  matrix  factorization  model  [19]  or  the  GNMF  model  [12],  the  cost 

Iterative Algorithm 

3.3 

(3) 

multiplier  method  used  in  [2]  is  also  applicable  here  to  give  an  iterative  algorithm. 
However  the  updating  rules  could  only  be  expected  converge  to  a  local  (not  global) 
minima. 

 

354 

Z. Xiong et al. 

Lagrangian is 

The cost function can be rewritten as 

(cid:2281)(cid:3404)(cid:1846)(cid:1870)(cid:4666)(cid:1850)(cid:3398)(cid:1847)(cid:1848)(cid:3021)(cid:4667)(cid:4666)(cid:1850)(cid:3398)(cid:1847)(cid:1848)(cid:3021)(cid:4667)(cid:3021)(cid:3397)(cid:2019)(cid:1846)(cid:1870)(cid:4666)(cid:1848)(cid:3021)(cid:1838)(cid:1848)(cid:4667)(cid:3397)(cid:2020)(cid:1846)(cid:1870)(cid:3435)(cid:1847)(cid:3021)(cid:1838)(cid:3560)(cid:1847)(cid:3439) 
(cid:3404)(cid:1846)(cid:1870)(cid:4666)(cid:1850)(cid:1850)(cid:3021)(cid:4667)(cid:3398)2(cid:1846)(cid:1870)(cid:4666)(cid:1850)(cid:1848)(cid:1847)(cid:3021)(cid:4667)(cid:3397)(cid:1846)(cid:1870)(cid:4666)(cid:1847)(cid:1848)(cid:3021)(cid:1848)(cid:1847)(cid:3021)(cid:4667)(cid:3397)(cid:2019)(cid:1846)(cid:1870)(cid:4666)(cid:1848)(cid:3021)(cid:1838)(cid:1848)(cid:4667)+ (cid:2020)(cid:1846)(cid:1870)(cid:3435)(cid:1847)(cid:3021)(cid:1838)(cid:3560)(cid:1847)(cid:3439) 
Here the basic properties (cid:1846)(cid:1870)(cid:4666)(cid:1827)(cid:4667) (cid:3404) (cid:1846)(cid:1870)(cid:4666)(cid:1827)(cid:3021)(cid:4667)  and (cid:1846)(cid:1870)(cid:4666)(cid:1827)(cid:1828)(cid:4667) (cid:3404) (cid:1846)(cid:1870)(cid:4666)(cid:1828)(cid:1827)(cid:4667) are used for 
any  matrices (cid:1827)  and (cid:1828).  Next  let (cid:2032)(cid:3036)(cid:3038)  be  the  Lagrange  multiplier  for  the  condition 
(cid:1873)(cid:3036)(cid:3038)(cid:3410)0,  and (cid:2038)(cid:3037)(cid:3038)   be  the  multiplier  for  the  condition (cid:1874)(cid:3037)(cid:3038)(cid:3410)0.  The  augmented 
 (cid:2278)(cid:3404)(cid:1846)(cid:1870)(cid:4666)(cid:1850)(cid:1850)(cid:3021)(cid:4667)(cid:3398)2(cid:1846)(cid:1870)(cid:4666)(cid:1850)(cid:1848)(cid:1847)(cid:3021)(cid:4667)(cid:3397)(cid:1846)(cid:1870)(cid:4666)(cid:1847)(cid:1848)(cid:3021)(cid:1848)(cid:1847)(cid:3021)(cid:4667)(cid:3397)(cid:2019)(cid:1846)(cid:1870)(cid:4666)(cid:1848)(cid:3021)(cid:1838)(cid:1848)(cid:4667)(cid:3397)(cid:2020)(cid:1846)(cid:1870)(cid:3435)(cid:1847)(cid:3021)(cid:1838)(cid:3560)(cid:1847)(cid:3439)(cid:3397)
(cid:1846)(cid:1870)(cid:4666)ΨU(cid:4667)(cid:3397)(cid:1846)(cid:1870)(cid:4666)Φ(cid:1848)(cid:3021)(cid:4667)                                                                                               (4) 
where Ψ(cid:3404)(cid:4666)(cid:2032)(cid:3036)(cid:3038)(cid:4667)(cid:3014)(cid:3400)(cid:3012)  and Φ(cid:3404)(cid:4666)(cid:2038)(cid:3037)(cid:3038)(cid:4667)(cid:3015)(cid:3400)(cid:3012). The partial derivatives are 
(cid:3105)(cid:2278)(cid:3105)(cid:3022)(cid:3404)(cid:3398)2(cid:1850)(cid:1848)(cid:3397)2(cid:1847)(cid:1848)(cid:3021)(cid:1848)(cid:3397)2(cid:2020)(cid:1838)(cid:3560)(cid:1847)(cid:3397)Ψ 
(cid:3105)(cid:2278)(cid:3105)(cid:3023)(cid:3404)(cid:3398)2(cid:1850)(cid:1847)(cid:3397)2(cid:1848)(cid:1847)(cid:3021)(cid:1847)(cid:3397)2(cid:2019)(cid:1838)(cid:1848)(cid:3397)Φ 
condition, (cid:2032)(cid:3036)(cid:3038)(cid:1873)(cid:3036)(cid:3038)(cid:3404)0,(cid:2038)(cid:3036)(cid:3038)(cid:1874)(cid:3036)(cid:3038)(cid:3404)0. The equations (5) and (6) become 
(cid:3398)(cid:4666)(cid:1850)(cid:1848)(cid:4667)(cid:3036)(cid:3038)(cid:3048)(cid:3284)(cid:3286)(cid:3397)(cid:4666)(cid:1847)(cid:1848)(cid:3021)(cid:1848)(cid:4667)(cid:3036)(cid:3038)(cid:3048)(cid:3284)(cid:3286)(cid:3397)(cid:2020)(cid:3435)(cid:1838)(cid:3560)(cid:1847)(cid:3439)(cid:3036)(cid:3038)(cid:3048)(cid:3284)(cid:3286)(cid:3404)0 
(cid:3398)(cid:4666)(cid:1850)(cid:3021)(cid:1847)(cid:4667)(cid:3037)(cid:3038)(cid:3049)(cid:3285)(cid:3286)(cid:3397)(cid:4666)(cid:1848)(cid:1847)(cid:3021)(cid:1847)(cid:4667)(cid:3037)(cid:3038)(cid:3049)(cid:3285)(cid:3286)(cid:3397)(cid:2019)(cid:4666)(cid:1838)(cid:1847)(cid:4667)(cid:3037)(cid:3038)(cid:3048)(cid:3285)(cid:3286)(cid:3404)0 

The  derivatives  vanish  at  local  minima.  Using  the  Karush-Kuhn-Tucker  (KKT) 

(7) 

(8) 

These equations give the following updating rules 

(cid:1873)(cid:3036)(cid:3038)(cid:1370)(cid:1873)(cid:3036)(cid:3038) (cid:4666)(cid:3025)(cid:3023)(cid:2878)(cid:3091)(cid:3024)(cid:3561)(cid:3022)(cid:4667)(cid:3284)(cid:3286)
(cid:4666)(cid:3022)(cid:3023)(cid:3269)(cid:3023)(cid:2878)(cid:3091)(cid:3005)(cid:3561)(cid:3022)(cid:4667)(cid:3284)(cid:3286) 
(cid:1874)(cid:3037)(cid:3038)(cid:1370)(cid:1874)(cid:3037)(cid:3038) (cid:3435)(cid:3025)(cid:3269)(cid:3022)(cid:2878)(cid:3090)(cid:3024)(cid:3023)(cid:3439)(cid:3285)(cid:3286)
(cid:3435)(cid:3023)(cid:3022)(cid:3269)(cid:3022)(cid:2878)(cid:3090)(cid:3005)(cid:3023)(cid:3439)(cid:3285)(cid:3286) 

 

 

 

 

 

 

 

 

(5) 

(6) 

(9) 

(10) 

(11) 

The updating rules of our model actually lead to convergence sequences, which are 

justified by Theorem 1 and its proof below. 

Theorem 1. Under the updating rules (9) and (10), the objective function (3) is non-
increasing. 

As in [12] and [18], the proof of Theorem 1 is essentially based on the existence of 
a proper auxiliary function for the ANMF. We give a simple proof on the ground of 
the following results from [12]. 

Lemma 2. Under the updating rule 

(cid:1874)(cid:3037)(cid:3038)(cid:1370)(cid:1874)(cid:3037)(cid:3038) (cid:3435)(cid:3025)(cid:3269)(cid:3022)(cid:2878)(cid:3090)(cid:3024)(cid:3023)(cid:3439)(cid:3285)(cid:3286)
(cid:3435)(cid:3023)(cid:3022)(cid:3269)(cid:3022)(cid:2878)(cid:3090)(cid:3005)(cid:3023)(cid:3439)(cid:3285)(cid:3286) 

 

is non-increasing. 

  Document Clustering with an Augmented Nonnegative Matrix Factorization Model 

355 

(cid:2281)(cid:2869)(cid:3404)(cid:1313)(cid:1850)(cid:3398)(cid:1847)(cid:1848)(cid:3021)(cid:1313)(cid:2870)(cid:3397)(cid:2019)(cid:1846)(cid:1870)(cid:4666)(cid:1848)(cid:3021)(cid:1838)(cid:1848)(cid:4667) 

(cid:2281)(cid:3022)(cid:3404)(cid:2281)(cid:3398)(cid:2019)(cid:1846)(cid:1870)(cid:4666)(cid:1848)(cid:3021)(cid:1838)(cid:1848)(cid:4667)(cid:3404)(cid:1313)(cid:1850)(cid:3398)(cid:1847)(cid:1848)(cid:3021)(cid:1313)(cid:2870)(cid:3397) (cid:2020)(cid:1846)(cid:1870)(cid:3435)(cid:1847)(cid:3021)(cid:1838)(cid:3560)(cid:1847)(cid:3439)
(cid:3404)(cid:1313)(cid:1850)(cid:3021)(cid:3398)(cid:1848)(cid:1847)(cid:3021)(cid:1313)(cid:2870)(cid:3397)(cid:2020)(cid:1846)(cid:1870)(cid:3435)(cid:1847)(cid:3021)(cid:1838)(cid:3560)(cid:1847)(cid:3439) 

The cost function (cid:2281)(cid:2869)  in GNMF, i.e. 
Proof of Theorem 1. Consider the objective function (cid:2281)  under the updating of (cid:1848)  by 
(11).  Then  the  last  term (cid:2020)(cid:1846)(cid:1870)(cid:3435)(cid:1847)(cid:3021)(cid:1838)(cid:3560)(cid:1847)(cid:3439)  in (cid:2281)  will  not  change.  It  suffices  to  prove 
(cid:2281)(cid:3023)(cid:3404)(cid:2281)(cid:3398)(cid:2020)(cid:1846)(cid:1870)(cid:3435)(cid:1847)(cid:3021)(cid:1838)(cid:3560)(cid:1847)(cid:3439)(cid:3404)(cid:2281)(cid:2869)  is non-increasing, which is exactly given by Lemma 2. 
Next  consider (cid:2281)  under  the  updating  of (cid:1847).  Since (cid:1834)  is  not  changed,  it  suffices  to 
Now  interchange (cid:1847), (cid:1848)  and  replace (cid:2020)  by (cid:2019), (cid:1850)  by (cid:1850)(cid:3021)  in  Lemma  2, (cid:2281)(cid:3022)  is  not 
increasing under the updating of (cid:1849)  by (9).                                                             (cid:1494) 
One  problem  of  the  objective  function  of  ANMF  is  that  the  solutions (cid:1847)  and (cid:1848) 
are not unique. If (cid:1847)  and (cid:1848)  are the solutions, then (cid:1847)(cid:1830)  and (cid:1848)(cid:1830)(cid:2879)(cid:2869)  can also be the 
matrix (cid:1847)  as one. This approach can be achieved by 
(cid:1873)(cid:3036)(cid:3038)(cid:1370) (cid:3048)(cid:3284)(cid:3286)(cid:3495)∑(cid:3048)(cid:3284)(cid:3286)(cid:3118)(cid:3284)
(cid:1874)(cid:3037)(cid:3038)(cid:1370)(cid:1874)(cid:3037)(cid:3038)(cid:3493)∑(cid:1873)(cid:3036)(cid:3038)(cid:2870)(cid:3036)
Input: the data matrix (cid:1850), regularization parameter (cid:2019)  and (cid:2020). 
Output: the data-topic matrix (cid:1847), and the topic-feature matrix (cid:1848). 
Construct weighting matrix (cid:1849)  and (cid:1849)(cid:3561) , compute the diagonal matrix (cid:1830)  and (cid:1830)(cid:3561); 

solutions  of  the  objective  function.  To  obtain  unique  solutions,  we  refer  to  the 
approach  from  [12]  that  enforces  the  Euclidean  distance  of  the  column  vectors  in 

Table 2 shows the simple algorithm of ANMF model.   

Table 2. Algorithm of ANMF 

                 

 

(12) 

(13) 

(34) 

consider 

Method: 

Random initialize U and V; 
Repeat (9) and (10) until convergence; 
Normalize U and V using (13) and (14). 

3.4  Complexity Analysis 

In  this  section,  the  computational  cost  of  NMF,  GNMF  and  ANMF  algorithms  are 

discussed. Supposing the algorithm stops after (cid:1872)  iterations, the overall cost for NMF 
is (cid:1841)(cid:4666)(cid:1872)(cid:1839)(cid:1840)(cid:1837)(cid:4667). For GNMF, the adjacency matrix needs (cid:1841)(cid:4666)(cid:1840)(cid:2870)(cid:1839)(cid:4667)  to construct, so the 
overall  cost  for  GNMF  is (cid:1841)(cid:4666)(cid:1872)(cid:1839)(cid:1840)(cid:1837)(cid:3397)(cid:1840)(cid:2870)(cid:1839)(cid:4667).  As  ANMF  adds  one  more  adjacency 
matrix on the other dimension, so the overall cost for ANMF is (cid:1841)(cid:4666)(cid:1872)(cid:1839)(cid:1840)(cid:1837)(cid:3397)(cid:1840)(cid:2870)(cid:1839)(cid:3397)
(cid:1839)(cid:2870)(cid:1840)(cid:4667). 

 

356 

Z. Xiong et al. 

4 

Experiments 

4.1  Data Sets and Evaluation Metrics 

Before  applying  CiteULike  data  set,  four  data  sets  were  chosen  as  the  benchmark, 
which were Coil20, ORL, TDT2, and Reuters-21578. Two of them are image data and 
the other two are text data.   

The  results  of  our  experiments  were  evaluated  by  Clustering  Accuracy  [20]  and 
normalized mutual information (NMI) [21]. Both of the evaluation metrics range from 
zero to one, and a high value indicates better clustering result. 

4.2 

Parameter Settings 

In  this  section,  we  compared  our  proposed  method  with  the  following  methods,  K-
means [22], NMF [4], and GNMF [12]. For both GNMF and ANMF, we normalized 

To  fairly  compare  algorithms,  each  algorithm  was  run  under  different  parameter 
settings, and the best results were selected to compare with each other. The number of 
clusters was set equal to the true number of standard categories for all the data sets 
and clustering algorithms.   

the vectors on columns of (cid:1849)  and (cid:1834).   
Here  we set the nearest neighborhood (cid:1868)  as 7 for both the algorithms. The value of   
(cid:1868)  determines the construction of the adjacency  matrix  for both GNMF and ANMF, 
the  performance  of  GNMF  and  ANMF  are  supposed  to  decrease  as (cid:1868)  increases, 
GNMF,  the  parameter  was  set  by  the  grid (cid:4668)10(cid:2879)(cid:2871),10(cid:2879)(cid:2870),10(cid:2879)(cid:2869),1,10,10(cid:2870),10(cid:2871),10(cid:2872)(cid:4669). 
For  ANMF  algorithm,  there  are  two  regularization  parameters (cid:2019)  and (cid:2020).  Both  of 
them were set by the grid (cid:4668)10(cid:2879)(cid:2871),10(cid:2879)(cid:2870),10(cid:2879)(cid:2869),1,10,10(cid:2870),10(cid:2871),10(cid:2872)(cid:4669).   

The 0-1 weighting was applied in GNMF and ANMF algorithms for convenience. 

which lies on the assumption that the neighboring data points share the same topic. So 

which was verified by [12] for GNMF. There is only one regularization parameter in 

The aforementioned algorithms were repeated 20 times for each parameter setting, 
the average results were computed. The best average results are shown in Table 3 and 
Table 4. 

4.3  Clustering Results 

Table 3 and 4 display the Accuracy and NMI of all algorithms on the four data sets 
respectively.  We  can see that overall both GNMF and ANMF performed much better 
than K-means and NMF algorithms. Note that both GNMF and ANMF consider the 
geometrical structure of the data through  the  local  invariance  assumption,  the  results 
imply the importance of the geometrical structure in mining the latent features of the 
data.  Besides,  ANMF  shows  the  best  performance  in  all  the  four  data  sets,  which 
indicates that by adding the geometrical structure for the two dimensions of the data, 
the algorithm can achieve better performance. 
 

 

  Document Clustering with an Augmented Nonnegative Matrix Factorization Model 

357 

Table 3. Clustering Accuracy (%) 

Data Sets 

Coil20 
ORL 
TDT2 

Reuters-21578  

(cid:2193)-means 
95.56%
96.40%
90.92%
74.04%

NMF 
95.90%
96.01%
90.11%
73.68%

GNMF 
97.80%
96.61%
95.09%
74.68%

ANMF 
97.82%
97.19%
95.35%
75.13%

Table 4. Normalized Mutual Information (%) 

Data Sets 

Coil20 
ORL 
TDT2 

Reuters-21578  

(cid:2193)-means 
73.86%
71.82%
64.54%
33.90%

NMF 
74.36%
66.80%
58.75%
29.98%

GNMF 
89.17%
72.01%
83.49%
34.41%

ANMF 
90.14%
75.24%
84.65%
36.31%

 

 

5 

Study on the CiteULike Data Set 

5.1  Data Processing 

CiteULike is a social bookmarking platform that allows researchers to share scientific 
references,  so  nearly  all  the  bookmarks  in  CiteULike  are  academic  papers.  The 
CiteULike data was crawled during January-December 2008. We extracted the article 
id, journal name of the articles, user id and tag information from the original data. The 
journal  name  of  the  articles  was  used  for  setting  evaluation  standard.  Before 
processing the dataset, we unified the format of the tags. Tags such as “data_mining”, 
“data-mining”, “data.mining”, “datamining”, etc. were all considered as the same one. 
Here  we  excluded  the  articles,  users  and  tags  with  less  than  four  bookmarks.  To 
evaluate the CiteULike dataset, we utilized the subject categories in Web of Science 
[23]. There are a total of 176 top-level subject categories for science journals. Under 
each subject category, they display a list of the afflicted journals. By overlapping the 
journals of all articles from CiteULike with the journals under the categories in Web 
of  Science,  we  could  discover  the  subject  categories  of  the  articles  in  CiteULike 
dataset.  Under  the  176  subject  categories,  we  only  kept  the  44  biggest  subject 
categories  with  the  largest  articles  numbers.  Finally,  we  had  3,296  bookmarks  with 
2406 articles, 1220 users and 4593 tags. 

5.2  Clustering Results 

We construct two matrices for CiteULike data set, article-user matrix and article-tag 
matrix. Besides, in order to test if combining the article vectors from article-user and 
article-tag vectors can get a better performance, we also construct a new matrix that 
consists  of  the  linear  combination  of  the  article-user  vectors  and  article-tag  vectors. 
Just as the experiments in section 4, we compare the clustering results of ANMF with 
GNMF, NMF and k-means based on the Clustering Accuracy and NMI. The settings 

for the parameters and the value of the nearest neighborhood (cid:1868)  are all the same as in 

section 4. 

 

358 

Z. Xiong et al. 

Table 5. The Evaluation Results for CiteULike Data Set 

Data Sets 

(cid:2193)-means 

article-user matrix 
article -tag matrix 

the combination matrix 

article -user matrix 
article -tag matrix 

the combination matrix 

30.04%
73.42%
68.65%

10.83%
25.00%
26.24%

NMF 

GNMF 
Clustering Accuracy (%)
44.22%
86.57%
88.46%
76.24%
68.94%
85.48%

Normalized Mutual Information (%)

19.03%
27.72%
23.85%

27.24%
32.07%
36.91%

ANMF 

87.17% 
88.43% 
87.60% 

28.55% 
36.85% 
42.35% 

 

Table  5  displays  the  evaluation  scores  of  the  four  algorithms  with  CiteULike 

dataset. The experiments reveal several interesting points: 
•  ANMF  still  performs  the  best  among  the  four  algorithms.  Specifically,  the 
improvement is significant in NMI results. This shows that ANMF is efficient not 
only in image and text data, but also in the data from social tagging systems, which 
suggests the potential of ANMF in collaborative filtering area. 

•  The evaluation results of the combination matrix are rather poor for k-means and 
NMF algorithms for the article-user matrix and article-tag matrix. For GNMF and 
ANMF,  their  NMI  scores  are  better  than  the  other  two  matrices,  while  the 
Clustering Accuracy scores are a little lower. 

6 

Conclusion 

In this paper, we have explored a graph regularized nonnegative matrix factorization 
model  for  document  clustering.  First,  we  applied  our  algorithm  in  four  benchmark 
data sets, and compared it with three canonical algorithms to evaluate its performance 
in clustering. Then the algorithm was used in CiteULike dataset by applying user and 
tag  information  for  analysis.  The  experiment  results  demonstrate  that  our  algorithm 
outperforms  GNMF,  NMF  and  k-means  models  in  both  benchmark  data  sets  and 
CiteULike data set. 

References 

1.  Jain,  A.K.,  Murty,  M.N.,  Flynn,  P.J.:  Data  clustering:  a  review.  ACM  Comput.  Surv. 

CSUR 31(3), 264–323 (1999) 

2.  Guy,  I.,  Carmel,  D.:  Social  recommender  systems.  In:  Proceedings  of  the  20th 

International Conference Companion on World Wide Web, pp. 283–284 (2011) 

3.  Lee, D.D., Seung, H.S.: Learning the parts of objects by non-negative matrix factorization. 

Nature 401(6755), 788–791 (1999) 

4.  Seung,  D.,  Lee,  L.:  Algorithms  for  non-negative  matrix  factorization.  Adv.  Neural  Inf. 

Process. Syst. 13, 556–562 (2001) 

 

  Document Clustering with an Augmented Nonnegative Matrix Factorization Model 

359 

5.  Hofmann, T.: Probabilistic latent semantic indexing. In: Proceedings of the 22nd Annual 
International  ACM  SIGIR  Conference  on  Research  and  Development  in  Information 
Retrieval, pp. 50–57 (1999) 

6.  Law,  M.H.,  Jain,  A.K.:  Incremental  nonlinear  dimensionality  reduction  by  manifold 

learning. Pattern Anal. Mach. Intell. IEEE Trans. 28(3), 377–391 (2006) 

7.  Balasubramanian,  M.,  Schwartz,  E.L.:  The  isomap  algorithm  and  topological  stability. 

Science 295(5552), 7–7 (2002) 

8.  Bengio, Y., Paiement, J.-F., Vincent, P., Delalleau, O., Le Roux, N., Ouimet, M.: Out-of-
sample  extensions  for  lle,  isomap,  mds,  eigenmaps,  and  spectral clustering.  Adv.  Neural 
Inf. Process. Syst. 16, 177–184 (2004) 

9.  Samko, O., Marshall, A.D., Rosin, P.L.: Selection of the optimal parameter value for the 

Isomap algorithm. Pattern Recognit. Lett. 27(9), 968–979 (2006) 

10.  Tenenbaum,  J.B.,  De  Silva,  V.,  Langford,  J.C.:  A  global  geometric  framework  for 

nonlinear dimensionality reduction. Science 290(5500), 2319–2323 (2000) 

11.  Belkin,  M.,  Niyogi,  P.:  Laplacian  eigenmaps  for  dimensionality  reduction  and  data 

representation. Neural Comput. 15(6), 1373–1396 (2003) 

12.  Cai, D., He, X., Han, J., Huang, T.S.: Graph regularized nonnegative matrix factorization 

for data representation. Pattern Anal. Mach. Intell. IEEE Trans. 33(8), 1548–1560 (2011) 

13.  Gaussier,  E.,  Goutte,  C.:  Relation  between  PLSA  and  NMF  and  implications.  In: 
Proceedings  of  the  28th  Annual  International  ACM  SIGIR  Conference  on  Research  and 
Development in Information Retrieval, pp. 601–602 (2005) 

14.  Xu,  W.,  Liu,  X.,  Gong,  Y.:  Document  clustering  based  on  non-negative  matrix 
factorization. In: Proceedings of the 26th Annual International ACM SIGIR Conference on 
Research and Development in Informaion Retrieval, pp. 267–273 (2003) 

15.  Chung, F.R.: Spectral graph theory, vol. 92. AMS Bookstore (1997) 
16.  Ramage, D., Heymann, P., Manning, C.D., Garcia-Molina, H.: Clustering the tagged web. 
In:  Proceedings  of  the  Second  ACM  International  Conference  on  Web  Search  and  Data 
Mining, pp. 54–63 (2009) 

17.  Lu, C., Hu, X., Park, J.: Exploiting the social tagging network  for Web clustering. Syst. 

Man Cybern. Part Syst. Humans IEEE Trans. 41(5), 840–852 (2011) 

18.  Matlab Codes and Datasets for Feature Learning, 

http://www.cad.zju.edu.cn/home/dengcai/Data/data.html (accessed: 
September 18, 2013) 

19.  Koren,  Y.,  Bell,  R.,  Volinsky,  C.:  Matrix  factorization  techniques  for  recommender 

systems. Computer 42(8), 30–37 (2009) 

20.  Gu, Q., Zhou, J.: Co-clustering on manifolds. In: Proceedings of the 15th ACM SIGKDD 

International Conference on Knowledge Discovery and Data Mining, pp. 359–368 (2009) 

21.  Strehl,  A.,  Ghosh,  J.:  Cluster  ensembles—a  knowledge  reuse  framework  for  combining 

multiple partitions. J. Mach. Learn. Res. 3, 583–617 (2003) 

22.  MacQueen, J.: Some methods for classification and analysis of multivariate observations. 
In:  Proceedings  of  the  Fifth  Berkeley  Symposium  on  Mathematical  Statistics  and 
Probability, vol. 1, p. 14 (1967) 

23.  Journal Search - IP & Science - Thomson Reuters, 

http://www.thomsonscientific.com/cgi-bin/jrnlst/ 
jlsubcatg.cgi?PC=D (accessed: October 01, 2013) 

 


