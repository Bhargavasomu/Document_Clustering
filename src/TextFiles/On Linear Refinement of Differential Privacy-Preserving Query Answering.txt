On Linear Reﬁnement of Diﬀerential
Privacy-Preserving Query Answering

Xiaowei Ying, Xintao Wu, and Yue Wang

University of North Carolina at Charlotte

{xying,xwu,ywang91}@uncc.edu

Abstract. Recent work showed the necessity of incorporating a user’s
background knowledge to improve the accuracy of estimates from noisy
responses of histogram queries. Various types of constraints (e.g., lin-
ear constraints, ordering constraints, and range constraints) may hold
on the true (non-randomized) answers of histogram queries. So the idea
was to apply the constraints over the noisy responses and ﬁnd a new set
of answers (called reﬁnements) that are closest to the noisy responses
and also satisfy known constraints. As a result, the reﬁnements expect
to boost the accuracy of ﬁnal histogram query results. However, there
is one key question: is the ratio of the distributions of the results after
reﬁnements from any two neighbor databases still bounded? In this pa-
per, we introduce a new deﬁnition, ρ-diﬀerential privacy on reﬁnement,
to quantify the change of distributions of reﬁnements. We focus on one
representative reﬁnement, the linear reﬁnement with linear constraints
and study the relationship between the classic -diﬀerential privacy ( on
responses) and our ρ-diﬀerential privacy on reﬁnement. We demonstrate
the conditions when the ρ-diﬀerential privacy on reﬁnement achieves the
same -diﬀerential privacy. We argue privacy breaches could incur when
the conditions do not meet.

Keywords: diﬀerential privacy, linear constraint, reﬁnement, background
knowledge.

1

Introduction

Research on diﬀerential privacy [1, 2] has shown that it is possible to carry out
data analysis on sensitive data while ensuring strong privacy guarantees. Diﬀer-
ential privacy is a paradigm of post-processing the output of queries. Diﬀerential
privacy is deﬁned as a property of a query answering mechanism, and a query
answering mechanism satisfying diﬀerential privacy must meet the requirement
that the distribution of its noisy query responses change very little with the
addition or deletion of any record, so that the analyst can not infer the presence
or absence of some record from the responses. Formally, diﬀerential privacy uses
a user-speciﬁed privacy threshold  to bound the ratio of the probabilities of the
noisy responses from any two neighbor databases (diﬀering one record).

Recent work [3–5] showed the necessity of incorporating a user’s background
knowledge to improve the accuracy of estimates from noisy responses of his-
togram queries. Various types of constraints (e.g., linear constraints, ordering

J. Pei et al. (Eds.): PAKDD 2013, Part II, LNAI 7819, pp. 353–364, 2013.
c(cid:2) Springer-Verlag Berlin Heidelberg 2013

354

X. Ying, X. Wu, and Y. Wang

constraints, and range constraints) may hold on the true (non-randomized) an-
swers of histogram queries. So the idea was to apply the constraints over the
noisy responses and ﬁnd a new set of answers (called reﬁnements) that are clos-
est to the noisy responses and also satisfy known constraints. As a result, the
reﬁnements expect to boost the accuracy of ﬁnal histogram query results.

However, there is one key question: is the ratio of the distributions of the
results after reﬁnements from any two neighbor databases still bounded? In this
paper, we introduce a new deﬁnition, ρ-diﬀerential privacy on reﬁnement, to
quantify the change of distributions of reﬁnements. We focus on one represen-
tative reﬁnement, the linear reﬁnement with linear constraints and study the
relationship between the classic -diﬀerential privacy (on responses) and our
ρ-diﬀerential privacy on reﬁnement. We demonstrate the conditions when the
ρ-diﬀerential privacy on reﬁnement achieves the same -diﬀerential privacy. We
argue privacy breaches could incur when the conditions do not meet.

2 Diﬀerential Privacy Revisited

We revisit the formal deﬁnition and the mechanism of diﬀerential privacy. We
denote the original database as D, and its neighboring database as D(cid:3)
. We will
concentrate on pairs of databases (D, D
) diﬀering only in one row, meaning one
is a subset of the other and the larger database contains just one additional row.
Deﬁnition 1. (-diﬀerential privacy) [1]. A mechanism K is -diﬀerentially pri-
vate if for all databases D and D
diﬀering on at most one element, and any
subsets of outputs S ⊆ Range(K),

(cid:3)

(cid:3)

Pr[K(D) ∈ S] ≤ e × Pr[K(D

(cid:3)

) ∈ S]

(1)
[1] For f : D → Rd, the mechanism Kf that adds independently
Theorem 1.
generated noise with distribution Lap(Δf /) to each of the d output terms satis-
ﬁes -diﬀerential privacy, where the sensitivity, Δf , is Δf = maxD,D(cid:2)(cid:6)f (D) −
f (D

diﬀering in at most one element.

)(cid:6)1 for all D, D

(cid:3)

(cid:3)

The mechanism for achieving diﬀerential privacy computes the sum of the true
answer and random noise generated from a Laplace distribution. The magnitude
of the noise distribution is determined by the sensitivity of the computation and
the privacy parameter speciﬁed by the data owner. Diﬀerential privacy maintains
composability, i.e., diﬀerential privacy guarantees can be provided even when
multiple diﬀerentially-private releases are available to an adversary, and can
extend to group privacy, i.e., changing a group of k records in the data set
induces a change of at most a multiplicative ek in the corresponding output
distribution [6].

3 ρ-Diﬀerential Privacy on Reﬁnement

In this section, we ﬁrst describe the notations and then formally deﬁne reﬁnement
based on background knowledge. We present deﬁnitions of unbiased reﬁnement

On Linear Reﬁnement of Diﬀerential Privacy-Preserving Query Answering

355

and constrained reﬁnement. We ﬁnally introduce our key concept, ρ-diﬀerential
privacy on reﬁnement, and use an illustrating example to show the diﬀer-
ence between the proposed ρ-diﬀerential privacy on reﬁnement and the classic
-diﬀerential privacy.In a diﬀerentially private query answering mechanism, the
analyst submits queries, the mechanism generates true values for the query, and
perturbs them with calibrated noise to derive the responses, then returns the
responses to the analyst. Usually, the analyst may possess some background
knowledge about the database. With background knowledge, the analyst can
reﬁne the responses given by the mechanism, and may obtain more accurate
values for his queries.

3.1 Deﬁnition
We denote the original database as D, and its neighboring database as D(cid:3)
which
diﬀers from the original database by a single record. The vector-valued query is
denoted as Q, Q = (q1, q2,··· , qn)T . We denote the true value from database
D for the query as μ, μ = (μ1, μ2,··· , μn)T , and the response from database D
for the query as X, X = (X1, X2,··· , Xn)T . And we denote the true value from
database D(cid:3)
(cid:3)
for the query as μ(cid:3)
n)T , and the response from
database D(cid:3)
(cid:3)
n)T . The randomization
for the query as X
mechanism satisﬁes -diﬀerential privacy, i.e., for an arbitrary set of integers
S = {i, j, . . . , k} ⊆ {1, . . . , n},

2,··· , μ
(cid:3)
(cid:3)
1, μ
(cid:3)
= (X
1, X

2,··· , X
(cid:3)

, μ(cid:3)
(cid:3)
, X

= (μ
(cid:3)

− ≤ Pr[XS]
(cid:3)
Pr[X
S]

≤ e

e

(2)
Assume that the user knows some background knowledge about D and D(cid:3)
denoted by B and B(cid:3)
respectively. For database D, we denote the estimated
value as (cid:2)X derived by the analyst from the response using background knowl-
edge, (cid:2)X = ( (cid:2)X1, (cid:2)X2,··· , (cid:2)Xn)T , for database D(cid:3)
=

, we denote it as (cid:2)X

, (cid:2)X

(cid:3)

(cid:3)

(cid:3)
1, (cid:2)X

2,··· , (cid:2)X
(cid:3)

(cid:3)
n)T .

( (cid:2)X
Deﬁnition 2. (Reﬁnement) Given the background knowledge B on database
D, the reﬁnement (cid:2)X = ( (cid:2)X1, . . . , (cid:2)Xn)T is the user’s estimation on the true value
of query Q(D) based on the response X: (cid:2)X = rf(X|B,D).

Similarly, given response X
rf(X

,D(cid:3)

(cid:3)|B(cid:3)

).

(cid:3)

from D(cid:3)

, the reﬁnement to estimate Q(D(cid:3)

) is (cid:2)X

(cid:3)

=

Deﬁnition 3. (Unbiased Reﬁnement) The reﬁnement (cid:2)X is unbiased if E( (cid:2)X) =
μ stands for any μ.

Deﬁnition 4. (Constrained Reﬁnement) The reﬁnement (cid:2)X is a constrained
reﬁnement if (cid:2)X always satisﬁes the background knowledge B for any response X.
The two reﬁnements, (cid:2)X = ( (cid:2)X1, . . . , (cid:2)Xn) from D and (cid:2)X
D(cid:3)

(cid:3)
n) from
, may be mapped to two disjoint spaces by the reﬁnement function rf(). In this

(cid:3)
1, . . . , (cid:2)X

= ( (cid:2)X

(cid:3)

356

X. Ying, X. Wu, and Y. Wang

case, either the numerator or the denominator of ratio Pr( (cid:2)X=x)
is 0. However,
Pr( (cid:2)X(cid:2)=x)
this diﬀerence is due to the reﬁnement strategy and does not disclose any privacy
information.

Deﬁnition 5. (ρ-diﬀerential privacy on reﬁnement) Given the reﬁnements
and an arbitrary set of integers S = {i, j, . . . , k} ⊆ {1, . . . , n}, deﬁne
(cid:2)X and (cid:2)X

(cid:3)

(cid:2)XS = ( (cid:2)Xi, (cid:2)Xj, . . . , (cid:2)Xk) and (cid:2)X

(cid:3)
S = ( (cid:2)X

(cid:3)
i, (cid:2)X

(cid:3)
j, . . . , (cid:2)X

(cid:3)
k).

Let RS and R(cid:3)
The reﬁnement satisﬁes diﬀerential privacy, if RS ∩R(cid:3)
Ω ⊆ R ∩ R(cid:3)

the following inequality stands

S be the sets of all possible values of (cid:2)XS and (cid:2)X

(cid:3)
S respectively.
S (cid:8)= ∅ and for any subset

−ρ ≤ Pr[ (cid:2)XS ∈ Ω]
S ∈ Ω]
(cid:3)

Pr[ (cid:2)X

e

≤ eρ.

(3)

3.2 An Illustrating Example

Example 1. The analyst submits a vector-valued query Q, Q = (q1, q2)T ,
max|μ − μ(cid:3)| = 1, and σ = 1
 . The analyst has the background knowledge that
(cid:3)
(cid:3)
μ1 + μ2 = c and μ
2 = c
. One method to reﬁne the response is shown in (4):
(X1 + c − X2), (cid:2)X2 =

(X2 + c − X1).

(cid:3)
1 + μ

(cid:2)X1 =

(4)

1
2

1
2

Equivalently expressed in matrix:

(cid:3)

(cid:4)

(cid:2)X1
(cid:2)X2

=

(cid:5) 1
2

− 1

2

− 1

2

1
2

(cid:6) (cid:5)

(cid:6)

X1
X2

+

(cid:6)

c

(cid:5) 1
2
1
2

(5)

The reﬁnement in (5) belongs to constrained reﬁnement. So we can calculate the
(cid:3)
1 = x1) to obtain the bound. First, from formula (5),
ratio Pr( (cid:2)X1 = x1)/ Pr( (cid:2)X
(cid:3)
we derive the probability density function of (cid:2)X1 and (cid:2)X
1, shown in (6) and (7).

f (cid:2)X1 (x1) =

f (cid:2)X(cid:2)

1

(x1) =

1
2σ
1
2σ

exp

(cid:7)

R

(cid:7)

exp

R

(cid:8)−|2x1 + x2 − c − μ1| + |x2 − μ2|
(cid:8)−|2x1 + x2 − c
| + |x2 − μ
(cid:3)
2

(cid:3)
1

σ
(cid:3) − μ
σ

(cid:9)

dx2

|

(cid:9)

dx2

(6)

(7)

Without loss of generality, we assume that μ1 − μ

(cid:3)
1 = 1. When x1 is suﬃciently
large, we can simplify formulas (6) and (7) to formulas (8) and (9) respectively.

f (cid:2)X1 (x1) =

f (cid:2)X(cid:2)

1

(x1) =

(σ + 2x1 − 2μ1) exp
(σ + 2x1 − 2μ

(cid:3)
1) exp

1
2σ
1
2σ

(cid:8)− 2(x1 − μ1)
(cid:8)− 2(x1 − μ
(cid:3)
1)

σ

σ

(cid:9)

(cid:9)

(8)

(9)

On Linear Reﬁnement of Diﬀerential Privacy-Preserving Query Answering

357

The ratio of the two PDFs can then be calculated as shown in (10), which tends
to be e2 for large value of response X1.
(σ + 2x1 − 2μ1)
(σ + 2x1 − 2μ
(cid:3)
1)
(σ + 2x1 − 2μ1)
(σ + 2x1 − 2μ
(cid:3)
1)

e2 → e2 (as x1 → ∞)

f (cid:2)X1 (x1)
f (cid:2)X(cid:2)
(x1)

2(μ1 − μ

(cid:3)
1)

(10)

exp

(cid:8)

(cid:9)

1

=

=

σ

So we can conclude that the ratio between the distributions of reﬁnements for
databases D and D(cid:3)
could be diﬀerent from the ratio between the distributions of
responses. In this example, the classic -diﬀerential privacy incurs 2-diﬀerential
(cid:2)
privacy on reﬁnement.

4 Background Knowledge and Reﬁnement Analysis

In this section, we formally the linear constraint based background knowledge
and conduct theoretical analysis on how reﬁnement strategies aﬀect diﬀerential
privacy on reﬁnement. We will use the following scenario as a running example
throughout this section. Consider that a data publisher (such as a school) has
collected grade information about a group of students and would like to allow
the third party to query the data while preserving the privacy of the individuals
involved. Assume the analyst submits a simple vector query: Q =(qA, qB, qC , qD,
qF , qp, qt). qA, qB, qC , qD, and qF represent the numbers of students receiving
grades A, B, C, D, and F respectively; qp represents the number of passing
students (grade D or higher) and qt represents the query for the number of all
the students.

⎧

⎪

⎨

⎪

⎩

μA + μB + μC + μD − μp = 0
μF + μp − μt = 0
μA + μB = 80

(11)

The analyst may have the background knowledge in terms of the linear con-
straints shown in (11). The ﬁrst two constraints are by the deﬁnition and inde-
pendent of the underlying database whereas the third constraint holds speciﬁ-
cally on the current database.
The analyst may have the background knowledge in terms of the ordering
constraint, e.g., μA ≤ μp. Ordering constraint can also be enforced when the
user submits the vector query. For example, the analyst may submit a simple
ascending ordering query that shows the number of students in each category.

In other words, the analyst knows for sure that μ1 ≤ μ2 ≤ ... ≤ μn although the

responses may not hold the order constraints due to calibrated noises. Similarly
the range constraint denotes the true answer of a particular query is within some
ﬁnite range, e.g., μA ∈ [1, 10]. Range constraints are often implicitly used in the
post-process of noisy output. For example, users apply non-negative constraints
when dealing with responses with negative values for attributes like age or salary.
We will study these types of background knowledge in our future work.

358

X. Ying, X. Wu, and Y. Wang

4.1 Reﬁnement with Linear Constraint

Assume that the user knows m linear combinations of the true answers:

i = 1, . . . , m.

b1iμ1 + b2iμ2 + ··· + bniμn = ci,
i μ = ci, where bji is the j-th entry of bi. Let B = [b1, . . . , bm]

Equivalently, bT
and c = (c1, . . . , cm)T .
Deﬁnition 6. (Linear Constraint) The background knowledge with linear
constraint can be expressed as
where B is an n × m matrix and c is an m-dimensional constant vectors.
Under the linear constraint based background knowledge, a constrained reﬁne-
ment (cid:2)X must satisfy BT

(cid:2)X = c for any response X.

BT μ = c,

Deﬁnition 7. (Reﬁnement with Linear Constraint) The reﬁnement (cid:2)X is
linear if it can be expressed as

(12)
where A and D are n × n and n × m matrices respectively, and h is an n-
dimensional constant vector.

(cid:2)X = AX + Dc + h,

4.2 A General Result

= c(cid:3)

Theorem 2. Suppose that the user possesses the linear background knowledge
BT μ = c and BT μ(cid:3)
respectively, and he implements
some constrained linear reﬁnement as shown in (12) to estimate μ. Assume
rank(B) = m and rank(A) = r = n − m. Let U = (U1, U2) , V = (V1, V2) , Σ =
U T . Adding

,be the SVD of A: A = U ΣV T , and A∗

for database D and D(cid:3)

= V

Σ−1

(cid:16)

(cid:17)

(cid:15)

(cid:14) Σ1 0
0 0

0
1
0 Im

noise from distribution Lap(σ), σ = ΔQ/, would result in

(cid:6)A∗D(c − c(cid:3)

) − (μ − μ(cid:3)

)(cid:6)1

(cid:6)μ − μ(cid:3)(cid:6)1

,

ρ =

are from the two databases that achieves (cid:6)μ − μ(cid:3)(cid:6)1 = ΔQ.

where μ and μ(cid:3)
Proof. Let Ω = {ω1, . . . , ωk} ⊆ {1, 2, . . . , n}, and PΩ be the n × k matrix with
P (ωi, i) = 1 and 0 elsewhere. Similarly, ¯Ω = {1, . . . , n} − Ω. with n × (n − k)
matrix P ¯Ω deﬁned likewise. We can rewrite the reﬁnement function to

(cid:2)X = U ΣV T X + Dc + h.

(cid:14) Σ1 0
0 I

(cid:15) V T X. With V T = V −1 and |V | = 1, we can have that

(cid:15)

(cid:14) Z1
Z2

Let Z =
=
the PDF of Z is

fZ(z) =

1|Σ1| fX (V Σ∗z), where Σ∗

=

(cid:5)Σ−1

1 0
0 I

(cid:6)

.

On Linear Reﬁnement of Diﬀerential Privacy-Preserving Query Answering

359

Let W = U Z, S = (cid:2)X − Dc − h = U1Z1, and T = U2Z2. Then, W = S + T ,
S ∈ U∞, S ∈ U∈, and the PDF of W is given by

fW (w) =

1|Σ1| fX(V Σ∗U T w) =

1|Σ1| fX(A∗w).

Notice that S and T are actually the projection of W onto the space spanned
by U1 and U2 respectively, and the two spaces are orthogonal. For any s ∈ U∞
and t ∈ U∈, W = s + t would always give S = s. Therefore, if s ∈ U∞, the PDF
of S is given by

fS(s) =

=

(cid:7)

U∈

(cid:7)

1|Σ1|
1|Σ1|

fX [A∗

(s + t)]dt =

1|Σ1|
fX (A∗s + V2z2) dz2.

(cid:7)

(cid:18)A∗ (cid:14)s + U

fX

(cid:14) 0
z2

(cid:15)(cid:15)(cid:19)

dU

(cid:15)

(cid:14) 0
z2

Hence, the PDF of (cid:2)X can be given by

f (cid:2)X (x) =

(cid:7)

1|Σ1|

fX [A∗

(x − Dc − h) + V2z2] dz2,

if U T

2 (x − Dc − h) = 0, and f (cid:2)X (x) = 0 otherwise.

Similarly, the PDF of (cid:2)X

is given by

(cid:3)

f (cid:2)X(cid:2) (x) =

(cid:7)

1|Σ1|

fX(cid:2) [A∗

(x − Dc(cid:3) − h) + V2z2] dz2,

if U T

2 (x − Dc(cid:3) − h) = 0, and f (cid:2)X(x) = 0 otherwise.
Notice that (cid:2)XΩ = P T
(cid:2)XΩ can be expressed as

Ω (cid:2)X, (cid:2)X ¯Ω = P T
¯Ω

(cid:2)X and (cid:2)X = PΩ (cid:2)XΩ + P ¯Ω (cid:2)X ¯Ω. The PDF of

f (cid:2)XΩ (xΩ) =

(cid:7) (cid:7)

dz2dx ¯ΩfX [A∗

(PΩxΩ + P ¯Ωx ¯Ω − Dc − h) + V2z2] ,

1|Σ1|

D(xΩ )

(13)

where D(xΩ) = {x ¯Ω : U T

2 (PΩxΩ + P ¯Ωx ¯Ω − Dc − h) = 0}.

The PDF of (cid:2)X

(cid:3)
Ω can be derived in a similar manner. When the ratio of the

integral kernels in (13) is bounded, i.e.,

− ≤ fX [A∗

(x − Dc − h) + V2z2]
fX(cid:2) [A∗(x − Dc(cid:3) − h) + V2z2]

e

≤ e,

(14)

360

X. Ying, X. Wu, and Y. Wang

the ratio of the integrals, f (cid:2)XΩ (xΩ)/f (cid:2)X(cid:2)
that fX and fX are the Laplace distribution p.d.f., and hence

(xΩ), is also bounded by [e

Ω

−, e]. Note

f (cid:2)XΩ (xΩ)
(xΩ)
f (cid:2)X(cid:2)

Ω

(x − Dc − h) + V2z2]
≤ fX [A∗
fX(cid:2) [A∗(x − Dc(cid:3) − h) + V2z2]
≤ exp{ 1
σ(cid:6)A∗
(x − Dc − h) + V2z2 − μ(cid:6)1}
σ(cid:6)A∗(x − Dc(cid:3) − h) + V2z2 − μ(cid:3)(cid:6)1}
exp{ 1
≤ exp
(cid:8)± 1
σ

(cid:6)A∗D(c − c(cid:3)

) − (μ − μ(cid:3)

)(cid:6)1

.

(cid:9)

Therefore, when the noise is added according to the classical schema , i.e., take
±ρ where
σ =

(cid:6)μ−μ(cid:3)(cid:6)1, we can have f (cid:2)XΩ (xΩ)/f (cid:2)X(cid:2)

(xΩ) is bounded by e

ΔQ
 = 1

Ω

(cid:6)A∗D(c − c(cid:3)

) − (μ − μ(cid:3)

(cid:6)μ − μ(cid:3)(cid:6)1

ρ =

)(cid:6)1

.

(cid:10)(cid:11)

A special case of the above result is that ρ =  when c = c(cid:3)
(no diﬀerence
on constants of linear background constraints over two neighbor databases).
However, in practice, c could be diﬀerent from c(cid:3)
(refer to the example shown in
Appendix), the ρ-diﬀerential privacy on reﬁnement is generally diﬀerent from the
-diﬀerential privacy. A direct result from the above theorem is that, in order
 maxD,D(cid:2)
to guarantee e
(cid:6)A∗D(c − c(cid:3)

− ≤ f (cid:2)XΩ (xΩ)/f (cid:2)X(cid:2)
) − (μ − μ(cid:3)

(xΩ) ≤ e, we can choose σ = 1

)(cid:6)1.

Ω

4.3 The Best Linear Reﬁnement

Consider the following least square reﬁnement based on the linear background
knowledge:

min(cid:6) (cid:2)X − X(cid:6)2

s.t. BT

(cid:2)X = c.

(15)

Theorem 3. The least square reﬁnement from the optimization problem in (15)
is given by

(cid:18)I − B(BT B)

−1BT (cid:19)

(cid:2)X =

X + B(BT B)

−1c.

(16)

The reﬁnement shown in (16) is a constrained unbiased reﬁnement. It has the
minimum variance of (cid:2)Xi, i = 1, . . . , n, among all linear unbiased reﬁnements.

Proof. The Lagrange function of (16) is

L = ( (cid:2)X − X)T ( (cid:2)X − X) − 2Λ(BT

(cid:2)X − c).

Taking ∂L

∂ (cid:2)X = 0, we can have (cid:2)X = X + BT Λ, and hence

BT
Λ = (BT B)

(cid:2)X = BT (X + BΛ) = c
−1(c − BT X)

(cid:2)X = X + B[(BT B)

−1(c − BT X)]

On Linear Reﬁnement of Diﬀerential Privacy-Preserving Query Answering

361

Equivalently, (cid:2)X can be expressed as follows:

(cid:2)X = [I − B(BT B)

−1BT ]X + B(BT B)

−1c.

Next, we show that (cid:2)X is a unbiased constrained reﬁnement:

(cid:2)X = BT X − BT B(BT B)

BT
E( (cid:2)X) = [I − B(BT B)
= μ − B(BT B)

−1BT X + BT B(BT B)

−1c = c.

−1BT ] E(X) + B(BT B)
−1BT μ + B(BT B)

−1c
−1c = μ.

Next, we prove the minimal variance property. We use M to denote the matrix
I − B(BT B)
−1BT . Then we have M = M T , MM T = M . We can further
show that (A − I)M = 0. Notice that the following equalities stand for any μ,

E( (cid:2)X) = A E(X) + Dc + h ⇒ μ = Aμ + DBT μ + h.

We can thus have I − A = DBT and h = 0. Therefore,

(A − I)M = −DBT [B(BT B)

−1BT − I] = 0.

Since Cov( (cid:2)X) = A Cov(X)A(cid:3)
of matrix AAT . With MM T = M and (A − I)M = 0, we can have

= 2σ2AAT , V( (cid:2)Xi)/2σ2 is the i-th diagonal entry

AAT = [(A − M ) + M ][(A − M )T + M T ] = (A − M )(A − M )T + M .

Since (A−M )(A−M )T is the semi-positive deﬁnite matrix, and the the diagonal
entries are non-negative, and hence (AAT )ii ≥ Mii with A = M minimizes
(AAT )ii, i = 1, . . . , n.

5 Conclusion and Further Discussion

In this paper we have introduced a new deﬁnition, ρ-diﬀerential privacy on re-
ﬁnement, to quantify the change of distributions of results after reﬁnements.
We focus on one representative reﬁnement, the linear reﬁnement with back-
ground knowledge as linear constraints and investigate the relationship between
the classic -diﬀerential privacy (on responses) and our ρ-diﬀerential privacy on
reﬁnement.

Three techniques were proposed to use constraints to boost accuracy of an-
swering range queries over histograms [3–5]. The reﬁnement approach (also called
constrained inference) [3] focused on using consistency constraints, which should
hold over the noisy output, to improve accuracy for a variety of correlated his-
togram queries. The idea was to ﬁnd a new set of answers ¯q that is the closet set
to the set of noisy answers ˜q and that also satisﬁes the consistency constraints.
The proposed approach, the minimum least squares solution, was a special case
of our linear reﬁnement with linear constraints presented in this paper. Hay et al.
in [3] also showed that the inferred ¯q based on the minimum L2 solution satisﬁes

362

X. Ying, X. Wu, and Y. Wang

-diﬀerential privacy. In our work, we introduced the general linear reﬁnement
and showed the conditions on when the reﬁnement based on the general linear
constraints achieves the same -diﬀerential privacy as deﬁned over distributions
of responses. The authors extended to reﬁne degree distribution of networks un-
der the context of publishing private network data [7]. Xiao et al. in [4] proposed
an approach based on the Haar wavelet. In [5], the authors uniﬁed the two ap-
proaches [3, 4] in one general framework based on the matrix mechanism that
can answer a workload of predicate counting queries.

One key question is whether background knowledge can be exploited by ad-
versaries to breach privacy. It is well known that for the pre-processing based
privacy preserving data mining models, several works [8, 9] showed the risks of
privacy disclosure by incorporating a user’s background knowledge in the rea-
soning process. In contrast, in the context of diﬀerential privacy, the authors
in [1, 2] stated that diﬀerential privacy provides formal privacy guarantees that
do not depend on an adversary’s background knowledge (including access to
other databases) or computational power. In [10], the authors gave an explicit
formulation of resistance to background knowledge. The formulation follows the
implicit statement: Regardless of external knowledge, an adversary with access
to the sanitized database draws the same conclusions whether or not my data
is included in the original data. They presented a mathematical formulation of
background knowledge and belief. The belief is modeled by the posteriori distri-
bution: given a response, the adversary draws his belief about the database using
Baye’s rule to obtain a posterior reﬁnement. In [3–5], the authors also stated that
the reﬁnement has no impact on the diﬀerential privacy guarantee. This is be-
cause the analyst performs the reﬁnement without access to the private data,
using only the constraints and the perturbed responses. The perturbed responses
are simply the output of a diﬀerentially private mechanism and post-processing
of responses cannot diminish the rigorous privacy guarantee.

In [11], the authors examined the assumptions of diﬀerential privacy from the
data generation perspective and proposed a participation-based guideline - does
deleting an individual’s tuple erase all evidence of the individual’s participation
in the data-generation process? - for determining the applicability of diﬀerential
privacy. They showed that the privacy guarantee from diﬀerential privacy can
degrade when applied to social networks or when deterministic statistics (of a
contingency table) have been previously released. The deterministic statistics
can be modeled as linear constraints with ﬁxed c values. In this case, c could be
diﬀerent from c(cid:3)
. Based on our Theorem 2, the ρ-diﬀerential privacy on reﬁne-
ment is diﬀerent from the -diﬀerential privacy and we have to add larger noise
to prevent privacy breaches. In practice, the adversary may possess any kind of
background knowledge, which may even include the a-priori knowledge of the
exact values of all other n− 1 individuals. We refer readers to the example shown
in Appendix where the adversary can exploit the background knowledge of the
other n− 1 individuals in the database to infer the value of a speciﬁc individual.
We argue that the privacy breach is caused by the combination of the random-
ization mechanism and the background knowledge. In our future work, we would

On Linear Reﬁnement of Diﬀerential Privacy-Preserving Query Answering

363

explore whether reﬁnements with some particular background knowledge (e.g.,
ordering or range constraints) can incur privacy breaches, i.e., enabling the ad-
versary to draw signiﬁcantly diﬀerent beliefs about the databases.

Acknowledgments. This work was supported in part by U.S. National Science
Foundation IIS-0546027 and CCF-0915059.

References

1. Dwork, C., McSherry, F., Nissim, K., Smith, A.: Calibrating Noise to Sensitivity in
Private Data Analysis. In: Halevi, S., Rabin, T. (eds.) TCC 2006. LNCS, vol. 3876,
pp. 265–284. Springer, Heidelberg (2006)

2. Dwork, C.: A Firm Foundation for Private Data Analysis. Communications of the

ACM (January 2011)

3. Hay, M., Rastogi, V., Miklau, G., Suciu, D.: Boosting the Accuracy of Diﬀerentially
Private Histograms Through Consistency. Proceedings of the VLDB Endowment
3(1) (2010)

4. Xiao, X., Wang, G., Gehrke, J.: Diﬀerential Privacy via Wavelet Transforms. In:
Proceedings of the 26th IEEE International Conference on Data Enginering, pp.
225–236. IEEE (2010)

5. Li, C., Hay, M., Rastogi, V., Miklau, G., McGregor, A.: Optimizing Linear Count-
ing Queries Under Diﬀerential Privacy. In: Proceedings of the 29th ACM SIGMOD-
SIGACT-SIGART Symposium on Principles of Database Systems of Data, pp.
123–134. ACM (2010)

6. Dwork, C., Lei, J.: Diﬀerential Privacy and Robust Statistics. In: Proceedings of
the 41st Annual ACM Symposium on Theory of Computing, pp. 371–380. ACM
(2009)

7. Hay, M., Li, C., Miklau, G., Jensen, D.: Accurate Estimation of the Degree Dis-
tribution of Private Networks. In: Proceedings of the 9th IEEE International Con-
ference on Data Mining, pp. 169–178. IEEE (2009)

8. Martin, D., Kifer, D., Machanavajjhala, A., Gehrke, J., Halpern, J.: Worst-Case
Background Knowledge for Privacy-Preserving Data Publishing. In: Proceedings
of the 26th IEEE International Conference on Data Enginering. IEEE (2007)

9. Du, W., Teng, Z., Zhu, Z.: Privacy-MaxEnt: Integrating Background Knowledge
in Privacy Quantiﬁcation. In: Proceedings of the ACM SIGMOD International
Conference on Management of Data, ACM (2008)

10. Ganta, S., Kasiviswanathan, S., Smith, A.: Composition Attacks and Auxiliary In-
formation in Data Privacy. In: Proceeding of the 14th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, pp. 265–273. ACM (2008)
11. Kifer, D., Machanavajjhala, A.: No Free Lunch in Data Privacy. In: Proceedings
of the ACM SIGMOD International Conference on Management of Data, pp. 193–
204. ACM (2011)

A Appendix
A.1 Example When c (cid:2)= c(cid:2)
Database D with n records is obtained by adding one record to database D0.
Every record in D belongs to one of two categories. The attacker knows that

364

X. Ying, X. Wu, and Y. Wang

in D0, kμ1 = μ2, where μi denotes the count of category i in D0, i = 1, 2.
The added record belongs to either of the two categories, denoted by D(cid:3)
and
D(cid:3)(cid:3)
and D(cid:3)(cid:3)
respectively. The background knowledge can be expressed as:

be the counts of D(cid:3)

respectively. Let μ(cid:3)

and μ(cid:3)(cid:3)

(cid:16) μ(cid:2)(cid:2)
μ(cid:2)(cid:2)

(cid:16) μ(cid:2)
μ(cid:2)

=

=

2

2

1

1

(cid:17)

(cid:17)

if D(cid:3)
if D(cid:3)(cid:3)

(cid:3)
is true: kμ
1
(cid:3)(cid:3)
1

is true: kμ

− μ
(cid:3)
2 = BT μ(cid:3)
− μ
(cid:3)(cid:3)
2 = BT μ(cid:3)(cid:3)

= c(cid:3)
= c(cid:3)(cid:3)

= k,
= −1,

(cid:15)

where B =
that, for D(cid:3)
Consider the following reﬁnement:

(cid:14) k−1
.
and D(cid:3)(cid:3)

, the reﬁnements (cid:2)X

(cid:3)

Response X = (X1, X2) is obtained by adding noise Lap( 2

(cid:3)(cid:3)

 ). Next, we show
do not satisfy diﬀerential privacy.

and (cid:2)X

For D(cid:3)
For D(cid:3)(cid:3)

: (cid:2)X1 =

: (cid:2)X1 =

X1 + X2 + k
X1 + X2 − 1

k + 1

k + 1

, and (cid:2)X2 = X2;

, and (cid:2)X2 = X2.

(17)

(18)

Comparing (17) and (18) with the general linear reﬁnement formula in (12), we
can have

(cid:6)

(cid:5)

(cid:6)

(cid:6)

A =

(cid:5) 1

k+1
0

, D =

(cid:5) 1

k+1
0

, and h =

0
0

.

When X1 satisﬁes x1 ≥ μ(cid:2)

, we can have

1

k+1
1
1+μ(cid:2)
k+1

2+k

fX1(z)fX2 [(k + 1)x1 − k − z]dz
|
| + |z − (k + 1)x1 + k + μ
(cid:3)
2

(cid:3)
1

(cid:9)

dz

σ

(cid:9)

[(k + 1)x1 − k − n]

(cid:7)

(x1) =

1

f (cid:2)X(cid:2)
∝ (cid:7)

exp

R

(cid:8)

= exp

R

(cid:8)−|z − μ
n + k − (k + 1)x1

(note n = μ

σ
(cid:3)
1 + μ

(cid:3)
2).

For D(cid:3)(cid:3)

, we can similarly have that when x1 ≥ μ(cid:2)(cid:2)

2 −1

1 +μ(cid:2)(cid:2)
k+1

,

(x1) ∝ exp

(cid:8)

f (cid:2)X(cid:2)(cid:2)

1

n − 1 − (k + 1)x1

σ

(cid:9)

[(k + 1)x1 + 1 − n].

With σ = 2

 (satisfying -diﬀerential privacy), we can have:

lim
x1→∞

1

f (cid:2)X(cid:2)
f (cid:2)X(cid:2)(cid:2)

1

(x1)

(x1)

= exp

(cid:20)

(k + 1)

(cid:21)

2

.

/f (cid:2)X(cid:2)(cid:2)

for suﬃciently large X1 +X2, which
Therefore, the ratio f (cid:2)X(cid:2)
indicates the adversary can tell which database of D(cid:3)
the response is
from. In other words, the adversary can derive the value of the added record by
reﬁnement.

and D(cid:3)(cid:3)

reaches e

1

1

2

(k+1)


