Learning Gradients with Gaussian Processes(cid:2)

Xinwei Jiang1, Junbin Gao2,(cid:2)(cid:2), Tianjiang Wang1, and Paul W. Kwan3

1 Intelligent and Distributed Computing Lab,
School of Computer Science and Technology,

Huazhong University of Science and Technology, Wuhan, 430074, China

ysjxw@hotmail.com

2 School of Computing and Mathematics,

Charles Sturt University, Bathurst, NSW 2795, Australia

jbgao@csu.edu.au

3 School of Science and Technology,

University of New England, Armidale, NSW 2351, Australia

kwan@turing.une.edu.au

Abstract. The problems of variable selection and inference of statistical
dependence have been addressed by modeling in the gradients learning
framework based on the representer theorem. In this paper, we propose
a new gradients learning algorithm in the Bayesian framework, called
Gaussian Processes Gradient Learning (GPGL) model, which can achieve
higher accuracy while returning the credible intervals of the estimated
gradients that existing methods cannot provide. The simulation examples
are used to verify the proposed algorithm, and its advantages can be seen
from the experimental results.

1 Introduction

Analyzing data sets associated with many variables or coordinates has become
increasingly challenging in many circumstances, especially in biological and phys-
ical sciences [1]. A wide range of machine learning algorithms based on the
regularization theory such as support vector machines (SVMs) [2] have been
proposed to solve the predictive problems in the past two decades. Although
these approaches demonstrate quite acceptable and robust performances in a
lot of experiments and applications, sometimes one also wants to get an insight
into the relationships between the coordinates and the inﬂuence of the coor-
dinates/attributes/features on the outputs. For example, it is very interesting
to investigate which covariant is most signiﬁcant for prediction and how the
variables vary with respect to each other in estimation.

The gradient of the target function provides a valuable measure to charac-
terize the relationships [3,4,5,1] and it has been used in many approaches and

(cid:2) This research was supported by the National High Technology Research and Devel-

opment (863) Program of China (2007AA01Z161).

(cid:2)(cid:2) The author to whom all the correspondence should be addressed.

M.J. Zaki et al. (Eds.): PAKDD 2010, Part II, LNAI 6119, pp. 113–124, 2010.
c(cid:2) Springer-Verlag Berlin Heidelberg 2010

114

X. Jiang et al.

applications. For example, the minimum average variance estimation (MAVE)
method and the outer product of gradients (OPG) estimation approach pro-
posed by [3] focus on ﬁnding the eﬀective dimension reduction (e.d.r.) space
in the data sets by using the gradients of the training data implicitly and
explicitly, respectively. These models show better performance in the estima-
tion of e.d.r. space than others, but learning gradient information would fail
in the “large m (dimension), small n (size of the dataset)” paradigm [6]. Re-
cently, [4] and [5] proposed a method to learn the gradient of a target function
directly from a given data set based on the Tikhonov regularization method,
which avoided the overﬁtting problem in the “large m, small n” settings. The
most signiﬁcant statistical measure we can get by those nonparametric
kernel based models is the gradient outer product (GOP) matrix, which can
interpret the importance of the coordinates for the prediction and the covari-
ation with respect to each other. In addition, with the assistance of spectral
decomposition of the gradient outer product matrix, the e.d.r. directions can
be directly estimated [1]. Furthermore [7] extended gradient learning algorithm
from the Euclidean space to the manifolds setting, and provided the conver-
gence rate dependent on the intrinsic dimension of the manifold rather than
the dimension of the ambient space. This is very important in the “large m,
small n” settings. Except for the application examples proposed in the avail-
able literature, gradient learning from scattered data sets is particularly im-
portant
reconstruction in computer graphics where, when
visually scaled geometric surfaces constructed from scattered data, analytical
expression (or rules) of gradients was highly desirable in calculating the nor-
mals at any given point needed in most surface reconstruction algorithms
(see [8]).

surfaces

for

However, these direct gradient learning methods cannot oﬀer any reasonable
error bars for the estimated gradients because essentially the task of estimating
gradients is the problem of point estimation. In many application scenarios, a
conﬁdence interval on the estimates is very important, such as found in computer
graphics.

In this paper, we propose a new gradient learning approach under the Bayesian
framework based on Gaussian Processes (GPs) [9]. Compared to the learning
gradients method in [4], not only can our algorithm apply in the “large m, small
n” cases and achieve higher accuracy, but it can also return the error bars of
the estimated gradients, which provide us with an estimate of the uncertainty
of stability. We will verify these features in Sections 5.

The rest of this paper is organized as follows. In Section 2, we introduce
the statistical foundation for learning gradients. The gradients learning method
with Gaussian Processes will be proposed in Section 3, which includes a brief
introduction of the Gaussian Processes regression. The algorithm derivation
is illustrated in Section 4. Then, simulated data are used to verify our algo-
rithm in Section 5. Finally, closing remarks and comments will be given in
Section 6.

Learning Gradients with Gaussian Processes

115

2 The Statistical Foundation for Learning Gradients

m and yi ∈ R

2.1 Notations
Denote data D = {(xi, yi)}n
i=1 where xi is a vector in a m-dimensional compact
metric subspace X ⊂ R
p is a vector too. Without loss of generality,
we will assume that p = 1. Our approach can be easily extended to the case
of vectorial value outputs y. Typically we assume that the data are drawn i.i.d.
from a joint distribution, (xi, yi) ∼ p(X, Y ). In the standard regression problem,
we want to model the regression function F deﬁned by the conditional mean of
Y |X, i.e., F = EY [Y |X]. The gradient of F is a vectorial value function with m
components, if all the partial derivatives exist,

(cid:2)

(cid:3)T

f(x) (cid:2) ∇F = (f1(x), ..., fm(x))T =

∂F (x)
∂x1

,··· ,

∂F (x)
∂xm

(1)

where xi are the components of the vector x and f(x) = (f1(x), ..., fm(x)).

The gradient and the issues of variable selection and coordinate covariation

are relevant because the gradient can provide following information [4]:

(cid:4)(cid:4)(cid:4)(cid:4)(cid:4) ∂F

∂xi

(cid:4)(cid:4)(cid:4)(cid:4)(cid:4) indicates the

1. Variable selection: the norm of the partial derivative

signiﬁcance of the variables for the prediction because a small norm implies
a slight change in the function F with respect to the i-th coordinate.

2. Coordinate covariation: the inner product of the partial derivatives with

respect to diﬀerent dimensions
i-th and j-th coordinates.

indicates the covariance of the

(cid:6)

(cid:5) ∂F

∂xi

,

∂F
∂xj

A central concept in all gradients learning approaches, called the gradient outer
product (GOP) matrix, is deﬁned by

Γij = E(cid:6) ∂F
∂xi

,

(cid:7)

∂F
∂xj

(2)

The GOP has a deep relation with the so-called eﬀective dimension reduction
(e.d.r.) space and the relationship was exploited in several gradient regression
methods such as MAVE and OPG [3] and [4,5].

2.2 Learning Gradients

To propose our approach for gradients learning, let us focus on the introduction
of those available algorithms of learning the gradients from the data. Recall that
the MAVE and OPG suﬀer from the problem of the overﬁtting in the “large
m, small n” paradigm, so the so-called regularized framework has been used to
overcome the overﬁtting based on the kernel representer theorem. Actually the

116

X. Jiang et al.

kernel representer theorem is also the motivation for our algorithm in the next
section. The algorithms based on the kernel representer theorem show better
performance than the MAVE and OPG [1].

Our goal is to design a model for the gradient estimate directly from data. The
conventional methods usually take a two-steps procedure by ﬁrst learning a re-
gression function F and then calculating the gradients of the F . However a direct
gradients learning algorithm may have more advantages than the conventional
ways, as demonstrated in [4,1].

Essentially, all of those kinds of models are motivated by the Taylor expansion

of the target function:

yi ≈ yj + f(xi)T (xi − xj) for xi ≈ xj

(3)

(4)

f

n(cid:10)

⎫⎬
A model for gradients leaning from an observation dataset D is deﬁned as
⎭
(cid:15)
(cid:14)
− (cid:3)xi−xj(cid:3)2

wij[yi − yj + f(xi)T (xj − xi)]2 + λ(cid:9)f(cid:9)2

⎧⎨
⎩ 1

f := argmin

where
where wij is called weights and deﬁned as wij = 1
σ2 is set to the median of the input data. When xj is far away from xi, the
Taylor expansion of the function F (xj) at xi makes less contribution to the
regression objective.

σm+2 exp

2σ2

i,j=1

n2

According to the representer theorem [10], the optimal solution to (4) is the
linear combination of kernel function deﬁned on the data points, thus the prob-
lem is actually transformed to solving a linear systems problem, see [4].

Due to regularization, this model can prevent overﬁtting in the “large m, small
n” paradigm and obtain fairly remarkable performance. However, sometimes it
is also important that we want to know the error bars of the point estimation
for the gradient, which can not be provided by those kinds of models.

An alternative method is to deﬁne the model under the Bayesian learning
and inference framework. We aim to use the Gaussian Processes (GPs) model
which is also based on the kernel and can be viewed as the exhibition of the
representer theorem. So motivated by the model in [4] and associated it with the
GPs, we will show how to improve the accuracy and compute the error bars of
the estimated gradient in the following section.

3 Gradients Learning with Gaussian Processes

3.1 Gaussian Processes Regression
Given the data set which consists of the i.i.d. samples from unknown distribu-
1),··· , (xn, yn)} ⊂ R
tion D = {(x1, y
p The standard Gaussian Process
regression is concerned with the case when p = 1. The goal is to estimate the
p(y|x∗) for a test data x∗. In the standard Gaussian processes (GPs) regression
model, a latent variable f is introduced in the model deﬁned by

m × R

y = f(x) + 

Learning Gradients with Gaussian Processes

117

Denote by X = {xi}n

where  is the additive noise, speciﬁed by the likelihood p(y|f, x) = p(y|f). This
model is nonparametric because the latent variable f is random function which
follows the Gaussian Process with zero mean and covaiance function k(·,·). Also
t .
the likelihood follows a Gaussian distribution with zero mean and covariance σ2
i=1. Due to the independence of the samples D, its
likelihood under the model is the product of p(yi|f(xi)) which is a Gaussian
too. Given a test point x∗, it is easy to check that the joint distribution of the
(cid:16)
(cid:17)
latent function is given by, see [9],
∼ N

(cid:17)(cid:19)

(cid:18)

(cid:16)

(5)

0,

KXX KXx∗
Kx∗X Kx∗x∗

f
f∗

where K are matrix of the kernel function values at the corresponding points
and N (μ, Σ) denotes the Gaussian distribution with mean μ and covariance Σ.
Under the Gaussian likelihood assumption, we can simply add the covariance
of the noise to the GP prior due to the independence assumption of the noise.
So the predictive distribution on the observation is
f∗|x∗, X, y ∼ N (Kx∗X(KXX + σ2
t I)

−1y, Kx∗x∗ − Kx∗X(KXX + σ2
t I)

−1KXx∗)
(6)

where the variance of the conditional distribution illustrates the uncertainty of
the prediction and the mean can be written as f(x∗) =
αiK(xi, x∗), where
t I)−1y. This form of the prediction exhibits the fact that the GP
α = (KXX + σ2
can be represented in terms of a number of basis function is one feature of the
representer theorem.

i=1

(cid:20)n

3.2 Gradients Estimation Model with Gaussian Processes
To apply the Gaussian Process model in the case of gradients learning, we have
to overcome two hurdles. First, the regression model (4) shows that we are
dealing with a multi-task regression problem as the gradient f is a vectorial
function, so we have to generalize the standard Gaussian Process regression to
multi-task case. This has been done in the recent works such as [11]. Second, the
i.i.d. assumption for the data set can not be used to produce a joint likelihood
which is the product of individual likelihood at each data point. In fact, when we
transform (4) into probabilistic formulation, we see that the coupling between
data makes learning and inference more complicated. However, we can still deﬁne
a likelihood for the whole data set D rather than a likelihood for each data pair.
Under the above modiﬁcation, we can formulate the gradients learning model
in the Bayesian framework based on the GPs, named Gaussian Process Gradient
Learning (GPGL) model, and we will show the advantages in Section 5.

Based on the analysis we have just given, a new likelihood formulation by
extending the datum-based likelihood to dataset-based likelihood is deﬁned as

p(Y |X, f) ∝ exp

wij[yi − yj + f(xi)T (xj − xi)]2

(7)

⎫⎬
⎭

⎧⎨
⎩− 1

2

n(cid:10)

i,j=1

(cid:20)n

118

X. Jiang et al.

(cid:20)n

j=1

j=1

i,j=1

(cid:20)n

wij(yi − yj)2, the
Let us introduce the following notation, the scalar c =
wij(xi − xj)(xi − xj)T , and the m dimensional
m × m matrices Bi =
wij(yi − yj)(xi − xj), i = 1, 2, ..., n. We will use the same
vectors hi =
weights wij as [4] for comparison. Furthermore deﬁne B = U T diag(B1, B2,··· ,
Bn)U and a column vector of dimension mn h = U T [hT
n ]T , where U
1
is a permutation matrix. Similarly deﬁne the column vector (of dimension mn)
f = [f T
1
(cid:21)
observation dataset D can be written as

Under the above notation, it is easy to validate that the likelihood (7) of the

,··· , hT

, ..., f T

m]T where f i = [fi(x1), fi(x2), ..., fi(xn)]T .

, f T
2

, hT
2

p(Y |X, f) =

1
M exp

(cid:22)
− 1
(f T Bf − 2hT f + c)
2

where M is the normalized constant.

The variable f collects the information of m partial derivatives over the given
input data X. In our model formulation, the variable f is assumed to be a
Gaussian Processes while the covariance function is Σ = Kf f ⊗ KXX. So the
Gaussian processes prior is

(cid:21)

(cid:22)

(8)

(9)

p(f|X, θ) =

1

|2πΣ|1/2

exp

− 1
f T Σ−1f
2

m×m is the coordinate-similarity matrix, KXX ∈ R

where Kf f ∈ R
n×n is the
covariance matrix of the samples X, and θ is the parameters of the covariance
function.

By using the Bayesian formulation, the posterior of f given the dataset is

p(f|X, Y, θ) =

p(Y |X, f)p(f|X, θ)

p(Y |X, θ)

(10)

As all the densities in the above relation are Gaussian, it is easy to derive, see
Appendix A of [9], the posterior of f

(cid:21)

(cid:22)
− 1
(f − Eh)T E−1(f − Eh)
2

(11)

p(f|X, Y, θ) =

1

|2πE|1/2

exp

where E = (B + Σ−1)−1.

For a new data x∗, we want to estimate f∗ = f(x∗) based on the observation

data. According to the predictive distribution of Gaussian processes, we have
f∗|f , x∗, X, θ∼N ((K f⊗K∗X)T Σ−1f , K f⊗ K∗∗−(K f ⊗ K∗X)T Σ−1(K f⊗K∗X))
(12)

where K∗X = K(X, x∗), K∗∗ = K(x∗, x∗). By integrating over the uncertainty
f according to the posterior (11), we can get the gradients predictive distribution

(cid:23)

p(f∗|x∗, X, Y ) =

Learning Gradients with Gaussian Processes

119

p(f∗|f , x∗, X)p(f|X, Y )df

(cid:21)
(cid:22)
− 1
(f∗ − P )T Q−1(f∗ − P )
2

=

1

|2πQ|1/2

exp

(13)

That is, the gradients predictive distribution is a Gaussian with the mean P and
the covariance Q. Thus the gradient estimate is given by
−1h

P = (Kf f ⊗ K∗X)T (BΣ + I)

(14)

and the error bar is given by

Q = Kf f ⊗ K∗∗ − (Kf f ⊗ K∗X)T (Σ + B−1)

−1(Kf f ⊗ K∗X).

(15)

4 Learning Kernel Hyperparameters

To develop an approach for learning coordinate-similarity matrix Kf f and the
kernel hyperparameters, we use gradient-based optimization of the marginal like-
lihood p(Y |X, θ). Without loss of generality, we just consider Kf f as unit matrix.
Since Kf f controls the correlations between m dimensions of the gradients, the
simplicity means that we are assuming the independence of diﬀerent coordinates.
Actually the optimization with respect to the parameters in Kf f can be dealt
with in the same way as follows [11].

Then the log marginal likelihood log p(Y |X, θ) is given by

L = − 1
2

log|B−1 + Σ| − 1
hT (B−1 + Σ)
2

−1h + C.

(16)

(cid:25)

where C is a constant independent of the parameter θ, which can be ignored in
optimizing L with respect to θ. To work out a formula for the derivatives of L
with respect to θ, we refer to the matrix reference manual for the notation [12].
Denote by F1 = − log|B−1 + Σ|, then dF1 = −(B−1 + Σ)−1 :T d(Σ) : .
:T d(Σ) : , where

(cid:24)
(B−1 + Σ)−1hhT (B +−1 +Σ)−1

Similarly, we have dF2 =
F2 = −hT (B−1 + Σ)−1h,
According to the derivative formula for the Kronecker product of matrices,
we have d(Σ) = d(Kf f ⊗ KXX) = (Im,m ⊗ Tn,m ⊗ In,n)(Kf f : ⊗In2,n2)dKXX :,
where Tm,n, called the vectorized transpose matrix, is the mn× mn permutation
matrix whose (i, j)-th elements is 1 if j = 1 + m(i − 1) − (mn − 1)(cid:12) i−1
n (cid:13) or 0
otherwise.
(cid:27)

So the derivatives of L with respect to θ is

∂L
∂θ =

1
−1 :)T + ((B−1 + Σ)
2
(Im,m ⊗ Tn,m ⊗ In,n)(Kf f : ⊗In2,n2)

−1hhT (B−1 + Σ)
dKXX

−1 :)T

(cid:26)
−((B−1 + Σ)

.

dθ

(17)

120

X. Jiang et al.

In our experiments, we learn the parameters of the models so as to maximize
the marginal likelihood using gradient-based search. The code is based on Neil
D. Lawrence’s MATLAB packages Kern and Optimi1.
We have seen that (B−1 + Σ)−1 needs to be inverted for both making pre-
dictions and learning the hyperparameters in time O(m3n3). This can lead to
computational problems if mn is large. Although we only use cholesky decompo-
sition and singular value decomposition to accelerate computation, the eﬃcient
approximation method in [11] can be directly used in our GPGL algorithm to
reduce the computational complexity.

5 Experiments

In this section we will verify our GPGL algorithm in two simulated data sets to
show the higher accuracy of the estimation and the credible intervals that the
gradient learning methods in [4], named Mukherjee’s algorithm in the following,
can not gain. In the ﬁrst data set, we generate some samples from four simple
functions which can compute the real gradients for comparison. Another high-
dimensional data set is used to test that our algorithm can be applied to show
the variable selection and coordinate covariance like Mukherjee’s algorithm.

5.1 Error Bar Estimation

i=1

We illustrate how GPGL can be used to estimate the credible intervals of the
estimated gradient and compare Mukherjee’s algorithm with GPGL to show
higher accuracy that GPGL demonstrates.
Given four representative elementary regression models y = exp(x); y =
ln(x); y = x2; y = sin(x), where {(xi, yi)}n
∈ R × R and xi ∼ N(1, 0.1). In
our experiment, we sampled 100 points from the Gaussian distribution. The
true derivatives are given by y(cid:5) = exp(x); y(cid:5) = 1/x; y(cid:5) = 2 ∗ x; y(cid:5) = cos(x),
respectively. The comparison of the results between proposed GPGL algorithm
and Mukherjee’s algorithm is shown in Figures 1 to 4. We use the mean squared
error between the true derivative and learned derivative to measure the quality
of learning algorithm. The smaller MSE means that a better performance of the
algorithm. All the MSEs for those four functions with diﬀerent algorithms are
collected in Table 1. It can be seen that the proposed GPGL algorithm gives
better performance in terms of lower MSEs for three out of the four functions.
Although for the functions y = exp(x) and y = x2, Mukherjee’s algorithm
gives slightly better results, the proposed GPGL algorithm outperforms Mukher-
jee’s Algorithm in other cases. However, in the experiment, we ﬁnd that Mukher-
jee’s algorithm is sensitive to the value of regularization parameter and the
percentage of the eigenvalues parameters (see the code in [4]) that need to
be chosen manually, especially the regularization parameter. Sometimes, it is
hard to choose them optimally, although a standard cross validation can be

1

http://www.cs.manchester.ac.uk/~neil/software.html

Learning Gradients with Gaussian Processes

121

8

6

4

2

0

s
t

i

n
e
d
a
r
G

(a) The comparison between 

 GPGL and Mukherjee’s algorithm.

 

The real gradient
Mukherjee’s algorithm
GPGL algorithm

−2

 
0

20

40
60
Samples

80

100

s
t

i

n
e
d
a
r
G

8

6

4

2

0

−2

 

(b) The error bar of the 

estimated gradients with GPGL.

 

The real gradient
Error bar
GPGL algorithm

20

40

60
Samples

80

100

Fig. 1. The Result for function y = exp(x)

3.5

3

2.5

2

1.5

1

0.5

i

s
t
n
e
d
a
r
G

(a) The comparison between 

 GPGL and Mukherjee’s algorithm.

 

The real gradient
Mukherjee’s algorithm
GPGL algorithm

0

 
0

20

40
60
Samples

80

100

i

s
t
n
e
d
a
r
G

4

3

2

1

0

−1

 

(b) The error bar of the 

estimated gradients with GPGL.

 

The real gradient
Error bar
GPGL algorithm

20

40

60
Samples

80

100

Fig. 2. The Result for function y = ln(x)

4

3

2

1

0

s
t

i

n
e
d
a
r
G

(a) The comparison between 

 GPGL and Mukherjee’s algorithm.

 

The real gradient
Mukherjee’s algorithm
GPGL algorithm

−1

 
0

20

40
60
Samples

80

100

s
t

i

n
e
d
a
r
G

5

4

3

2

1

0

−1

−2

 

(b) The error bar of the 

estimated gradients with GPGL.

 

The real gradient
Error bar
GPGL algorithm

20

40

60
Samples

80

100

Fig. 3. The Result for function y = x2

122

X. Jiang et al.

(a) The comparison between 

 GPGL and Mukherjee’s algorithm.

 

s
t

i

n
e
d
a
r
G

1

0.8

0.6

0.4

0.2

0

−0.2

−0.4

 
0

1.5

1

0.5

0

s
t

i

n
e
d
a
r
G

(b) The error bar of the 

estimated gradients with GPGL.

 

The real gradient
Error bar
GPGL algorithm

The real gradient
Mukherjee’s algorithm
GPGL algorithm

20

40
60
Samples

80

100

−0.5

 

20

40

60
Samples

80

100

Fig. 4. The Result for function y = sin(x)

Table 1. The Mean Squared Error

Algorithm

GPGL

Mukherjee’s

y = exp(x)
19.6275
12.0330

y = ln(x)
12.8840
80.8199

y = x2
7.9557
2.7621

y = sin(x)
1.3999
15.9319

applied. However, the proposed GPGL method does not suﬀer from this prob-
lem and is more stable with ability to automatically adapt parameters. In ad-
dition, the error bars can be obtained from our algorithm along with gradient
estimation.

5.2 High-Dimensional Data Set
Deﬁnition 1. The empirical gradient matrix (EGM), Fz, is the m × n matrix
whose columns are f(xj) with j = 1,··· , n. The empirical covariance matrix
(ECM), is the m × m matrix of inner products of the directional derivative of
two coordinates, which can be denoted as Cov(f) := [(cid:6)(f)p, (f)q(cid:7)K]m

The ECM gives us the covariance between the coordinates while the EGM
provides us information about how the variables diﬀer over diﬀerent sections of
the space.

p,q=1.

For a fair comparison, we construct the same artiﬁcial data as those used in
[4]. By creating a function in an m = 80 dimensional space which consists of
three linear functions over diﬀerent partitions of the space, we generate n = 30
samples as follows:
1. For samples {xi}10
i=1,

xj ∼ N (1, σx), for j = 1,··· , 10;
xj ∼ N (0, σx), for j = 11,··· , 80;

Learning Gradients with Gaussian Processes

123

(a) X.

(b) Y.

i

n
o
s
n
e
m
D

i

10

20

30

40

50

60

70

80

l

e
u
a
v
−
y

30

20

10

0

−10

−20

−30

0

 

10

20

Samples

30

(d) EGM.

10

20

Samples

30

0

0

(e) ECM.

(c) Norm.

50

40

30

20

10

m
r
o
N

20

40

60

80

Dimension

 

10

5

0

−5

i

n
o
s
n
e
m
D

i

20

40

60

80

 

10

20
Samples

30

5

0

−5

−10

i

n
o
s
n
e
m
D

i

20

40

60

80

 

20

40

60

80

Dimension

Fig. 5. a). The data matrix x; b). The vector of y values; c). The RKHS norm for each
dimension; d). An estimate of the gradient at each sample; e). The empirical covariance
matrix

2. For samples {xi}20

i=11,
xj ∼ N (1, σx), for j = 11,··· , 20;
xj ∼ N (0, σx), for j = 1,··· , 10, 21,··· , 80;

3. For samples {xi}30

i=21

xj ∼ N (1, σx), for j = 41,··· , 50;
xj ∼ N (0, σx), for j = 1,··· , 40, 51,··· , 80;

A representation of this X matrix is shown in Figure 5(a). Three vectors with
support over diﬀerent dimensions were constructed as follows:

w1 = 2 + 0.5 sin(2πi/10) for i = 1,··· , 10 and 0 otherwise,
w2 = −2 + 0.5 sin(2πi/10) for i = 11,··· , 20 and 0 otherwise,
w3 = 2 − 0.5 sin(2πi/10) for i = 41,··· , 50 and 0 otherwise,

Then the function is deﬁned by
1. For samples {yi}10
2. For samples {yi}20
3. For samples {yi}30
A draw of the y values is shown in Figure 5(b). In Figure 5(c), we plot the norm
of each component of the estimate of the gradient using the GPGL algorithm.

yi = xi (cid:3) w1 + N (0, σy),
yi = xi (cid:3) w2 + N (0, σy),
yi = xi (cid:3) w3 + N (0, σy).

i=1
i=11
i=21

124

X. Jiang et al.

The norm of each component gives an indication of the importance of a variable
and variables with small norms can be eliminated. Note that the coordinates
with nonzero norm are the ones we expect, l = 1,··· , 20, 41,··· , 50. In Figure
5(d) we plot the EGM, while the ECM is displayed in Figure 5(e). The blocking
structure of the ECM indicates the coordinates that covary. The similar result
can be found in [4].

6 Conclusions

In this paper we have proposed a direct gradient learning algorithm from sample
dataset in the Bayesian framework. The Gaussian Processes Gradient Learning
(GPGL) model we propose can be seen as the manifestation of the representer
theorem which is the basis of Mukherjee’s algorithm. However, only the GPGL
model can provide the error bars of the estimated gradients which characterize
the uncertainty of the estimation. Besides, the GPGL model is stable and shows
higher accuracy than Mukherjee’s algorithm in terms of MSE in some circum-
stances. Another advantage is that GPGL model is more stable with automatical
parameter adapting while the result from Mukherjee’s algorithm heavily depends
on the better tuning of the regularization parameters. In future work we plan to
extend GPGL to sparse model to improve the generalization capability that is
especially useful in the “large m, small n” setting.

References

1. Wu, Q., Guinney, J., Maggioni, M., Mukherjee, S.: Learning gradients: predictive
models that infer geometry and dependence. Technical report, Duke University
(2007)

2. Vapnik, V.: Statistical Learning Theory. Wiley, Chichester (1998)
3. Xia, Y., Tong, H., Li, W.K., Zhu, L.X.: An adaptive estimation of dimension re-

duction space. Journal of Royal Statistical Society 64(3), 363–410 (2002)

4. Mukherjee, S., Zhou, D.X.: Learning coordinate covariances via gradients. Journal

of Machine Learning Research 7, 519–549 (2006)

5. Mukherjee, S., Wu, Q.: Estimation of gradients and coordinate covariation in clas-

siﬁcation. Journal of Machine Learning Research 7, 2481–2514 (2006)

6. West, M.: Bayesian factor regression models in the ”large p, small n” paradigm. In:

Bayesian Statistics, vol. 7, pp. 723–732. Oxford University Press, Oxford (2003)

7. Mukherjee, S., Wu, Q., Zhou, D.X.: Learning gradients and feature selection on

manifolds. Technical report, Duke University (2007)

8. Dollar, P., Rabaud, V., Belongie, S.: Non-isometric manifold learning: Analysis
and an algorithm. In: International Conference on Machine Learning, pp. 241–248
(2007)

9. Rasmussen, C.E., Williams, C.K.I.: Gaussian Processes for Machine Learning. The

MIT Press, Cambridge (2006)

10. Schoelkopf, B., Smola, A.: Learning with Kernels: Support Vector Machines, Reg-

ularization, Optimization, and Beyond. The MIT Press, Cambridge (2001)

11. Bonilla, E.V., Chai, K.M.A., Williams, C.K.I.: Multi-task gaussian process predic-
tion. In: Advances in Neural Information Processing Systems, vol. 20, pp. 153–160
(2008)

12. Brookes, M.: The Matrix Reference Manual (2005)


