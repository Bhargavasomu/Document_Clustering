Bulk Loading Hierarchical Mixture Models

for Eﬃcient Stream Classiﬁcation

Philipp Kranen, Ralph Krieger, Stefan Denker, and Thomas Seidl

Data Manangement and Exploration Department

RWTH Aachen University, Germany

{kranen,krieger,denker,seidl}@cs.rwth-aachen.de

Abstract. The ever growing presence of data streams led to a large
number of proposed algorithms for stream data analysis and especially
stream classiﬁcation over the last years. Anytime algorithms can deliver
a result after any point in time and are therefore the natural choice
for data streams with varying time allowances between two items. Re-
cently it has been shown that anytime classiﬁers outperform traditional
approaches also on constant streams. Therefore, increasing the anytime
classiﬁcation accuracy yields better performance on both varying and
constant data streams. In this paper we propose three novel approaches
that improve anytime Bayesian classiﬁcation by bulk loading hierarchical
mixture models. In experimental evaluation against four existing tech-
niques we show that our best approach outperforms all competitors and
yields signiﬁcant improvement over previous results in term of anytime
classiﬁcation accuracy.

1 Introduction

With an abundance of streaming data due to widely deployed sensors or other
data gathering devices, analysis of streaming data such as stream classiﬁcation
has recently received much attention in data mining research. Algorithms that
work on data streams have to cope with the limited and often also varying
amount of computation time. Traditionally they got for a certain task a ﬁxed
time budget which was known in advance, i.e. they were tailored to the speciﬁc
application. These budget algorithms can neither provide a results in less time
nor exploit additional time to improve their result. In contrast, so called anytime
algorithms can provide a result after a very short initialization, improve their
result incrementally when more time is available and hold the most recent result
ready at any time. In data mining anytime solutions have been proposed for
many tasks such as clustering [10], top-k processing [2] and classiﬁcation [5,16].
Anytime algorithms are the natural choice for varying data streams since they
ﬂexibly exploit all available time to improve the quality of their result. Recently it
has been shown in [12] that also on constant data streams anytime classiﬁers can
improve the classiﬁcation accuracy over that of traditional budget approaches.
With their superiority on varying and constant data streams, applications for
anytime classiﬁers are numerous and range from industrial applications, such as

M.J. Zaki et al. (Eds.): PAKDD 2010, Part II, LNAI 6119, pp. 325–334, 2010.
c(cid:2) Springer-Verlag Berlin Heidelberg 2010

326

P. Kranen et al.

machine monitoring, over sorting tasks to robotics and health applications. [11].
Our focus is on improving the performance of anytime Bayesian classiﬁcation.
Improving the anytime accuracy together with the results from [12] leads to
better classiﬁcation results on both constant and varying data streams.

2 Related Work

Classiﬁcation aims at determining the class label of unknown objects based on
training data. Diﬀerent classiﬁcation approaches are discussed in the literature
including nearest neighbor classiﬁers, decision trees or support vector machines.
Bayes classiﬁers constitute a statistical approach that has been successfully used
in numerous application domains. Another classiﬁcation approach is represented
by Bayesian classiﬁcation using kernel density estimation [9]. Especially for huge
data sets the estimation error using kernel densities is known to be very low and
even asymptotically optimal [3].

Anytime classiﬁcation is real time classiﬁcation up to a point of interruption.
In addition to high classiﬁcation accuracy as in traditional classiﬁers, anytime
classiﬁers have to make best use of the limited time available, and, most notably,
they have to be interruptible at any given point in time. This point in time is
usually not known in advance and may vary greatly. Anytime classiﬁcation has
for example been discussed for support vector machines [5] or nearest neighbor
classiﬁcation [16].

For Bayesian classiﬁcation based on kernel densities an anytime algorithm
called Bayes tree has been proposed in [15]. The Bayes tree is a balanced tree
structure and is basically an extension of the R-tree [8]. It stores in each entry
a pointer and a minimum bounding rectangle (MBR) and additionally a cluster
feature representing the corresponding subtree. In [15] the tree is constructed
trough iterative insertion, i.e. no optimization is performed with respect to over-
lapping or quality of the resulting mixture densities. In this paper we propose
several methods for bulk loading mixture densities in the Bayes tree and show
in experimental evaluation that they outperform the results from [15].

3 Bulk Loading Mixture Densities

Before going into detail on the diﬀerent bulk loading approaches in Sections 3.2
and 3.3 we brieﬂy review Bayesian classiﬁcation and describe the structure and
working of the Bayes tree proposed in [15].

3.1 Bayesian Classiﬁcation and the Bayes Tree
Given a set of classes C and an object space X a classiﬁer is a function G that
assigns the class label G(x) to an object x ∈ X. Based on a statistical model
of the distribution of class labels the Bayes classiﬁer assigns to an object x the
class ci with the highest posterior probability P (ci|x). With Bayes rule it holds:

G(x) = argmax

ci∈C

{P (ci|x)} = argmax
ci∈C

{P (ci) · p(x|ci)}

Bulk Loading Hierarchical Mixture Models for Eﬃcient Stream Classiﬁcation

327

In the Bayes tree the class-conditional density p(x|ci) is estimated using
Gaussian mixture models in the inner nodes and Gaussian kernel estimators at
the leaf level. Iterative reﬁnement of mixture components enables anytime kernel
density estimation for eﬃcient and interruptible classiﬁcation. The general idea
of the Bayes tree is a hierarchy of mixture densities stored in a multidimensional
index. Each level of the tree stores a complete model of the entire data at a dif-
ferent granularity. The node entries consist of pointers to a subtree and a single
Gaussian representing the objects in the subtree. All objects stored in the leaves
of the Bayes tree are d-dimensional kernels. The mean μs and the variance vector
s can be computed from the cluster features.
σ2
Answering a probability density query uses a complete model which is avail-
able at each level of the tree. Besides these full models, the Bayes tree allows
for local reﬁnement of the model (to adapt ﬂexibly to the query) and thus pro-
vides models composed of coarser and ﬁner representations. The current mixture
model components, i.e. their corresponding entries, are stored in a frontier. The
current entries in the frontier have to represent each stored object exactly once.
This is made sure by removing the entry es that is reﬁned from the frontier and
adding its child entries es◦j, j = 1 . . . νs instead. The probability density for a
query object is then calculated with respect to the current frontier.

For tree traversal best ﬁrst descent using a probabilistic priority measure has
proven to yield the best results in [15]. One Bayes tree is built per class, therefore
several improvement strategies have been proposed to decide which tree has the
right to reﬁne its model in the next time step. Extensive experiments showed
that reﬁning the k most probable classes (qbk) in turns yielded the best results
throughout. k = min{2,(cid:3)log(m)(cid:4)}, where m is the number of classes, showed the
best performance on all tested data sets. For more details please refer to [15].

3.2 Machine Learning and Statistical Approaches
Our goal in this work is to improve the performance of the Bayes tree. The ac-
curacy of the Bayes tree results is based on the quality of the mixture densities
stored in its entries. The iterative insertion performed in [15] does not consider
the quality of the resulting Gaussian components. We develop and evaluate sev-
eral bulk loading approaches that try to overcome this shortcoming and improve
the quality of the mixture densities.
Goldberger. Since the Bayes tree is a statistical approach to classiﬁcation we
looked for statistical methods to create a smaller mixture model from a given
mixture model. Starting bottom up with a mixture model that contains a kernel
estimator for each training set item we create successively coarser models that
represent good approximations.

Our ﬁrst statistical approach is based on [7] and is called Goldberger in the
following. The Goldberger approach assumes two initial mixture models f and
g to be given, where f is the ﬁner model with r components and g an approxi-
mation with s components, hence r > s. Each component is assigned a weight
and is speciﬁed by its mean and covariance matrix. To measure the quality of
the approximation [7] deﬁnes the distance between two mixture densities as:

328

P. Kranen et al.

(cid:2)
r

(cid:2)
s

Deﬁnition 1. Let f =
j=1 βjgj be two mixture densities
containing r and s Gaussian components fi and gj with their respective weights
αi and βj. The distance between f and g is then deﬁned using the Kullback-
Leibler divergence KL [4] as follows

i=1 αifi and g =

d(f, g) =

r(cid:3)

i=1

αi ·

s

min
j=1

{KL(fi, gj)}

The optimal model ˆg reducing f to s components is ˆg = arg ming(d(f, g)). Since
there is no closed form to compute ˆg, a local optimum is computed iterating the
following two steps until the distance d(f, g) does no longer decrease. Therein
π(i) : {1 . . . r} → {1 . . . s} is a mapping function that assigns each component in
f to a component in g.

– Regroup: update π: π(i) = arg mins
– Reﬁt: for each component gj recompute weight βj, mean μj and covariance

j=1

{KL(fi, gj)}

(cid:2)

matrix Σj as follows
• βj =
i,π(i)=j αi
(cid:2)
• μj = 1
• Σj = 1

(cid:2)

i,π(i)=j αiμi
(cid:4)
i,π(i)=j αi

βj

βj

Σj + (μi − μj)2

(cid:5)

We devise a bulk loading technique based on [7] as follows. To initialize the
mixture g we compute a ﬁrst mapping π0 by assigning 0.75· M components from
f to one component in g according to the z-curve order of their mean values. M
is given through the fanout, which in turn is dictated by the page size. When
no more changes occur in step 2, the resulting components gj are converted
to Bayes tree nodes containing the entries fi with π(i) = j. Since the ﬁnal
π might map more than M components from f to a single component in g, we
investigated several strategies to restrict the fanout to the given boundaries. First
we reformulated the regroup step into an integer linear program with constraints
regarding the resulting fanout. However, for realistic problem sizes, this approach
took way too long to compute a complete bulk loading. Hence, we decided for a
post processing after the mapping π was computed, which splits the nodes that
contain too many entries. Therefore two representatives are computed by moving
the mean along the dimension a with the highest variance σa by an  = σa/2
in both direction. A Gaussian is placed over the two representatives and the
mapping of the entries to the representatives is computed as in the regroup step.
If a node contains too few entries it is merged with the node closest to it in
terms of the Kullback-Leibler divergence.
Virtual sampling. The second approach, called virtual sampling, uses the work
presented in [17] and does not rely on the KL divergence. The virtual sampling
i=1..r αi · fi containing r com-
approach assumes a given mixture model f =
j=1..s αj · gj with s < r
ponents and computes a coarser mixture model g =
components. The components fi = G(x, μi, σi) constitute multivariate Gaus-
sian normal distributions with their respective weight αi (analogue for gj). To

(cid:2)

(cid:2)

Bulk Loading Hierarchical Mixture Models for Eﬃcient Stream Classiﬁcation

329

derive an algorithm the following model is utilized: the mixture g can be com-
puted using samples R1 . . . Rr from each component in f with R = ∪i=1..rRi
and |Ri| = αi · |R|. Assuming independence of the sample points from diﬀerent
components in f yields the assumption that they can be assigned to diﬀerent
components in g while samples from the same fi are likely to be assigned to
the same gj. Based on this assumption hidden variables zij are introduced that
indicate for each component fi its assignment to the corresponding gj. While the
zij are binary during initialization, they can take values between 0 and 1 during
the iterations. The hidden variables are used in a modiﬁed Expectation Maxi-
mization algorithm to compute the coarser mixture g as follows (superscripts f
and g are added for readability to indicate the origin of the components):

– Expectation: zij =

(cid:2)

(cid:6)

G(μf
(cid:6)

i ,μg
G(μf

s
k=1

j ,Σg

j )e

i ,μg

k,Σg

k)e

− 1

2 trace{(Σ

)−1

}

Σ

f
i

g
j

(cid:7)|Ri

|

− 1

2 trace{(Σ

)−1 Σ

f
i

}

g
k

j

·αg
(cid:7)|Ri
|·αg

k

– Maximization:
(cid:2)
r
i=1 zij

j = 1

r

• αg
• Σg

j =

1(cid:2)

i=1 zij|Ri|

r

(cid:2)

r
(cid:2)

i=1 zij|Ri|μf
i=1 zij|Ri|
(cid:2)
r

μg
j =
i=1 zij|Ri|Σf

i +

r

i

(cid:6)

(cid:2)
r

i=1 zij|Ri| (cid:4)

μf
i

(cid:7)

(cid:5)2

− μg

j

The above equations are independent of the actual samples Ri and can be com-
puted directly from the mixture components in f, hence virtual sampling. To
use the described bottom up method for bulk loading we have to provide an
initialization for the hidden variables zij. The initialization of the mixture g is
done as in the goldberger approach described above. After getting the ﬁnal val-
ues for zij from the virtual sampling algorithm, we assign each fi to that gj with
the maximum zij for all j. Moreover, the result has to comply with the fanout
parameters m and M of the Bayes tree. This is achieved through merging and
splitting of the resulting components gj. If a component gj is assigned less than
m components fi, these components from f are assigned to the gj(cid:2) with the
second highest zij(cid:2). If more than M components fi are assigned to one gj, gj
is duplicated while moving the resulting two means in opposite direction along
the dimension with the highest variance. The respective fi are reassigned to the
more probable candidate according to the density of their mean μi. After merg-
ing and splitting the corresponding mixture parameters are adapted following
the above equations.
EMTopDown. Besides the above mentioned bottom up approaches we imple-
mented a top down approach that recursively splits the training set into several
clusters. In contrast to the previous approach, where Gaussian components were
merged and mapped, we now operate solely on the data objects. More precisely,
we start by applying the EM [6] algorithm to the complete training set. The
desired number M of resulting clusters is always set to the fanout which is again
given through the page size. If the EM returns less than m clusters, the biggest
resulting cluster is split again such that the total number of resulting clusters is
at most M. In the rare case that the EM returns a single cluster, this cluster is

330

P. Kranen et al.

split by picking the two farthest elements and assigning the remaining elements
to the closest of the two. Finally, if a resulting cluster contains more than L
objects (the capacity of a leaf node), the cluster is recursively split using the
procedure described above. Otherwise the items contained in that cluster are
stored in a leaf node, its corresponding entry is calculated and returned to build
the Bayes tree. The EM approach may result in an unbalanced tree, which diﬀers
from the primary Bayes tree idea. However, as we will see in the experimental
section, the results show that this is not a drawback but even leads to better
anytime classiﬁcation performance.

3.3 Data Base Driven Approaches

Since the Bayes tree extends the R-tree, we employ traditional R-tree bulk load-
ing algorithms for comparison. We implemented two types of space ﬁlling curves,
namely Hilbert curve and z-curve. We brieﬂy describe the Hilbert curve ap-
proach, the z-curve bulk loading works analogously. The bulk loading according
to the Hilbert curve is a bottom up approach where in the ﬁrst step the Hilbert
value for each training set item is calculated. Next the items are ordered ac-
cording to their Hilbert value and put into leaf nodes w.r.t. the page size. After
that the corresponding entry for each resulting node is created, i.e. MBR, cluster
features (CF) and the pointer. These steps are repeated using the mean vectors
as representatives until all entries ﬁt into one node, the root node. Theory on
creating multidimensional Hilbert curves can be found in [1], for implementa-
tion guide lines see [13]. Additionally we implemented the partitioning approach
presented in [14] that is called sort-tile-recursive. The basic idea is to build a hier-
archy of rectangles which have, at the same level of the hierarchy, approximately
the same expansion in each dimension. For details please refer to [14].

4 Experiments

The three proposed bulk loading techniques Goldberger, virtual sampling and
EMTopDown are compared to the existing R-tree bulk loading approaches
Hilbert, z-curve and STR and the previous results from [15] (called Iterative
in the graphs since it performs iterative insertion of objects). We also used the
same settings as in [15], i.e. we use the same data sets, performed 4-fold cross
validation and show the classiﬁcation accuracy after each node averaged over
the four folds. We used global best descent and the qbk improvement strategy
as they showed the best results in [15]. Please note that the bulk loading is done
per fold once and oﬄine and the resulting classiﬁer is then used on the data
stream. Since our focus is on anytime classiﬁcation, we do not study the time
performance of the bulk loading algorithm but the performance of the resulting
classiﬁer, i.e. its anytime classiﬁcation accuracy.

The top left part of ﬁgure 1 shows the results for the pendigits data set. The
Goldberger approach fails to improve the accuracy over the iterative insertion
for the ﬁrst 50 nodes. After that it performs slightly better, but cannot increase
the accuracy by more than 1%. Virtual sampling performs worst on this data set.

Bulk Loading Hierarchical Mixture Models for Eﬃcient Stream Classiﬁcation

331

Pendigits

0,9

Letter

y
c
a
r
u
c
c
a

0,85

EMTopDown
Goldberger
Goldberger
vSampling
Hilbert
ZCurve
str
Iterativ

0,8

0,75

0,7

0,65

0,6

0 6

2
1

8
1

4
2

0
3

6
3

2
4

8
4

4
5

0
6

6
6

2
7

8
7

4
8

0
9

6
9

nodes

0 6

2
1

8
1

4
2

0
3

6
3

2
4

8
4

4
5

0
6

6
6

2
7

8
7

4
8

0
9

6
9

nodes

Area(cid:3)values(cid:3)for(cid:3)Pendigits

Area(cid:3)values(cid:3)for(cid:3)Letter

0,88
0,86
0,84
0,82
0,8
0,78
0,76
0 74
0,74
0,72
0,7
0,68

y
c
a
r
u
c
c
c
a

0,99

0,97

0,95

0,93

0,91

0,89

0,87

0,85

0,99
0,98
0,97
0,96
0,95
0,94
0,93
0,92
0,91
0,9
0,89
0,88

Fig. 1. Anytime classiﬁcation accuracy and a ranking of all approaches according to
the area for Pendigits (left) and Letter (right)

The Hilbert and z-curve bulkload yield comparable results, their corresponding
curves show a steep increase similar to the iterative insertion and show bet-
ter performance in most cases. After falling behind during the ﬁrst nodes, STR
performs equaly well compared to Iterative. The EMTopDown bulkload outper-
forms all other approaches and improves the accuracy over the iterative insertion
constantly by 3% or more on this data set.

The performance of the Goldberger bulkload stayed below the iterative inser-
tion in the majority of our experiments. Just on the Letter data set it improved
the accuracy for larger time allowances (cf. Figure 1, right). For the ﬁrst 40
nodes Goldberger and Iterative perform equally well, after that the accuracy of
Iterative stays behind that of Goldberger. While the virtual sampling and STR
bulkload shows similar performance to Iterative, Hilbert and z-curve (which are
again in close proximity to each other) show constantly better accuracy than It-
erative. The EMTopDown again constantly yields the best accuracy up to 13%
better than the iterative insertion.

To facilitate an easier comparison between the diﬀerent approaches we report
the values for the normalized area under the anytime curves for Pendigits, Letter,
Vowel, USPS and Verbmobil in Figures 1 (bottom) and 2 (top) respectively.
Throughout the data sets Hilbert and z-curve show nearly the same performance,
while z-curve is usually slightly behind Hilbert except for the USPS data set.
STR ranges between these two and the iterative insertion; it is never better
than the former and never beaten by the latter. Surprisingly both statistical
approaches exhibit the same weakness as STR, i.e. they never outperform the
z-curve bulk load (except Goldberger on Letter) and several times show even
worse performance than iterative insertion. This is especially interesting since
both approaches are initialized using the z-curve, however, only with 0.75 · M
entries per node (cf. Section 3.2). We discuss the reasons for this shortcoming of

332

P. Kranen et al.

Area(cid:3)values(cid:3)for(cid:3)Vowel

Area(cid:3)values(cid:3)for(cid:3)USPS

Area(cid:3)values(cid:3)for(cid:3)Verbmobil

0,96

0,95

0,94

0,93

0,92

0,91

0,9

0,97

0,96

0,95

0,94

0,93

0 92
0,92

0,91

0,9

0,75

0,74

0,73

0,72

0,71

0 7
0,7

0,69

0,68

Gender

Covertype

0,85

y
y
c
a
r
u
c
c

0,8a

0,85

y
y
c
a
r
u
c
c

0,8a

0,75

0,7

0,65

0,6

EMTopDown glo
EMTopDown(cid:3)glo
EMTopDown(cid:3)bft
Hilbert(cid:3)glo
Hilbert(cid:3)bft
Iterativ(cid:3)glo

0,75

0,7

0,65

0,6

EMTopDown glo
EMTopDown(cid:3)glo
EMTopDown(cid:3)bft
Hilbert(cid:3)glo
Hilbert(cid:3)bft
Iterativ(cid:3)glo

0 5

0
1

5
1

0
2

5
2

0
3

5
3

0
4

5
4

0
5

5
5

0
6

5
6

0
7

5
7

0
8

5
8

0
9

5
9

0
0
1

nodes

0 5

0
1

5
1

0
2

5
2

0
3

5
3

0
4

5
4

0
5

5
5

0
6

5
6

0
7

5
7

0
8

5
8

0
9

5
9

0
0
1

nodes

Fig. 2. Top: Comparison of all approaches on Vowel, USPS and Verbmobil data sets.
Bottom: Anytime classiﬁcation accuracy on Gender (left) and Covertype (right).

the statistical approaches at the end of the section where we analyze the structure
of the resulting trees. Finally, the EMTopDown bulk load shows constantly the
best performance on all data sets despite the unbalanced resulting trees. Again
we defer the analysis to the end of the section and ﬁrst discuss a diﬀerent issue.
Figure 2 (bottom) shows the results for the gender and covertype data sets.
For readability only the results for Hilbert and EMTopDown are shown. For both
data sets k = 2 for the qbk improvement strategy (cf. Section 3.1). The graphs for
EMTopDown and Hilbert using the global best descent (glo) show an oscillating
behavior on both data sets. For comparison we recapitulated the breadth ﬁrst
traversal (bft), the results are plotted as well. As was found in [15], the global
best descent performs better than breadth ﬁrst traversal. However, the graphs
for bft do not show the oscillating behavior mentioned above. Since k = 2, there
is obviously a certain percentage of object whose class decision changes in favor
of (or against) the tree which is currently reﬁned. More precisely, these objects
are likely positioned on the decision boundary between the two most probable
classes. In global best descend reﬁning mixture components close to the objects,
and hence close to the decision boundary, aﬀects the corresponding posterior
probabilities more heavily than reﬁnement of a farther component as in breadth
ﬁrst traversal. If we assume the oscillation to be a higher frequency added to
a smooth underlying anytime curve, the percentage of these borderline objects
corresponds to the amplitude. However, on balance, the oscillating behavior does
not aﬀect the superiority of the bulk loading over the iterative insertion.

To ﬁnd reasons for the surprising ranking of the individual algorithms, we
analyzed the structure of the resulting trees. Since more entries in a node cor-
respond to more detailed information compared to less entries, we looked at the
degree to which the nodes were ﬁlled in the diﬀerent approaches. To this end we
computed the average fanout per level of the trees from root to leaf. However,
the resulting ﬁgures did not reveal any correlation to the found ranking of the ap-
proaches. For example, both space ﬁlling curve approaches always ﬁll the nodes

Bulk Loading Hierarchical Mixture Models for Eﬃcient Stream Classiﬁcation

333

Letter

Gender

e
c
n
a
i
r
a
V

1e
0,9
0,8
0,7
0,6
0,5
0,4
0,3
0,2
0,1
0

e
c
n
a
i
r
a
V

1e
0,9
0,8
0,7
0,6
0,5
0,4
0,3
0,2
0,1
0

EMTopDown
Hilbert
G ldb
Goldberger
Iterativ

EMTopDown
Hilbert
G ldb
Goldberger
Iterativ

0

1

2

3

4

5

6 Level

0

2

4

6

8

10 Level

Fig. 3. Variances per level for Letter (left) and Gender (right)

to nearly 100% (except for the last one per level and the root), but the EMTop-
Down sometimes produces less than M entries for a given node.

We found a correlation between the performance of the algorithm and the
variance of the mixture components in the resulting trees. More precisely, we
calculated the average variance of all entries per level, Figure 3 shows the result-
ing numbers for Hilbert, Goldberger, EMTopDown and Iterative on the Letter
and Gender data sets. The variances are normalized by the variance of the entire
data set per class. Level 0 corresponds to the leaves, Level 1 is above the leaves
etc. The trees resulting from diﬀerent approaches can have diﬀerent heights as
can be seen in the graphs. Hilbert bulk load ﬁlls each node to 100% and con-
sequently yields the smallest trees, while the unbalanced trees resulting from
EMTopDown are up to twice as high on the Gender data set.

EMTopDown and Hilbert show signiﬁcantly smaller average variances com-
pared to the iterative insertion. While the corresponding variances for Gold-
berger are smaller than those of Iterative for the Letter data set they are larger
on the Gender data set. This is in line with the observed anytime classiﬁcation
performances. We found comparable similarities between the two measures on
the other data sets. The average variances per level achieved by the EMTopDown
bulk load were constantly amongst the lowest compared to all other approaches.
This explains and underlines the superior performance of EMTopDown.

In general the EMTopDown shows the best results in terms of anytime classi-
ﬁcation accuracy on all tested data sets and continuously improves the accuracy
over that of the previous results in [15] up to 13%. This proves the eﬀectiveness
of our bulk loading approach for hierarchical anytime classiﬁers.

5 Conclusion

We proposed three bulk loading approaches for hierarchical mixture models to
improve Bayesian classiﬁcation on data streams using the Bayes tree. We com-
pared our approaches to the previously proposed iterative insertion [15] and
three known R-tree bulk loading algorithms on a range of real world data sets.
Experimental results showed that our novel EMTopDown bulk load constantly
outperformed all other approaches and improved the accuracy by up to 13%.
Surprisingly our two statistical approaches were outperformed by existing R-
tree bulk loadings based on space ﬁlling curves. Further analysis attributed this

334

P. Kranen et al.

shortcoming to a structural property of the resulting Bayes trees. The results of
the analysis were in line with the classiﬁcation results found in the experiments
conﬁrming the superior performance of our new EMTopDown bulk loading in
terms of anytime classiﬁcation accuracy.

Acknowledgments. This work has been supported by the UMIC Research
Centre, RWTH Aachen University.

References

1. Alber, J., Niedermeier, R.: On multi-dimensional hilbert indexings. In: 4th Annual

International Conference on Computing and Combinatorics COCOON (1998)

2. Arai, B., Das, G., Gunopulos, D., Koudas, N.: Anytime measures for top-k algo-

rithms on exact and fuzzy data sets. VLDB Journal 18(2), 407–427 (2009)

3. Bouckaert, R.: Naive Bayes Classiﬁers that Perform Well with Continuous
Variables. In: Webb, G.I., Yu, X. (eds.) AI 2004. LNCS (LNAI), vol. 3339,
pp. 1089–1094. Springer, Heidelberg (2004)

4. Chen, J.-Y., Hershey, J., Olsen, P., Yashchin, E.: Accelerated monte carlo for
kullback-leibler divergence between gaussian mixture models. In: ICASSP (2008)
5. DeCoste, D.: Anytime interval-valued outputs for kernel machines: Fast support

vector machine classiﬁcation via distance geometry. In: ICML (2002)

6. Dempster, A.P., Laird, N.M.L., Rubin, D.B.: Maximum likelihood from incomplete
data via the em algorithm. Journal of the Royal Statistical Society, Series B 39(1),
1–38 (1977)

7. Goldberger, J., Roweis, S.T.: Hierarchical clustering of a mixture model. In: NIPS

(2004)

8. Guttman, A.: R-trees: A dynamic index structure for spatial searching. In:

SIGMOD, pp. 47–57 (1984)

9. John, G., Langley, P.: Estimating continuous distributions in bayesian classiﬁers.

In: UAI. Morgan Kaufmann, San Francisco (1995)

10. Kranen, P., Assent, I., Baldauf, C., Seidl, T.: Self-adaptive anytime stream clus-

tering. In: Proc. of the 9th IEEE ICDM (2009)

11. Kranen, P., Kensche, D., Kim, S., Zimmermann, N., M¨uller, E., Quix, C., Li,
X., Gries, T., Seidl, T., Jarke, M., Leonhardt, S.: Mobile mining and information
management in healthnet scenarios. In: Proc. of the 9th IEEE MDM (2008)

12. Kranen, P., Seidl, T.: Harnessing the strengths of anytime algorithms for constant

data streams. DMKD Journal, ECML PKDD Special Issue 2(19) (2009)

13. Lawder, J.: Calculation of mappings between one and n-dimensional values us-
ing the hilbert space-ﬁlling curves. Technical Report JL1/00 Birkbeck College,
University of London (2000)

14. Leutenegger, S.T., Edgington, J.M., Lopez, M.A.: Str: A simple and eﬃcient algo-

rithm for r-tree packing. In: ICDE, pp. 497–506 (1997)

15. Seidl, T., Assent, I., Kranen, P., Krieger, R., Herrmann, J.: Indexing density
models for incremental learning and anytime classiﬁcation on data streams. In:
EDBT/ICDT (2009)

16. Ueno, K., Xi, X., Keogh, E.J., Lee, D.-J.: Anytime classiﬁcation using the nearest

neighbor algorithm with applications to stream mining. In: ICDM (2006)

17. Vasconcelos, N., Lippman, A.: Learning mixture hierarchies. In: NIPS (1998)


