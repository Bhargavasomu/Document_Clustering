Rough Margin Based Core Vector Machine

Gang Niu1, Bo Dai2, Lin Shang1, and Yangsheng Ji1

1 State Key Laboratory for Novel Software Technology,

Nanjing University, Nanjing 210093, P.R.China
{niugang,jiyangsheng,lshang}@ai.nju.edu.cn

2 NLPR/LIAMA, Institute of Automation, Chinese Academy of Science,

Beijing 100190, P.R.China

bdai@nlpr.ia.ac.cn

Abstract. 1The recently proposed rough margin based support vector
machine (RMSVM) could tackle the overﬁtting problem due to outliers
eﬀectively with the help of rough margins. However, the standard solvers
for them are time consuming and not feasible for large datasets. On the
other hand, the core vector machine (CVM) is an optimization tech-
nique based on the minimum enclosing ball that can scale up an SVM
to handle very large datasets. While the 2-norm error used in the CVM
might make it theoretically less robust against outliers, the rough margin
could make up this deﬁciency. Therefore we propose our rough margin
based core vector machine algorithms. Experimental results show that
our algorithms hold the generalization performance almost as good as
the RMSVM on large scale datasets and improve the accuracy of the
CVM signiﬁcantly on extremely noisy datasets, whilst cost much less
computational resources and are often faster than the CVM.

1 Introduction

People in computer science societies have been questing for faster algorithms
since long before. When come to mind the solving techniques of SVMs, there
are several approaches ranged from the chunking method [1] to the sequential
minimal optimization [2], as well as scaling down the training data and low-rank
kernel matrix approximations [3]. Eventually, the Core Vector Machine (CVM)
algorithms [4, 5, 6, 7] have gone to an extreme that they have linear asymptotic
time complexity and constant asymptotic space complexity, since they transform
the quadratic programming (QP) involved in SVMs to the minimum enclosing
ball (MEB) problems. In order to perform this transformation the CVM takes
the 2-norm error, which may cause it less robust and thus hurt the accuracy.
Fortunately the notion of the rough margin in the Rough Margin based Support
Vector Machine (RMSVM) [8] could make SVMs less sensitive to noises and
outliers, and consequently reduce the negative eﬀects of outliers. For this reason
we propose our Rough Margin based Core Vector Machine (RMCVM), which
unites the merits of the two aforementioned methods.
1 Supported by National Natural Science Foundation of China (No. 60775046) and

Natural Science Foundation of Jiangsu Province (No. BK2009233).

M.J. Zaki et al. (Eds.): PAKDD 2010, Part I, LNAI 6118, pp. 134–141, 2010.
c(cid:2) Springer-Verlag Berlin Heidelberg 2010

Rough Margin Based Core Vector Machine

135

After brief introductions to the CVM and the RMSVM in Section 2 and 3, we
will ﬁrst of all deﬁne the primal problem of the RMCVM. Next we shall elaborate
how to solve an RMCVM through the approximate MEB ﬁnding algorithm. We
will also investigate the loss functions used by the RMSVM and RMCVM. In
the end experimental results are shown in Section 5.

R2

min
R,c

2 Core Vector Machine with Minimum Enclosing Ball
Given a set of points S = {x1, . . . , xm}, where xi ∈ Rd for some integer d,
the minimum enclosing ball of S (denoted MEB(S)) is the smallest ball which
contains all the points in S [5]. Formally, let ϕ be a kernel induced feature map,

s.t. (cid:3)c − ϕ(xi)(cid:3)2 ≤ R2, i = 1, . . . , m.

(1)
Let B(c∗, R∗) be the exact MEB(S). Given an  > 0, a (1 + )-approximation
of B(c∗, R∗) is a ball B(c, (1 + )R) such that R ≤ R∗ and S ⊂ B(c, (1 + )R). A
subset Q of S is called a core-set, if S ⊂ B(c, (1+)R) where B(c, R) is MEB(Q).
The approximate MEB ﬁnding algorithm [9] uses a simple iterative scheme: at
the t-th iteration, Qt is expanded by including the farthest point of S from ct,
then optimize (1) to get B(ct+1, Rt+1); this is repeated until S ⊂ B(ct, (1+)Rt).
A surprising property is that the number of iterations, and thus the size of the
ﬁnal core-set, depend only on  but not on d or m [9, 5].
The dual of (1) is maxα α(cid:4)diag(K)− α(cid:4)Kα s.t. α ≥ 0, α(cid:4)1 = 1, where α is
the Lagrange multiplier and K is the kernel matrix. Conversely, any QP of this
form can be regarded as an MEB problem [4]. In particular when

k(x, x) = κ,

(2)

where κ is a constant (this is true for many popular kernels), we can drop the
linear term α(cid:4)diag(K) and obtain a simpler QP,

−α(cid:4)

Kα s.t. α ≥ 0, α(cid:4)

max

α

1 = 1.

(3)

Deﬁnition 1 (CVM [4])
(cid:3)w(cid:3)2 + b2−2ρ+ C

m(cid:2)

min
w,b,ξ,ρ

i=1

s.t. yi(w

(cid:4)ϕ(xi)+ b) ≥ ρ− ξi, i = 1, . . . m, (4)

ξ2
i

where C is a regularization parameter and ξi are slack variables.
The dual of (4) is analogous to the dual (3), in which K is replaced with ˜K
e(cid:4)
i ](cid:4) (ei is all
that ˜Kij = ˜ϕ(xi, yi)(cid:4) ˜ϕ(xj , yj) where ˜ϕ(xi, yi) = [yiϕ(xi)(cid:4), yi, 1√
0 except that the i-th component is 1). Hence the CVM is an MEB if (2) is
true. To deal with the situation that (2) is not satisﬁed, Tsang et al. extend
the MEB to the center-constrained MEB [10], and propose the generalized core
vector machine [11] which is applicable for any kernel and can also be applied
to kernel methods such as SVR and ranking SVM.

C

Note that the 2-norm error is used here. It could be less robust in the presence

of outliers in theory to some extent [5].

136

G. Niu et al.

3 Rough Margin Based Support Vector Machine

The rough set theory, which is based on the concept of the lower and upper
approximation of a set, is a mathematical tool to cope with uncertainty and
incompleteness [12]. The rough margins [8] are expressed as a lower margin 2ρl(cid:6)w(cid:6)
and an upper margin 2ρu(cid:6)w(cid:6) where 0 ≤ ρl ≤ ρu. They are corresponding with the
lower and upper approximations of the outlier set, such that the samples in the
lower margin are considered as outliers, the samples outside the upper margin are
not outliers, and the samples between two rough margins are possibly outliers.
When the training procedure takes place, the RMSVM tries to give major
penalty to samples lying within the lower margin, and give minor penalty to
other samples [8]. Notice that the Universum SVM [13] uses a similar strategy.
In practice, the RMSVM introduces slack variables ζi and penalize them τ times
larger than ξi, since ζi > 0 means that ϕ(xi) is in the lower margin. Formally,

Deﬁnition 2 (RMSVM [8])

min

w,b,ξ,ζ,ρl,ρu

1
2

m(cid:2)

(cid:3)w(cid:3)2 − νρl − νρu +

ξi +
(cid:4)ϕ(xi) + b) ≥ ρu − ξi − ζi,

1
m

i=1

τ
m

m(cid:2)

i=1

ζi

s.t.

yi(w
0 ≤ ξi ≤ ρu − ρl, ζi ≥ 0, ρl ≥ 0, ρu ≥ 0, i = 1, . . . m,

(5)

where ν ∈ (0, 1), τ > 1 are regularization parameters and ξi, ζi are slack variables.

3.1 Justiﬁcation of the Rough Margin

Apparently the RMSVM should encounter severer overﬁtting problem since it
emphasizes more on outliers than the ν-SVM. However, the dual of (5) is

min
α

α(cid:4)

1
2

m

(cid:4)α = 0, α(cid:4)

1 ≥ 2ν, 0 ≤ αi ≤ τ

Qα s.t. y

, i = 1, . . . m,

(6)
where Qij = yiyjϕ(xi)(cid:4)ϕ(xj). Hence 2ν/τ is the fraction of samples permitted to
lie within the lower margin. For ﬁxed ν, the larger the τ is, the less overﬁtting the
RMSVM is, and ultimately the RMSVM would become an underﬁtting classiﬁer.
Likewise the loss function, which was an absent topic in [8], could justify the
rough margin well. Let f(xi) = w(cid:4)ϕ(xi) + b,
Proposition 1. The loss function of the RMSVM is
ρu + (τ − 1)ρl − τ yif(xi)
ρu − yif(xi)
0

, yif(xi) ≤ ρl
, ρl < yif(xi) ≤ ρu
, yif(xi) > ρu

(xi, yi, f) =

⎧
⎪⎨
⎪⎩

Lρl,ρu

1

Proof. (Sketch) In fact ξi increases before ζi, while ζi has to stay at zero until
(cid:7)(cid:8)
ξi arrives at ρu − ρl, since ξi suﬀers less penalty in (5).

Rough Margin Based Core Vector Machine

137

1

< αi ≤ τ

m when yif(xi) ≤ ρl, ρl is smaller than
In other words, though 1
m
ρ, the loss Lρl,ρu
(xi, yi, f) may still not be large, and the number of samples
satisfying yif(xi) ≤ ρl is usually smaller than the number of samples satisfying
yif(xi) ≤ ρ in the ν-SVM. Therefore the notion of the rough margin is a tool
and technique to avoid the overﬁtting problem.
There is a side eﬀect that ρu is usually larger than ρ, which makes the RMSVM
generate much more support vectors satisfying ρl < yif(xi) ≤ ρu such that ϕ(xi)
lies between two rough margins with tiny αi. This phenomenon slows down the
speed and increases the storage of the RMSVM.

4 Rough Margin Based Core Vector Machine

For the sake of using the approximate MEB ﬁnding algorithm to solve the rough
margin based SVM, we use 2-norm error because it allows a soft-margin L2-SVM
to be transformed to a hard-margin one. Subsequently we have

Deﬁnition 3 (RMCVM)

(cid:3)w(cid:3)2 + b2 − 2ρl − 2ρu + C

m(cid:2)

m(cid:2)

ζ2
i

i + τ C
ξ2

min

w,b,ξ,ζ,ρl,ρu

s.t.

(cid:4)ϕ(xi) + b) ≥ ρu − ξi − ζi, ξi ≤ ρu − ρl, i = 1, . . . m,

yi(w

i=1

i=1

(7)

where C > 0, τ > 1 are regularization parameters and ξi, ζi are slack variables.
The dual of (7) is

− α(cid:4)

max
α,β
s.t. α(cid:4)

(cid:7)
K ◦ yy
(cid:4)
1 = 1, α ≥ 0, β ≥ 0,
1 = 2, β(cid:4)

(cid:8)
1
τ C I

(cid:4)
+ yy

+

α − 1
C

(cid:3)α − β(cid:3)2

(8)

where y = [y1, . . . , ym](cid:4) and the operator ◦ denotes the Hadamard product.
Remark 1. We omit the constraint ζi ≥ 0 since it is dispensable for L2-SVMs.
We omit the constraints ρl, ρu ≥ 0 based on the fact that certain inequality
constraints in the dual problem can be replace by the corresponding equality
constraints [14, 15]. Finally we omit ξi ≥ 0, otherwise there will be 1
(cid:3)α−β+γ(cid:3)2
in the objective and additional constraint γ ≥ 0, and the optimal γ∗ = 0
obviously. The constraint ρu ≥ ρl is indeed implicated by (7) already.
Remark 2. Note that the regularization parameter of the original SVM [16] and
the ν-SVM [17] is C and ν respectively. In the CVM it is C through which we
control the trading oﬀ between the ﬂatness and training errors. Since the order of
(cid:3)w(cid:3) and ξi are equal in (4), their coeﬃcients would change simultaneously under
scaling, which means that the coeﬃcient of ρ does not inﬂuence very much.

C

Remark 3. It is obvious that there is only ν-RMSVM insofar as it applies 1-norm
error. Similarly the RMSVM using 2-norm error would be C-RMSVM inherently.

138

G. Niu et al.

Therefore we demonstrate that Deﬁnition 3 is proper. Furthermore,

Proposition 2. The loss function of the RMCVM is

Lρl,ρu

2

(xi, yi, f) =

⎧
(ρu − ρl)2 + τ(ρl − yif(xi))2
⎪⎨
τ +1(ρu − yif(xi))2
⎪⎩
0

τ

, yif(xi) ≤ τ +1

ρl − 1

, τ +1
, yif(xi) > ρu

τ

τ

τ

ρl − 1

ρu

ρu < yif(xi)≤ ρu

τ

Proof. (Sketch) We have ξi = τ ζi when 0 < ξi < ρu− ρl, by equalling the partial
(cid:7)(cid:8)
derivatives of the objective function of (7) w.r.t. ξi and ζi respectively.

4.1 Solving Rough Margin Based CVM

From now on we will proof that (7) can be solved approximately by the CVM.

Proposition 3. The RMSVM cannot be transformed to an MEB problem unless
we drop the group of constraints αi ≤ τ
Lemma 1. Given a non negative vector α ∈ Rm, the optimal value of

m in (6).

minβ (cid:3)α − β(cid:3)2
(cid:3)α(cid:3)2.

4m(α(cid:4)1)2 and 1

4

is between 1

s.t. β(cid:4)

1 = 1
2

α(cid:4)

1, β ≥ 0

(9)

Proof. (Sketch) The upper bound is quite straightforward by setting β = 1
Let V = {β : β(cid:4)1 = 1
2
bound is given by minβ∈V (cid:3)α − β(cid:3) =
Actually βi = 0 iﬀ cot(α, ei) ≥ √

α.
α(cid:4)1}, then V consists of a hyperplane in Rm. The lower
(cid:7)(cid:8)

2 + 1. In other words, β is even sparser than

|α(cid:2)1− 1
2 α(cid:2)1|
1(cid:2)1

|α(cid:2)1|
√
m .
2

α, which is consistent with that there are less outliers than support vectors.

=

√

2

Theorem 1. The optimum of (8) is bounded by

(cid:7)
K ◦ yy
(cid:4)
(cid:7)
K ◦ yy
(cid:4)

(cid:8)
τ + 4
4τ C I
11(cid:4)
41(cid:4)1C +

−α(cid:4)

max

α

(cid:4)
+ yy

+

α s.t. α(cid:4)

1 = 2, α ≥ 0,

(10)

α

−α(cid:4)

(cid:4)
+ yy

(cid:8)
1
(11)
max
τ C I
Proof. (Sketch) Denote the objective function of (8) as maxα,β −h(α, β) =
− minα,β h1(α) + h2(α, β) where h1(α) = α(cid:4) (cid:9)
α and
h2(α, β) = 1
C

(cid:10)
K ◦ yy(cid:4) + yy(cid:4) + 1
τ C I

(cid:3)α − β(cid:3)2. Then

1 = 2, α ≥ 0.

α s.t. α(cid:4)

+

h(α, β) ⇐⇒ min

α

min
α,β

h1(α) + min
β

h2(α, β)

(cid:7)

(cid:8)

since h(α, β) is convex. According to Lemma 1, minβ h2(α, β) for given α is
(cid:7)(cid:8)
bounded by

4mC (α(cid:4)1)2 and 1

α(cid:4)α.

4C

1

Rough Margin Based Core Vector Machine

139

Corollary 1. The RMCVM can be solved approximately using the approximate
MEB ﬁnding algorithm.

Proof. Let Q(1), ϕ1, Q(2), ϕ2 be

Q(1)

ij = ϕ1(xi, yi)

(cid:4)ϕ1(xj , yj), ϕ1(xi, yi) =

Q(2)

ij = ϕ2(xi, yi)

(cid:4)ϕ2(xj , yj), ϕ2(xi, yi) =

(cid:11)
yiϕ(xi)

(cid:4), yi,

(cid:14)
yiϕ(xi)

(cid:4), yi,

(cid:12)

τ + 4
(cid:4)
4τ C e
i
√
1
mC

,

2

(cid:13)(cid:4)

,

(cid:15)(cid:4)

.

(cid:4)
e
i

1√
τ C

Then (10) and (11) can be regarded as MEB problems with parameters

(cid:12)

Rt =

1
2

α(cid:4)diag(Q(t)) − 1
4

α(cid:4)Q(t)α,

ct =

1
2

m(cid:2)

i=1

αiϕt(xi, yi),

t = 1, 2.

(cid:7)(cid:8)

The convergence of (10) and (11) are as same as the CVM but we omit the proof
here. Hence the approximate RMCVM algorithms based on (10) and (11) have
linear asymptotic time complexity and constant asymptotic space complexity.
Recall that the RMSVM is slower than the ν-SVM since it generates more sup-
port vectors. Surprisingly the RMCVM most often generates less core vectors
and are faster than the CVM, even though we solve two MEB problems.

5 Experiments

We implement the RMSVM using LibSVM [18] and the RMCVM using LibCVM.
The parameters are ﬁxed to τ = 5 as [8] suggested, and  = 10−6 as [5] rec-
ommended. We tune ν ∈ {0.1, 0.2, . . . , 0.9} providing it is feasible. The kernel is
Gaussian and its width is the better one computed by the default methods of
LibSVM and LibCVM. The computation method of kernel width is kept unchanged
over one dataset.
αiyik(x, xi)+ b from (10)
and RCu for fu(x) from (11), where b = y(cid:4)α respectively. We denote RCavg
as the average solution of them. For multi-class tasks the default one-versus-one
strategy is used. The datasets2 are listed in Table 1.

The notation RCl stands for the solution fl(x) =

(cid:16)

To begin with, we conduct experiments on a small dataset shown in Table 2.
Our accuracy is almost always higher than the CVM. We ﬁnd that RCavg is
not always the best and RCu is usually better than RCl. The next are results
on large datasets in the top of Table 3, where the time for reading input and
writing output ﬁles is excluded from the training time. Note that the CVM and
the RMCVM are very fast on extended-usps, on which all the RMSVM fail
to give a solution in 24 hours. Moveover, the RMCVM is usually better than
the CVM and even beat the RMSVM once on web. At last we display results

2 Download from http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/

and http://www.cse.ust.hk/~ivor/cvm.html

140

G. Niu et al.

Table 1. Data sets

name

satimage

web
ex-usps
intrusion

SensIT Vehicle: acoustic
SensIT Vehicle: seismic

#class #training #testing #feature
36
300
676
127
50
50

4,435
49,749
266,079
4,898,431
78,823
78,823

2,000
14,951
75,383
311,029
19,705
19,705

6
2
2
2
3
3

Table 2. Experimental results on satimage (accuracy (%))

−5

 = 10

−6

 = 10

C CVM RCl RCu RCavg CVM RCl RCu RCavg ν ν-SVM RMSVM
0.1 82.5 83.5 84.8 84.1
1
85.7 86.9 87.7 86.8
10 89.0 88.3 88.8 88.3
100 89.3 88.3 84.9 88.4

82.5 83.5 84.8 84.1
85.6 86.9 87.8 86.9
88.6 89.2 89.4 89.3
89.6 89.8 90.0 90.1

0.5 81.9
0.4 84.8
0.3 87.2
0.2 88.9

88.8
89.5
89.8
89.6

Table 3. Experimental results on large datasets

accuracy (%)

training time (seconds)

C

CVM RCl RCu RCavg RMSVM CVM RCl RCu RCavg RMSVM

10
100
1,000
10,000

98.6
98.9
96.2
95.9

98.7 98.8
99.0 98.9
97.1 97.3
96.1 93.0

98.7
99.1
98.2
97.4

web

98.9

79
23

218
29

295
423
39
53
20.7 11.6 12.9 24.6
8.88 7.75 4.53 12.22

134

100
1,000

99.46 99.50 99.49 99.49
99.45 99.47 99.46 99.47

-

166
81

121 109
92
108

229
201

> 1 day

extended-usps

100

91.8
91.8
1,000,000 91.8

10,000

intrusion

92.0 92.1
91.7 91.8
92.0 88.3

92.1
92.1
92.3

-

36

13

49
209
3.61 3.45 3.58 6.77
enough
2.41 1.65 1.61 2.84 memory

not

SensIT Vehicle: acoustic

100
1,000
10,000

50.0
39.4
40.9
1,000,000 40.6

26.7 31.9
37.1 52.4
42.1 41.0
50.6 42.4

41.9
58.4
46.6
47.7

66.1

832
3923 706 138
68.5 23.6 15.1 38.2
9.47 7.46 6.98 14.1
4.90 5.45 4.37 9.64

SensIT Vehicle: seismic

100
1,000
10,000

50.1
28.1
51.2
1,000,000 34.6

23.2 23.5
46.5 59.1
41.6 50.3
46.7 41.1

26.2
57.3
55.0
45.9

64.6

3882 796 164
951
55.0 21.5 11.6 33.3
8.50 7.17 6.61 13.7
4.60 4.25 4.67 8.70

79434

35194

Rough Margin Based Core Vector Machine

141

on the extremely noisy SensIT Vehicle in the bottom of Table 3. Perhaps the
RMCVM could always choose the right core vector and have less iteration before
convergence as a result. When C is too small the RMCVM is inferior to the CVM
since it is underﬁtting.

6 Conclusion

Motivated by the rough margin, we propose the rough margin based core vector
machine and demonstrate that it can be solved eﬃciently. Experimental results
show that the derived algorithms can handle very large datasets and the accuracy
is almost comparable to the rough margin based SVM.

References

[1] Vapnik, V.N.: Statistical Learning Theory. John Wiley & Sons, New York (1998)
[2] Platt, J.C.: Fast training of support vector machines using sequential minimal op-
timization. In: Advances in Kernel Methods, pp. 185–208. MIT Press, Cambridge
(1999)

[3] Smola, A.J., Sch¨okopf, B.: Sparse greedy matrix approximation for machine learn-

ing. In: 17th ICML, pp. 911–918 (2000)

[4] Tsang, I.W., Kwok, J.T., Cheung, P.M.: Very large svm training using core vector

machines. In: 20th AISTATS (2005)

[5] Tsang, I.W., Kwok, J.T., Cheung, P.M.: Core vector machines: Fast svm training

on very large data sets. JMLR 6, 363–392 (2005)

[6] Tsang, I.W., Kocsor, A., Kwok, J.T.: Simpler core vector machines with enclosing

balls. In: 24th ICML, pp. 911–918 (2007)

[7] Asharaf, S., Murty, M.N., Shevade, S.K.: Multiclass core vector machine. In: 24th

ICML, pp. 41–48 (2007)

[8] Zhang, J., Wang, Y.: A rough margin based support vector machine. Information

Sciences 178, 2204–2214 (2008)

[9] B˘adoiu, M., Clarkson, K.L.: Optimal core-sets for balls. In: DIMACS Workshop

on Computational Geometry (2002)

[10] Tsang, I.W., Kwok, J.T., Lai, K.T.: Core vector regression for very large regression

problems. In: 22nd ICML, pp. 912–919 (2005)

[11] Tsang, I.W., Kwok, J.T., Zurada, J.M.: Generalized core vector machines. IEEE

Transactions on Neural Networks 17, 1126–1140 (2006)

[12] Pawlak, Z.: Rough sets. International Journal of Computer and Information Sci-

ences 11, 341–356 (1982)

[13] Weston, J., Collobert, R., Sinz, F., Bottou, L., Vapnik, V.: Inference with the

universum. In: 23rd ICML, pp. 1009–1016 (2006)

[14] Crisp, D.J., Burges, C.J.C.: A geometric interpretation of ν-svm classiﬁers. In:

NIPS, vol. 12, pp. 244–250 (1999)

[15] Chang, C.C., Lin, C.J.: Training ν-support vector classiﬁers: Theory and algo-

rithms. Neural Computation 13, 2119–2147 (2001)

[16] Cortes, C., Vapnik, V.: Support-vector networks. Machine Learning 20, 273–297

(1995)

[17] Sch¨olkopf, B., Smola, A.J., Williamson, R.C., Bartlett, P.L.: New support vector

algorithms. Neural Computation 12, 1207–1245 (2000)

[18] Chang, C.C., Lin, C.J.: LIBSVM: a library for support vector machines (2001),

http://www.csie.ntu.edu.tw/~cjlin/libsvm


