Relevant Gene Selection Using Normalized Cut

Clustering with Maximal Compression

Similarity Measure

Rajni Bala1, R.K. Agrawal2, and Manju Sardana2

1 Deen Dayal Upadhyaya College, University of Delhi,

2 School of Computer and System Science, Jawaharlal Nehru University,

Delhi, India

New Delhi, India

Abstract. Microarray cancer classiﬁcation has drawn attention of re-
search community for better clinical diagnosis in last few years. Mi-
croarray datasets are characterized by high dimension and small sample
size. To avoid curse of dimensionality good feature selection methods
are needed. Here, we propose a two stage algorithm for ﬁnding a small
subset of relevant genes responsible for classiﬁcation in high dimensional
microarray datasets. In ﬁrst stage of algorithm, the entire feature space
is divided into k clusters using normalized cut. Similarity measure used
for clustering is maximal information compression index. The informa-
tive gene is selected from each cluster using t-statistics and a pool of non
redundant genes is created. In second stage a wrapper based forward
feature selection method is used to obtain a set of optimal genes for a
given classiﬁer. The proposed algorithm is tested on three well known
datasets from Kent Ridge Biomedical Data Repository. Comparison with
other state of art methods shows that our proposed algorithm is able to
achieve better classiﬁcation accuracy with less number of features.

Keywords: Cancer Classiﬁcation, Microarray, Normalized Cut, Repre-
sentative Entropy, Gene Selection.

1 Introduction

DNA microarrays have provided the opportunity to measure the expression lev-
els of thousands of genes simultaneously. One of the most common application
of microarray is to classify the samples such as healthy versus diseased by com-
paring the gene expression levels. Microarray data which is characterized by
high dimension and small sample size suﬀers from curse of dimensionality[1].
For better classiﬁcation there is a need to reduce the dimension. In general,
among thousands of genes(features) which are monitored simultaneously only a
fraction of them are biologically relevant. Therefore, eﬃcient feature selection
methods are needed to identify a set of discriminatory genes that can be used for
eﬀective class prediction and better clinical diagnose. In literature, various fea-
ture selection methods have been proposed. These methods broadly fall into two

M.J. Zaki et al. (Eds.): PAKDD 2010, Part II, LNAI 6119, pp. 81–88, 2010.
c(cid:2) Springer-Verlag Berlin Heidelberg 2010

82

R. Bala, R.K. Agrawal, and M. Sardana

categories[2]: ﬁlter and wrapper methods. Most ﬁlter methods independently
measure the importance of features without involving any classiﬁer. So, they
may not select the most relevant set of features for the learning algorithm. Also,
the features set selected by ﬁlter methods may contain correlated(redundant)
features which may degrade the performance of classiﬁer. On the other hand,
wrapper methods directly use the classiﬁcation accuracy of some classiﬁer as the
evaluation criteria. They tend to ﬁnd features better suited to the predetermined
learning algorithm resulting in better performance. But, they are computation-
ally more expensive . The conventional wrapper methods are hard to apply
directly to high dimensional datasets as they require large computation time.
Reducing the search space for wrapper methods will decrease the computation
time. This can be achieved by ﬁrst selecting a reduced set of non-redundant
features from the original set of features without losing any informative feature.
In this paper, a novel two-stage approach is proposed to determine a subset of
relevant and non-redundant genes for better cancer classiﬁcation. Our approach
ﬁrst groups correlated genes and then select one informative gene from each one
of these groups to reduce redundancy. This requires partitioning of the original
gene set into some distinct clusters so that the genes within a cluster are highly
similar(correlated) while those in diﬀerent clusters are dissimilar. At the second
stage a Sequential Forward Feature Selection(SFFS) method is applied to select
a smaller set of discriminatory genes which can provide maximum classiﬁcation
accuracy.

This paper is organized as follows. Section 2 describes related work. In sec-
tion 3 we present our proposed algorithm for selecting a set of informative and
non-redundant genes. Experimental results on some well-known datasets are
presented in Section 4. Section 5 contains conclusions.

2 Related Work

In order to achieve better classiﬁcation of high dimensional microarray data,
we need to determine a smaller set of discriminatory genes from a given set
of genes without loosing any information. In literature, many gene selection
methods have been proposed which are based on a gene ranking that assigns a
score for each gene which approximates the relative strength of the gene. These
methods return a set of top ranked genes and classiﬁer is built on these genes.
Among them, Golub et. al.[3] selected top genes using measure of correlation
which emphasizes that a discriminatory gene must have close expression levels
in samples within a class, but signiﬁcantly diﬀerent expression levels in samples
across diﬀerent classes. Other approaches that adopt the same principle with
modiﬁcations and enhancements include[4] and [5]. Using ranking method, one
cannot select a smallest set of discriminatory genes as the selected subset may
contain many correlated genes. Few wrapper based approaches are also suggested
in literature which works better for small and middle dimensional data. How-
ever they cannot be applied directly on high dimensional microarray dataset as it
is computationally expensive. We can overcome this by determining a smaller set

Relevant Gene Selection Using Normalized Cut Clustering

83

of genes for wrapper approach. This is possible if we can group correlated or
similar genes into clusters and then select a gene from each cluster which can
provide us a reduced set of independent and informative genes.

In literature clustering has been employed for grouping correlated or simi-
lar genes. Many diverse clustering techniques have been suggested in literature.
The most widely used techniques include hierarchical[6], k-means clustering[7]
and Self-organized- maps(SOM)[8]. Each one of them is associated with advan-
tages and disadvantages. Shi and Malik[9] have proposed an eﬃcient normalized
cut(NCUT) method based on graph theoretic approach for image segmentation.
The normalized cut criterion measures both the total dissimilarity between the
diﬀerent groups as well as total similarity with in the groups. This can also be
used for clustering of correlated genes in microarray data. In NCUT a given
graph G=(V, E), where vi V represents a gene and e(vi, vj) E represents sim-
ilarity between two genes vi and vj, is divided into two disjoint sets A and B.
For partitioning of the genes into A and B, the capacity of the normalized cut,
Ncut is deﬁned as

N cut(A, B) = cut(A, B)
assoc(A, V )

+ cut(A, B)
assoc(B, V )

(1)

where cut(A, B) = ΣuA,vBw(u, v) and assoc(A, V ) = ΣuA,tV w(u, t)

To determine a better partition of a cluster the value of Ncut should be
minimized which is a NP-hard problem. Shi and Malik[9] have shown that this
problem can be reformulated as eigenvalue problem which is given by

−1/2(D − W )D

−1/2x = λx

D

(2)

It has been shown by Shi and Malik[9] that second smallest eigenvector of the
above generalized eigenvalue system is the real valued solution to our minimum
normalized cut problem. Hence, the second smallest eigenvector can be used to
partition the original cluster into two clusters.

In general euclidean distance and Pearsons correlation are used as the distance
or similarity measure for clustering. However, euclidean distance is not suitable
to capture functional similarity such as positive and negative correlation, and
interdependency[10]. It is also pointed out that it is suitable only for a data
which follows a particular distribution[11]. On other hand, Pearson coeﬃcient
is not robust to outliers and it may assign a high similarity score to a pair of
dissimilar genes[12]. Also both these measures are sensitive to scaling and rota-
tion. A similarity measures called maximal information compression index[13] is
suggested in literature for measuring redundancy between two features. Given
two random variables x1 and x2, the maximal information compression index
λ2(x1, x2) is deﬁned as

λ2(x1, x2) = σ1 + σ2 +

2

(cid:2)
((σ1 + σ2)2 − 4σ1σ2(1 − ρ(x1, x2)2)

(3)

where σ1, σ2 are the variance of x1, and x2 respectively and ρ(x1, x2) is the
correlation between x1 and x2.

84

R. Bala, R.K. Agrawal, and M. Sardana

The value of λ2 is zero when the features are linearly dependent and increases
as the amount of dependency decreases. The measure λ2 possesses several desir-
able properties such as symmetry, sensitivity to scaling and invariance to rotation
which are not present in the commonly used euclidean distance and correlation
coeﬃcient.

Further splitting of a cluster, from a set of available clusters, can be decided on
the basis of representative entropy measure. Representative entropy measures the
amount of redundancy among genes in a given cluster. For a cluster containing p
genes with covariance matrix Σ, representative entropy, HR of a cluster is given
by

HR = −Σ

p

l=1λllog(λl)

(4)

where λl = λl
Σp

l=1λl

and λl, l = 1, 2, . . . , p are the eigen values of the matrix Σ.

HR attains a minimum value(zero) when all the eigenvalues except one are
zero, or in other words when all the information is present along a single di-
rection. If all the eigenvalues are equal, i.e. information is equally distributed
among all the genes, HR is maximum. High value of HR represents low redun-
dancy in the cluster. Since we are interested in partitioning the original subspace
into homogeneous clusters, each cluster should have low HR. So we split a clus-
ter which has maximum HR among a given set of clusters as it contains more
non-redundant genes.

3 Proposed Method for Gene Selection

Here we propose a two stage algorithm to select a set of discriminatory genes
to achieve better classiﬁcation. Our proposed algorithm consists of two phases.
The ﬁrst phase involves partitioning of the original gene set into some distinct
clusters so that the genes within a cluster are highly correlated to each other
while those in diﬀerent clusters are less correlated. The similarity measure used
in NCUT clustering algorithm is maximal information compression index. We
have used a hierarchical clustering in which we start with a single cluster. We
split the original cluster into two clusters such that the normalized cut value is
minimized. To determine which candidate cluster to further partition from the
existing set of clusters, we have used representative entropy. The cluster with
the maximum HR(low redundancy) is partitioned. This process is repeated till
we get the required number of clusters. Representative gene from each cluster
is chosen using t-statistics. In the second phase a Sequential Forward Feature
selection(SFFS) method is applied to select a smaller set of discriminatory genes
which provides maximum accuracy. The criterion used in the SFFS is the accu-
racy of the classiﬁer. The outline of the proposed algorithm is the following:

Proposed Algorithm
Input : Initial Set of genes, Class Labels C, Classifier M,

Cluster_Size

PHASE 1 // to determine a subset of relevant and independent

genes S

Relevant Gene Selection Using Normalized Cut Clustering

85

1. Intialization : Set G=initial set of genes ;
2. S = empty set; No of clusters=2; /*Set of Selected Attributes*/
3. Calculate the Similarity Matrix W using Maximal information compression

index.

cluster C into two clusters.

−1/2x = λ

−1/2(D − W )D

4. Deﬁne D where D(i) = σjw(i, j)
5. Solve eigenvalue problem D
6. Use the eigenvector with second smallest eigenvalues to divide the original
7. While (no of clusters≤Cluster Size)
8. Begin
9. For each cluster calculate the representative entropy HR
10. Choose the Cluster Ci having the maximum entropy
11. Repeat step (3)-(6) for Cluster Ci
12. No of clusters=No of clusters+1
13. End
14. For each cluster
15. Find the informative gene gi from cluster Ci using t-statistics
16. S=S U gi

PHASE 2 // to determine subset of genes which provides max accuracy

1.Initialization R=empty set
2.For each xj ∈ S calculate classiﬁcation accuracy for classiﬁer M.
3.[xk, max acc] = maxj Classif ication accuracy(xj);
4.R = R ∪ xk; S = S − xk; R min = R
5. For each xj calculate classiﬁcation accuracy of S ∪ xj for classiﬁer M
6. [xk, max acc] = maxj Classif ication accuracy(xj);
7. R = R ∪ xk; S = S − xk
8. If new max acc ≥ max acc then R min=R;max acc=new max acc;
9. Repeat 5-9 until max acc=100 or S = empty set
10. Retum R min, max acc

4 Experimental Setup and Results

To test the eﬀectiveness of our proposed algorithm, we have carried out ex-
periments on three well known datasets from Kent Ridge Biomedical Data
Repository[14]. The details of these datasets are given in Table 1. Datasets are
normalized using Z-score before carrying out experiments.

Table 1. Datasets Used

Dataset
Colon
SRBCT
Prostate

Samples
62
83
102

Genes
2000
2308
5967

Classes
2
4
2

86

R. Bala, R.K. Agrawal, and M. Sardana

Table 2. Maximum classiﬁcation accuracy along with number of genes for diﬀerent
classiﬁers using diﬀerent cluster size methods

No.of Clusters LDC
30
40
50
60

93.54(18)
91.93(4)
91.93(4)
98.38(32)

No.of Clusters LDC
30
40
50
60

97.59 (20)
100 (31)
100 (33)
98.79 (9)

No.of Clusters LDC
30
40
50
60

93.13 (3)
96.07 (8)
96.07 (8)
97.05 (5)

QDC
91.93(24)
93.54(8)
95.16(6)
95.16(7)

KNN
95.16(13)
95.16(6)
96.77(11)
95.16(8)

a. Colon dataset

QDC
96.38 (10)
97.59 (11)
97.59 (11)
97.59 (12)

KNN
100 (7)
100 (6)
100 (6)
100 (6)

b. SRBCT dataset

QDC
96.07 (3)
96.07 (3)
96.07 (3)
97.05 (19)

KNN
98.03 (7)
96.07 (3)
96.07 (3)
99.01 (7)

c. Prostate dataset

SVM
95.16(14)
93.54(5)
95.16(10)
96.77(19)

SVM
100 (4)
100 (4)
100 (5)
100 (6)

SVM
97.06 (14)
99.01 (15)
98.03 (17)
96.07 (3)

Table 3. Comparison of Maximum Classiﬁcation accuracy and number of genes se-
lected with other state of art methods

SRBCT

PROSTATE

COLON

Proposed Method 100(4) Proposed Method 99.01(7) Proposed method 98.38(32)

GS2+SVM[4] 100(96)
GS1+SVM[4] 98.8(34)
Chos+SVM[4] 98.8(80)
Ftest + SVM[4] 100(78)
Fu and Liu[15] 100(19)

Tibsrani[19] 100(43)

Khan[16] 100(96)

GAKNN[17] 96.3(79)

PSO+ANN[4] 88.7

BIRS[18] 91.2(3)

Yuechui and Yao[20] 90.3
BIRSW[18] 85.48(3.50)
BIRSF[18] 85.48(7.40)

Genes are clustered using NCUT based on maximal information compression
index as similarity measure. From each cluster the most informative gene is
selected using t-statistics. After collecting a pool of genes, a Forward Feature
Selection method is applied to get a sub-optimal set of genes which provides
maximum classiﬁcation accuracy. Classiﬁcation accuracy is calculated using leave-
one-out cross validation. The diﬀerent classiﬁers used in our experiments are lin-
ear discriminant
classiﬁer(QDC),
k-nearest neighbor(KNN) and support vector machine(SVM). For KNN the op-
timal value of k is chosen. Linear kernel is used in SVM. The experiment was
conducted for diﬀerent cluster sizes. The cluster sizes considered in our experi-
ments are 30, 40, 50 and 60. Table 2 depicts the maximum classiﬁcation accuracy
along with the number of genes obtained by our proposed algorithm for diﬀerent
cluster sizes. We can observe the following from Table 2:

classiﬁer(LDC), quadratic discriminant

Relevant Gene Selection Using Normalized Cut Clustering

87

1. For Colon dataset a maximum accuracy of 98.38% is achieved with 32 genes
for LDC classiﬁer. The maximum accuracy of 96.77% is achieved for KNN
and SVM with 11 and 19 genes respectively. For QDC a maximum accuracy
of 95.16% is achieved with 6 genes.

2. For SRBCT dataset maximum classiﬁcation accuracy of 100% is achieved
for LDC, KNN and SVM with 31 , 6 and 4 genes respectively. For QDC a
maximum accuracy of 97.59% is achieved with 11 genes.

3. For prostate dataset maximum classiﬁcation accuracy of 99.01% is achieved
for KNN and SVM with 7 and 15 genes respectively. For QDC and LDC a
maximum accuarcy of 97.05% is achieved with 19 and 5 genes respectively.
4. The performance of KNN is better in terms of number of genes in comparison

to other classiﬁers LDC, QDC and SVM for all three data sets.

It is observed that our proposed algorithm is able to achieve a high classiﬁcation
accuracy with small number of genes. In Table 3, we have also compared per-
formance of our proposed method in terms of classiﬁcation and number of genes
with some already existing gene selection methods in literature[15],[16],[17],[18],
[19],[4]and [20]. From Table 3, it can be observed that the performance of our
proposed algorithm is signiﬁcantly better in terms of both classiﬁcation accuracy
and number of genes selected.

5 Conclusion

In this paper, we have proposed a two stage algorithm for ﬁnding a small subset of
discriminatory genes responsible for classiﬁcation in high dimensional microarray
datasets. The ﬁrst stage involves partitioning of the original gene set into some
distinct subsets or clusters so that the genes within a cluster are highly correlated
to each other while those in diﬀerent clusters are less correlated. We have used
NCUT clustering algorithm which is based on graph theoretic approach and
requires computation of similarity measures between genes. We have used a
novel similarity measure maximal information compression index which is not
used for microarray datasets earlier. Most informative gene from each cluster
is then selected to create a pool of non-redundant genes. The size of this set
is signiﬁcantly small which allows us to use a wrapper approach at the second
stage. The use of wrapper method at the second stage gives a smaller subset of
genes which provides better classiﬁcation accuracy. Experimental results show
that our proposed method is able to achieve a better accuracy with a small
number of genes. Comparisons with other state of art methods show that our
proposed algorithm is able to achieve better or comparable accuracy with less
number of genes with all the three datasets.

References

1. Bellman, R.: Adaptive Control Processes. A Guided Tour. Princeton University

Press, Princeton (1961)

2. Guyon, I., Elisseeﬀ, A.: An Introduction to Variable and feature Selection. Journal

of Machine Learning Research (3), 1157–1182 (2003)

88

R. Bala, R.K. Agrawal, and M. Sardana

3. Golub, T.R., Slonim, D.K., Tamayo, P., Huard, C., Gaasenbeek, M., Mesirov, J.P.:
Molecular classiﬁcation of cancer: Class discovery and class prediction by gene
expression monitoring. Science 286, 531–537 (1999)

4. Yang, K., Cai, Z., Li, J., Lin, G.H.: A stable gene selection in microarray data

analysis. BMC Bioinformatics 7, 228 (2006)

5. Cho, J., Lee, D., Park, J.H., Lee, I.B.: New gene selection for classiﬁcation of cancer

subtype considering within-class variation. FEBS Letters 551, 3–7 (2003)

6. Eisen, M.B., Spellman, T.P.: Cluster analysis and display of genome-wide expres-

sion patterns. Proc. Natl. Acad. Sci. USA 95(25), 14863–14868 (1998)

7. Tavazoie, S., Huges, D., Campbell, M.J., Cho, R.J., Church, G.M.: Systematic

determination of genetic network architecture. Nature Genet., 281–285 (1999)

8. Kohonen, T.: Self-organizing maps. Springer, Berlin (1995)
9. Shi, J., Malik, J.: Normalized Cuts and Image Segmentation. IEEE Transactions

on Pattern analysis and machine Intelligence 22(8), 888–903 (2000)

10. Jiang, D., tang, C., Zhang, A.: Cluster Analysis for gene expression data: A survey.

IEEE Trans. Knowledge and Data Eng. 16, 1370–1386 (2004)

11. Yu, J., Amores, J., Sebe, N., Tian, Q.: Toward Robust Distance Metric analysis
for Similarity Estimation. In: Proc. IEEE Int’l Conf. Computer Vision and Pattern
Recognition (2006)

12. Heyer, L.J., Kruglyak, S., Yooseph, S.: Exploring Expression Data: identiﬁcation

and analysis of coexpressed genes. Genome Research 9, 1106–1115 (1999)

13. Mitra, P., Murthy, C.A., Pal, S.: Unsupervised feature selection using feature sim-
ilarity. IEEE Trans. Pattern Analysis and Machine Intelligence 24(3), 301–312
(2002)

14. Kent Ridge Biomedical Data Repository,

http://datam.i2r.a-star.edu.sg/datasets/krbd/

15. Fu, L.M., Liu, C.S.F.: Evaluation of gene importance in microarray data based

upon probability of selection. BMC Bioinformatics 6(67) (2005)

16. Khan, J., Wei, S., Ringner, M., Saal, L.H., Ladanyi, M., Westermann, F.: Clas-
siﬁcation and diagnosis prediction of cancers using gene expression proﬁling and
artiﬁcial neural networks. Nat. Med. 7, 673–679 (2001)

17. Li, L., Weinberg, C.R., Darden, T.A., Pedersen, L.G.: Gene Selection for sam-
ple classiﬁcation based on gene expression data: Study of sensitivity to choice of
parameters of the GA/KNN method. Bioinformatics 17(12), 1131–1142 (2001)

18. Ruiz, R., Riqueline, J.C., Aguilar-Ruiz, J.S.: Incremental wrapper based gene se-
lection from microarray data for cancer classiﬁcation. Pattern Recognition 39(12),
2383–2392 (2006)

19. Tibsrani, R., Hastie, T., Narasimhan, B., Chu, G.: Diagnosis of multiple cancer
types by shrunken centriods of gene expression. Proc. Natl Acad. Sci., USA (99),
6567–6572 (2002)

20. Yuechui, C., Yaou, Z.: A novel ensemble of classiﬁers for microarray data classiﬁ-

cation. Applied Soft computing (8), 1664–1669 (2008)


