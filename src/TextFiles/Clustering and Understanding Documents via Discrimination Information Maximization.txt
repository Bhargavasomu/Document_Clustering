Clustering and Understanding Documents

via Discrimination Information Maximization

Malik Tahir Hassan and Asim Karim

Dept. of Computer Science, LUMS School of Science and Engineering

{mhassan,akarim}@lums.edu.pk

Lahore, Pakistan

Abstract. Text document clustering is a popular task for understanding and sum-
marizing large document collections. Besides the need for efﬁciency, document
clustering methods should produce clusters that are readily understandable as
collections of documents relating to particular contexts or topics. Existing cluster-
ing methods often ignore term-document semantics while relying upon geomet-
ric similarity measures. In this paper, we present an efﬁcient iterative partitional
clustering method, CDIM, that maximizes the sum of discrimination informa-
tion provided by documents. The discrimination information of a document is
computed from the discrimination information provided by the terms in it, and
term discrimination information is estimated from the currently labeled docu-
ment collection. A key advantage of CDIM is that its clusters are describable by
their highly discriminating terms – terms with high semantic relatedness to their
clusters’ contexts. We evaluate CDIM both qualitatively and quantitatively on
ten text data sets. In clustering quality evaluation, we ﬁnd that CDIM produces
high-quality clusters superior to those generated by the best methods. We also
demonstrate the understandability provided by CDIM, suggesting its suitability
for practical document clustering.

1 Introduction

Text document clustering discovers groups of related documents in large document col-
lections. It achieves this by optimizing an objective function deﬁned over the entire
data collection. The importance of document clustering has grown signiﬁcantly over
the years as the world moves toward a paperless environment and the Web continues
to dominate our lives. Efﬁcient and effective document clustering methods can help in
better document organization (e.g. digital libraries, corporate documents, etc) as well
as quicker and improved information retrieval (e.g. online search).

Besides the need for efﬁciency, document clustering methods should be able to han-
dle the large term space of document collections to produce readily understandable
clusters. These requirements are often not satisﬁed in popular clustering methods. For
example, in K-means clustering, documents are compared in the term space, which
is typically sparse, using generic similarity measures without considering the term-
document semantics other than their vectorial representation in space. Moreover, it is
not straightforward to interpret and understand the clusters formed by K-means clus-
tering; the similarity of a document to its cluster’s mean provides little understanding
of the document’s context or topic.

P.-N. Tan et al. (Eds.): PAKDD 2012, Part I, LNAI 7301, pp. 566–577, 2012.
c(cid:2) Springer-Verlag Berlin Heidelberg 2012

Clustering and Understanding Documents

567

In this paper, we present a new document clustering method based on discrimination
information maximization (CDIM). CDIM’s semantically motivated objective function
is maximized via an efﬁcient iterative procedure that repeatedly projects documents
onto a K-dimensional discrimination information space and assigns documents to the
cluster along whose axis they have the largest value. The discrimination information
space is deﬁned by term discrimination information estimated from the labeled docu-
ment collection produced in the previous iteration. This procedure maximizes the sum
of discrimination information provided by all documents. A key advantage of using
term discrimination information is that each cluster can be interpreted by a list of highly
discriminating terms. These terms serve as units of understanding, as demonstrated in
linguistics studies [1,2], describing a cluster in the document collection. We evaluate
the performance of CDIM on ten popular text data sets. In clustering quality evalua-
tion, CDIM is found to produce high quality clusters superior to those produced by
non-negative matrix factorization (NMF) and several K-means variants. Our results
suggest the practical suitability of CDIM for clustering and understanding of document
collections.

The rest of the paper is organized as follows. We discuss the related work and moti-
vation for our method in Section 2. CDIM, our document clustering method is described
in detail in Section 3. Section 4 presents our experimental setup. Section 5 discusses the
results of our experiments, and we conclude with future directions in Section 6.

2 Motivation and Related Work

Content-based document clustering continues to be challenging because of (1) the high
dimensionality of the term-document space, (2) the sparsity of the documents in the
term-document space, and (3) the difﬁculty of incorporating appropriate term-document
semantics for improved clustering quality and understandability. Moreover, real-world
document clustering often involves large document collections thus requiring the clus-
tering method to be efﬁcient.

The K-means algorithm continues to be popular for document clustering due to its
efﬁciency and ease of implementation [3]. It is a partitional clustering method that opti-
mizes an objective function via an iterative two-step procedure. Usually, documents are
represented by terms’ weights, and documents are compared in the term space by the
cosine similarity measure. Several clustering objective functions can be optimized [4]
with the traditional objective of maximizing the similarity of documents to their clus-
ter means producing reliable clusterings. The Repeated Bisection clustering method,
which splits clusters into two until the desired number of clusters are obtained, has
been shown to produce better clusterings especially when K is large (greater than 20)
[5]. These K-means based methods are efﬁcient and accurate for many practical ap-
plications. Their primary shortcoming is poor interpretability of the clusters where the
cluster mean vector is often not a reliable indicator of the documents in a cluster.

Some researchers have used external knowledge bases to semantically enrich the
document representation for document clustering [6,7]. In [6], Wikipedia’s concepts
and categories are adopted to enhance the document representation, while in [7] several
ontology-based (e.g. WordNet) term relatedness measures are evaluated for semanti-
cally smoothing the document representation. In both works, it has been shown that

568

M.T. Hassan and A. Karim

the quality of clusterings produced by the K-means algorithm improves over the base-
line (“bag of words”) document representation. However, extracting information from
knowledge bases is computationally expensive. Furthermore, these approaches suffer
from the same shortcomings of K-means regarding cluster understandability.

The challenge of high dimensional data clustering, including that of document clus-
tering, has been tackled by clustering in a lower dimensional space of the original
term space. One way to achieve this is through Non-Negative Matrix Factorization
(NMF). NMF approximates the term-document matrix by the product of term-cluster
and document-cluster matrices [8]. Extensions to this idea, with the goal of improv-
ing the interpretability of the extracted clusters, have also been proposed [9,10]. An-
other way is to combine clustering with dimensionality reduction techniques [11,12].
Nonetheless, these methods are restricted by their focus on approximation rather than
semantically useful clusters, and furthermore, dimensionality reduction based tech-
niques are often computationally expensive.

Recently, it has been demonstrated that the relatedness of a term to a context or
topic in a document collection can be quantiﬁed by its discrimination information [2].
Such a notion of relatedness, as opposed to the traditional term-to-term relatedness,
can be effectively used for data mining tasks like classiﬁcation [13]. Meanwhile, mea-
sures of discrimination information, such as relative risk, odds ratio, risk difference,
and Kullback-Leibler divergence, are gaining popularity in data mining [14,15]. In the
biomedical domain, on the other hand, measures like relative risk have been used for a
long time for cohort studies and factor analysis [16,17].

3 CDIM – Our Document Clustering Method

CDIM (Clustering via Discrimination Information Maximization) is an iterative par-
titional document clustering method that ﬁnds K groups of documents in a K-
dimensional discrimination information space. It does this by following an efﬁcient
two-step procedure of document projection and assignment with the goal of maximiz-
ing the sum of documents’ discrimination scores. CDIM’s clusters are describable by
highly discriminating terms related to the context/topic of the documents in the cluster.
We start our presentation of CDIM by formally stating the problem.

3.1 Problem Statement
Let X = [x1, x2, . . . , xN ] ∈ (cid:3)M×N be the term-document matrix in which the ith
document xi = [x1i, x2i, . . . , xMi]T is represented by an M -dimensional vector (ith
column of matrix X). M is the total number of distinct terms in the N documents.
The weight of term j in document i, denoted by xji, is equal to the count of term j in
document i.
Our goal is to ﬁnd K (usually in practice K (cid:4) min{M, N}) clusters Ck (k =
1, 2, . . . , K) of documents such that if a document x ∈ Ck then x (cid:5)∈ Cj,∀j (cid:5)= k. Thus,
we assume hard partitioning of the documents among the clusters; however, this as-
sumption can be relaxed trivially in CDIM but we do not discuss this further in our

Clustering and Understanding Documents

569

current work. In addition to the cluster composition, we will also like to ﬁnd signiﬁ-
cant describing terms for each cluster. Let Tk be the index set of signiﬁcant terms for
cluster k.

3.2 Clustering Objective Function

CDIM ﬁnds K clusters in the document collection by maximizing the sum of discrimi-
nation scores of documents for their respective clusters. If we denote the discrimination
information provided by document i for cluster k by dik and the discrimination informa-
tion provided by document i for all clusters but cluster k by ¯dik, then the discrimination
score of document i for cluster k is deﬁned as ˆdik = dik − ¯dik. CDIM’s objective
function can then be written as

J =

K(cid:2)

(cid:2)

k=1

xi∈Ck

rik(dik − ¯dik)

(1)

where rik = 1 if document i is assigned to cluster k and zero otherwise. Document
discrimination information (dik and ¯dik) is computed from term discrimination infor-
mation that in turn is estimated from the current labeled document collection. These
computations are discussed in the following subsections.

Intuitively, CDIM seeks a clustering in which the discrimination information pro-
vided by documents for their cluster is higher than the discrimination information pro-
vided by them for the remaining clusters. It is not sufﬁcient to maximize just the dis-
crimination information of documents for their respective clusters as they may also
provide high discrimination information for the remaining clusters.
The objective function J is maximized by using a greedy two-step procedure. In one
step, given a cluster assignment deﬁned by rik,∀i, k, J is maximized by estimating
dik,∀i, k and ¯dik,∀i, k from the labeled document collection. This estimation is done
using maximum likelihood estimation. In the other step, given estimated discrimina-
tion scores ˆdik,∀i, k of documents, J is maximized by assigning each document to the
cluster k for which the document’s discrimination score is maximum. This two-step
procedure continues until the change in J from one iteration to the next drops below a
speciﬁed threshold value. Convergence is guaranteed because J is non-decreasing from
one iteration to the next and J is upper-bounded by a local maxima.

3.3 Term Discrimination Information

The discrimination information provided by a document is computed from the discrimi-
nation information provided by the terms in the document. The discrimination informa-
tion provided by a term for cluster k is quantiﬁed with the relative risk of the term for
cluster k over the remaining clusters. Mathematically, the discrimination information
of term j for cluster k and term j for all clusters but k is given by
p(xj| ¯Ck) when p(xj|Ck) − p(xj| ¯Ck) > t
p(xj|Ck)

(cid:3)

wjk =

0

otherwise

and

(2)

570

M.T. Hassan and A. Karim

(cid:3)

p(xj|Ck) when p(xj| ¯Ck) − p(xj|Ck) > t
p(xj| ¯Ck)

¯wjk =

0

otherwise

(3)
where p(xj|Ck) is the conditional probability of term j in cluster k and ¯Ck denotes all
clusters but cluster k. The term discrimination information is either zero (no discrimina-
tion information) or greater than one with a larger value signifying higher discriminative
power. The conditional probabilities in Equations 2 and 3 are estimated via smoothed
maximum likelihood estimation.

3.4 Relatedness of Terms to Clusters
In Equations 2 and 3, t ≥ 0 is a term selection parameter that controls the exclusion
of terms that provide insigniﬁcant discrimination information. As the value of t is in-
creased from zero, fewer terms will have a discrimination information greater than one.
The index set of terms that provide signiﬁcant discrimination information for cluster
k (Tk) is deﬁned as Tk = {j|wjk > 0,∀j}. These terms and their discrimination
information provide a good understanding of the context of documents in cluster k
in contrast with those in other clusters in the document collection. In general, Tk ∩
Tj (cid:5)= ∅,∀j (cid:5)= k. That is, there may be terms that provide signiﬁcant discrimination
information for more than one cluster. Also, depending on the value of t, there may be
terms that do not provide signiﬁcant discrimination information for all clusters.

In a study discussed in [1], it has been shown that humans comprehend text by asso-
ciating terms with particular contexts or topics. These relationships are different from
the traditional lexical relationships (e.g synonymy, antonymy, etc), but are more funda-
mental in conveying meaning and understanding. Recently, it has been shown that the
degree of relatedness of a term to a context is proportional to the term’s discrimination
information for that context in a corpus [2]. Given these studies, we can consider all
terms in Tk to be related to cluster k and the strength of this relatedness is given by the
term’s discrimination information. This is an important characteristic of CDIM whereby
each cluster’s context is describable by a set of related terms. Furthermore, these terms
and their weights (discrimination information) deﬁne a K-dimensional space in which
documents are comparable by their discrimination information.

3.5 Document Discrimination Information

A document i is describable by the terms it contains. Each term j in the document
vouches for the context or cluster k according to the value of the term’s discrimina-
tion information wjk. Equivalently, each term j in the document has a certain degree
of relatedness to context or cluster k according to the value wjk. The discrimination
information provided by document i for cluster k can be computed as the average term
discrimination information for cluster k:
(cid:4)

dik =

j∈Tk xjiwjk
(cid:4)

j∈Tk xji

.

(4)

Clustering and Understanding Documents

571

A similar expression can be used to deﬁne ¯dik. The document discrimination informa-
tion dik can be thought of as the relatedness (discrimination) of document i to cluster
k. The document discrimination score is given by ˆdik = dik − ¯dik; the larger this value
is, the more likely that document i belongs to cluster k. Note that a term contributes
to the discrimination information of document i for cluster k only if it belongs to Tk
and it occurs in document i. If such a term occurs multiple times in the document then
each of its occurrence contributes to the discrimination information. Thus, the discrim-
ination information of a document for a particular cluster increases with the increase in
occurrences of highly discriminating terms for that cluster.

3.6 Algorithm

CDIM can be described more compactly in matrix notation. CDIM’s algorithm, which
is outlined in Algorithm 1, is described next.
Let W ( ¯W) be the M × K matrix formed from the elements wjk,∀j, k ( ¯wjk,∀j, k),
ˆD be the N×K matrix formed from the elements ˆdik,∀i, k, and R be the N×K matrix
formed from the elements rik,∀i, k. At the start, each document is assigned to one of the
K randomly selected seeds using cosine similarity, thus deﬁning the matrix R. Then,
a loop is executed consisting of two steps. In the ﬁrst step, the term discrimination
information matrices (W and ¯W) are estimated from the term-document matrix X and
the current document assignment matrix R. The second step projects the documents
onto the relatedness or discrimination score space to create the discrimination score
matrix ˆD. Mathematically, this transformation is given by

ˆD = (XΣ)T (W − ¯W)

(5)

where Σ is a N × N diagonal matrix deﬁned by elements σii = 1/
ˆD represents the documents in the K-dimensional discrimination score space.
Documents are re-assigned to clusters based on their discrimination scores. A doc-
ument i is assigned to cluster k if ˆdik ≥ ˆdij ,∀j (cid:5)= k (ties are broken arbitrarily). In
matrix notation, this operation can be written as

j xji. The matrix

(cid:4)

R = maxrow( ˆD)

(6)

where ‘maxrow’ is an operator that works on each row of ˆD and returns a 1 for the
maximum value and a zero for all other values. The processing of Equations 5 and 6
are repeated until the absolute difference in the objective function becomes less than a
speciﬁed small value. The objective function J is computed by summing the maximum
values from each row of matrix ˆD.

The algorithm outputs the ﬁnal document assignment matrix R and the ﬁnal term
discrimination information matrix W. It is easy to see that the computational time
complexity of CDIM is O(KM N I) where I is the number of iterations required to
reach the ﬁnal clustering. Thus, the computational time of CDIM depends linearly on
the clustering parameters.

572

M.T. Hassan and A. Karim

4 Experimental Setup

Our evaluations comprise of two sets of experiments. First, we evaluate the clustering
quality of CDIM and compare it with other clustering methods on 10 text data sets.
Second, we illustrate the understanding that is provided by CDIM clustering. The results
of these experiments are given in the next section. Here, we describe our experimental
setup.

Algorithm 1. CDIM – Document Clustering via Discrimination Information Maxi-
mization
Require: X (term-document matrix), K (no. of clusters)
1: R(0) ← initial assignment of documents to clusters
2: τ ← 0
3: J (0) ← 0
4: repeat
5: W(τ ), ¯W(τ ) ← term discrimination info estimated from X and R(τ ) (Eqs. 2 and 3)
6:
7: R(τ +1) ← maxrow( ˆD(τ +1))
8:
9:
10: until (|J (τ ) − J (τ−1)| < )
11: return R (document assignment matrix), W (term discrimination info matrix)

ˆD(τ +1) ← (XΣ)T (W(τ ) − ¯W(τ ))
J (τ +1) ← sum of max discrimination scores from each row of ˆD(τ +1)
τ ← τ + 1

4.1 Data Sets

Our experiments are conducted on 10 standard text data sets of different sizes, contexts,
and complexities. The key characteristics of these data sets are given in Table 1. Data
set 1 is obtained from the Internet Content Filtering Group’s web site1, data set 2 is
available from a Cornell University web page2, and data sets 3 to 10 are obtained from
Karypis Lab, University of Minnesota3. Data sets 1 (stopword removal) and 3 to 10
(stopword removal and stemming) are available in preprocessed formats, while we per-
form stopword removal and stemming of data set 2. For more details on these standard
data sets, please refer to the links given above.

4.2 Comparison Methods

We compare CDIM with ﬁve clustering methods. Four of them are K-means variants
and one of them is based on Non-Negative Matrix Factorization (NMF) [8].

The four K-means variants are selected from the CLUTO Toolkit [18] based on
their strong performances reported in the literature [5,3]. Two of them are direct K-
way clustering methods while the remaining two are repeated bisection methods. For

1 http://labs-repos.iit.demokritos.gr/skel/i-conﬁg/downloads/
2 http://www.cs.cornell.edu/People/pabo/movie-review-data/
3 http://glaros.dtc.umn.edu/gkhome/cluto/cluto/download

Clustering and Understanding Documents

573

Table 1. Data sets and their characteristics

# Name Documents (N) Terms (M) Categories (K)
1 pu
2 movie
3 reviews
4 hitech
5 tr31
6 tr41
7 ohscal
8 re0
9 wap
10 re1

19868
38408
23220
13170
10128
7454
11465
2886
8460
3758

672
1200
4069
2301
927
878
11162
1504
1560
1657

2
2
5
6
7
10
10
13
20
25

each of these two types of methods, we consider two different objective functions. One
objective function maximizes the sum of similarities between documents and their clus-
ter mean. The direct and repeated bisection methods that use this objective function are
identiﬁed as Direct-I2 and RB-I2, respectively. The second objective function that we
consider maximizes the ratio of I2 and E1, where I2 is the intrinsic (based on cluster
cohesion) objective function deﬁned above and E1 is an extrinsic (based on separa-
tion) function that minimizes the sum of the normalized pairwise similarities of docu-
ments within clusters with the rest of the documents. The direct and repeated bisection
methods that use this hybrid objective function are identiﬁed as Direct-H2 and RB-H2,
respectively.

For NMF, we use the implementation provided in the DTU:Toolbox4. Speciﬁcally,
we use the multiplicative update rule with Euclidean measure for approximating the
term-document matrix.

In using the four K-means variants, the term-document matrix is deﬁned by term-
frequency-inverse-document-frequency (TF-IDF) values and the cosine similarity mea-
sure is adopted for document comparisons. For NMF, the term-document matrix is
deﬁned by term frequency values.

4.3 Clustering Validation Measures

We evaluate clustering quality with the BCubed metric [19]. In [20], it has been shown
that the BCubed precision and recall are the only measures that satisfy all desirable
constraints for a good clustering validation measure.

(cid:3)); otherwise Correct(o, o

in the clustering is equal to one, Correct(o, o

The BCUbed F-measure is computed as follows. Let L(o) and C(o) be the cate-
gory and cluster of an object o. Then, the correctness of the relation between objects
(cid:3)) ↔
(cid:3)
o and o
(cid:3)) = 0. BCubed precision (BP ) and BCubed re-
C(o) = C(o
(cid:3))]] and
call (BR) can now be deﬁned as: BP = Avgo[Avgo(cid:2).C(o)=C(o(cid:2))[Correct(o, o
(cid:3))]]. The BCubed F-measure is then given
BR = Avgo[Avgo(cid:2).L(o)=L(o(cid:2))[Correct(o, o
by BF = 2 × BP×BR
BP +BR . The BCubed F-measure (BF ) ranges from 0 to 1 with larger
values signifying better clusterings.

(cid:3)) = 1, iff L(o) = L(o

4 http://cogsys.imm.dtu.dk/toolbox/

574

M.T. Hassan and A. Karim

5 Results and Discussion

5.1 Clustering Quality

Table 2 gives the results of the clustering quality evaluation. The desired number of
clusters K for each data set is set equal to the number of categories in that data set
(see Table 1). The shown values are average BCubed F-measure ± standard deviation,
computed from 10 generated clusterings starting with random initial partitions.

These results show that CDIM outperforms the other algorithms on the ten data sets
with ﬁve highest performance scores (shown in bold) and within 0.005 of the highest
scores on three more data sets. CDIM is much better than NMF while its performances
are closer to those of the K-means variants. We veriﬁed the consistency of these results
using the Freidman’s test, which is a non-parametric test recommended for evaluat-
ing multiple algorithms on multiple data sets [21]. At 0.05 signiﬁcance level, CDIM
is found to be signiﬁcantly better than Direct-H2, RB-H2, and NMF, while its perfor-
mance difference with Direct-I2 and RB-I2 is not statistically signiﬁcant at this level.

An observation from our analysis is that CDIM consistently produces higher qual-
ity clusterings when the desired number of clusters is small (e.g. K < 5). This is
attributable to the lesser resolution power of the multi-way comparisons ( ˆdik = dik −
¯dik,∀k) that are required for document assignment. One potential way to overcome this
shortcoming for larger number of clusters is to use a repeated bisection approach rather
than a direct K-way partitioning approach.

Table 2. Clustering quality evaluation (average BCubed F-measure ± standard deviation)

RB-I2

RB-H2

NMF

CDIM Direct-I2 Direct-H2

Data
0.706±0.06 0.565±0.02 0.553±0.02 0.565±0.02 0.553±0.02 0.612±0.04
pu
movie 0.581±0.02 0.533±0.02 0.522±0.01 0.533±0.02 0.522±0.01 0.510±0.01
reviews 0.667±0.05 0.627±0.06 0.626±0.06 0.609±0.04 0.669±0.03 0.552±0.03
hitech 0.433±0.04 0.391±0.02 0.380±0.02 0.394±0.02 0.390±0.03 0.399±0.02
0.636±0.11 0.585±0.05 0.575±0.05 0.553±0.07 0.572±0.05 0.362±0.03
tr31
0.603±0.05 0.608±0.02 0.584±0.03 0.602±0.05 0.590±0.04 0.361±0.04
tr41
ohscal 0.429±0.02 0.422±0.02 0.417±0.03 0.432±0.01 0.427±0.01 0.250±0.02
0.417±0.02 0.382±0.02 0.382±0.01 0.397±0.03 0.375±0.01 0.345±0.02
re0
0.442±0.05 0.462±0.01 0.444±0.01 0.465±0.02 0.438±0.02 0.299±0.02
wap
0.393±0.03 0.443±0.02 0.436±0.02 0.416±0.01 0.418±0.03 0.301±0.03
re1

5.2 Cluster Understanding and Visualization

A key application of data clustering is corpus understanding. In the case of document
clustering, it is important that clustering methods output information that can readily
be used to interpret the clusters and their documents. CDIM is based on term discrim-
ination information and each of its cluster is describable by the highly discriminating
terms in it. We illustrate the understanding provided by CDIM’s output by displaying

Clustering and Understanding Documents

575

Table 3. Top 10 most discriminating terms (stemmed words) for clusters in ohscal data set

k Top 10 terms in cluster k
1 ‘platelet’, ‘kg’, ‘mg’, ‘dose’, ‘min’, ‘plasma’, ‘pressur’, ‘ﬂow’, ‘microgram’, ‘antagonist’
2 ‘carcinoma’, ‘tumor’, ‘cancer’, ‘surviv’, ‘chemotherapi’, ‘stage’, ‘recurr’, ‘malign’, ‘resect’, ‘therapi’
3 ‘antibodi’, ‘antigen’, ‘viru’, ‘anti’, ‘infect’, ‘hiv’, ‘monoclon’, ‘ig’, ‘immun’, ‘sera’
4 ‘patient’, ‘complic’, ‘surgeri’, ‘ventricular’, ‘infarct’, ‘oper’, ‘eye’, ‘coronari’, ‘cardiac’, ‘morta’
5 ‘pregnanc’, ‘fetal’, ‘gestat’, ‘matern’, ‘women’, ‘infant’, ‘deliveri’, ‘birth’, ‘labor’, ‘pregnant’
6 ‘risk’, ‘alcohol’, ‘age’, ‘children’, ‘cholesterol’, ‘health’, ‘factor’, ‘women’, ‘preval’, ‘popul’
7 ‘gene’, ‘sequenc’, ‘dna’, ‘mutat’, ‘protein’, ‘chromosom’, ‘transcript’, ‘rna’, ‘amino’, ‘structur’
8 ‘contract’, ‘muscle’, ‘relax’, ‘microm’, ‘calcium’, ‘effect’, ‘respons’, ‘antagonist’, ‘releas’, ‘action’
9 ‘il’, ‘receptor’, ‘cell’, ‘stimul’, ‘bind’, ‘growth’, ‘gamma’, ‘alpha’, ‘insulin’, ‘0’
10 ‘ct’, ‘imag’, ‘comput’, ‘tomographi’, ‘scan’, ‘lesion’, ‘magnet’, ‘reson’, ‘cerebr’, ‘tomograph’

the top 10 most discriminating terms (stemmed words) for each cluster of the ohscal
data set in Table 3. The ohscal data set contains publications from 10 different medical
subject areas (antibodies, carcinoma, DNA, in-vitro, molecular sequence data, preg-
nancy, prognosis, receptors, risk factors, and tomography). By looking at the top ten
terms, it is easy to determine the category of most clusters: cluster 2 = carcinoma, clus-
ter 3 = antibodies, cluster 4 = prognosis, cluster 5 = pregnancy, cluster 6 = risk factors,
cluster 7 = DNA, cluster 9 = receptors, cluster 10 = tomography. The categories molec-
ular sequence data and in-vitro do not appear to have a well-deﬁned cluster; molecular
sequence data has some overlap with cluster 7 while in-vitro has some overlap with
clusters 1 and 9. Nonetheless, clusters 2 and 8 still give coherent meaning to the docu-
ments they contain.

As another example, in hitech data set, the top 5 terms for two clusters are: (1)
‘health’, ‘care’, ‘patient’, ‘hospit’, ‘medic’, and (2) ‘citi’, ‘council’, ‘project’, ‘build’,
‘water’. The ﬁrst cluster can be mapped to the health category while the second clus-
ter does not have an unambiguous mapping to a category but it still gives sufﬁcient
indication that these articles discuss hi-tech related development projects.

Since CDIM ﬁnds clusters in a K-dimensional discrimination information space,
the distribution of documents among clusters can be visualized via simple scatter plots.
The 2-dimensional scatter plot of documents in the pu data set is shown in Figure 1 (left
plot). The x- and y-axes in this plot correspond to document discrimination information
for cluster 1 and 2 (di1 and di2), respectively. and the colored makers give the true
categories. It is seen that the two clusters are spread along the two axes and the vast
majority of documents in each cluster belong to the same category. Similar scatter plots
for Direct-I2 and NMF are shown in the middle and right plots, respectively, of Figure
1. However, these methods exhibit poor separation between the two categories in the pu
data set.
Such scatter plots can be viewed for any pair of clusters when K > 2. Since CDIM’s
document assignment decision is based upon document discrimination scores ( ˆdik,∀k),
scatter plots of documents in this space are also informative; each axis quantiﬁes how
relevant a document is to a cluster in comparison to the remaining clusters.

576

M.T. Hassan and A. Karim

CDIM

2

 
r
e
t
s
u
l
C

 
r
o
f
 
n
o
i
t
a
n
i
m

i
r
c
s
i
D

0.3

0.25

0.2

0.15

0.1

0.05

0

0.2

0
Doc. Disc. Info for Cluster 1

0.1

NMF

Direct−I2

0.05

0.045

0.04

0.035

0.03

0.025

0.02

0.015

0.01

0.005

2
 
r
e
t
s
u
l
C

 
f
o

 

 

n
a
e
m
o
t
 
y
t
i
r
a
l
i

m
S

i

0.5

0.45

0.4

0.35

0.3

0.25

0.2

0.15

0.1

0.05

2
 
r
e
t
s
u
l
C

 
r
o
f
 
t
h
g
i
e

W

0

0

0.02

0.04

0

0

0.1

0.2

Similarity to mean of Cluster 1

Weight for Cluster 1

Fig. 1. Scatter plot of documents projected onto the 2-D discrimination information space
(CDIM), similarity to cluster mean space (Direct-I2), and weight space (NMF). True labels are
indicated by different color markers.

6 Conclusion and Future Work

In this paper, we propose and evaluate a new document clustering method, CDIM, that
ﬁnds clusters in a K-dimensional space in which documents are well discriminated. It
does this by maximizing the sum of the discrimination information provided by doc-
uments for their respective clusters minus that provided for the remaining clusters.
Document discrimination information is computed from the discrimination informa-
tion provided by the terms in it. Term discrimination information is estimated from the
document collection via its relative risk. An advantage of using a measure of discrim-
ination information is that it also quantiﬁes the degree of relatedness of a term to its
context in the collection. Thus, CDIM produces clusters that are readily interpretable
by their highly discriminating terms.

Our experimental evaluations conﬁrm the effectiveness of CDIM as a practically
useful document clustering method. Its core idea of clustering in spaces deﬁned by
corpus-based discrimination or relatedness information holds much potential for future
extensions and improvements. In particular, we would like to investigate other measures
of discrimination/relatedness information, extend and evaluate CDIM for soft cluster-
ing, and develop a hierarchical and repeated bisection version of CDIM.

References

1. Morris, J., Hirst, G.: Non-classical lexical semantic relations. In: Proceedings of the HLT-
NAACL Workshop on Computational Lexical Semantics, pp. 46–51. Association for Com-
putational Linguistics (2004)

2. Cai, D., van Rijsbergen, C.J.: Learning semantic relatedness from term discrimination infor-

mation. Expert Systems with Applications 36, 1860–1875 (2009)

3. Tan, P.N., Steinbach, M., Kumar, V.: Introduction to Data Mining. Addison Wesley, New

York (2006)

Clustering and Understanding Documents

577

4. Zhao, Y., Karypis, G.: Criterion functions for document clustering: experiments and analysis.

Technical Report 01-40, University of Minnestoa (2001)

5. Steinbach, M., Karypis, G.: A comparison of document clustering techniques. In: Proceed-

ings of the KDD Workshop on Text Mining (2000)

6. Hu, X., Zhang, X., Lu, C., Park, E., Zhou, X.: Exploiting wikipedia as external knowledge for
document clustering. In: Proceedings of the 15th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, pp. 389–396. ACM (2009)

7. Zhang, X., Jing, L., Hu, X., Ng, M., Zhou, X.: A Comparative Study of Ontology Based Term
Similarity Measures on PubMed Document Clustering. In: Kotagiri, R., Radha Krishna, P.,
Mohania, M., Nantajeewarawat, E. (eds.) DASFAA 2007. LNCS, vol. 4443, pp. 115–126.
Springer, Heidelberg (2007)

8. Xu, W., Liu, X., Gong, Y.: Document clustering based on non-negative matrix factorization.
In: Proceedings of the 26th Annual International ACM SIGIR Conference on Research and
Development in Informaion Retrieval, pp. 267–273. ACM (2003)

9. Xu, W., Gong, Y.: Document clustering by concept factoriz ation. In: Proceedings of the 27th
Annual International ACM SIGIR Conference on Research and Development in Information
Retrieval, pp. 202–209. ACM (2004)

10. Cai, D., He, X., Han, J.: Locally consistent concept factorization for document clustering.

IEEE Transactions on Knowledge and Data Engineering (2010)

11. Tang, B., Shepherd, M., Heywood, M.I., Luo, X.: Comparing Dimension Reduction Tech-
niques for Document Clustering. In: K´egl, B., Lee, H.-H. (eds.) Canadian AI 2005. LNCS
(LNAI), vol. 3501, pp. 292–296. Springer, Heidelberg (2005)

12. Ding, C., Li, T.: Adaptive dimension reduction using discriminant analysis and k-means
clustering. In: Proceedings of the 24th International Conference on Machine Learning,
pp. 521–528. ACM (2007)

13. Junejo, K., Karim, A.: A robust discriminative term weighting based linear discriminant
method for text classiﬁcation. In: Eighth IEEE International Conference on Data Mining,
pp. 323–332 (2008)

14. Li, H., Li, J., Wong, L., Feng, M., Tan, Y.P.: Relative risk and odds ratio: a data mining
perspective. In: PODS 2005: Proceedings of the 24th ACM SIGMOD-SIGACT-SIGART
Symposium on Principles of Database Systems (2005)

15. Li, J., Liu, G., Wong, L.: Mining statistically important equivalence classes and delta-
discriminative emerging patterns. In: KDD 2007: Proceedings of the 13th ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining (2007)

16. Hsieh, D.A., Manski, C.F., McFadden, D.: Estimation of response probabilities from aug-
mented retrospective observations. Journal of the American Statistical Association 80(391),
651–662 (1985)

17. LeBlanc, M., Crowley, J.: Relative risk trees for censored survival data. Biometrics 48(2),

411–425 (1992)

18. Karypis, G.: CLUTO-a clustering toolkit. Technical report, Dept. of Computer Science, Uni-

versity of Minnesota, Minneapolis (2002)

19. Bagga, A., Baldwin, B.: Entity-based cross-document coreferencing using the vector space
model. In: Proceedings of the 36th Annual Meeting of the Association for Computa-
tional Linguistics and 17th International Conference on Computational Linguistics, vol. 1,
pp. 79–85. ACL (1998)

20. Amig´o, E., Gonzalo, J., Artiles, J., Verdejo, F.: A comparison of extrinsic clustering evalua-

tion metrics based on formal constraints. Information Retrieval 12(4), 461–486 (2009)

21. Demsar, J.: Statistical comparisons of classiﬁers over multiple data sets. Journal of Machine

Learning Research 7, 1–30 (2006)


