Data Transformation for Sum Squared Residue

Hyuk Cho

Computer Science, Sam Houston State University,

Huntsville, TX 77341-2090, USA

hyukcho@shsu.edu

http://www.cs.shsu.edu/~hxc005/

Abstract. The sum squared residue has been popularly used as a clus-
tering and co-clustering quality measure, however little research on its
detail properties has been performed. Recent research articulates that the
residue is useful to discover shifting patterns but inappropriate to ﬁnd
scaling patterns. To remedy this weakness, we propose to take speciﬁc
data transformations that can adjust latent scaling factors and eventu-
ally lead to lower the residue. First, we consider data matrix models with
varied shifting and scaling factors. Then, we formally analyze the eﬀect
of several data transformations on the residue. Finally, we empirically
validate the analysis with publicly-available human cancer gene expres-
sion datasets. Both the analytical and experimental results reveal column
standard deviation normalization and column Z-score transformation are
eﬀective for the residue to handle scaling factors, through which we are
able to achieve better tissue sample clustering accuracy.

Keywords: Data Transformation, Sum Squared Residue, Z-score Trans-
formation, Scaling Pattern, Shifting Pattern.

1 Introduction

Hartigan’s pioneering work, direct clustering [13], stimulated a vast amount of
research on co-clustering. Co-clustering aims at identifying homogeneous local
patterns, each of which consists of a subset of rows and a subset of of columns
in a given two dimensional matrix. This idea has attracted genomic researchers,
because it is compatible with our understanding of cellular processes, where
a subset of genes are coregulated under a certain experimental conditions [5].
Madeira and Oliveira [15] surveyed biclustering algorithms and their applications
to biological data analysis.

Cheng and Church [7] are considered to be the ﬁrst to apply co-clustering,
biclustering, to gene expression data. The greedy search heuristic generates bi-
clusters, one at a time, which satisfy a certain homogeneity constraint, called
mean squared residue. Since then, several similar approaches have been proposed
to enhance the original work. For example, Cho et al. [8] developed two mini-
mum sum squared co-clustering (MSSRCC) algorithms: one objective function
is based on the partitioning model proposed by Hartigan [13] and the other one

M.J. Zaki et al. (Eds.): PAKDD 2010, Part I, LNAI 6118, pp. 48–55, 2010.
c(cid:2) Springer-Verlag Berlin Heidelberg 2010

Data Transformation for Sum Squared Residue

49

is based on the squared residue formulated by Cheng and Church [7]. The later
is the residue of interest and deﬁned in the next chapter.

Recently, Aguilar-Ruiz [1] shows that the mean squared residue depends on
the scaling variance in the considered data matrix. This ﬁnding issues the weak-
ness of the residue and the need of new approaches to discover scaling patterns.
Motivated by the work, we propose a simple remedy to ﬁnd scaling patterns, still
using the same residue measure. We suggest to take speciﬁc data transforma-
tions so as to handle hidden scaling factors. In this paper, we apply several data
transformations to data matrix models derived from varied scaling and shifting
factors and analyze the eﬀect of data transformations on the the second residue,
RESIDUE(II) [8]. Furthermore, using MSSRCC, we empirically demonstrate the
advantage of the data transformations with publicly available human cancer mi-
croarrays. Both analysis and experimental results reveal that column standard
deviation normalization and column Z-score transformation are eﬀective.

The rest of this paper is organized as follows: In Section 2 we introduce
some deﬁnitions and facts used in this paper. We describe the considered data
transformations in Section 3. Then, we formally analyze the eﬀects of data trans-
formations and summarize the analysis results in Section 4. We discuss the ex-
perimental results with human cancer gene expression datasets in Section 5.
Finally, the paper is concluded with some remark.

2 Deﬁnition

We adapt the following deﬁnitions in Agular-Ruiz [1], Cheng and Church [7],
and Cho et al. [8] to ﬁt for our context.
Data matrix. A data matrix is deﬁned as a real-valued rectangular matrix
A ∈ R

m×n, whose (i, j)-th element is denoted by aij.

For example, a microarray can be deﬁned with two ﬁnite sets, the set of genes
and the set of experimental conditions. Note that Aguilar-Ruiz [1] describes the
microarray whose rows represent experimental condition and columns represent
genes. However, in this paper, we will consider a microarray which consists of
examples of genes in rows and attributes as experimental conditions in columns.
Co-cluster. Let I ⊆ {1, 2, . . . , m} and J ⊆ {1, 2, . . . , n} denote the set of indices
of the rows in a row cluster and the set of indices of the columns in a column
cluster. A submatrix of A induced by the index sets I and J is called a co-cluster
and denoted as AIJ ∈ R
, where |I| and |J| denote the cardinality of index
In reality, rows and columns in a co-cluster are not
set I and J, respectively.
necessary to be consecutive. However, for brevity we consider the co-cluster,
AIJ, whose entries consist of ﬁrst |I| rows and ﬁrst |J| columns in A.

|I|×|J|

2.1 Sum Squared Residue

In order to evaluate the coherence of such a co-cluster, we deﬁne RESIDUE(II)
of an element aij in the co-cluster determined by index sets I and J as below.

50

H. Cho

(cid:2)

(cid:2)

Residue. RESIDUE(II) is deﬁned as hij = aij − aiJ − aIj + aIJ , where the
mean of the entries in row i whose column indices are in J is computed by
aiJ = 1|J|
j∈J aij , the mean of the entries in column j whose row indices are in
I by aIj = 1|I|
i∈I aij , and the mean of all the entries in the co-cluster whose
row and column indices are in I and J by aIJ = 1|I||J|
Sum squared residue (SSR). Let HIJ ∈ R
be the residue matrix whose
entries are described by RESIDUE(II). Then, the sum squared residue of HIJ
(cid:2)
is deﬁned as SSR = (cid:4)HIJ(cid:4)2 =
ij, where (cid:4)X(cid:4) denotes the Frobenius
norm of matrix X, i.e., (cid:4)X(cid:4)2 =

i∈I,j∈J h2
(cid:2)
i,j x2
ij .

i∈I,j∈J aij.

|I|×|J|

(cid:2)

2.2 Patterns

We assume A contains both scaling and shifting factors. We borrow the concepts
of “local” and “global” scaling and shifting from Cheng and Church [7], Cho et
al. [8], and Aguilar-Ruiz [1] and generalize the deﬁnition of data patterns in [1].
Global/local scaling and global/local shifting patterns. A bicluster con-
tains both scaling and shifting patterns when it expresses aij = πi × αj + βj,
where πi is the base value for row (e.g., gene) i, αj is the scaling factor for column
(e.g., experimental condition) j, and βj is the shifting factor for column (e.g.,
experimental condition) j. We classify the expression into the following four pat-
terns: global scaling (gsc) and global shifting pattern (gsh) when aij = πi×α+β;
global scaling (gsc) and local shifting pattern (lsh) when aij = πi × α + βj; local
scaling (lsc) and global shifting pattern (gsh) when aij = πi × αj + β; and local
scaling (lsc) and local shifting pattern (lsh) when aij = πi × αj + βj.

3 Data Transformations

Raw data values have a limitation that raw values do not disclose how they vary
from the central tendency of the distribution. Therefore, transformation of the
raw data is considered one of the most important steps for various data mining
processes since the variance of a variable will determine its importance in a given
model [16]. In this study, we investigate the following data transformations.
No transformation (NT). No centering or scaling is taken. In other words,
ij = aij, ∀i and ∀j, i.e., the raw matrix is directly input to MSSRCC.
a(cid:4)
ij = aij − ai· − a·j + a··, ∀i and ∀j.
Double centering (DC). DC is deﬁned as a(cid:4)
ij = (πi − μπ) (αj − μα).
Through DC, each entry of a data matrix A becomes a(cid:4)
Note that we have a(cid:4)
·· = 0, since DC transforms
the data matrix to have both row means and column means to be 0.
ij = aij −
Column/row mean centering (MC). Column MC is deﬁned as a(cid:4)
ij = πiαj + βj − μ·j.
a·j, ∀i and ∀j. Through column MC, each entry becomes a(cid:4)
i· = πiμα+μβ−a··,
Therefore, row mean, column mean, and whole mean become a(cid:4)
·· = μπμα+μβ−a··, respectively. Similarly, row MC is
·j = μπαj +βj−a·j, and a(cid:4)
a(cid:4)
deﬁned similarly with ai·. Therefore, row mean, column mean, and whole mean
·j = μπαj + βj − a··, and a(cid:4)·· = μπμα + μβ − a··.
become a(cid:4)

·j = 0 and consequently a(cid:4)

i· = πiμα + μβ − ai·, a(cid:4)

i· = a(cid:4)

Data Transformation for Sum Squared Residue

51

ij = aij

σ·j , ∀i and ∀j. Similarly, row SDN is deﬁned with σ2

Column/row standard deviation normalization (SDN). Column SDN is
deﬁned as a(cid:4)
i·. Through
column and row SDN each column and row has a unit variance, respectively.
Column/row Z-score transformation (ZT). Column ZT is deﬁned as a(cid:4)
aij−a·j

ij =
i·. It is also called
“autoscaling”, where the measurements are scaled so that each column/row has
a zero mean and a unit variance [14]. Through ZT, the relative variation in
intensity is emphasized, since ZT is a linear transformation, which keeps the
relative positions of observations and the shape of the original distribution.

, ∀i and ∀j. Similarly, row ZT is deﬁned with ai· and σ2

σ·j

4 Analysis

Now, we analyze the eﬀect of the data transformations on the sum squared
residue, RESIDUE(II). Because of space limitation, we focus on analysis on the
three data transformations including NT, column SDN, and column ZT, which
clearly demonstrate the eﬀect of the speciﬁc data transformation.

(cid:2)

i∈I πi. and the mean of the scaling factors by μαJ = 1|J|

4.1 No Transformation (NT)
(i, j)-th entry of row i ∈ I and column j ∈ J of co-cluster AIJ is described
as aij = πiαj + βj. Then, the mean of the base values of AIJ is computed by
μπI = 1|I|
j∈J αj,
and the mean of the shifting factors by μβJ = 1|J|
j∈J βj. Also, the mean of row
i is obtained by aiJ = πiμαJ + μβJ , the mean of column j by aIj = μπαj +
βj, and the mean of all the elements by aIJ = μπμαJ + μβJ . Using these val-
ues, we obtain RESIDUE(II), hij = (πi − μπI )(αj − μαJ ). Consequently, the sum
squared residue (SSR) can be computed as SSR = (cid:4)HIJ(cid:4)2 =
ij =
i∈I,j∈J h2
(cid:2)
i∈I(πi − μπI )2
πI = 1|I|

i∈I,j∈J (πi − μπI )2(αj − μαJ )2 =|I||J|σ2

, where σ2

(cid:2)
(cid:2)

σ2
αJ

(cid:2)

(cid:2)

and σ2

αJ = 1|J|

(cid:2)

j∈J (αj − μαJ )2.

πI

In fact, SSR shown above is a revisit of Theorems in Aguilar-Ruiz [1]. It shows
with no data transformation that SSR is dependent on both the variance of base
values and the variance of scaling factors, but independent from shifting factors.
Accordingly, any shifting operations such as DC and MC to the given data ma-
trix should not contribute to RESIDUE(II). As also shown in [1], RESIDUE(II)
itself has an ability to discover shifting patterns.

4.2 Column Standard Deviation Normalization (SDN)

Column SDN transforms A to have the constant global scaling factor, i.e., 1, and
the local shifting factors, i.e., βj
αj . To be more speciﬁc, (i, j)-th entry is trans-
. Then, row mean, column mean,
formed as aij = 1

σ·j (πiαj + βj) = 1
σπ

πi + βj
αj

(cid:3)

(cid:4)

52

H. Cho

1

(cid:2)

σ·j (πiαj +βj),
and whole mean of co-cluster AIJ are computed by aiJ = 1|J|
j∈J
aIj = 1|I|
σ·j (πiαj + βj), re-
1
j∈J
spectively. Therefore, using RESIDUE(II), we can capture the perfect co-cluster,
i.e., zero RESIDUE(II), for all the four expression patterns.

σ·j (πiαj + βj), and aIJ = 1|I||J|

i∈I

i∈I

(cid:2)

(cid:2)

(cid:2)

1

4.3 Column Z-Score Transformation (ZT)

Column ZT transforms A to have the constant global scaling factor, i.e., 1, and
the constant global shifting factor, i.e., −μπ. To be more speciﬁc, (i, j)-th entry
σπ (πi − μπ). Then, row mean of
σ·j (πiαj + βj − a·j) = 1
is transformed as aij = 1
σπ (μπi − μπ) = aij, and column mean and
co-cluster AIJ is obtained by aiJ = 1
σπ (μπI − μπ) = aIJ. Like column SDN, we obtain zero
whole mean by aIj = 1
REDIDUE(II) for all the possible combinations of scaling and shifting patterns.

5 Experimental Results

Now, we empirically show the eﬀect of data transformations on the four pub-
licly available human cancer microarray datasets including Colon cancer [2],
Leukemia [12], Lung cancer [3], and MLL [3]. With MSSRCC [8][9], we generate
100 × 2 or 100 × 3 co-clusters with random and spectral initializations, setting
τ = 10−3(cid:4)A(cid:4)2 and τ = 10−6(cid:4)A(cid:4)2 for batch and local search, respectively. De-
tailed algorithmic strategies and their contributions are discussed in [9].
Data preprocessing. Since utilizing sophisticated feature selection algorithms
is not a main focus, we just apply the simple preprocessing steps usually adopted
in microarray experiments as in [6][10][11] to detect diﬀerential expression. De-
tails are summarized in Table 1. Further, the gene expression values in Colon
dataset were transformed by taking the base-10 logarithm.

Table 1. Description of microarray datasets used in our experiments

2000
62
2

# original genes
# samples
# sample classes

Colon Leukemia
7129
72
2

MLL
Lung
12582
12533
72
181
3
2
ALL(24)
Normal(20) ALL(47) ADCA(150)
AML(25)
Sample class names Tumor(42) AML(25) MPM(31)
MLL(23)
|max/min|
5
|max − min|
5500
# remaining genes
2474
Abbreviations: ALL – Acute Lymphoblastic Leukemia; AML – Acute
Myeloid Leukemia; ADCA – Adenocarcinoma; MPM – Malignant
Pleural Mesothelioma; and MLL – Mixed-Lineage Leukemia. The
number after each sample class name denotes the number of samples.

15
500
1096

5
500
3571

5
600
2401

Data Transformation for Sum Squared Residue

53

 

 

I

R
+
T
Z

I

R
+
N
D
S

I

S
+
T
Z

I

S
+
N
D
S

I

I

L
L
M

S
L
 
t
u
o
h
t
i

w

 
l
a
n
F

i

l

a

i
t
i
n

I

S
L
h
t
i

 

w

 
l
a
n
F

i

I

R
+
C
M

V
C
C
R
S
S
M

I

R
+
C
D

I

R
+
T
N

)
d
(

L
L
M

S
L
 
t
u
o
h
t
i

w

 
l
a
n
F

i

l

a

i
t
i
n

I

S
L
h
t
i

 

w

 
l
a
n
F

i

I

S
+
C
M

V
C
C
R
S
S
M

I

S
+
C
D

I

S
+
T
N

0
1
1

0
0
1

0
9

0
8

0
7

0
6

0
5

0
4

 

0
3

0
1
1

0
0
1

0
9

0
8

0
7

0
6

0
5

0
4

 

0
3

Accuracy (%)

Accuracy (%)

 

 

I

R
+
T
Z

I

R
+
N
D
S

I

S
+
T
Z

I

S
+
N
D
S

I

I

g
n
u
L

S
L
 
t
u
o
h
t
i

w

 
l
a
n
F

i

l

a

i
t
i
n

I

S
L
h
t
i

 

w

 
l
a
n
F

i

I

R
+
C
M

V
C
C
R
S
S
M

I

R
+
C
D

I

R
+
T
N

)
c
(

g
n
u
L

S
L
 
t
u
o
h
t
i

w

 
l
a
n
F

i

l

a

i
t
i
n

I

S
L
h
t
i

 

w

 
l
a
n
F

i

I

S
+
C
M

V
C
C
R
S
S
M

I

S
+
C
D

I

S
+
T
N

0
1
1

0
0
1

0
9

0
8

0
7

0
6

 

0
5

0
1
1

0
0
1

0
9

0
8

0
7

0
6

 

0
5

Accuracy (%)

Accuracy (%)

 

 

I

R
+
T
Z

I

R
+
N
D
S

I

S
+
T
Z

I

S
+
N
D
S

I

I

I

R
+
C
M

V
C
C
R
S
S
M

S
L
 
t
u
o
h
t
i

w

 
l

a
n
F

i

S
L
h
t
i

 

w

 
l

a
n
F

i

l
a
i
t
i
n

I

I

R
+
C
D

I

R
+
T
N

)
b
(

a
i
m
e
k
u
e
L

S
L
 
t
u
o
h
t
i

w

 
l

a
n
F

i

S
L
h
t
i

 

w

 
l

a
n
F

i

l
a
i
t
i
n

I

I

S
+
C
M

V
C
C
R
S
S
M

I

S
+
C
D

I

S
+
T
N

0
1
1

0
0
1

0
9

0
8

0
7

0
6

 
0
5

0
1
1

0
0
1

0
9

0
8

0
7

0
6

 
0
5

Accuracy (%)

Accuracy (%)

 

 

I

R
+
T
Z

I

R
+
N
D
S

I

S
+
T
Z

I

S
+
N
D
S

I

I

I

R
+
C
M

V
C
C
R
S
S
M

S
L
 
t
u
o
h
t
i

w

 
l

a
n
F

i

S
L
h
t
i

 

w

 
l

a
n
F

i

l

a

i
t
i
n

I

I

R
+
C
D

I

R
+
T
N

)
a
(

l

n
o
o
C

S
L
 
t
u
o
h
t
i

w

 
l

a
n
F

i

S
L
h
t
i

 

w

 
l

a
n
F

i

l

a

i
t
i
n

I

I

S
+
C
M

V
C
C
R
S
S
M

I

S
+
C
D

I

S
+
T
N

a
i
m
e
k
u
e
L

l

n
o
o
C

0
1
1

0
0
1

0
9

0
8

0
7

0
6

 

0
5

0
1
1

0
0
1

0
9

0
8

0
7

0
6

 

0
5

Accuracy (%)

Accuracy (%)

)
h
(

)
g
(

)
f
(

)
e
(

e
n
e
g
0
0
1
o
t
1
r
e
v
o
d
e
g
a
r
e
v
a
e
r
a
s
e
u
l
a
v
y
c
a
r
u
c
c
a
e
h
T

.
)
I
I
(
E
U
D
I
S
E
R
h
t
i
w
C
C
R
S
S
M
g
n
i
s
u
y
c
a
r
u
c
c
a

g
n
i
r
e
t
s
u
l
c

e
l
p
m
a
s

e
u
s
s
i
t

e
g
a
r
e
v
A

.
1
.
g
i
F

I

R

:
s
n
o
i
t
a
i
v
e
r
b
b
A

.

n
o
i
t
a
z
i
l
a
i
t
i
n

i

l
a
r
t
c
e
p
s

i

c
i
t
s
i
n
m
r
e
t
e
d

h
t
i
w
d
e
n
i
a
t
b
o

e
r
a

)
h
(
-
)
e
(

d
n
a

s
n
u
r

m
o
d
n
a
r

0
1

r
e
v
o

d
e
g
a
r
e
v
a

e
r
a

)
d
(
-
)
a
(

.
s
r
e
t
s
u
l
c

;
g
n
i
r
e
t
n
e
C
n
a
e
M

)
n
m
u
l
o
c
(

–
C
M

;
g
n
i
r
e
t
n
e
C
e
l
b
u
o
D
–
C
D

;

n
o
i
t
a
m
r
o
f
s
n
a
r
T
o
N
–
T
N

;

n
o
i
t
a
z
i
l
a
i
t
i
n
I

l
a
r
t
c
e
p
S

–

I
S

;

n
o
i
t
a
z
i
l
a
i
t
i
n
I

m
o
d
n
a
R
–

.

h
c
r
a
e
S

l
a
c
o
L

–

S
L

d
n
a

;

n
o
i
t
a
m
r
o
f
s
n
a
r
T
e
r
o
c
s
-
Z

)
n
m
u
l
o
c
(

–
T
Z

;

n
o
i
t
a
z
i
l
a
m
r
o
N
n
o
i
t
a
i
v
e
D
d
r
a
d
n
a
t
S

)
n
m
u
l
o
c
(

–
N
D
S

54

H. Cho

Tissue sample clustering evaluation measure. To evaluate the performance
of sample clusterings, we quantify tissue sample clustering performance using the
(cid:4)×100, where
following clustering accuracy measure: accuracy(%) = 1
T
T denotes the total number of samples, l the number of sample clusters, and ti
the numbers of the samples correctly clustered into a sample class i.
Performance comparison. Figure 1 illustrates the average tissue sample ac-
curacy using MSSRCC with RESIDUE(II). As reported in [9], spectral initial-
ization and local search strategy play a signiﬁcant role in improving MSSRCC
performance. However, in this paper, we are more interested in how data trans-
formations aﬀect the tissue sample accuracy performance.

(cid:3)(cid:2)l

i=1

ti

NT, DC, and MC with random initialization ((a)-(d)) and NT and MC
with spectral initialization ((e)-(h)) result in nearly similar accuracy. Note that
RESIDUE(II) is not aﬀected by shifting factors, but still aﬀected by the scal-
ing factors as ﬁrst articulated in [1] and also revisited in the analysis. To be
more speciﬁc, the residue with NT on data matrices with local scaling factors is
(πi − μπI ) (αj − μαJ ), on which interestingly the residue with DC or MC is also
dependent. In our experiment, DC with random initialization generates com-
patible accuracy with that of other data transformations ((a)-(d)), however it is
relatively less eﬀective with spectral initialization ((f)-(h)). For all the consid-
ered datasets, MC presents compatible performance that of NT, but not better
than that of either SDN or ZT.

As analyzed in the previous section, both column SDN and column ZT help
MSSRCC with RESIDUE(II) capture perfect co-clusters, thus MSSRCC with
column SDN or column ZT is supposed to generate similar accuracy and also
better accuracy than those with NT, DC, or MC. Accordingly, they lead to the
best accuracy values for most cases ((a)-(h)).

6 Conclusion and Remark

Aguilar-Ruiz [1] issues the need of a new metric to discover both scaling and
shifting patterns, showing that the sum squared residue can discover any shifted
patterns but may not capture some scaled patterns. To answer this need, we
propose a simple remedy that helps the residue resolve its dependency on scaling
variances. We suggest to take speciﬁc data transformations through which the
hidden scaling factors are implicitly removed. We analyze the eﬀect of various
data transformation on RESIDUE(II) [8] for data matrices with global/local
scaling and global/shifting factors.

Both analysis and experimental results reveal that column standard deviation
normalization and column Z-score transformation are eﬀective for RESIDUE(II).
To be more speciﬁc, through MSSRCC with RESIDUE(II) and the two data trans-
formations, we are able to discover coherent patterns with both scaling and shifting
factors. The transformed matrix contains the constant global scaling factor 1 and
local shifting factors and gives the perfect residue score, i.e., zero RESIDUE(II).
Note that RESIDUE(II) is a special case (scheme 6) of the six Euclidean
co-clustering schemes in Bregman co-clustering algorithms [4]. Our formal

Data Transformation for Sum Squared Residue

55

analysis can be applicable to any clustering/co-clustering algorithm that has a
closed-form of objective function, thus our potential future direction is to apply the
proposed analysis to the remaining co-clustering models in Bregman co-clustering
algorithms and formally characterize each of Bregman co-clustering algorithms.

References

1. Aguilar-Ruiz, J.S.: Shifting and scaling patterns from gene expression data. Bioin-

formatics 21(20), 3840–3845 (2005)

2. Alon, U., et al.: Broad patterns of gene expression revealed by clustering analysis
of tumor and normal colon tissues probed by oligonucleotide arrays. PNAS 96(12),
6745–6750 (1999)

3. Armstrong, S.A., et al.: MLL translocations specify a distinct gene expression

proﬁle that distinguishes a unique leukemia. Nature Genetics 30, 41–47 (2002)

4. Banerjee, A., Dhillon, I.S., Ghosh, J., Merugu, S., Modha, D.: A Generalized maxi-
mum entropy approach to Bregman co-clustering and matrix approximation. Jour-
nal of Machine Learning Research 8, 1919–1986 (2007)

5. Ben-Dor, A., Chor, B., Karp, R., Yakhini, Z.: Discovering Local Structure in Gene
Expression Data: The Order-Preserving Submatrix Problem. Journal of Computa-
tional Biology 10(3-4), 373–384 (2003)

6. Bø, T.H., Jonassen, I.: New feature subset selection procedures for classiﬁcation of

expression proﬁles. Genome Biology 3(4) (2002)

7. Cheng, Y., Church, G.M.: Biclustering of expression data. In: Proceedings of
the Eighth International Conference on Intelligent Systems for Molecular Biology
(ISMB), vol. 8, pp. 93–103 (2000)

8. Cho, H., Dhillon, I.S., Guan, Y., Sra, S.: Minimum sum squared residue based
co-clustering of gene expression data. In: Proceedings of the Fourth SIAM Inter-
national Conference on Data Mining (SDM), pp. 114–125 (2004)

9. Cho, H., Dhillon, I.S.: Co-clustering of human cancer microarrays using minimum
sum-squared residue co-clustering (MSSRCC) algorithm. IEEE/ACM Transactions
on Computational Biology and Bioinformatics (IEEE/ACM TCBB) 5(3), 114–125
(2008)

10. Dettling, M., B¨uhlmann, P.: Supervised clustering of genes. Genome Biology 3(12)

(2002)

11. Dudoit, S., Fridlyand, J.: A Prediction-based Resampling Method for Estimating

the Number of Clusters in a Dataset. Genome Biology 3(7) (2002)

12. Golub, T.R., et al.: Molecular Classiﬁcation of Cancer: Class Discovery and Class

Prediction by Gene Expression Monitoring. Science 286, 531–537 (2002)

13. Hartigan, J.A.: Direct clustering of a data matrix. Journal of the American Statis-

tical Association 67(337), 123–129 (1972)

14. Kowalski, B.R., Bender, C.F.: Pattern recognition: A powerful approach to inter-
preting chemical data. Journal of the American Chemical Society 94(16), 5632–5639
(1972)

15. Madeira, S.C., Oliveira, A.L.: Biclustering algorithms for biological data analysis:
a survey. IEEE Transactions on Computational Biology and Bioinformatics (IEEE
ACM/TCBB) 1(1), 24–45 (2004)

16. S´anchez, F.C., Lewi, P.J., Massart, D.L.: Eﬀect of diﬀerent preprocessing methods
for principal component analysis applied to the composition of mixtures: detec-
tion of impurities in HPLC-DAD. Chemometrics and Intelligent Laboratory Sys-
tems 25(2), 157–177 (1994)


