Clustering Patient Medical Records
via Sparse Subspace Representation

Budhaditya Saha1, Duc-Son Pham2, Dinh Phung1, and Svetha Venkatesh1,2

1 Center for Pattern Recognition and Data Analytics

School of Information Technology, Deakin University, Geelong, Australia

2 Institute for Multi-sensor Processing and Content Analysis

Department of Computing, Curtin University, Western Australia

budhaditya.saha@deakin.edu.au

Abstract. The health industry is facing increasing challenge with “big
data” as traditional methods fail to manage the scale and complexity.
This paper examines clustering of patient records for chronic diseases
to facilitate a better construction of care plans. We solve this problem
under the framework of subspace clustering. Our novel contribution lies
in the exploitation of sparse representation to discover subspaces auto-
matically and a domain-speciﬁc construction of weighting matrices for
patient records. We show the new formulation is readily solved by ex-
tending existing (cid:2)1-regularized optimization algorithms. Using a cohort
of both diabetes and stroke data we show that we outperform existing
benchmark clustering techniques in the literature.

Keywords: subspace clustering, medical data, sparse representation.

1

Introduction

Traditional methods fail to manage the scale and complexity of “big data”. The
health sector is at the epicenter of this “big data” - data on admissions, diagnosis,
outcomes, spanning a bewildering and disconnected web of images, computerized
records and registries. There are no systems to manage this big data. The result
is “write only data”, mostly unused. Critically it has potential to identify critical
safety issues, as well as service and clinical eﬃciency. This paper explores the
pressing need, to construct data analytic to inform such clinical decisions. The
outcomes are critically important from economic, patient safety and systems
perspectives.

Historically, classical statistical methods have been used to verify stated hy-
potheses. This requires a priori assumption, for example, on data distributions.
As the scale, distribution and diversity of data increase, this approach leads to
sub-optimal use of this information. This paper examines new ways to analyze
cohorts of patients with chronic diseases, such as Diabetes mellitus (diabetes)
and stroke. Chronic care is expensive to administer. One crucial problem in the
management of chronic patients is to deliver care plans, such that in major-
ity of cases patients can be manged in the community without hospitalization.

J. Pei et al. (Eds.): PAKDD 2013, Part II, LNAI 7819, pp. 123–134, 2013.
c(cid:2) Springer-Verlag Berlin Heidelberg 2013

124

B. Saha et al.

This requires us to ﬁnd sub-groups of patients with same disease characteristics,
without any prior assumptions on grouping.

Considering the complexity and nature of the datasets, we propose to model
the data by a union of subspaces [1] where each subspace corresponds to pa-
tients with similar diagnostic conditions. This model has been used in many
applications, such as lossy compression of images [2][3], motion segmentation
in video sequences [4,5,6,7,8] etc. Early subspace clustering methods include
mixture of Gaussian, factorization, algebraic, compressed sensing/low-rank [9]
methods, and examples range from K subspaces [10], mixture of probabilistic
PCA [11], multi-stage learning [12] etc. These algorithms typically require prior
knowledge about the subspaces - the number of subspaces or their dimensions
[13]. The computation is also exponential with the number and/or dimensions.
Recently, Elhamifar and Vidal [13] propose sparse subspace clustering (SSC), in
which the clustering is solved by seeking a sparse representation of data points.
By computing an aﬃnity graph on the sparse representations for all data points,
SSC automatically discovers the subspaces and their dimensions. However, the
previous results by SSC show that there are many instances in which the sparse
coeﬃcients corresponding to points outside a cluster of interest are signiﬁcantly
non-zero. This suggests that enforcing constraints that discourage points fur-
ther apart will prevent them from entering the same cluster [14]. This was also
exploited in [5], who propose a weighted version (WL-SSC).

Inspired by the related success of sparse subspace clustering in computer vi-
sion, this paper proposes a novel application of this powerful approach in the
context of health care data. Here, it requires a careful modeling and interpreta-
tion of subspaces in health care data as well as novel construction of weighting
matrices. The weighting matrix acts as the prior knowledge on the similarity
between patient records and is computed directly from the data. We explore
the decomposition into union of linear subspaces (WL-SSC) and extend the
model to consider decomposition of a union of aﬃne subspaces (WA-SSC). To
decide on the weighting constraints, we consider three diﬀerent ways of spec-
ifying proximity of points in a k-neighborhood - RBF, cosine and 0-1 matrix.
We apply the models across a cohort of 1580 diabetes patients with 551 disease
codes, and 1159 stroke patients with 805 codes. The data is collected over a
period of 5 years, and each time the patient comes to hospital, a diagnosis code
is assigned. Evaluation of such algorithms, with real-world data is notoriously
hard. We propose the use of the recently introduced ρ-measure- this method
allows ground-truth to be allocated based on degree of similarity between two
points. Using this measure, we can compute the Rand-Index and F -measure for
a given ρ. We show that our methods outperform the unweighted version and
many competitive clustering methods such as aﬃnity propagation (AP) [15],
locality-preserving projection (LPP)[16] and k-means [17]. We show that further
improvement can be achieved with a weighted union of aﬃne subspace model.
We also show tag clouds for clusters in the diabetes cohort and demonstrate how
the sub-groups discovered are qualitatively meaningful.

Clustering Patient Medical Records via Sparse Subspace Representation

125

The novelty in our paper is threefold: (a) it applies weighted sparse subspace
clustering to a unique medical dataset problem to improve service eﬃciency,
(b) it proposes a new aﬃne, weighted subspace clustering method, and (c) uses
a novel principled way to evaluate real world clustering results for which no
ground-truth can be obtained.

The signiﬁcance of the problem lies in the ability to save costs with eﬃcient
sub-group identiﬁcation, leading to targeted care plans. Both chronic diseases
chosen have reached epidemic status. For example, Diabetes mellitus (diabetes)
is spreading so rapidly that recent studies show that the total number of diabetic
people across the world was 171 million in 2001 and it is estimated to 230 million
by the end of 2030 [18,19].

2 Related Background

2.1 Sparse Subspace Clustering (SSC)
Consider a set of N data points collected in a D×N matrix X = [x1, x2, . . . , xN ]
where xi ∈ R
D and D is the number of features. SSC [4] clusters the datapoints
via the subspace principle. Intuitively, a linear representation of a datapoint with
respect to the whole set gives more preferences to those points that belong to
the same subspace. Denote as Si the subspace (cluster) that xi belongs to. Then,
the linear representation of a datapoint can be written as follows:

xi =

(cid:2)

j(cid:3)=i

cijxj =

(cid:2)

i∈Si,j(cid:3)=i

cij xj +

(cid:2)

j /∈Si

cij xj = Xci

(1)

.
= [ci1, ci2, . . . , ciN ]T are the coeﬃcients of the representation. In the
Here, ci
ideal case, the coeﬃcients in the second summation of the right term are zero,
giving rise to sparse coeﬃcient vector ci. However, the solution of (1) is generally
not unique when the number of features D is usually much less than the number
of observations N . Recent advances in sparse learning [20,21] show that it is
possible to regularize the solution and at the same time achieve sparse solution,
which is consistent to the ideal case, by enforcing the (cid:3)1-norm of the coeﬃcient
vector, (cid:3)ci(cid:3)1 =

(cid:3)|cik|, to be small. Using this principle, SSC [4] advocates to

ﬁnd the solution with two variations as follows.

Linear Sparse Subspace formulation (L-SSC). Under this formulation, we
assume that data points in X are sampled from a union of linear subspaces. Then
the sparse coeﬃcients are obtained by solving following optimization problem
without employing any others constraints on coeﬃcient vector ci.

||ci||1

arg min

ci

s.t. xi = Xci, cii = 0

(2)

Aﬃne Sparse Subspace formulation (A-SSC). L-SSC can be extended to
union of aﬃne subspaces by enforcing an additional equality constraint over the
sparse coeﬃcient vector ci as follows:

126

B. Saha et al.

||ci||1

arg min

ci

s.t. xi = Xci, cT

i 1 = 1 cii = 0

(3)

The coeﬃcients are then used to compute a balanced aﬃnity matrix for ﬁnal
spectral clustering: ¯C = (C + CT )/2. Then, the Laplacian matrix L = I −
−1/2 is computed, with I being the identity matrix and D being a
−1/2 ¯CD
D
j=1 ¯cij. The smallest eigenvalues of L is used to
diagonal matrix where Dii =
estimate number of subspaces and the corresponding data points are obtained
using the k-means algorithm.

(cid:3)N

3 Proposed Method

3.1 Weighted Sparse Subspace Clustering (W-SSC)

In the ideal case, the coeﬃcients cij are zero if data points xi and xj are sampled
from two diﬀerent subspaces. However, there are cases where they signiﬁcantly
deviate from zero due to numerical properties of the data matrix X [5]. To avoid
undesirable sparse solutions, it has been suggested to introduce a weighting
scheme in the sparse formulation [5]. Under this scheme, a weight matrix W ∈
N×N is used to enforce sparse coeﬃcients to better fall into the same subspace
R
they deem to belong to. Such a desired solution is encouraged by minimizing
the weighted (cid:3)1-norm (cid:3)wi (cid:4) ci(cid:3)1 instead of (cid:3)ci(cid:3)1. Here, (cid:4) denotes element-wise

product of two vectors. Inspired by this principle, we also propose to employ
the weighting scheme in our method. The remaining challenge is to construct a
suitable weighting matrix for the data, which we detail next.

3.2 Construction of Weighting Matrix W

An optimal weighting matrix can be constructed if we have ground-truth knowl-
edge of the clusters to suppress cross-cluster coeﬃcients (by setting wij large
or small for inter- or intra-cluster coeﬃcients respectively). However, as this
knowledge is not available, we propose to use the information within the data
to approximate the optimal weighting matrix. We rely on the principle that the
weights for inter-cluster coeﬃcients are large whilst those for intra-cluster coef-
ﬁcients are small. Denote as xi ∈ N (xj ) as xi is k-nearest neighbor of xj , and I
the indicator (0/1) function. We propose the following choices:

– Inverse RBF : wij = Ixi(cid:3)∈N (xj) × exp
– 0-1 : wij = Ixi(cid:3)∈N (xj)
– Cosine: wij = Ixi(cid:3)∈N (xj ) × (cid:5)xi,xj(cid:6)
||xi||2||xj||2

||xi−xj||2

2

2σ2

Clustering Patient Medical Records via Sparse Subspace Representation

127

3.3 Weighted Formulation

Extending the basic SSC algorithms, we propose to adapt to the idea in [5] and
solve the following basic weighted formulation with linear subspace assumption

||wi (cid:4) ci||1,

arg min

ci

s.t.xi = Xci, cii = 0

(4)

The above basic formulation assumes noiseless data generation. Considering
noise while modeling the data points sampled from the union of subspaces, we
assume that each data points xi is contaminated with noise ei. i.e. xi = xtrue
i +ei
is the true value of the i-th variable and ei is bounded: ||ei||2 ≤ .
where xtrue
Thus, it is more realistic to extend the basic model to account for noise by
considering the noise-aware version of the formulation

i

||wi (cid:4) ci||1

s.t. ||xi − Xci||2

2

≤ , cii = 0

arg min

ci

(5)

This can be more conveniently written in a Lagrangian form
−ici||2

λ||wi (cid:4) ci||1 +

||xi − X

ci

arg min

(6)
−i is X with the ith column removed, and
Here, λ is regularization parameter, X
we implicitly ignore the ith entry of ci. When considering the aﬃne subspace
modeling, the above Lagrangian formulation can be extended to account for the
additional aﬃne constraints as follows

1
2

2

arg min

ci

λ||wi (cid:4) ci||1 +

1
2

||xi − X

−ici||2

2, cT

i 1 =1,

(7)

Next, we discuss optimization algorithms to solve (7) (note that (6) can be
readily solved by a slight modiﬁcation of many eﬃcient compressed sensing
−i by the inverse of the corresponding
solver, such as reweighting the column of X
weights and working on the reweighted variables [5]). As they are convex prob-
lems, oﬀ-the-shelf solvers, such as CVX, can be used, but we do not seek to use
them because they are rather ineﬃcient. We show that it is possible to solve (7)
more eﬃciently with the alternative direction method of multipliers (ADMM)
[22]. For notational simplicity, we drop the subscript/superscript of ci, xi and
−i. Under the ADMM framework, we decouple the (cid:3)1 regularization term from
X
the quadratic terms by introducing a new variable z such that z − c = 0 and
consider the augmented Lagrangian

L(c, z, y, v) =

1
2

(cid:3)x − Xc(cid:3)2

2 + λ(cid:3)z(cid:3)1 + yT (c − z) +

(cid:3)c − z(cid:3)2

2

ρ1
2

+v(1T c − 1) +

(1T c − 1)2.

(8)

ρ2
2

Here, y and v are the dual parameters corresponding to the inequality constraints

c−z = 0 and 1T c−1 = 0 respectively; ρ1 and ρ2 are small parameters to improve

128

B. Saha et al.

numerical stability (see [22] for ADMM background). By using the normalized
dual variables u1 = (y/ρ1) and u2 = (v/ρ2) we derive the following ADMM
updates that solve (7)

−1(XT x + ρ1(zk − uk

1) + ρ21(1 − u2))

ck+1 = (XT X + ρ1I + ρ211T )
zk+1 = Sλ/ρ1 (ck+1 + u1)
1 + (ck+1 − zk+1)
uk+1
1 = uk
2 + (1T ck+1 − 1).
uk+1
2 = uk

(9)

(10)

(11)

(12)

Here Sτ (c) is the soft-thresholding shrinkage operator, deﬁned as a vector r such
that ri = sign(ci) max(|ci| − τi, 0) (see [22]).

Once the coeﬃcient vectors ci’s are found, the spectral clustering part pro-

ceeds in the same way as the original SSC algorithm [4].

4 Experiments

4.1 Datasets

We validate our approach on two real-world datasets collected from patients
having diabetes and heart (stroke) diseases collected over a period of ﬁve years
from 2007 to 2011 and has diagnosis records from 9878 patients. Each patient
has been diagonised several times over a period of ﬁve years and assigned unique
diagnosis code(s). An example of a record for a patient over time might be
(E1172, I10, E1172, Z9222). Table 1 and 2 shows the description of some
codes. Patients may be assigned similar code more than once over time.

We remove records without codes, patients diagonised less than twice and
also duplicated codes. This results in 1580 diabetes patients with 551 unique
codes. We construct a code-patient matrix, where codes are used as features and
each patient is an observation, analogous to term-document matrix for text data
analysis. In our second data set (stroke patients), there are 1159 patients with
805 diagnostic codes.

4.2 Evaluation Method

As no ground-truth is available for latent groups, it is impossible to measure the
clustering performance by standard evaluation metrics. Thus, we evaluate the
performance using a novel ρ-measure method as follows:
1. Each data point xi ∈ R
2. Compute relative similarity metric sρ(¯xi, ¯xj)

N is mapped to a binary vector ¯xi where ¯xij = Ixij(cid:3)=0.

(cid:3)
(¯xi (cid:4) ¯xj)
k=1 ¯xjk − (cid:3)
(cid:3)N
3. Construct a ground-truth matrix Gρ ∈ R
N×N with element gij = Isρ(¯xi,¯xj)≥ρ
4. Construct a cluster membership matrix V with element vij = IIDK (i)=IDK (j)

(¯xi (cid:4) ¯xj )

k=1 ¯xik +

sρ(¯xi, ¯xj) =

(cid:3)N

(13)

Clustering Patient Medical Records via Sparse Subspace Representation

129

Table 1. Examples of code description

Codes Description of Codes

E1172 Type 2 diabetes mellitus with features of insulin resistance

I10 Essential (primary) hypertension

Z9222 Personal history of long-term (current) use of other medicament, insulin
R63Z Chemotherapy

Table 2. Diabetes dataset

Patient Id

Diagnosis Codes

P1
P2
P3

E1172,I10,E1172,Z9222
M81403,Z511,R63Z,R63Z

E1023,E1023,E1012

Next, we compute the standardPrecision (P), Recall (R) and F-measure (F ):

P =

T P

T P + F P

, R =

T P

T P + F N

, F =

2 × P × R
P + R

(14)

Here, true positive (TP) is scored when two similar data points in the ground-
truth are grouped together in the obtained results, a true negative (TN) is scored
when two dissimilar data points are grouped separately, a false positive (FP) is
scored when two dissimilar data points are grouped together and a false negative
(FN) is scored when two similar data points are grouped separately. Similarly,
the rand index (RI) is deﬁned as

RI =

T P + T N

T P + F P + F N + T N

where high RI and F indicates the better accuracy.
Algorithm 1 show the overall method of computing F -measure. Note that, we
compute F measure over a matrix of N × N variables, instead of N number of
data points.

4.3 Results and Comparisons

Performance against Other Methods. We compare our proposed cluster-
ing method against competitive sparse subspace clustering and baseline alter-
natives, including aﬃnity propagation (AP) [15], locality preserving projection
(LPP) [16], and k-means [17]. In all experiments, we set ρ to 0.9, regularization
parameter λ to 0.001.

Table 3 presents the clustering results obtained from SSC methods for diabetes
and stroke data. Clearly, our proposed method outperforms both L-SSC and A-
SSC variants by obtaining larger RI and F scores. The F measure scores of
WL-SSC and WA-SSC have improved over L-SSC and A-SSC by large margins
of 47% and 45% for the diabetes data and 236% and 257% for the stroke data
respectively.

130

B. Saha et al.

Algorithm 1. Computing F measure
Input: Groundtruthed Matrix Gρ and Cluster Index matrix V.
Output: F − measure
Intialize: Set TP=TN=FP=FN=0.

– for i = 1 to N

• for k = 1to N

grouped together.

∗ if (gik = 1) and (vik = 1) T P = T P + 1; // Two similar data points
∗ else if (gik = 0) and (vik = 0) T N = T N + 1; // Two dissimilar data
∗ else if (gik = 0) and (vik = 1) F P = F P + 1;//Two dissimilar data points
∗ else (gik = 1) and (vik = 0) F N = F N + 1;//Two similar data points

points grouped separately.

grouped similar.

grouped separately.

• end

– end
– Calculate F -measure following the equation 14.

Likewise, the F measure is improved by 275% (AP), 85% (LPP), 388% (k-
means) for diabetics datasets, whereas the betterment in RI is 87% (AP), 14%
(LPP), 10% (k-means) respectively. For the strokes data, F measure is improved
by 173% (AP), 54% (LPP), 465% (k-means) and Rand Index is 71% (AP), 13%
(LPP), 139% (k-means) respectively.

Table 3. Performance comparison

Datasets
Methods F measure Rand Index F measure Rand Index

Diabetics Data

Strokes Data

AP
LPP

k-means
L-SSC
A-SSC

0.0423
0.0854
0.0325

0.0951
0.1092

WL-SSC 0.1401
WA-SSC 0.1587

0.4639
0.7654
0.4312

0.7817
0.7862

0.8652
0.8982

0.062
0.11

0.0294

0.0475
0.0619

0.1597
0.1697

0.522
0.8045
0.3845

0.5210
0.7324

0.90
0.91

Table 4. Performance analysis using diﬀerent weighting schemes

Datasets

Diabetes Data

Strokes Data

Weighting Schemes WL-SSC WA-SSC WL-SSC WA-SSC

RBF
0-1

cosine

0.1401 0.1587 0.1597 0.1697
0.1201
0.1199
0.1352
0.1382

0.1221
0.1444

0.1191
0.1390

Clustering Patient Medical Records via Sparse Subspace Representation

131

1500

1000

500

0

0

1500

1000

500

0

0

0.35

0.3

0.25

0.2

0.15

0.1

 

e
r
s
u
s
a
e
m

F

 

500

1000

L−SSC

1500

1000

500

1500

0

0

1500

1000

500

500

1000

1500

A−SSC

500

1000
WL−SSC

1500

0

0

500

1000
WA−SSC

1500

(a) Aﬃnity Matrices C

 

WL−SSC
L−SSC
A−SSC
WA−SSC

0.04

0.035

0.03

0.025

0.02

0.015

0.01

0.005

Zero Eigenvalues

0.2

0.4

0.6

ρ

0.8

1

0
1520

1530

1540

1550

Eigenvalues

1560

1570

1580

(b) F -measure

(c) Eigenvalues of L

Fig. 1. Plots for Qualitative Evaluations

132

B. Saha et al.

(a) Cluster 1: Type2 diabetes with Heart
disease

(b) Cluster 2: Post surgery: Diabetic
Neuropathy

(c) Cluster 3: Type2 diabetes with Hyper-
tensions

(d) Cluster 4: Cancer Treatment

(e) Cluster 5: Type 1 diabetes with Ke-
toacdosis

(f) Cluster 6: Diagnosis for vascular
complications

(g) Cluster 7 : Diabetes with Lym-
phoma

(h) cluster 8: Diabetic Nephropathy

(i) Cluster 9: Diabetes with Psychiatric Disorders

Fig. 2. Diagnostic Clouds

Inﬂuence of Weighting Schemes. Table 4 include the performance for dif-
ferent weighting schemes and it is found that the RBF choice provides better
performance than the other choices.

Discovered Clusters. The number of clusters K equals to the number of zero
eigenvalues of of Laplacian matrix L. Fig. 1(c) shows the eigenvalue plot of L
for the diabetes data where the number of zero eigenvalue equals to 9. Similarly,
we found 12 sub-groups for stroke data.

Since ρ is the relative similarity between the two data points, which means
high value of ρ denotes two observations are highly similar, we vary ρ varies
from 0.1 to 1 in a separate experiment on diabetes data and plots are shown in .
Figure 1(b). As expected, F -measure is high for small values of ρ and F -measure
is low when ρ is increasing.

Clustering Patient Medical Records via Sparse Subspace Representation

133

Figures 1 and 2 show the qualitative evaluation of clusters for the diabetes
data. Figure 1(a) shows the aﬃnity matrices, whilst Figure 2 shows the tag clouds
of the diagnosis codes in each cluster. As anticipated the clusters are qualitatively
diﬀerent in terms of disease diﬀerentiation within diabetes: diabetes with heart
disease, with cancer, with dialysis. Type 1 and 2 are clearly diﬀerentiated.

5 Conclusion

We have demonstrated a novel application of the sparse subspace clustering the-
ory in solving the clustering problem of health care data. Our novel contributions
includes special construction of the weighting matrices to obtain better sparse so-
lution and the eﬃcient algorithm to solve the formulation with aﬃne constraints.
To evaluate realistic health care data where no ground-truth is available, we have
also suggested a novel evaluation method of clustering results. Compared with
competitive alternatives in the literature, our proposed method achieve much
better F and RI scores, and discovers meaningful patients subgroups.

References

1. Eldar, Y., Mishali, M.: Robust recovery of signals from a structured union of sub-

spaces. IEEE Transactions on Information Theory 55(11), 5302–5316 (2009)

2. Hong, W., Wright, J., Huang, K., Ma, Y.: A multiscale hybrid linear model for

lossy image representation. In: Proc. ICCV, pp. 764–771 (2005)

3. Yang, A., Wright, J., Ma, Y., Sastry, S.: Unsupervised segmentation of natu-
ral images via lossy data compression. Computer Vision and Image Understand-
ing 110(2), 212–225 (2008)

4. Elhamifar, E., Vidal, R.: Sparse subspace clustering. In: Proc. CVPR, pp. 2790–

2797. IEEE (2009)

5. Pham, D.-S., Saha, B., Phung, D., Venkatesh, S.: Improved subspace clustering via

exploitation of spatial constraints. In: Proc. CVPR. IEEE (2012)

6. Wang, S., Yuan, X., Yao, T., Yan, S., Shen, J.: Eﬃcient subspace segmentation via

quadratic programming. In: Proc. AAAI (2011)

7. Yu, Y., Schuurmans, D.: Rank/norm regularization with closed-form solutions:

Application to subspace clustering. Arxiv preprint arXiv:1202.3772 (2012)

8. Vidal, R., Tron, R., Hartley, R.: Multiframe motion segmentation with missing

data using power factorization and GPCA. IJCV 79(1), 85–105 (2008)

9. Liu, G., Lin, Z., Yu, Y.: Robust subspace segmentation by low-rank representation.

In: Proc. ICML (2010)

10. Ho, J., Yang, M., Lim, J., Lee, K., Kriegman, D.: Clustering appearances of objects
under varying illumination conditions. In: Proc. CVPR, vol. 1, pp. I–11. IEEE
(2003)

11. Tipping, M., Bishop, C.: Mixtures of probabilistic principal component analyzers.

Neural Computation 11(2), 443–482 (1999)

12. Gruber, A., Weiss, Y.: Multibody factorization with uncertainty and missing data

using the EM algorithm. In: Proc. CVPR (2004)

13. Elhamifar, E., Vidal, R.: Sparse subspace clustering. In: Proc. CVPR, pp. 2790–

2797 (2009)

134

B. Saha et al.

14. Candes, E., Wakin, M., Boyd, S.: Enhancing sparsity by reweighted l1 minimiza-

tion. Journal of Fourier Analysis and Applications 14(5), 877–905 (2008)

15. Frey, B., Dueck, D.: Clustering by passing messages between data points. Sci-

ence 315(5814), 972–976 (2007)

16. He, X., Cai, D., Liu, H., Ma, W.: Locality preserving indexing for document rep-

resentation. In: Proc. ACM SIGIR, pp. 96–103 (2004)

17. Kanungo, T., Mount, D., Netanyahu, N., Piatko, C., Silverman, R., Wu, A.: An
eﬃcient k-means clustering algorithm: Analysis and implementation. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence 24(7), 881–892 (2002)

18. Fabris, P., Floreani, A., Tositti, G., Vergani, D., De Lalla, F., Betterle, C.: Type
1 diabetes mellitus in patients with chronic hepatitis c before and after interferon
therapy. Alimentary Pharmacology & Therapeutics 18(6), 549–558 (2003)

19. Young, J., McAdam-Marx, C.: Treatment of type 1 and type 2 diabetes melli-
tus with insulin detemir, a long-acting insulin analog. Clinical Medicine Insights.
Endocrinology and Diabetes 3, 65 (2010)

20. Cand`es, E., Romberg, J., Tao, T.: Robust uncertainty principles: Exact signal
reconstruction from highly incomplete frequency information. IEEE Transactions
on Information Theory 52(2), 489–509 (2006)

21. Donoho, D.: Compressed sensing. IEEE Transactions on Information Theory 52(4),

1289–1306 (2006)

22. Boyd, S., Parikh, N., Chu, E., Peleato, B., Eckstein, J.: Distributed Optimization
and Statistical Learning via the Alternating Direction Method of Multipliers. In:
Jordan, M. (ed.) Foundations and Trends in Machine Learning, vol. 3(1), pp. 1–122.
Now Publisher (2011)


