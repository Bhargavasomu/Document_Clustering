Feature Weighting by RELIEF Based on Local

Hyperplane Approximation

Hongmin Cai1,(cid:2) and Michael Ng2

1 South China University of Technology, Guangdong, P.R. China

2 Department of Mathematics, Hong Kong Baptist University, Hong Kong

caihongm@sysu.edu.cn

Abstract. In this paper, we propose a new feature weighting algorithm
through the classical RELIEF framework. The key idea is to estimate
the feature weights through local approximation rather than global mea-
surement, as used in previous methods. The weights obtained by our
method are more robust to degradation of noisy features, even when the
number of dimensions is huge. To demonstrate the performance of our
method, we conduct experiments on classiﬁcation by combining hyper-
plane KNN model (HKNN) and the proposed feature weight scheme.
Empirical study on both synthetic and real-world data sets demonstrate
the superior performance of the feature selection for supervised learning,
and the eﬀectiveness of our algorithm.

Keywords: Feature weighting, local hyperplane, RELIEF, Classiﬁca-
tion, KNN.

1

Introduction

Feature weighting plays an important step in the preprocessing of data, es-
pecially in data classiﬁcation. In general, the feature weights are obtained by
assigning a continuous relevance value to each feature via a learning algorithm
by stressing on the context or domain knowledge. The feature weighting pro-
cedure is particularly useful for instance based learning models, which usually
construct the distance metric by using all features. Moreover, feature weighting
can reduce the risk of over-ﬁtting by removing noisy features, thereby improve
the predictive accuracy. Existing feature selection methods broadly falls into two
categories, wrapper and ﬁlter methods. Wrapper methods use the predictive ac-
curacy of predetermined classiﬁcation algorithms (called base classiﬁer), such as
SVMs, as the criteria to determine the goodness of a subset of features [9,15].
Filter methods select features based on discriminant criteria that relies on the
characteristics of data, independent of any classiﬁcation algorithm [7,14,17]. The
common discriminant criteria includes entropy measurement [18], Chi-squared

(cid:2) This work was supported in part by NSFC under award number 60902076, NSF
of Guangdong Province under award number 9451027501002551, and China Funda-
mental Research Funds for the Central Universities under award number 11lgpy33.

P.-N. Tan et al. (Eds.): PAKDD 2012, Part II, LNAI 7302, pp. 335–346, 2012.
c(cid:2) Springer-Verlag Berlin Heidelberg 2012

336

H. Cai and M. Ng

measurement [21], Fisher ratio measurement [10], mutual information measure-
ment [20,4,3], and RELIEF-based measurement [19,27,28].

Due to the emerging needs in biomedical and bioinformatics areas, researchers
are particularly interested in algorithms which can process of data with feature
being of large (or huge) dimensions, such as, microarray scanning in cancer
research. Therefore, ﬁlter methods are widely used due to its eﬃciency in com-
putation. Among the existing ﬁlter methods in feature weighting, the RELIEF
algorithm [19] is considered as one of the most successful ones due to its sim-
plicity and eﬀectiveness. The main idea behind RELIEF is to iteratively update
feature weights by a distance margin to estimate the diﬀerence between neigh-
boring patterns. It has been further generalized to average multiple, instead
of just one, nearest neighbors when computing the sample margins, and was
named as RELIEF-F [19]. The authors have shown that RELIEF-F can achieve
signiﬁcant improvement on performance of the original RELIEF [19]. Sun sys-
tematically proved that RELIEF is indeed an online algorithm for a convex
optimization problem [27]. Through maximizing an averaged margin of nearest
patterns in feature scaled space, RELIEF could estimate the feature weight in a
straightforward and eﬃcient manner. Based on the theoretical framework, one
can impose outlier removal scheme called I-RELIEF since the margin averaging
is sensitive to large variations [27]. To accomplish sparse feature weighting, the
author introduced the l1 penalty into optimization of I-RELIEF [28].

In this paper, we present a new feature weighting algorithm to extend classical
RELIEF model. The main contribution of the proposed algorithm is that the
feature weights are estimated from local patterns other than global ones, as
used in exiting methods [19,27,28]. Therefore, the proposed feature weighting
scheme is particularly useful when combined with local pattern based classiﬁers,
such as HKNN [30], ADAMENN [8] and discriminant adaptive nearest neighbor
(DANN) [16]. Besides, local patterns are more robust to the noises and outliers.
It is promising to be used in applications where data are severely contaminated
by noises or rich of redundance.

This paper is organized as follows. Section 2 introduces the background of
the classical RELIEF method and its variations, including F-RELIEF and I-
RELIEF. The main result is reported in this section. Section 3 demonstrates the
performance of the proposed model. Extensive experiments have been conducted
to compare with the classical methods on benchmark data sets. Conclusion is
presented in Section 4.

2 The Proposed Method

2.1 RELIEF

The RELIEF algorithm has been successfully applied in feature weighing due to
its simplicity and eﬀectiveness [19,28]. The main idea of RELIEF is to iteratively
adjust feature weights according to their ability to discriminate among neigh-
boring patterns. Mathematically, suppose that x is a randomly selected sample
of a binary class data. One can estimates its two nearest neighbors, wherein one

Feature Weighting by RELIEF Based on Local Hyperplane Approximation

337

is from its same class (called the nearest hit or NH) and the other is from a
diﬀerent class (called the nearest miss or NM). Then the weight wi for the i-th
feature is updated by a heuristic estimation:

wi = wi + |x(i) − N M (i)| − |x(i) − N H (i)|

(1)

Since there is no exhaustive or iterative search evolved in RELIEF updating, this
scheme is very eﬃcient for the processing of data with huge dimensions, thus it
is particularly promising for large-scale problems such as analysis of microarray
data [24,28,7]. The authors have generalized the RELIEF model by averaging
k, instead of just one in Eq. (1), nearest neighbors when computing the sample
margins and was named as RELIEF-F model [19]. Experimental results have
shown that RELIEF-F achieves superior performance over the original RELIEF.
Its success is due to the robustness of margin estimation on multiple samples.
However, the optimal number of nearest neighbors needs to be estimated empir-
ically. Besides, RELIEF-F is also sensitive to noise degradation and the outliers.
An benchmark achievement has been reported in [27], in which the author ﬁrstly
proved that RELIEF is a convex optimization problem with a margin-based ob-
jective function,

n(cid:2)

n=1

max

w

ρn(w)

(cid:2)w(cid:2)2

(cid:3)N

n=1(

:=

(cid:3)I

i=1 ωi|x(i)

n − N M (i)(xn)| − (cid:3)I

i=1 ωi|x(i)

n − N H (i)(xn)|)

w ≥ 0

2 = 1,

s.t.
where ρn = d(xn−N M (xn))−d(xn−N H(xn)) is deﬁned as margin of a sample
i |xi|. N M (xn) and N H(xn) are the nearest
xn for distance function d(x) =
miss and hit for a sample xn, respectively.

(2)

(cid:2)

To tackle the drawbacks of RELIEF, such as outlier detection and inaccurate
updating, Sun reformulated the above problem as maximization of expected
margin through scaling of features [27,28]:

E[ρ(w)] = wT ( E
i∈NM
(cid:2)

= wT

i∈NM

= wT zn

|[xn − xi|)]

[|xn − xi|] − E
i∈NH
P (xi = N M (xn)|w)|xn − xi| − (cid:2)
i∈Hn

P (xi = N H(xn)|w)|xn − xi|

(3)

where N M = {i : 1 ≤ i ≤ N, yi (cid:3)= yn}, N H = {i : 1 ≤ i ≤ N, yi = yn, i = n} are
the sets of the nearest miss and the nearest hit, respectively. P (x = N M (xn)|W )
(or P (x = N H(xn)|W )) are the probabilities of the sample x being in the set
of N M (xn) (or N H(xn)) in the feature space scaled by weights w. Though the
probability distributions are unknown in prior, they can be estimated via kernel
density estimation [6]. Empirical study has shown that the I-RELIEF achieves
signiﬁcant improvements over the traditional models. Task of classiﬁcation on
feature scaled dataset achieves higher accuracy than standard techniques such
as SVM [12,15,9,26] and NN model [25]. Task of feature weighting is also robust

338

H. Cai and M. Ng

to noisy features. In applications with a huge dimension of features, economic
feature weights are appreciated not only because of computational consideration,
but also most features being irrelevant [14,17]. To obtain sparse and economic
feature weighting, the author introduced the l1 penalty into the optimization of
I-RELIEF [28].

However, since the expectation in Eq. (3) is carried out on the set of nearest
miss or hit, which consisted of the nearest neighbors of all observed samples, the
feature weight estimation may be less inaccurate if the samples contain many
outliers, or most of the features are being irrelevant. In both cases, the distance
between the tested one and its nearest neighbors are in large value. It follows
that large bias will be introduced in margin estimation via averaging operation.
Although one can reduce the inﬂuence of the abnormal samples by introducing
kernel distribution estimation [27,28], it will introduce additional free parameter
estimation. Moreover, probability estimation via kernel approximation is sen-
sitive to the sample size [6,13]. Therefore, it limits the empirical applications
such as in analysis of microarray data, in which the data is notoriously known
for that the dimension of sample observation is far less than that of the sample
feature [11]. In this paper, we propose to use a local hyperplane to approximate
the set of the nearest hit and miss and then estimate the feature weight through
maximization of an expected margin deﬁned by the hyperplane. The contribution
of this approximation is that the hyperplane is more robust for noisy features
degradation than averaging over all neighbors [19,27,28].

2.2 Approximation by Local Hyperplane

Given a sample x, it can be represented by a local hyperplane of class c by:

LHc(x) = {s | s = Hα},

(4)
where H is a I × n matrix composed by n NNs of the sample x: H =
{h1, h2,··· , hn}, with hi being the i-th nearest neighbor (called prototype)
of class c. The parameter of α = (α1, . . . , αn)T is the weights of the proto-
types {hi, i = 1, 2, . . . , n}. It can be viewed as spanning coeﬃcients of the
subspace LHc(x). Therefore, the hyperplane can be represented as: {· |Hα =
α1h1 + α2h2 + . . . + αnhn}. The value of α is solved by minimizing the distance
between the sample x and its local hyperplane of LHc(x) within feature scaled
space:

Jc(α) = arg min

1
2

I(cid:3)

i=1

ω(i)(xi − si)2 =

1
2

(x − Hα)T W (x − Hα)

Subject to :

k(cid:3)

αi = 1 , α ≥ 0

(5)

i=1

where s = (s1, s2, . . . , sI ) = Hα ∈ LHc(x). W is a diagonal matrix with

diagonal elements wi being the weight of the i-th feature.

Feature Weighting by RELIEF Based on Local Hyperplane Approximation

339

We are proposing to use the hyper plane to represent the set of nearest miss
N M (x) and nearest hit N H(x) for the given sample x. The beneﬁciary of the
representation is to characterize the local sample patterns robustly. Then the
distance between the sample to its N H (or N M ) set can be estimated from
its local hyperplane other than averaging across over all samples within the set.
Therefore, we redeﬁne the margin for a sample x as ρn = d(xn − LHN M (xn))−
d(xn − LHN H(xn)). The feature weights are now estimated through maximiza-
tion of total margins:

max

w

E[ρ(w)] =

N(cid:2)

I(cid:2)

max

w

(
n=1

i=1

1
N

ωi|x(i)

n − LH

(i)

NM (xn)| − I(cid:2)
NM (xn)| − I(cid:2)

i=1

|x(i)

n − αH (i)

N(cid:2)

I(cid:2)

(

n=1

i=1

max

w

= wT 1
N
= wT zn

ωi|x(i)

n − LH

(i)

NH (xn)|)

|x(i)

n − βH (i)

NH (xn)|)

i=1

(6)

N M and LH (n)

where HN M (xn) and HN H (xn) are the nearest neighbors for the set of the
nearest miss and hit of the sample xn. αn and βn are the coeﬃcients for spanning
of hyperplane LH (n)
N H. w is a vector with its i-th element w(i) being
the weight of the i-th feature, for i = 1, 2, . . . , I. To solve the minimization
problem of Eq. (6), one should estimate the parameters of αn, βn, which are
dependent on the nearest neighborhoods. The main problem of the estimation,
however, is that the nearest neighbors of a given sample are unknown before
learning. In the presence of many thousands of irrelevant features, the nearest
neighbors deﬁned in the original space can be completely diﬀerent from those in
the induced space. Therefore, the nearest neighbors deﬁned in the original feature
space may not be true in the weighted feature space. To solve the diﬃculties,
we have designed an iterative algorithm, similar to the EM algorithm and I-
RELIEF [27], to achieve the goal.
Step 1: In t-th iteration, for a given sample x, we estimate the parameter of
α by constructing the local hyperplane of the nearest hit set within induced
feature space. It is trivial to show that the minimization of Eq. (5) is equivalent
to solving the following quadratic programming:

1
2

α

αT ¯Hα + f T α
min
s.t. 1T α = 1, α ≥ 0

(7)
where ¯H = H T W (i)H, f = −xT W (i)H, and 1 is an unitary vector whose
elements are all being 1. The matrix of W (i) is the t-th feature weight matrix,
satisfying W (i)1 = w. The parameter of β for nearest miss hyperplane is obtained
similarly. Minimization of Eq. (5) is a constrained quadratic program problem
and standard techniques can be used to obtain its solution. In particular, since

340

H. Cai and M. Ng

the matrix of ¯H is symmetric and non-negative, the minimization could be solved
eﬃciently through standard techniques, such as active set [23].
Step 2: Estimation of the total margin with respect to w(i).

ρ(w(i)) =

1
N

N(cid:3)

I(cid:3)
(
n=1

i=1

ωi|x(i)

n − αH (i)

N M (xn)| − I(cid:3)

ωi|x(i)

n − βH (i)

N H(xn)|)

i=1

Step 3: Estimation of the weight W in (i + 1)-th iteration.

w = arg max

ρ(w(i))

w
N(cid:3)

I(cid:3)
(
n=1

i=1

=

1
N

ωi|x(i)

n − αH (i)

N M (xn)| − I(cid:3)

ωi|x(i)

n − βH (i)

N H(xn)|)

i=1

(8)

(9)

The above steps iterate alternatively until their convergence. The last two steps
are similar to the one used in I-RELIEF [27], and we name our scheme as LH-
RELIEF since it requires a local hyperplane approximation.

The pseudo-code for the LH-RLIEF is summarized in Alg. (2.1)

Algorithm 2.1: LH-RELIEF algorithm(V, W, λ)

comment: Variables Initialization: w= 1
for t ← 1 to T
while (cid:2)w(t+1) − w(t)(cid:2) > 

I , stopping criteriaand number of iterations T

⎧
⎨
⎩

do

1. Estimate the coeﬃcients for hyperplane of nearest miss and hit α, β
2. Calculate the margin by Eq .(8)
3. Update the weights by Eq. (9)

return (w)

3 Experimental Results

We shall demonstrate the performance of the proposed scheme through classiﬁca-
tion evaluation on both synthetic and empirical problems. In particular, we are
interested in its: 1) performance of classiﬁcation compared with other feature
weighting scheme; 2) robustness when processing the samples with irrelevant
features of large dimension.

3.1 Selection of Classiﬁer

In our experiments, we selected the hierarchical k-nearest neighbor (HKNN)
algorithm to conduct the comparison on feature weighting [30]. HKNN could be
viewed as a localized approximation of K-nearest neighbor model. In this model,
each class is modeled as a smooth and low-dimensional manifold embedded in the
high-dimensional data space by assuming that the manifolds are locally linear.

Feature Weighting by RELIEF Based on Local Hyperplane Approximation

341

There are two steps involved in classiﬁcation by HKNN. In the ﬁrst step, for
each tested sample, it constructs local hyperplanes for each class. The label of the
tested sample is assigned to the class whose local hyperplane to the tested sample
is minimized. Empirical study has shown that the HKNN produced a comparable
or even better performance of classiﬁcation than standard techniques, including
KNN and SVM [30,8,29]. One may note that the HKNN model shares the similar
idea with our approach in that the sample information is inferred from local
structure, which is the main reason for us to choose this particular classiﬁer.

Since the HKNN model does not consider the inﬂuence of feature weights,
the test data will be ﬁrstly scaled into feature space before the classiﬁcation is
carried out. The hyper-parameters used in training phase are estimated through
ten-fold cross validation.

3.2 Fermat’s Spiral Problem

In the ﬁrst example, we shall test the performance of the proposed method on the
well-known Fermat’s Spiral problem. The test dataset consists of two classes with
200 samples for each class. The labels of the Spiral are completely determined by
its ﬁrst two features. The shape of the Fermat’s Spiral distribution is shown in
Fig. 1(a). Heuristically, the label of a sample will be inferred easily from its local
neighbors. Classiﬁcation based on local information will give more accurate as-
signment than global measurement based prediction (or classiﬁcation) does since
the later one is sensitive to noise degradation. To tackle this drawback, Sun pro-
posed to lower the inﬂuence of the samples nearby through modeling of their prob-
ability distribution via kernel techniques [27]. This strategy is straightforward and
successful. However, if the dominant (informative) features are buried by the ir-
relevant (less informative) ones, estimation of the probability via distance will be
less accurate since the irrelevant feature may introduce a large variation to dis-
tance, for instance, the irrelevant features are being in a huge dimension. In
order to show this, irrelevant features following standard norm distribution are
added to the Spiral for classiﬁcation testing. The dimensions of irrelevant fea-
tures are ranging from {0, 300, 600, 900, 1200, 1500, 1800, 2100, 2400, 2700, 3000}.
Two feature weighting scheme, I-RELIEF and LH-RELIEF were ﬁrstly applied
to quantify the importance of feature. Then the classiﬁcation was performed on
dataset scaled by the feature weights. For each experiment, ten folds cross val-
idation scheme is used to compute the accuracy of classiﬁcation. To eliminate
the statistical variations, we have conducted ten times experiments indepen-
dently on each dataset and averaged classiﬁcation error is recorded and, shown
in Fig. 1(b). We observe that, the performance of the two methods are very
similar when the dimension of the irrelevant features is small. However, if the di-
mension of irrelevant features tends to be large, the performance of I-RELIEF is
severely degraded by the noises. In comparison, the performance of LH-RELIEF
is very stable and produces superior outcomes.

342

H. Cai and M. Ng

8

6

4

2

0

−2

−4
−6

)

%

(
 
r
o
r
r

 

E
n
o

i
t

a
c
i
f
i
s
s
a
C

l

13

12

11

10

9

8

7

6

 
0

LH−RELIEF
I−RELIEF

 

500

1000

1500

2000

2500

3000

−4

−2

0

(a)

2

4

6

8

Number of Irrelevant Feature

(b)

Fig. 1. Experiment on Fermat’s Spiral. (a) Distribution of binary Fermat’s Spiral prob-
lem. Each class has 200 samples and is labeled by diﬀerent colors; (b) Irrelevant fea-
tures with veriﬁed dimensions are added to test the robustness of the feature weighting
schemes. The LH-RELIEF outperforms I-RELIEF with respect to classiﬁcation error.
With the increase of dimension of the irrelevant features, the performance of I-RELIEF
is degraded while LH-RELIEF keeps stable.

3.3 UCI Data Sets

In the second experiment, we tested the proposed technique on ten medium
sized datasets. The tested benchmark data sets were downloaded from the UCI
Machine Learning Repository [1], and they have been widely tested by various
classiﬁcation benchmark models. The characteristics of the datasets are sum-
marized in Table 1. We compare our algorithm with four other algorithms, in-
cluding Iterative Search Margin Based Algorithm (Simba) [2], sparse Bayesian
multinomial logistic regression (SBMLR) [5] and I-RELIEF [27]. Simba is a local
learning based algorithm similar to RELIEF. SBMLR is a special kind of sparse
multinomial logistic regression models with Bayesian regularization. Multinomial
logistic regression algorithm has been successfully used in text processing [31]
and microarray classiﬁcation [22]. The beneﬁciary of adding regularization pa-
rameter into sparse multinomial logistic regression via a Laplace prior is that an
analytical solution could be obtained. Besides, its performance is similar to us-
ing cross-validation based model selection, thus greatly reducing computational
expense.

For each dataset, the optimal parameters were estimated by ten-fold cross
validation. The obtained feature weights under optimal parameters were used
to scale the raw datasets. Twenty times experiments on each dataset were per-
formed independently and classiﬁcation errors were averaged to evaluate the
performance of the feature weighting scheme. We will use the classiﬁcation error
to quantify the discrimination power of weighting scheme. Furthermore, statisti-
cal testing is also useful to fully comprise the performance of feature weights [27].
We selected the Students paired two-tailed t-test to achieve the goal. The p-value
of the t-test represents the probability that two sets of compared results come
from distributions with an equal mean. In this experiment, a p-value of 0.05 is
considered statistically signiﬁcant.

Feature Weighting by RELIEF Based on Local Hyperplane Approximation

343

The results are summarized in Table 2. We observe that that LH-RELIEF
and I-RELIEF are statistically diﬀerent from the tested ten datasets. The per-
formance of classiﬁcation after LH-RELIEF is better than after I-RELIEF in
9 of 10 experiments. Among the four feature weighting schemes, LH-RELIEF
outperforms others in 5 of 10 datasets, while almost is suboptimal in other ﬁve
dataset.

Table 1. Summary of tested datasets and their characteristics

Data set

#Instances #Classes

#Feature

Bupa
Teach
Sonar
Cancer
Prokaryotic
Eukaryotic
Haberman
Page block
Pima
Spambase

345
151
208
198
997
2427
306
5473
768
4601

46

44

42

40

38

36

34

32

30

)

%

(
 
y
c
a
r
u
c
c
A

28
 
0

100

200

300

400

500

600

700

Number of Irrelevant Feature

2
3
2
6
3
4
2
5
2
2

34

33

32

31

30

29

28

27

26

25

)

%

(
 
y
c
a
r
u
c
c
A

 

LH−RELIEF
I−RELIEF
Simba
SBMLR

800

900

1000

24
 
0

100

200

6
5
60
32
20
20
3
10
8
57

 

LH−RELIEF
I−RELIEF
Simba
SBMLR

800

900

1000

300

400

500

600

700

Number of Irrelevant Feature

(a)

(b)

Fig. 2. Experiment on benchmark dataset of Bupa and Pima by adding irrelevant
features in veriﬁed dimension, extending from 0 to 1000. (a) Bupa; (b) Pima.

In the last experiment, we are willing to test the performance of the algorithm
on data in huge dimensions. More speciﬁcally, we are interested in the robustness
of the algorithm on feature weighting with respect to the dimension of the irrel-
evant features. We selected two test datasets: Bupa and Pima. For each dataset,
irrelevant features are added to the raw dataset. The added irrelevant features
are independently sampled from zero-mean and unit-variance Gaussian distribu-
tion. Their dimensions are ranged from 0 to 1000. Including useless features is

344

H. Cai and M. Ng

Table 2. Classiﬁcation accuracies (%) on 10 real data sets. The LH-RELIEF shows
to be statistically diﬀerent from the I-RELIEF in 9 among 10 datasets. The P -value
for each dataset is shown in parenthesis. Overall, the better results are subscripted by
star under diﬀerent feature weighting scheme. The LH-Relief outperforms the standard
ones in most cases when the two methods show a statistically diﬀerence.

Dataset

Bupa
Teach
Sonar
Cancer
Prokaryotic
Eukaryotic
Haberman
Page Block
Pima
Spambase

∗
∗
∗

∗

LH-RELIEF I-RELIEF
(P -value)
66.7 (0.00)
46.3 (0.00)
84.3 (0.00)
76.0 (0.48)
89.8 (0.00)
81.2 (0.00)
72.3 (0.00)
94.1 (0.00)
70.3 (0.00)
78.0 (0.00)

69.7
64.4
86.7
76.2
90.5
82.8
69.3
94.5
74.0
84.8

∗

SBMLR

Simba

56.2
34.4
82.7
76.9
90.4
83.5
69.9
95.7
68.9
79.3

∗

∗

∗

66.8
62.3
85.7
76.4
89.3
81.3
68.7
89.8
74.5
39.4

∗

less appreciated in applications where the acquisition of data is quite expensive.
For example, it may complicate the pathway research if irrelevant genes are in-
cluded in microarray data analysis [27]. We would welcome such complication in
order to show the robustness of the algorithm.

The hyper-parameters, such as the kernel size σ in I-RELIEF and the number
of nearest neighbors k in LH-RELIEF are estimated through ten-fold cross vali-
dation. To eliminate statistical variations, each algorithm is run for twenty times
on each noisy dataset. In each run, a dataset is randomly partitioned into train-
ing and testing. The averaged testing errors serve as the criterion to quantify the
performance of the algorithm, and the results are drawn in Fig.2. For Bupa, the
classiﬁcation error of the classiﬁer after LH-RELIEF is smaller than that after
I-RELIEF in all dimensions, Fig. 2(a). This observation is coincided with the
results in Table. 2, implying that the feature weights estimated by LH-RELIEF
are more accurate and robust to the noises. For Pima, the performance of the two
scheme is almost comparable when the dimension of the the irrelevant features
is small, Fig. 2(b). However, the testing error after LH-RELIEF dramatically de-
creased with respect to the dimension of the irreverent features. In comparison,
the classiﬁcation error after I-RELIEF tends to be greater. The experiment fur-
ther demonstrates that the proposed feature weighting scheme is more immune
to the noisy features by showing surprising high degree of robustness.

4 Discussion

In this paper, we proposed a new feature weight scheme to tackle the com-
mon drawbacks of the RELIEF family. The nearest miss and hit subset are

Feature Weighting by RELIEF Based on Local Hyperplane Approximation

345

approximated by constructing a local hyperplane. Then the updating of feature
weights is achieved by measuring the margin between the sample and its hy-
perplane under general RELIEF framework. The main contribution of the new
variation is that the margin is more robust to the noises and the outliers than ear-
lier works do. Therefore, the feature weights can characterize the local structure
more accurately. Experimental results on both synthetic and real-world datasets
validate our ﬁndings. The proposed weighting scheme performs superior on most
test data with respect to classiﬁcation error. We also observed that the algorithm
was convergent in most cases, though theoretical justiﬁcation is needed.

References

1. Asuncion, A., Newman, D.: UCI machine learning repository (2007),

http://www.ics.uci.edu/~mlearn/MLRepository.html

2. Bachrach, G.R., Navot, A., Tishby, N.: Margin Based Feature Selection - The-
ory and Algorithms. In: Proc. 21st International Conference on Machine Learning
(ICML), pp. 43–50 (2004)

3. Brown, G.: An Information Theoretic Perspective on Multiple Classiﬁer Systems.
In: Benediktsson, J.A., Kittler, J., Roli, F. (eds.) MCS 2009. LNCS, vol. 5519, pp.
344–353. Springer, Heidelberg (2009)

4. Brown, G.: Some Thoughts at the Interface of Ensemble Methods and Feature
Selection. In: El Gayar, N., Kittler, J., Roli, F. (eds.) MCS 2010. LNCS, vol. 5997,
pp. 314–314. Springer, Heidelberg (2010)

5. Cawley, G.C., Talbot, N.L.C., Girolami, M.: Sparse Multinomial Logistic Regres-
sion via Bayesian L1 Regularisation. Advances in Neural Information Processing
Systems 19 (2007)

6. Christopher, A., Andrew, M., Stefan, S.: Locally weighted learning. Artiﬁcial In-

telligence Review 11, 11–73 (1997)

7. Ding, C., Peng, H.: Minimum redundancy feature selection from microarray gene
expression data. Journal of Bioinformatics and Computational Biology 3(2), 185–
205 (2005)

8. Domeniconi, C., Peng, J., Gunopulos, D.: Locally adaptive metric nearest-
neighbor classiﬁcation. IEEE Transactions on Pattern Analysis and Machine Intel-
ligence 24(9), 1281–1285 (2002)

9. Duan, K.B.B., Rajapakse, J.C., Wang, H., Azuaje, F.: Multiple SVM-RFE for
gene selection in cancer classiﬁcation with expression data. IEEE Transactions on
Nanobioscience 4(3), 228–234 (2005)

10. Duda, R., Hart, P., Stork, D.: Pattern Classiﬁcation. Wiley (2001)
11. Fraley, C., Raftery, A.E.: Model-based clustering, discriminant analysis, and den-
sity estimation. Journal of the American Statistical Association 97(458), 611–631
(2002)

12. Furey, T.S., Cristianini, N., Duﬀy, N., Bednarski, D.W., Schummer, M., Haussler,
D.: Support vector machine classiﬁcation and validation of cancer tissue samples
using microarray expression data. BMC bioinformatics 16, 906–914 (2000)

13. Girolami, M., He, C.: Probability density estimation from optimally condensed
data samples. IEEE Transactions on Pattern Analysis and Machine Intelligence 25,
1253–1264 (2003)

14. Guyon, I.: An introduction to variable and feature selection. Journal of Machine

Learning Research 3, 1157–1182 (2003)

346

H. Cai and M. Ng

15. Guyon, I., Weston, J., Barnhill, S., Vapnik, V.: Gene selection for cancer classiﬁ-

cation using support vector machines. Machine Learning 46, 389–422 (2002)

16. Hastie, T., Tibshirani, R.: Discriminant adaptive nearest neighbor classiﬁcation.
IEEE Transactions on Pattern Analysis and Machine Intelligence 18, 607–616
(1996)

17. Huang, C.J., Yang, D.X., Chuang, Y.T.: Application of wrapper approach and
composite classiﬁer to the stock trend prediction. Expert Systems with Applica-
tions 34(4), 2870–2878 (2008)

18. Koller, D., Sahami, M.: Toward optimal feature selection. In: Saitta, L. (ed.) Pro-
ceedings of the Thirteenth International Conference on Machine Learning (ICML),
pp. 284–292. Morgan Kaufmann Publishers (1996)

19. Kononenko, I.: Estimating Attributes: Analysis and Extensions of RELIEF. In:
Bergadano, F., De Raedt, L. (eds.) ECML 1994. LNCS, vol. 784, pp. 171–182.
Springer, Heidelberg (1994)

20. Kwak, N., Choi, C.H.: Input feature selection by mutual information based on

parzen window. IEEE Trans. Pattern Anal. Mach. Intell. 24, 1667–1671 (2002)

21. Liu, H., Setiono, R.: Feature selection via discretization. IEEE Transactions on

Knowledge and Data Engineering 9, 642–645 (1997)

22. Narlikar, L., Hartemink, A.J.: Sequence features of dna binding sites reveal struc-
tural class of associated transcription factor. Bioinformatics 22(2), 157–163 (2006)

23. Nocedal, J., Wright, S.J.: Numerical Optimization. Springer (August 2000)
24. Peng, Y.H.: A novel ensemble machine learning for robust microarray data classi-

ﬁcation. Computers in Biology and Medicine 36, 553–573 (2006)

25. Shakhnarovich, G., Darrell, T., Indyk, P. (eds.): Nearest-Neighbor Methods in

Learning and Vision: Theory and Practice. MIT Press (2006)

26. Statnikov, A., Wang, L., Aliferis, C.F.: A comprehensive comparison of random
forests and support vector machines for microarray-based cancer classiﬁcation.
BMC Bioinformatics 9, 319–328 (2008)

27. Sun, Y.: Iterative relief for feature weighting: Algorithms, theories, and applica-

tions. IEEE Trans. Pattern Anal. Mach. Intell. 29(6), 1035–1051 (2007)

28. Sun, Y., Todorovic, S., Goodison, S.: Local-learning-based feature selection for
high-dimensional data analysis. IEEE Trans. Pattern Anal. Mach. Intell. 32(9),
1610–1626 (2010)

29. Tao, Y., Vojislav, K.: Adaptive local hyperplane classiﬁcation. Neurocomput-

ing 71(13-15), 3001–3004 (2008)

30. Vincent, P., Bengio, Y.: K-local hyperplane and convex distance nearest neighbor
algorithms. In: Advances in Neural Information Processing Systems, pp. 985–992.
The MIT Press (2001)

31. Zhang, T., Oles, F.J.: Text categorization based on regularized linear classiﬁcation

methods. Information Retrieval 4(1), 5–31 (2001)


