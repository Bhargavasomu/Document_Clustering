Neighborhood-Based Smoothing of External

Cluster Validity Measures

Ken-ichi Fukui and Masayuki Numao

The Institute of Scientiï¬c and Industrial Research (ISIR),
Osaka University, 8-1 Mihogaoka, Ibaraki, Osaka, Japan

fukui@ai.sanken.osaka-u.ac.jp

Abstract. This paper proposes a methodology for introducing a neigh-
borhood relation of clusters to the conventional cluster validity measures
using external criteria, that is, class information. The extended measure
evaluates the cluster validity together with connectivity of class distribu-
tion based on a neighborhood relation of clusters. A weighting function is
introduced for smoothing the basic statistics to set-based measures and
to pairwise-based measures. Our method can extend any cluster validity
measure based on a set or pairwise of data points. In the experiment, we
examined the neighbor component of the extended measure and revealed
an appropriate neighborhood radius and some properties using synthetic
and real-world data.

Keywords: cluster validity, neighborhood relation, weighting function.

1

Introduction

Clustering is a basic data mining task that discovers similar groups from given
multi-variate data. Validation of a clustering result is a fundamental but diï¬ƒcult
issue, since clustering is an unsupervised learning and is essentially to ï¬nd latent
clusters in the observed data[3,7,14]. Up until now, various validity measures
have been proposed from diï¬€erent aspects, and they are mainly separated into
two types whether based on internal or external criteria[7,10,8]:

â€“ Internal criteria evaluate compactness and separability[3] of the clusters
based only on distance between objects in the data space, that is learning
perspective. As such measures, older methods of Dunn-index[4], DB-index[2],
and recent CDbw[5] are well known. Surveys and comparisons of internal
cluster validity measures are [3,9].

â€“ External criteria evaluate how accurately the correct/desired clusters are
formed in the clusters, that is userâ€™s perspective. External criteria normally
uses class/category label together with cluster assignment. Purity, entropy,
F-measure, and mutual information are typical measures[10,12,14].

This paper focuses on using external criteria, that is provided by human inter-
pretation of data. It is more beneï¬cial to use external criteria when class labels
are available.

P.-N. Tan et al. (Eds.): PAKDD 2012, Part I, LNAI 7301, pp. 354â€“365, 2012.
c(cid:2) Springer-Verlag Berlin Heidelberg 2012

Neighborhood-Based Smoothing of External Cluster Validity Measures

355

In order to understand obtained clusters better, this work introduces a neigh-
borhood relation among clusters. A neighborhood relation is useful especially in
case of micro-clusters or, i.e., cluster number is larger than class number. Global
structure of clusters, which means not only individual (local) clusters, can be
evaluated with neighborhood relation of classes within each cluster.

The basic policies of introducing the neighborhood relation is as follows:

1. A data object which belongs to the same class should be in neighbor over
clusters. To evaluate this property, we introduce a weighting function based
on inter-cluster distance. The inter-cluster distance can be computed based
on either topology-based or Euclidean distance in the data space.

2. A weighting function is introduced into basic statistics that are commonly
used in the conventional measures. Therefore, our approach is generic, any
conventional cluster validity measure that uses these statistics can also be
extended in the same way.

Above mentioned conventional indices do not consider neighboring clusters,
while very few works introduce inter-cluster connectivity for prototype based
clustering[11]. The inter-cluster connectivity is introduced by the ï¬rst and the
second best matching units, but this work is based on internal criterion. The
contribution of this work is to introduce neighborhood relation over clusters into
conventional external cluster validity indices.

The reason why we assume the situation to evaluate an unsupervised learning
by class labels is as follows. The fundamental diï¬ƒculty of unsupervised learning
is that the features and the distance metric are derived from observation and
assumption, there is no information from human interpretation of data. On the
other hand, it is often the case that a small number of samples, or data from
the same domain, or simulated samples are available with class labels. In such
cases, an external validity measure works as a preliminary evaluation instead of
evaluating unlabeled target data.

This paper presents how to introduce the weighting function to smooth the
conventional clustering validity measures. In the experiment, we revealed the op-
timal smoothing radius and also examined several parameters, prototype (micro-
cluster) number, and class overlapping degree. We revealed the properties of our
extended measure and showed potential to validate a clustering result consider-
ing neighborhood relation of clusters.

R

2 Preliminaries
Deï¬nition 1. (Clustering) Given a set of v-dimensional objects S = {xi}N
v, a clustering produces a cluster set C = {Ci}K
c(i) âˆˆ C for each object xi.
Deï¬nition 2. (Class) Let a class set be T = {Ti}L
class assignment for xi. Classes are provided independent from a clustering.

âˆˆ
i=1 with a cluster assignment

i=1

i=1, and t(i) âˆˆ T denotes a

356

K. Fukui and M. Numao

Deï¬nition 3. (Inter-cluster distance) d(Ci, Cj) âˆˆ R is deï¬ned as inter-cluster
distance between clusters that can be computed either Euclidean-based or topology-
based distance.

Ex) Inter-cluster distance. Euclidean-based distances can be given by single
linkage, complete linkage, and other methods commonly used in an aggrega-
tive hierarchical clustering. While, topology-based distance by the number
of hops in a neighbor graph. The neighbor graph can be obtained by such
as a threshold on Euclidean-based distance or by k-nearest neighbor.

Note that though a neighbor graph is normally obtained independent from a
clustering process, some method produces cluster (vector quantization) with
topology preservation such as Self-Organizing Map(SOM)[6], which is also used
in this experiment.

The objective of this work is to evaluate density of class T within intra-cluster
C together with the neighbor relation based on inter-cluster distance d(Ci, Cj).

3 Neighborhood-Based Smoothing of Validity Measures

There are two types of cluster validity measures, namely set-based and pairwise-
based measures1. These two types of measures can be extended in diï¬€erent
manners.

3.1 Extension of Set-Based Cluster Validity Measures

First, the way to extend set-based cluster validity measures[10,12] such as cluster
purity and entropy are described in this section. The properties of each measure
were studied in the literature[1].

By considering neighborhood relation of clusters, the neighbor class distribu-
tion should be taken into account to the degree of certain class contained in a
cluster, that is, the data points of the same class in the neighbor clusters should
have a high weight, while those of distant clusters should have a low weight
based on the inter-cluster distance as the diagram is shown in Fig. 1.
Let f (u; l) be a density distribution of class label l âˆˆ T at u âˆˆ Î©, where
Î© denotes a data space, and h(u, v) : Î© Ã— Î© (cid:3)â†’ R be a weighting function
based on the neighborhood relation. Based on the above concept, a class density
distribution f (u; l), a data density distribution f (u), and a total volume of data
N are smoothed by the weighting function h(u, v) as follows:

(cid:2)
(cid:2)

Î©

Ë†f (u; l) =
(cid:3)
lâˆˆT

Ë†f (u; l) =

(cid:3)
lâˆˆT

Ë†f (u) =

h(u, v)f (v; l)dv,

h(u, v)f (v; l)dv,

Î©

(1)

(2)

1 This work introduces the smoothing function into several cluster validity measures,
in actual use, a measure should be selected according to the target application and
the aspects the user wants to evaluate.

Neighborhood-Based Smoothing of External Cluster Validity Measures

357

d i, j

C

j

C

i

(cid:70)(cid:79)(cid:88)(cid:86)(cid:87)(cid:72)(cid:85)(cid:3)(cid:70)(cid:72)(cid:81)(cid:87)(cid:85)(cid:82)(cid:76)(cid:71)

(cid:20)(cid:3)(cid:3)(cid:21)(cid:3)(cid:3)(cid:22)
(cid:70)(cid:79)(cid:68)(cid:86)(cid:86)

(cid:20)(cid:3)(cid:3)(cid:21)(cid:3)(cid:3)(cid:22)

(cid:20)(cid:3)(cid:3)(cid:21)(cid:3)(cid:3)(cid:22)

(cid:87)(cid:75)(cid:72)(cid:3)(cid:81)(cid:88)(cid:80)(cid:69)(cid:72)(cid:85)(cid:3)(cid:82)(cid:73)

(cid:82)(cid:69)(cid:77)(cid:72)(cid:70)(cid:87)(cid:86)(cid:3)(cid:76)(cid:81)(cid:3)(cid:72)(cid:68)(cid:70)(cid:75)

(cid:70)(cid:79)(cid:68)(cid:86)(cid:86)

(cid:90)(cid:72)(cid:76)(cid:74)(cid:75)(cid:87)(cid:72)(cid:71)(cid:3)(cid:86)(cid:88)(cid:80)(cid:80)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:82)(cid:73)(cid:3)(cid:81)(cid:72)(cid:76)(cid:74)(cid:75)(cid:69)(cid:82)(cid:85)(cid:76)(cid:81)(cid:74)(cid:3)(cid:71)(cid:68)(cid:87)(cid:68)(cid:3)(cid:69)(cid:92)

h i, j

Fig. 1. Extension of a set-based clustering measure. The basic statistics are weighted
by the neighborhood relation based on inter-cluster distance di,j. This example shows
topology-based distance.

(cid:2)

(cid:2)

(cid:2)

(cid:3)
lâˆˆT

Ë†N =

Ë†f (u)du =

Î©

Î©

h(u, v)f (v; l)dvdu.

Î©

(3)

Discretizing eqs. (1) to (3), let Nl,i be the number of objects with class l in the ith
cluster Ci âˆˆ C; Nl,i = #{xk|t(k) = l, c(k) = Ci}, where # denotes the number of
elements. Ni denotes the number of objects in cluster Ci; Ni = #{xk|c(k) = Ci}.
Also N denotes the total number of objects; N = #{xk|xk âˆˆ S}. Eqs. (1) to (3)
can be rewritten as follows:

N(cid:4)

l,i =

N(cid:4)

i =

N(cid:4)

=

(cid:3)
(cid:3)
CjâˆˆC
N(cid:4)
t,i =
(cid:3)
lâˆˆT
CiâˆˆC

hi,jNl,j,
(cid:3)
(cid:3)
lâˆˆT
CiâˆˆC

N(cid:4)
i =

(cid:3)
(cid:3)
CjâˆˆC
lâˆˆT

hi,jNl,j,
(cid:3)
CjâˆˆC

hi,jNl,j.

(4)

(5)

(6)

Here, hi,j can be used any monotonically decreasing function, for example, the
often encountered Gaussian function: hi,j = exp(âˆ’di,j/Ïƒ2), where di,j denotes
inter-cluster distance and Ïƒ(> 0) is a smoothing (neighborhood) radius.

Thus, weighted cluster purity and entropy, for example, are deï¬ned using the

weighted statistics of eqs. (4), (5), and (6) as follows:

weighted Cluster Purity (wCP)

wCP(C) =

(cid:3)
CiâˆˆC

1
N(cid:4)

N(cid:4)
l,i.

max
lâˆˆT

(7)

The original purity is an average of the ratio that a majority class occupies
in each cluster, whereas in the weighted purity a majority class is determined
by the neighbor class distribution {N(cid:4)

l,i}.

358

K. Fukui and M. Numao

d

c(i), c(j)

c(i)

(cid:91)
i

(cid:83)(cid:68)(cid:76)(cid:85)(cid:3)(cid:82)(cid:73)(cid:3)(cid:82)(cid:69)(cid:77)(cid:72)(cid:70)(cid:87)(cid:86)

c(j) (cid:70)(cid:79)(cid:88)(cid:86)(cid:87)(cid:72)(cid:85)
(cid:70)(cid:72)(cid:81)(cid:87)(cid:85)(cid:82)(cid:76)(cid:71)
j

(cid:91)

(a) Distance of data pair on a graph

likelihood( c(i) = c(j) )

hc(i),c(j)

d
(b) Likelihood function

Fig. 2. Extension of a pairwise-based clustering measure. A likelihood function is in-
troduced to represent a degree that a data pair belongs to the same cluster.

weighted Entropy (wEP)

wEP(C) =

1|C|
Entropy(Ci) = âˆ’ 1

log N(cid:4)

(cid:3)
CiâˆˆC
(cid:3)
lâˆˆT

Entropy(Ci),
N(cid:4)
N(cid:4)
N(cid:4)
N(cid:4)

log

l,i

i

l,i

,

i

(8)

(9)

where |C| denotes a cluster number. The original entropy indicates the degree
of unevenness of class distribution within a cluster, whereas the extended
entropy includes unevenness of the neighboring clusters.

3.2 Extension of Pairwise-Based Cluster Validity Indices

This
section describes an extension of pairwise-based cluster validity
measures[1,14]. Table 1 shows a class and cluster confusion matrix of data pairs,
where a, b, c, d are the number of data pairs where xi and xj do or do not belong
to the same class/cluster.

Table 1. Class and cluster confusion matrix of data pairs

t(i) = t(j) t(i) (cid:2)= t(j)

c(i) = c(j)
c(i) (cid:2)= c(j)

a
c

b
d

Here, we introduce likelihood(c(i) = c(j)) indicating a degree that a data
pair xi and xj belongs to the same cluster instead of the actual number of data
pairs. The likelihood is given by the inter-cluster distance of the data pair as
shown in Fig. 2(a). The same weighting function as in sec. 3.1 is available for the

Neighborhood-Based Smoothing of External Cluster Validity Measures

359

likelihood function (Fig. 2(b)); likelihood(c(i) = c(j)) = hc(i),c(j). Then, a, b, c, d
are replaced by summation of the likelihoods as follows:

{i,j|t(i)=t(j)}

{i,j|t(i)(cid:5)=t(j)}

(cid:3)
(cid:3)
(cid:3)
(cid:3)

hc(i),c(j),

hc(i),c(j),
(cid:4)
1 âˆ’ hc(i),c(j)
(cid:4)
1 âˆ’ hc(i),c(j)

(cid:5)
(cid:5)

= a + c âˆ’ a(cid:4),

= b + d âˆ’ b(cid:4).

(10)

(11)

(12)

(13)

a(cid:4)

=

b(cid:4)

c(cid:4)

=

=

d(cid:4)

=

{i,j|t(i)=t(j)}

{i,j|t(i)(cid:5)=t(j)}

With these extended a(cid:4), b(cid:4), c(cid:4)
F-measure are deï¬ned as follows:

and d(cid:4)

, weighted pairwise accuracy and pairwise

weighted Pairwise Accuracy (wPA)

wPA(C) =

a(cid:4)

+ d(cid:4)

a(cid:4) + b(cid:4) + c(cid:4) + d(cid:4) .

(14)

The original pairwise accuracy is a ratio of the number of pairs in the same
class belonging to the same cluster, or the number of pairs in diï¬€erent classes
belonging to diï¬€erent clusters, against all pairs. The weighted PA is the de-
gree to which pairs in the same class belong to the neighbor clusters or that
pairs in diï¬€erent classes belong to distant clusters.

weighted Pairwise F-measure (wPF)

wPF(C) =

2 Â· P Â· R
P + R

,

(15)

+b(cid:4)

where P = a(cid:4)/(a(cid:4)
) is precision, that is a measure of the same class among
each cluster, and R = a(cid:4)/(a(cid:4)
) is recall that is a measure of the same cluster
among each class. The original pairwise F-measure is a harmonic average of
the precision and the recall. While, the weighted PF is based on a degree
that the data pairs belong to the same cluster.

+c(cid:4)

3.3 Weighting Function

For the weighting function for smoothing in the set-based and the likelihood
in pairwise-based measures, any monotonically decreasing function hi,j â‰¥ 0 is
feasible, including the Gaussian or a rectangle function. Note that the extended
measures are exactly the same as the original measures when hi,j = Î´i,j (Î´ is the
Kronecker delta).

The neighborhood radius eï¬€ects the degree of smoothing and likelihood. Fig.
3 illustrates that the measure evaluates individual clusters, that is the original

360

K. Fukui and M. Numao

values, as the radius becomes zero (Ïƒ â†’ 0). On the other hand, as the radius
becomes larger (Ïƒ â†’ âˆ), the data space is smoothed by almost the same weights,
and all micro-clusters are treated as one big cluster. The way to ï¬nd the optimal
radius is described in section 3.4.

(cid:86)(cid:80)(cid:82)(cid:82)(cid:87)(cid:75)(cid:72)(cid:71)(cid:3)(cid:89)(cid:68)(cid:79)(cid:88)(cid:72)
(cid:25)
(cid:24)
(cid:23)
(cid:22)
(cid:21)
(cid:20)
(cid:19)

(cid:21)

(cid:20)
(cid:22)
(cid:81)(cid:72)(cid:76)(cid:74)(cid:75)(cid:69)(cid:82)(cid:85)(cid:76)(cid:81)(cid:74)(cid:3)(cid:70)(cid:79)(cid:88)(cid:86)(cid:87)(cid:72)(cid:85)(cid:86)

(cid:85)(cid:68)(cid:71)(cid:76)(cid:88)(cid:86)(cid:3)(cid:11)(cid:109)(cid:12)

(cid:24)
(cid:20)
(cid:19)(cid:17)(cid:24)
(cid:19)

(cid:11)(cid:32)(cid:82)(cid:85)(cid:76)(cid:74)(cid:76)(cid:81)(cid:68)(cid:79)(cid:3)(cid:89)(cid:68)(cid:79)(cid:88)(cid:72)(cid:12)

Fig. 3. Example of the eï¬€ect of smoothing radius. Values over the neighborhood rela-
tion of the clusters become smoother as the radius increases.

3.4 Optimal Smoothing Radius

Our smoothed measures include a neighborhood relation in the conventional
cluster validity measures. In order to evaluate a neighbor component within the
measure, we deï¬ned as:

Deï¬nition 4. (neighbor component) The quantity within the smoothed cluster
validity value (Eval) that are caused by the neighborhood relation.

Here, Eval refers to the output value of wCP, wEP, wPA, or wPF in this paper.
Then, the neighbor component (N C) can be computed by comparing Evals

with randomized neighborhood relation.

N C(Ïƒ) = |Eval âˆ’ lim

nâ†’âˆ Evalrnd(n)| = |Eval(di,j) âˆ’ Eval( Â¯di,j))|,

(16)

where Evalrnd(n) denotes an average of Eval when inter-cluster distances are n
times shuï¬„ed, and when n â†’ âˆ this value converges to Eval with the average of
all inter-cluster distances Â¯di,j. It is assumed that the smoothing radius that max-
imizes the neighbor component is the optimal one, i.e., Ïƒâˆ—
= arg maxÏƒ N C(Ïƒ).
Then, the optimal evaluation value can be Evalâˆ—
= Eval(Ïƒâˆ—
).

4 Evaluation of the Smoothed Validity Measures

This section describes the experiment to clarify the properties of the proposed
smoothed validity measures.

Neighborhood-Based Smoothing of External Cluster Validity Measures

361

4.1 Settings of Clustering and Neighborhood Relation

1. kmc-knn

Typical k-means clustering was used to produce a clustering and mutual k-
nearest neighbor (kmc-knn) was used to obtain the neighbor relation. With
parameters of the prototype (micro-cluster) number k1 and of nearest neigh-
bors k2, adjacent matrix A = (ai,j) can be given by:

(cid:6)

ai,j =

1 if Cj âˆˆ O(Ci) and Ci âˆˆ O(Cj)
0 otherwise,

(17)

where O(Ci) denotes a set of k-nearest neighbor clusters from Ci, where
d(Ci, Cj ) is given by Euclidean distance. Then, distance matrix D = (di,j)
can be given by topological distance, in this experiment the shortest path
between Ci and Cj is used, where the shortest path of all pairs are calculated
by Warshall-Floyd Algorithm.

2. SOM

Also the SOM[6] was used as an another type of producing micro-cluster
prototypes with neighbor relation. In the SOM, the neurons of prototypes
correspond to centroids of micro-clusters. The standard batch type SOM is
used in this work. A distance matrix D is given by di,j = ||ri âˆ’ rj||, where
r is a coordinate of a neuron within the topology space of the SOM.

4.2 Datasets

1. Synthetic data

In order to evaluate the proposed measure, two classes of two-dimensional
synthetic data were prepared, where 300 data points for each class were
generated from diï¬€erent Gaussian distributions. The data distribution and
examples of graphs are illustrated in Fig. 4.

2. Real-world data

Well-known open datasets2 were used as real-world data: Iris data (150
samples, 4 attributes, 3 classes), Wine data (178 samples, 13 attributes, 3
classes), and Glass Identiï¬cation data (214 samples, 9 attributes, 6 classes).

4.3 Eï¬€ect of Smoothing Radius - Finding the Optimal Radius

Fig. 5 shows the evaluation values of the smoothed validity measures for the
synthetic data using kmc-knn. The larger value is the better except entropy.
The values are average of 100 runs of randomized initial values.

Firstly, the total evaluation values (Eval) provides always better value than
that of random topology (Evalrnd) where neighborhood relation of the proto-
types is destroyed. This means that the proposed measures evaluate both cluster
validity and neighborhood relation of the clusters.

2 http://archive.ics.uci.edu/ml/

362

K. Fukui and M. Numao

(cid:3)(cid:22)

(cid:3)(cid:21)

(cid:3)(cid:20)

(cid:3)(cid:19)

(cid:16)(cid:20)

(cid:16)(cid:21)

(cid:16)(cid:22)

(cid:16)(cid:22) (cid:16)(cid:21) (cid:16)(cid:20) (cid:3)(cid:19) (cid:3)(cid:20) (cid:3)(cid:21) (cid:3)(cid:22) (cid:3)(cid:23) (cid:3)(cid:24) (cid:3)(cid:25)

(cid:3)(cid:22)

(cid:3)(cid:21)

(cid:3)(cid:20)

(cid:3)(cid:19)

(cid:16)(cid:20)

(cid:16)(cid:21)

(cid:16)(cid:22)

(cid:16)(cid:22) (cid:16)(cid:21) (cid:16)(cid:20) (cid:3)(cid:19) (cid:3)(cid:20) (cid:3)(cid:21) (cid:3)(cid:22) (cid:3)(cid:23) (cid:3)(cid:24) (cid:3)(cid:25)

(cid:3)(cid:22)

(cid:3)(cid:21)

(cid:3)(cid:20)

(cid:3)(cid:19)

(cid:16)(cid:20)

(cid:16)(cid:21)

(cid:16)(cid:22)

(cid:16)(cid:22) (cid:16)(cid:21) (cid:16)(cid:20) (cid:3)(cid:19) (cid:3)(cid:20) (cid:3)(cid:21) (cid:3)(cid:22) (cid:3)(cid:23) (cid:3)(cid:24) (cid:3)(cid:25)

(a) kmc-knn k1=10, k2=4

(b) kmc-knn k1=25, k2=4

(c) kmc-knn k1=50, k2=4

(cid:3)(cid:22)

(cid:3)(cid:21)

(cid:3)(cid:20)

(cid:3)(cid:19)

(cid:16)(cid:20)

(cid:16)(cid:21)

(cid:16)(cid:22)

(cid:16)(cid:22) (cid:16)(cid:21) (cid:16)(cid:20) (cid:3)(cid:19) (cid:3)(cid:20) (cid:3)(cid:21) (cid:3)(cid:22) (cid:3)(cid:23) (cid:3)(cid:24) (cid:3)(cid:25)
(d) kmc-knn k1=25, k2=8

(cid:3)(cid:22)

(cid:3)(cid:21)

(cid:3)(cid:20)

(cid:3)(cid:19)

(cid:16)(cid:20)

(cid:16)(cid:21)

(cid:16)(cid:22)

(cid:16)(cid:22) (cid:16)(cid:21) (cid:16)(cid:20) (cid:3)(cid:19) (cid:3)(cid:20) (cid:3)(cid:21) (cid:3)(cid:22) (cid:3)(cid:23) (cid:3)(cid:24) (cid:3)(cid:25)

(cid:3)(cid:22)

(cid:3)(cid:21)

(cid:3)(cid:20)

(cid:3)(cid:19)

(cid:16)(cid:20)

(cid:16)(cid:21)

(cid:16)(cid:22)

(e) kmc-knn k1=25, k2=4,
random topology

(cid:16)(cid:22) (cid:16)(cid:21) (cid:16)(cid:20) (cid:3)(cid:19) (cid:3)(cid:20) (cid:3)(cid:21) (cid:3)(cid:22) (cid:3)(cid:23) (cid:3)(cid:24) (cid:3)(cid:25)

(f) SOM 10x10

Fig. 4. Cluster prototypes (â€¢) with topology-based neighbor relation on two dimen-
sional synthetic data. The data points ((cid:2),(cid:3)) were generated from two Gaussian dis-
tributions; N (Âµ1, 1) and N (Âµ2, 1), where Âµ1 = (0, 0) and Âµ2 = (3, 0).

Secondly, as the smoothing radius becomes close to zero (Ïƒ â†’ 0), the extended
measure evaluates individual clusters without neighborhood relation. Whereas,
as the radius becomes larger (Ïƒ â†’ âˆ), the extended measure treats whole data
as one big cluster as mentioned before. Therefore, the solid and the broken lines
gradually become equal as the radius becomes close to zero or becomes much
larger.

Thirdly, the neighbor component has a monomodality against the radius in
all measures, since there exists an appropriate radius to the average class dis-
tribution. Since the smoothed measure is a composition of cluster validity and
neighborhood relation, the radius that gives the maximum Eval does not al-
ways match with that of neighbor component, for instance, wCP, wEP, and
wPF in Fig. 5. Therefore, the neighbor component should be examined to ï¬nd
the appropriate radius. Also the appropriate radius depends on function of the
measure such as purity, F-measure, or entropy. This means that the user should
use diï¬€erent radius for each measure.

These three trends appear also in SOM (omitted due to page limitation).

4.4 Eï¬€ect of Prototype Number

The eï¬€ect of prototype number is examined by changing k1 = 10, 25, 50 (Fig. 6).
In wPF, k1 = 25 provides the highest neighbor component (0.116 at Ïƒ = 1.4)
among three (Fig. 6(b)). wPF can suggest an optimal prototype number in terms
of maximizing the neighbor component in the measure, which means neighbor

(cid:79)

(cid:72)
(cid:88)
(cid:68)
(cid:89)
(cid:3)
(cid:81)
(cid:82)

(cid:76)
(cid:87)

(cid:79)

(cid:68)
(cid:88)
(cid:68)
(cid:89)
(cid:72)

Neighborhood-Based Smoothing of External Cluster Validity Measures

363

(cid:3)(cid:20)

(cid:3)(cid:19)(cid:17)(cid:27)

(cid:3)(cid:19)(cid:17)(cid:25)

(cid:3)(cid:19)(cid:17)(cid:23)

(cid:3)(cid:19)(cid:17)(cid:21)

(cid:3)(cid:19)

(cid:90)(cid:38)(cid:51)(cid:13)

(cid:109)

(cid:13)

(cid:3)(cid:19) (cid:3)(cid:20) (cid:3)(cid:21) (cid:3)(cid:22) (cid:3)(cid:23) (cid:3)(cid:24) (cid:3)(cid:25)

(cid:109)

(a) wCP

(cid:3)(cid:19)(cid:17)(cid:26)

(cid:3)(cid:19)(cid:17)(cid:25)

(cid:3)(cid:19)(cid:17)(cid:24)

(cid:3)(cid:19)(cid:17)(cid:23)

(cid:3)(cid:19)(cid:17)(cid:22)

(cid:3)(cid:19)(cid:17)(cid:21)

(cid:3)(cid:19)(cid:17)(cid:20)

(cid:3)(cid:19)

(cid:3)(cid:19)(cid:17)(cid:21)

(cid:3)(cid:19)(cid:17)(cid:20)

(cid:3)(cid:19)

(cid:16)(cid:19)(cid:17)(cid:20)

(cid:90)(cid:40)(cid:51)(cid:13)

(cid:109)(cid:13)

(cid:3)(cid:19) (cid:3)(cid:20) (cid:3)(cid:21) (cid:3)(cid:22) (cid:3)(cid:23) (cid:3)(cid:24)

(cid:109)

(b) wEP

(cid:90)(cid:51)(cid:36)(cid:13)

(cid:109)(cid:13)

(cid:3)(cid:19) (cid:3)(cid:21) (cid:3)(cid:23) (cid:3)(cid:25) (cid:3)(cid:27)(cid:3)(cid:20)(cid:19)(cid:3)(cid:20)(cid:21)(cid:3)(cid:20)(cid:23)

(cid:109)

(c) wPA

(cid:3)(cid:19)(cid:17)(cid:26)

(cid:3)(cid:19)(cid:17)(cid:25)

(cid:3)(cid:19)(cid:17)(cid:24)

(cid:3)(cid:19)(cid:17)(cid:23)

(cid:3)(cid:19)(cid:17)(cid:22)

(cid:3)(cid:19)(cid:17)(cid:21)

(cid:3)(cid:19)(cid:17)(cid:20)

(cid:3)(cid:19)

(cid:90)(cid:51)(cid:41)(cid:13)

(cid:109)(cid:13)

(cid:3)(cid:19) (cid:3)(cid:21) (cid:3)(cid:23) (cid:3)(cid:25) (cid:3)(cid:27)(cid:3)(cid:20)(cid:19)(cid:3)(cid:20)(cid:21)(cid:3)(cid:20)(cid:23)

(cid:109)

(d) wPF

(YDO

(YDO

UQG

1&

Fig. 5. The eï¬€ect of smoothing radius (synthetic data, kmc-knn(k1 = 25, k2 = 4));
total evaluation value (Eval), Eval with random topology (Evalrnd), neighbor com-
ponent (N C)

(cid:13)

(cid:13)

(cid:13)
(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:109)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:49)(cid:38)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:87)(cid:38)(cid:51)
(cid:20)(cid:17)(cid:3)(cid:19)(cid:17)(cid:28)(cid:3)(cid:3)(cid:19)(cid:17)(cid:20)(cid:24)(cid:28)(cid:3)(cid:3)(cid:19)(cid:17)(cid:27)(cid:20)(cid:19)
(cid:21)(cid:17)(cid:3)(cid:20)(cid:17)(cid:19)(cid:3)(cid:3)(cid:19)(cid:17)(cid:21)(cid:19)(cid:28)(cid:3)(cid:3)(cid:19)(cid:17)(cid:27)(cid:22)(cid:25)
(cid:22)(cid:17)(cid:3)(cid:20)(cid:17)(cid:20)(cid:3)(cid:3)(cid:19)(cid:17)(cid:21)(cid:25)(cid:24)(cid:3)(cid:3)(cid:19)(cid:17)(cid:27)(cid:24)(cid:28)

(cid:49)(cid:38)

(cid:3)(cid:19)(cid:17)(cid:22)
(cid:3)(cid:19)(cid:17)(cid:21)(cid:24)
(cid:3)(cid:19)(cid:17)(cid:21)
(cid:3)(cid:19)(cid:17)(cid:20)(cid:24)
(cid:3)(cid:19)(cid:17)(cid:20)
(cid:3)(cid:19)(cid:17)(cid:19)(cid:24)
(cid:3)(cid:19)

(cid:3)(cid:19) (cid:3)(cid:20) (cid:3)(cid:21) (cid:3)(cid:22) (cid:3)(cid:23) (cid:3)(cid:24) (cid:3)(cid:25)

(cid:109)
(a) wCP














(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:109)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)1&W3)
















(cid:109)

(b) wPF

N 




Fig. 6. The eï¬€ect of prototype number (synthetic data, kmc-knn(k2 = 4)). The maxi-
mum neighbor component (N Câˆ—
) and total values (wCP and wPF) are listed together
in the table.

relation of class distribution is maximized. However, the larger k1 the better in
wCP (Fig. 6(a)). This is because the function of cluster purity given by eq. (7),
that is, the smaller number of elements in some cluster tends to give better
purity.

4.5 Eï¬€ect of Class Overlap

âˆ’ Î¼x

The eï¬€ect of class overlap is examined (Fig. 7) by changing distance between
class centers Î¼d = Î¼x
1 from 2.0 to 3.0 in the synthetic data. Observing
2
Fig. 7, the lower class overlap is, the better the neighbor component and the
total values. However, the optimal radii are nearly the same even in diï¬€erent
class overlap. This means that our measure can determine the optimal radius
independent to class overlap, and can evaluate volume of overlap.

364

K. Fukui and M. Numao

(cid:49)(cid:38)

(cid:3)(cid:19)(cid:17)(cid:22)
(cid:3)(cid:19)(cid:17)(cid:21)(cid:24)
(cid:3)(cid:19)(cid:17)(cid:21)
(cid:3)(cid:19)(cid:17)(cid:20)(cid:24)
(cid:3)(cid:19)(cid:17)(cid:20)
(cid:3)(cid:19)(cid:17)(cid:19)(cid:24)
(cid:3)(cid:19)

(cid:13)

(cid:13)

(cid:13)
(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:109)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:49)(cid:38)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:87)(cid:38)(cid:51)
(cid:20)(cid:17)(cid:3)(cid:20)(cid:17)(cid:22)(cid:3)(cid:3)(cid:19)(cid:17)(cid:21)(cid:19)(cid:26)(cid:3)(cid:3)(cid:19)(cid:17)(cid:26)(cid:26)(cid:25)
(cid:21)(cid:17)(cid:3)(cid:20)(cid:17)(cid:22)(cid:3)(cid:3)(cid:19)(cid:17)(cid:21)(cid:25)(cid:28)(cid:3)(cid:3)(cid:19)(cid:17)(cid:27)(cid:23)(cid:25)
(cid:22)(cid:17)(cid:3)(cid:20)(cid:17)(cid:21)(cid:3)(cid:3)(cid:19)(cid:17)(cid:21)(cid:27)(cid:26)(cid:3)(cid:3)(cid:19)(cid:17)(cid:27)(cid:26)(cid:26)

(cid:3)(cid:19) (cid:3)(cid:21) (cid:3)(cid:23) (cid:3)(cid:25) (cid:3)(cid:27) (cid:3)(cid:20)(cid:19) (cid:3)(cid:20)(cid:21) (cid:3)(cid:20)(cid:23)

(cid:109)
(a) wCP


















(cid:109)1&W3)






     

(cid:109)

(b) wPF

(cid:43)(cid:3)(cid:3)(cid:27)G




Fig. 7. The eï¬€ect of class overlap (synthetic data, SOM(10Ã—10))

(cid:49)(cid:38)

(cid:3)(cid:19)(cid:17)(cid:23)
(cid:3)(cid:19)(cid:17)(cid:22)(cid:24)
(cid:3)(cid:19)(cid:17)(cid:22)
(cid:3)(cid:19)(cid:17)(cid:21)(cid:24)
(cid:3)(cid:19)(cid:17)(cid:21)
(cid:3)(cid:19)(cid:17)(cid:20)(cid:24)
(cid:3)(cid:19)(cid:17)(cid:20)
(cid:3)(cid:19)(cid:17)(cid:19)(cid:24)
(cid:3)(cid:19)

(cid:13)

(cid:13)

(cid:13)
(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:109)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:49)(cid:38)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:87)(cid:38)(cid:51)
(cid:20)(cid:17)(cid:3)(cid:20)(cid:17)(cid:19)(cid:3)(cid:3)(cid:19)(cid:17)(cid:22)(cid:22)(cid:21)(cid:3)(cid:3)(cid:19)(cid:17)(cid:27)(cid:25)(cid:21)
(cid:21)(cid:17)(cid:3)(cid:20)(cid:17)(cid:21)(cid:3)(cid:3)(cid:19)(cid:17)(cid:22)(cid:25)(cid:27)(cid:3)(cid:3)(cid:19)(cid:17)(cid:27)(cid:25)(cid:28)
(cid:22)(cid:17)(cid:3)(cid:19)(cid:17)(cid:27)(cid:3)(cid:3)(cid:19)(cid:17)(cid:20)(cid:22)(cid:28)(cid:3)(cid:3)(cid:19)(cid:17)(cid:25)(cid:23)(cid:25)
(cid:23)(cid:17)(cid:3)(cid:20)(cid:17)(cid:21)(cid:3)(cid:3)(cid:19)(cid:17)(cid:21)(cid:27)(cid:26)(cid:3)(cid:3)(cid:19)(cid:17)(cid:27)(cid:26)(cid:26)

(cid:3)(cid:19) (cid:3)(cid:21) (cid:3)(cid:23) (cid:3)(cid:25) (cid:3)(cid:27) (cid:3)(cid:20)(cid:19) (cid:3)(cid:20)(cid:21) (cid:3)(cid:20)(cid:23)

(cid:109)
(a) wCP














(cid:109)1&W3)





      

(cid:109)

(b) wPF

LULV
ZLQH
JODVV
V\QWKWLF

Fig. 8. The eï¬€ect of dataset, SOM (10Ã—10)

4.6 Real-World Data

Fig. 8 shows the result for real-world data using SOM. Though there exists an
optimal radius, the optimal radii vary depending on dataset, i.e., the number
of classes and the class distribution. This result indicates that depending on
dataset and measure, a user should use diï¬€erent radius that gives the maximum
volume of neighbor component.

5 Conclusion

This paper proposed a novel and generic smoothed cluster validity measures
based on neighborhood relation of clusters with external criteria. The experi-
ments revealed the existence of an optimal neighborhood radius which maxi-
mizes the neighbor component. A user should use an optimal radius depending
on a function of measure and a dataset. Our measure can determine the optimal
radius independent to class overlap, and can evaluate volume of class overlap.
In addition, feature selection, metric learning[13,15], and a correlation index for
multilabels to determine the most relevant class are promising future directions
for this work.

Neighborhood-Based Smoothing of External Cluster Validity Measures

365

Acknowledgment. This work was supported by KAKENHI (21700165).

References

1. AmigÂ´o, E., Gonzalo, J., Artiles, J., Verdejo, F.: A comparison of extrinsic clustering
evaluation metrics based on formal constraints. Information Retrieval 699(12), 461â€“
486 (2009)

2. Davies, D.L., Bouldin, D.W.: A cluster separation measure. IEEE Transactions on

Pattern Analsis and Machine Intelligence (TPAMI) 1(4), 224â€“227 (1979)

3. Deborah, L.J., Baskaran, R., Kannan, A.: A survey on internal validity measure
for cluster validation. International Journal of Computer Science & Engineering
Survey (IJCSES) 1(2), 85â€“102 (2010)

4. Dunn, J.C.: Well separated clusters and optimal fuzzy partitions. Journal of Cy-

bernetics 4, 95â€“104 (1974)

5. Halkidi, M., Vazirgiannis, M.: Clustering validity assessment using multi represen-
tatives. In: Proc. 2nd Hellenic Conference on Artiï¬cial Intelligence, pp. 237â€“248
(2002)

6. Kohonen, T.: Self-Organizing Maps. Springer (1995)
7. KovÂ´acs, F., LegÂ´any, C., Babos, A.: Cluster validity measurement techniques. En-

gineering 2006, 388â€“393 (2006)

8. Kremer, H., Kranen, P., Jansen, T., Seidl, T., Bifet, A., Holmes, G., Pfahringer, B.:
An eï¬€ective evaluation measure for clustering on evolving data streams. In: Proc.
the 17th SIGKDD Conference on Knowledge Discovery and Data Mining (KDD
2011), pp. 868â€“876 (2011)

9. Liu, Y., Li, Z., Xiong, H., Gao, X., Wu, J.: Understanding of internal clustering val-
idation measures. In: Proc. IEEE International Conference on Data Mining (ICDM
2010), pp. 911â€“916 (2010)

10. RendÂ´on, E., Abundez, I., Arizmendi, A., Quiroz, E.M.: Internal versus external
cluster validation indexes. International Journal of Computers and Communica-
tions 5(1), 27â€“34 (2011)

11. Tasdemir, K., MerÂ´enyi, E.: A new cluster validity index for prototype based clus-
tering algorithms based on inter- and intra-cluster density. In: Proc. International
Joint Conference on Neural Networks (IJCNN 2007), pp. 2205â€“2211 (2007)

12. Veenhuis, C., Koppen, M.: Data Swarm Clustering, ch. 10, pp. 221â€“241. Springer

(2006)

13. Weinberger, K.Q., Blitzer, J., Saul, L.K.: Distance metric learning for large margin
nearest neighbor classiï¬cation. Journal of Machine Learning Research (JMLR) 10,
207â€“244 (2009)

14. Xu, R., Wunsch, D.: Cluster Validity. Computational Intelligence, ch. 10, pp. 263â€“

278. IEEE Press (2008)

15. Zha, Z.J., Mei, T., Wang, M., Wang, Z., Hua, X.S.: Robust distance metric learning
with auxiliary knowledge. In: Proc. International Joint Conference on Artiï¬cial
Intelligence (IJCAI 2009), pp. 1327â€“1332 (2009)


