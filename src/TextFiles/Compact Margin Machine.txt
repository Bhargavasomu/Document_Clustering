Compact Margin Machine(cid:2)

Bo Dai1 and Gang Niu2

1 NLPR/LIAMA, Institute of Automation, Chinese Academy of Science,

Beijing 100190, P.R. China

2 State Key Laboratory for Novel Software Technology, Nanjing University,

bdai@nlpr.ia.ac.cn

Nanjing 210093, P.R. China

niugang@ai.nju.edu.cn

Abstract. How to utilize data more suﬃciently is a crucial considera-
tion in machine learning. Semi-supervised learning uses both unlabeled
data and labeled data for this reason. However, Semi-Supervised Sup-
port Vector Machine (S3VM) focuses on maximizing margin only, and it
abandons the instances which are not support vectors. This fact moti-
vates us to modify maximum margin criterion to incorporate the global
information contained in both support vectors and common instances.
In this paper, we propose a new method, whose special variant is a semi-
supervised extension of Relative Margin Machine, to utilize data more
suﬃciently based on S3VM and LDA. We employ Concave-Convex Pro-
cedure to solve the optimization that makes it practical for large-scale
datasets, and then give an error bound to guarantee the classiﬁer’s per-
formance theoretically. The experimental results on several real-world
datasets demonstrate the eﬀectiveness of our method.

1 Introduction

Semi-supervised learning paradigm, which blossoms out to utilize data suﬃ-
ciently, has attracted more and more attention in machine learning commu-
nity [1]. Semi-Supervised Support Vector Machine(S3VM) [2, 3], the semi
supervised extension of
is a state-of-the-art
semi-supervised learning algorithm. It uses all the data no matter labeled or not
to detect the margin and achieves signiﬁcant improvement in practice. However,
S3VM abandons the instances which are not support vectors. The framework [4]
utilizes general unlabeled data, but still wasteful for non-support vectors, which
stops them from further improving the performance.

support vector machine,

To utilize the data more suﬃciently and eﬃciently, we notice that compact-
ness of projected data provides global information and thus is important for
classiﬁcation. Linear Discriminant Analysis [5] is a successful algorithm to uti-
lize the information. Algorithm in [6] whitens the data when seeking decision
boundary. Gaussian Margin Machine [7] controls the projected data compact-
ness under a distribution assumption. Universum SVM [8] constrains range by

(cid:2) This work is supported in part by Natural Science Foundation of China (No.

60275025).

M.J. Zaki et al. (Eds.): PAKDD 2010, Part II, LNAI 6119, pp. 507–514, 2010.
c(cid:2) Springer-Verlag Berlin Heidelberg 2010

508

B. Dai and G. Niu

data that is related but not belonged to any category. Another criterion that
compresses the range of projected data is Relative Margin Machine[9] which is
motivated from an aﬃne invariance perspective and some probabilistic prop-
erties. Although these algorithms balance global and local information, these
methods merely exploit labeled data but turn blind eyes to unlabeled data.

In this paper, we propose a method to use the information contained by
instances suﬃciently. After brief introductions to S3VM and LDA in Section 2,
our framework incorporating LDA criterion with S3VM is presented in Section
3 and a relaxation of the criterion is given to make the optimization tractable.
We also derive a special variant which can be viewed as the semi-supervised
extension of Relative Margin Machine. The generalization bound is analyzed in
Section 4. Finally, the experimental results are reported in Section 5.

2 Background
Suppose that we are given labeled data set L = {(x1, y1), (x2, y2), . . . , (xl, yl)}
and unlabeled data set U = {xl+1, xl+2, . . . , xl+u}, both of which are drawn i.i.d
from a certain data distribution D , where u (cid:2) l, xi ∈ R
d (i = 1, 2, . . . , l + u)
and yj ∈ {−1, 1}, (j = 1, 2, . . . , l) is the label of instance xj. The problem we
d → {−1, +1} which can classify the
want to solve is seeking a hypothesis h : R
unlabeled data and unseen instances sampled from D.

2.1 Semi-supervised SVM(S3VM)

Many semi-supervised methods ﬁnd the suitable hypothesis by minimizing the
criterion which utilize both labeled data and unlabeled data as

(cid:5)f(cid:5)HK + C1

(cid:2)l

(cid:2)l+u

i=1

i=l+1

min
f

(cid:2)2(xi; f)

(cid:2)1(xi, yi; f) + C2

(1)
where H is the reproducing kernel Hilbert space introduced by kernel function K,
(cid:2)1 is a common classiﬁcation loss function, and (cid:2)2 is another loss function which
utilizes unlabeled data only. S3VM employs hinge loss as (cid:2)1 and symmetric hinge
loss as (cid:2)2. C1 and C2 are the parameters to balance the loss between labeled
data, unlabeled data and function complexity. We take the set of linear form of
d, b ∈ R), as the hypothesis space.
h = sign(f(x)), where f(x) = wT x + b(w ∈ R
So the formulation of (1) could be transformed as follow:

(cid:2)l

(cid:5)w(cid:5)2 + C1

1
ηi + C2
2
yif(xi)≥1 − ηi, i = 1, . . . , l,

i=1

min

w,b,η,ξ≥0

s.t.

(cid:2)l+u

ξi

i=l

|f(xi)|≥1 − ξi, i = l + 1, . . . , l + u.

(2)

2.2 Linear Discriminative Analysis(LDA)

The basic principle of LDA is projecting the data into a subspace in which the
instances in diﬀerent categories can be scattered and the instances in the same

Compact Margin Machine

509

(cid:3)
j∈Ci xj , i ∈ {−1, +1}
category can be concentrated together. Let mi = 1
is the sample mean of class Ci where ni is the number of samples in Ci, the
ni
(cid:3)
j∈Ci f(xj) = wT mi + b, i ∈
mean of projected instances is given by ˆmi = 1
{−1, +1}. So the distance between the projected means, between-class distance,
ni
is (cid:5) ˆm−1 − ˆm1(cid:5) = (cid:5)wT m−1 − wT m1(cid:5). LDA seeks the largest between-class
distance relative to total within-class variance which is deﬁned by
i=−1,+1 si
j∈Ci(f(xj)− ˆmi)2. The LDA criterion is deﬁned as the ratio of the
where si =
between-class distance to total within-class variance, Jw =

(cid:3)

(cid:3)

(cid:5) ˆm−1− ˆm1(cid:5)
(cid:3)
i=−1,+1 si .

3 Compact Margin Machine

The data compactness after projecting is very important in reﬂecting the data
structure and can indeed help in classifying the data. It is necessary to restrict
the range of projected data while seeking the largest margin. Within-class vari-
ance deﬁned in LDA provides us a natural way to measure the projected data
j∈Ci max{0,|f(xj) − ˆmi| − ε}, i = {−1, +1} in-
compactness. We modify si as
stead of 2− norm form and introduce it as
i=−1,+1 si to
S3VM. We propose the model, Compact Margin Machine, as follow (f(x) stands
for wT x + b):

(cid:3)l+u
i=1 (cid:2)c(xi;L,U, f) =

(cid:3)

(cid:3)

min

(cid:5)w(cid:5)2 + C1
w,b,{η,ξ,ζ}≥0,di={0,1}
s.t. yif(xi) ≥ 1 − ηi, i = 1, . . . , l

1
2

yi + 1

l(cid:2)

ηi + C2

l+u(cid:2)

ξi + C3

l+u(cid:2)

(ζ1,i + ζ0,i)

i=1

i=l+1

i=1

(3)

(4)

(5)

(6)

(cid:3)l

(cid:3)l+u

2 )f(xi)

(1 − di)

(cid:4)(cid:4)f(xi) −

(cid:4)(cid:4)f(xi) −

(cid:4)(cid:4)f(xi) −

2
1 − yi

|f(xi)| ≥ 1 − ξi, i = l + 1, . . . , l + u
(cid:4)(cid:4) ≤ ε + ζ0,i
(cid:4)(cid:4) ≤ ε + ζ1,i
(cid:4)(cid:4) ≤ ε + ζ0,i

(cid:3)l+u
i=l+1(1 − di)f(xi) +
i=1( yi+1
(cid:3)l
2u(1 − r) +
i=1( yi+1
2 )
(cid:3)l
i=1( 1−yi
i=l+1 dif(xi) +
2 )f(xi)
(cid:3)l
u(2r − 1) +
i=1( 1−yi
2 )
(cid:3)l
(cid:3)l+u
i=l+1(1 − di)f(xi) +
i=1( yi+1
(cid:3)l
i=1( yi+1
2 )
(cid:3)l
i=1( 1−yi
2 )f(xi)
(cid:3)l
(7)
i=1( 1−yi
2 )
(cid:3)l+u
i=l+1 di = 2r − 1 to avoid
where we introduce the class balancing constraint 1
u
the trivial solution that assigns all the instances the same label [10]. We can use
branch-and-bound algorithms to search the global optimal solution. However,
the computational complexity is too high. We relax the constraints to make the
optimization process easier and it can be proved that our relaxed constraints
imply upper bounds of original loss functions.

2u(1 − r) +
i=l+1 dif(xi) +
u(2r − 1) +

(cid:4)(cid:4) ≤ ε + ζ1,i

(cid:4)(cid:4)f(xi) −

2 )f(xi)

(cid:3)l+u

2

di

510

B. Dai and G. Niu

Relative Hinge Loss Function

Relative Symmetrical Loss Function

3.5

3.0

2.5

2.0

1.5

1.0

0.5

s
s
o
L

0.0

−2

−1

0

1

2

yf(x)

3

4

5

6

C2 = C3, epsilon = 0

3.0

2.5

2.0

s
s
o
L

1.5

C2 = C3, epsilon = 1

C2 = C3, epsilon = a

1.0

0.5

0.0

−a

−1

0

f(x)

1

a

Fig. 1. Relaxed compact ε-hinge loss and relaxed compact ε-symmetric hinge loss

Proposition 1. By replacing the constraints (4)-(7) with |wT xi+b| ≤ 1
the loss will be no less than (cid:2)c deﬁned above.

2(ε+ζi),

Proof. As the constraints (4)-(7) have the same form, without loss of generality,
we consider the constraint (7) and the others are similar. Obviously, if there is
a solution w that satisﬁes |wT xi + b| ≤ 1

2(ε + ζi), then we have

(cid:4)(cid:4)(cid:5)

di

(wT xi + b) −
(cid:5)|wT xi + b| +

(cid:4)(cid:4)

di

)(wT xj + b)

(cid:6)(cid:4)(cid:4) ≤

(cid:3)l+u

(cid:3)l+u

2

(cid:3)l+u

j=l+1 dj(wT xj + b) +
j=l dj +
j=l+1 dj(wT xj + b) +
(cid:3)l+u
j=l dj +
(cid:2)

(cid:3)l
j=1( 1−yj
(cid:3)l
j=1( 1−yj
)
(cid:3)l
j=1( 1−yj
(cid:3)l
j=1( 1−yj
)
|wT xj + b| ≤ ε +
1
2 ζi +

1
nj

2

2

2

j∈Cyi

|wT xi + b| +

)(wT xj + b)

(cid:4)(cid:4)(cid:6) ≤
(cid:2)

ζj

j∈Cyi

1
2nj

The inequality is derived from triangle inequality. We obtain that the relaxed
(cid:8)(cid:9)
constraints provide an upper bound and

(cid:3)n
i=1 (cid:2)c(xi;L,U, f) ≤ (cid:3)n

i=1 ζi.

Proposition 1 suggests that the relaxed constrains are helpful in reﬂecting the
projected data compactness. Mathematically, CMM is relaxed as:
l+u(cid:2)

l+u(cid:2)

l(cid:2)

min

w,b,{η,ξ,ζ}≥0

1
2

(cid:5)w(cid:5)2 + C1

ηi + C2
s.t. yi(wT xi + b) ≥ 1 − ηi,

i=1

ξi + C3

i=l+1
i = 1, . . . , l

ζi

i=1

(8)

|wT xi + b| ≥ 1 − ξi,
|wT xi + b| ≤ ε + ζi,

i = l + 1, . . . , l + u
i = 1, . . . , l + u

Note that relaxed constraints modify both of the loss functions for labeled and
unlabeled data given by S3VM actually. The loss function for the labeled data
C1 max{0,|f(x)| − ε} which we named as relaxed
is (cid:2)
compact ε-hinge loss. On the other hand, the loss function of the unlabeled data

1 = max{0, 1 − yf(x)} + C3

(cid:2)

Compact Margin Machine

511

(cid:2)

2 = max{0, 1 − |f(x)|} + C3

C2 max{0,|f(x)| − ε}, named as re-
is adapted as (cid:2)
laxed compact ε-symmetric hinge loss. The loss functions are shown above. It
is not easy to solve the optimization problem (8) directly because of the non-
convex property of relaxed compact ε-symmetric hinge loss. As [2], Mix Integer
Programming can ﬁnd the global optimal solution of (8). However, the compu-
tational complexity of MIP is usually very high. By employing Concave-Convex
Procedure(CCCP) [11], (8) can be solved eﬀciently [12]. We set xi = xi−u, i =
l + u + 1, . . . , l + 2u, yi = 1, i = l + 1, . . . , l + u, yi = −1, i = l + u + 1, . . . , l + 2u
and rewrite the relaxed Compact Margin Machine criterion (8) as the sum of a
convex part
Jvex = 1

(cid:5)w(cid:5)2 + C1
(cid:3)l+2u
2
i=1 max{0,|f(xi)| − ε}
+ C3
(cid:3)l+2u
i=l+1 max{0, δ − yif(xi)} where δ ∈ (−1, 0].

(cid:3)l
i=1 max{0, 1 − yif(xi)} + C2

(cid:3)l+2u
i=l+1 max{0, 1 − yif(xi)}

and a concave part Jcav = −C2

1 Initialize θ0 = (w0, b0) with a standard SVM solution on labeled data
2 Set

(cid:7)

C2 if yifθ0 (xi) < δ and i ≥ l + 1
0 otherwise

βi =
while βt+1 (cid:3)= βt do

(cid:5)

solve the convex problem where K is kernel
(α−β)(cid:2) y−γ+ˆγ
minα,γ , ˆγ≥0
subject to (β − α)T y + γ T 1 − ˆγ T 1 = 0, ˆγ + γ ≤ C31

(α−β) (cid:2) y−γ + ˆγ

(cid:6)T

(cid:5)

1
2

K

(cid:6)−αT 1+εγ T 1+εˆγT 1

αi ≤ C1, i = 1, . . . , l, αi ≤ C2, i = 1 + 1, . . . , l + 2u

compute bt+1 by fθ t+1 (xi) =
compute yifθ t+1 (xi), i = l + 1, . . . , l + 2u by KKT conditions
alternate β

Ki• + bt+1

(α − β) (cid:2) y − γ + ˆγ

(cid:6)T

(cid:5)

(cid:7)

C2 if yifθt+1 (xi) < δ and i ≥ l + 1
0 otherwise

3

4

5

6

βi =

7 end

Algorithm 1. Concave-Convex Procedure for Relaxed CMM

At each iteration, CCCP solves minw,b Jvex(w, b) + J (cid:6)

cav(wt, bt) · (w, b) until
convergence. The convergence of CCCP has been shown by [13]. The steps of
algorithm are shown in Algorithm 1.

3.1 Special Variant and the Relationship to RMM
In this section, we derive a special variant from (8) which can be solved more
eﬃciently. We set the parameters C2 = C3, ε = 0 in (8), so the loss function of
unlabeled data is the blue one in Fig.1. The mathematical form is as following:

512

B. Dai and G. Niu

l(cid:2)

(cid:5)w(cid:5)2

1
2

min

w,b,{η,ζ}≥0

2 + C1

ηi + C2
s.t. yi(wT xi + b) ≥ 1 − ηi,

i=1

l+u(cid:2)

(ζi + ˆζi)

i=1
i = 1, . . . , l

wT xi + b ≤ ε + ζi,−(wT xi + b) ≤ ε + ˆζi,

(9)

i = 1, . . . , l + u

This special variant of relaxed Compact Margin Machine utilizes the labeled
data to control the margin and both the labeled and unlabeled data to compress
the range of projected data. It can be viewed as a semi-supervised extension of
RMM. RMM introduces the relaxed constraints (8) from the other motivations
such as aﬃne invariance perspective and some probabilistic properties. From this
aspect, we can conclude that our model achieves the same properties that are
given by RMM.

4 Theoretical Analysis

2

(cid:5)wT zi(cid:5)2

Firstly, we deﬁne the function class of S3VM as HD = {wT x | 1

In this section, we derive the empirical transductive Rademacher complexity [14]
for function classes relaxed Compact Margin Machine which can be plugged into
uniform Rademacher error bound directly.
2 wT w ≤ D}
and the function class of relaxed Compact Margin Machine as FD,C3 = {wT x |
≤ D, 1 ≤ i ≤ n} where Z = {z1, . . . , zn} is an extra dataset
2 wT w+ C3
1
drawn from the same distribution for convenience of proof. However, in practice,
we may use training dataset and testing dataset as the extra dataset. We can
derive the empirical transductive Rademacher complexity of relaxed CMM and
S3VM.
Lemma 2. The transductive Rademacher complexity of Semi-Supervised SVM
Rl+u(HD) ≤ 2
D
i=1 are the singular eigenvalues of Gram
lu
(cid:8)(cid:9)
matrix of the data.

(cid:3)r
i=1 λi, where {λi}r

(cid:8)

2

Lemma 3. The transductive Rademacher complexity of relaxed Compact Mar-
gin Machine Rl+u(FD,C3) ≤ minα≥0
i=1 αi,
(cid:8)(cid:9)
where Kα,C3 =

i=1 xiKα,C3xi + lu

1
l+u
i αizizT
i .

(cid:3)n
i=1 αiI + C3

(m+u)2 D

(cid:3)l+u

(cid:3)n

(cid:3)n

Based on theorems in [14], the following corollary provides an upper bound on
the error rate:

(cid:8)

and
lu . Then with probability at least 1 − δ over the training set, the following

Corollary 4. Fix γ > 0, let F be the set of function. Let c0 =
Q = l+u
bound holds:

3

32 ln 4e

P [y (cid:10)= sign(f(x))] ≤ 1
nγ

l(cid:2)

i=1

ξi + Rl+u(F )

γ

(cid:9)

min{l, u} +

+ c0Q

where ξi = max{0, 1 − yif(xi)}.

(cid:10)

2Q ln

1
δ

Compact Margin Machine

513

Table 1. Experimental Results (Accuracy in percentage)

Performance Comparison on MNIST (mean%)

num #label/class

5

20

40

50

10

100

200
71.19 89.76 92.36 94.12 94.41 95.32 95.50
SVM
84.31 89.34 92.13 93.16 92.85 93.23 93.84
LDA
84.62 89.90 92.58 94.43 94.81 95.95 96.13
RMM
87.18 93.54 95.11 95.15 95.20 95.23 95.50
TSVM
LapSVM 88.83 91.68 93.22 94.52 94.63 95.11 95.50
88.29 93.80 95.07 95.60 95.76 96.10 96.13
CMM
TSVM
90.32 93.59 94.76 95.20 95.26 95.72 96.22
LapSVM 91.23 92.31 93.50 94.33 94.69 95.39 96.47
90.73 94.62 95.27 95.83 96.01 96.33 96.72
CMM
TSVM 94.52 93.86 94.65 95.41 95.45 96.12 96.42
LapSVM 92.13 93.44 94.13 95.00 95.20 95.87 96.67
91.37 94.99 95.70 96.10 96.31 96.63 96.89

CMM

400

600

800

Performance Comparison on TDT2 (mean%)

num #label/class

5

SVM
LDA
RMM
TSVM
LapSVM

CMM
TSVM
LapSVM

CMM

87.09
93.44
94.39
96.48
89.12
94.90
96.23
88.50
95.45

300

600

15

93.62
96.33
96.84
95.95
95.79
97.72
95.63
93.24
97.20

50

96.38
94.45
97.60
97.73
97.52
98.52
96.11
95.89
97.38

150
98.00
94.74
98.55
98.00
98.00
98.55
97.11
97.69
97.78

5 Experiment

The proposed algorithm is evaluated on two benchmark datasets, MNIST1 and
TDT22, to illustrate the eﬀectiveness. We compare our model with SVM, Ker-
nel LDA, RMM, LapSVM and S3VM. We implement our algorithm, RMM and
LapSVM based on CVX3. The SVM and S3VM are solved by SVM light 4. Among
all the experiments, we select the best kernel by 10-fold cross-validation. The other
parameters are also tuned beforehand. We chose the digits “8” vs “9” from MNIST
dataset for binary classiﬁcation problem because these two digits are diﬃcult to
discriminate visually. In TDT2 dataset, we chose categories randomly. We conduct
experiments on several diﬀerent amount of labeled and unlabeled data. The ﬁnal
test accuracy is given as the average of 10 independent trials on the test dataset.

1 The MNIST dataset can be download from http://yann.lecun.com/exdb/mnist/
2 The TDT2 dataset can is available at

http://www.nist.gov/speech/tests/tdt/tdt98
3 The CVX matlab code be obtained from http://www.stanford.edu/~boyd/cvx/
4 The package of SVM light can be download from http://svmlight.joachims.org/

514

B. Dai and G. Niu

The training data are selected randomly each time. We further select a certain num-
ber of labeled data from the selected training dataset randomly.

The weakness of large margin criterion can be observed from the results.
When labeled data are rare, the algorithms that compress the range of projected
data, such as LDA and RMM, perform better than SVM. Our semi-supervised
algorithm achieves signiﬁcant improvement on the two datasets with beneﬁt from
the compact margin. Fortunately, our algorithm suﬀers the slightest depression
compared with others when the unlabeled data hurt the classiﬁers.

6 Conclusion

We propose a novel semi-supervised algorithm which can utilize the data suﬃ-
ciently. To make our method capable to handle large-scale applications, we employ
Concave-Convex Procedure to solve the non-convex problem. A semi-supervised
extension of RMM can be derived from our model. Moreover, we provide theo-
retical analyses to guarantee the performance of our algorithm, and ﬁnally experi-
mental results show that the proposed algorithm improves the accuracy. A eﬃcient
global optimization method for exact solution is our future direction.

References

[1] Chapelle, O., Sch¨olkopf, B., Zien, A. (eds.): Semi-Supervised Learning. MIT Press,

Cambridge (2006)

[2] Bennett, K.P., Demiriz, A.: Semi-supervised support vector machines. In: 12th

NIPS, pp. 368–374 (1998)

[3] Joachims, T.: Transductive inference for text classiﬁcation using support vector

machines. In: 16th ICML, pp. 200–209 (1999)

[4] Huang, K., Xu, Z., King, I., Lyu, M.R.: Semi-supervised learning from gen-
eral unlabeled data. In: Perner, P. (ed.) ICDM 2008. LNCS (LNAI), vol. 5077,
pp. 273–282. Springer, Heidelberg (2008)

[5] Fisher, R.A.: The use of multiple measurements in taxonomic problems. Annals

Eugen. 7, 179–188 (1936)

[6] Xiong, R., Cherkassky, V.: A combined svm and lda approach for classiﬁcation.

In: IJCNN, pp. 157–171 (2005)

[7] Crammer, K., Mohri, M., Pereira, F.: Gaussian margin machines. In: 12th

AISTATS (2009)

[8] Weston, J., Collobert, R., Sinz, F., Bottou, L., Vapnik, V.: Inference with the

universum. In: 23rd ICML, pp. 1009–1016 (2006)

[9] Shivaswamy, P., Jebara, T.: Relative margin machines. In: 22nd NIPS (2008)

[10] Chapelle, O., Sindhwani, V., Keerthi, S.S.: Optimization techniques for semi-

supervised support vector machines. JMLR 9, 203–233 (2008)

[11] Yuille, A.L., Rangarajan, A.: The concave-convex procedure (cccp). In: 15th NIPS.

MIT Press, Cambridge (2001)

[12] Collobert, R., Sinz, F., Weston, J., Bottou, L.: Large scale transductive svms.

JMLR 7, 1687–1712 (2006)

[13] Sriperumbudur, B., Lanckriet, G.: On the convergence of the concave-convex pro-

cedure. In: 23rd NIPS (2009)

[14] El-Yaniv, R., Pechyony, D.: Transductive rademacher complexity and its applica-

tions. In: 20th COLT, pp. 157–171 (2007)


