Ranking Sequential Patterns
with Respect to Signiﬁcance

Robert Gwadera and Fabio Crestani

Universita della Svizzera Italiana

Lugano, Switzerland

Abstract. We present a reliable universal method for ranking sequential
patterns (itemset-sequences) with respect to signiﬁcance in the problem
of frequent sequential pattern mining. We approach the problem by ﬁrst
building a probabilistic reference model for the collection of itemset-
sequences and then deriving an analytical formula for the frequency for
sequential patterns in the reference model. We rank sequential patterns
by computing the divergence between their actual frequencies and their
frequencies in the reference model. We demonstrate the applicability of
the presented method for discovering dependencies between streams of
news stories in terms of signiﬁcant sequential patterns, which is an im-
portant problem in multi-stream text mining and the topic detection and
tracking research.

1 Introduction

1.1 Motivation
Frequent sequential pattern mining, introduced in [1], has established itself as
one of the most important data mining frameworks with broad applications
including analysis of time-related processes, telecommunications, bioinformatics,
business, software engineering, Web click stream mining, etc [9]. The problem is
deﬁned as follows. Given a collection of itemset-sequences (sequence database of
transactions) and a minimum frequency (support) threshold, the task is to ﬁnd
all subsequence patterns, occurring across the itemset-sequences in the collection,
whose frequency is greater than the minimum frequency threshold. The main
focus of the research on sequential pattern mining has been on devising eﬃcient
algorithms for discovering frequent sequential patterns (see [9] for a review).
Although state of the art mining algorithms can eﬃciently derive a complete set
of frequent sequential patterns under certain constraints, the main problem is
that the set of frequent sequential patterns is still too large for eﬀective usage [9].
The two most eﬀective methods for reducing the large set of frequent sequential
patterns have been: closed sequential pattern mining [12] and maximal sequential
pattern mining [5]. However no methods for assessing interestingness of sequential
patterns have been proposed while such methods are very important to advance
the applicability of frequent sequential pattern mining. By comparison, such
methods have been proposed for subsequence patterns in the sliding window
model [6] and for itemsets (see [11] for a review).

M.J. Zaki et al. (Eds.): PAKDD 2010, Part I, LNAI 6118, pp. 286–299, 2010.
c(cid:2) Springer-Verlag Berlin Heidelberg 2010

Ranking Sequential Patterns with Respect to Signiﬁcance

287

1.2 Overview of the Method

We approach the problem by ﬁrst building a probabilistic reference model for the
collection of itemset-sequences and then deriving an analytical formula for the
relative frequency for sequential patterns. Given such a model we discover se-
quential patterns that are under-represented or over-represented with respect to
the reference model, where a pattern is under-represented if it is too infrequent
in the input collection of itemset-sequences and a pattern is over-represented
if it is too frequent in the input collection of itemset-sequence. According to
this notion a sequential pattern is signiﬁcant if the probability that it would
occur by chance a speciﬁc number of times, in the reference model, is very
small. Note that the frequency of occurrence alone is not enough to determine
signiﬁcance, i.e., an infrequent sequential pattern can be more signiﬁcant than
a frequent one. Furthermore an occurrence of a subsequence pattern may be
meaningless [6] if it occurs in an sequence of an appropriately large size. Our
algorithm for ranking sequential patterns with respect to signiﬁcance works as
follows: (I) we ﬁnd frequent sequential patterns using PreﬁxSpan [10] for a given
minimum support threshold; (II) we compute their frequencies and variances of
the frequencies in the reference model and (III) we rank the frequent sequential
patterns with respect to signiﬁcance by computing the divergence (Z-score) be-
tween the empirical (actual) frequencies and frequencies in the reference model.
Given the reference model a presence of signiﬁcant divergence between the ac-
tual and computed frequency of a sequential pattern indicates that there is a
dependency between itemsets/items in that pattern. In order to capture these
dependencies our reference model consists of two sub-models: (I) sequence-wise
reference model: treats itemsets as alphabet symbols and represents an indepen-
dence model where itemsets occur independently of their order in an itemset-
sequence and (II) itemset-wise reference model: provides decorrelated frequencies
of itemsets for the sequence-wise reference model. By decorrelated frequencies we
mean that given an attribute (item) a1 and attribute a2 the frequency of itemset
(a1, a2) is computed using a maximum entropy model, where the marginal em-
pirical probabilities are preserved. The reason we use such a model for itemsets
is that unlike in the case of frequent itemset mining, we do not consider empty
itemsets (empty attribute sets) and therefore the independence model for item-
sets [3] is inappropriate as an itemset-wise reference model. In particular, using
the independence model for sparse non-empty itemsets (the average number of
ones in a row is much smaller than the number of attributes) would artiﬁcially
overestimate the probability of the empty itemset causing a distortion of proper
proportions of probabilities of non-empty itemsets. Note, that the sequence-wise
reference model can be easily extended to Markov models in the spirit of [7].
The reason we consider the sequence-wise model to be independence model in
this paper is because of the following reasons: (I) it is the model of choice if
the Markov reference model is not known; (II) it has an intuitive interpretation
as a method for discovering dependencies and (III) it leads to exact polynomial
formulas for computing the frequencies of sequential patterns.

288

R. Gwadera and F. Crestani

1.3 Multi-stream of News Stories and the Reference Model

We demonstrate the applicability of the presented method for discovering de-
pendencies between streams of news stories, which is an important problem in
multi-stream text mining and the topic detection and tracking research [2]. For
this purpose we generated a collection of itemset-sequences from a multi-stream
of news stories that was gathered from RSS feeds of major world news agencies
[8]. Every itemset-sequence in that collection consists of stream identiﬁers of
stories in a cross-stream cluster of news stories reporting the same news event,
where the sequence is ordered according to the timestamps of the stories. Every
itemset contains stream identiﬁers of documents published within the same time
granularity. As an example itemset-sequence in that collection consider [(AP,
MSNBC), UPI] that corresponds to three articles on the same news event (e.g.,
an earthquake in Italy), where the ﬁrst two of them were published by AP and
MSNBC within the same time granularity and followed by an article by UPI.
Thus, clearly the empty itemset () does not occur in our data set. We stated the
following research questions with respect to this collection of itemset-sequences:
(I) what is the relationship between frequency, signiﬁcance and content similarity
in the discovered signiﬁcant sequential patterns? and (II) what are the depen-
dencies between the news sources in terms of sequential patterns of reporting
the same news events?

As an example of the application of the reference model consider a case where
the input collection of itemset-sequences contains a frequent sequential pattern
s =[(AP, MSNBC), UPI], that consist of two itemsets s1 =(AP, MSNBC) and
s2 =UPI that are correlated by occurring frequently together. Then since the
sequence-wise reference model assumes independence between the elements, the
frequency of s computed from the sequence-wise reference model will be much
smaller then its actual frequency leading to a high signiﬁcance rank of s. Further-
more, s1 =(AP, MSNBC) contains two items a1 =AP and a2 =MSNBC which
are correlated by occurring frequently together in the same itemsets. Then there
are two possibilities for computing the frequency of s in the sequence-wise refer-
ence model: (I) we use the empirical frequency of s1 or (II) we use a frequency
of s1 provided by the itemset-wise reference model. Then since the itemset-wise
reference model provides decorrelated frequencies of itemsets while preserving
marginal frequencies of the items (the publishing rates of AP and MSNBC), the
frequency of s1 computed from the itemset-wise reference model will be smaller
that its empirical frequency leading to an even higher signiﬁcance rank of s.

1.4 Related Work and Contributions

Thus, we present a reliable universal method for ranking sequential patterns
with respect to signiﬁcance that builds on the previous work [6], where a frame-
work for assessing signiﬁcance of subsequence patterns in the sliding window
model was presented. The challenges of analysing itemset-sequences with re-
spect to the previous work on sequences in [6] stems from the following facts:
(I) itemset-sequences have variable sizes; (II) itemset-sequences contain itemsets
(unordered sets) and (III) we do not consider empty itemsets. We address the

Ranking Sequential Patterns with Respect to Signiﬁcance

289

ﬁrst problem by modeling the frequency of an itemset-sequence using a proba-
bilistic discrete mixture model and we approach the second and third problem
by using an appropriate maximum entropy itemset-wise reference model.

To the best of our knowledge this is the ﬁrst algorithm for ranking sequential
patterns with respect to signiﬁcance while there has been an extensive research
on mining frequent sequential patterns (see [9] for a review).

The paper is organized as follows. Section 2 reviews theoretical foundations,
Section 3 deﬁnes the problem, Section 4 presents the sequence-wise reference
model, Section 5 presents the itemset-wise reference model, Section 6 presents
the algorithm for ranking sequential patterns with respect to signiﬁcance, Section
7 presents experimental results and ﬁnally Section 8 presents conclusions.

2 Theoretical Foundations (Review)

In this section we review some concepts that are necessary in order to explain
our framework.

2.1 Sequential Pattern Mining

(cid:2)

m
i=1

(cid:3)
1, s

|S|

(cid:3)
im

. We also say that s

i2 ,. . . , sm ⊆ s
(cid:3)

In this section we review the problem of sequential pattern mining [1]. Let
A = {a1, a2,. . . , a|A|} be a set of items (alphabet). A subset I ⊆ A, where
I = {a1, a2,. . . , a|I|} is called an itemset or element and is also denoted by
(a1, a2,. . . , a|I|). An itemset-sequence s = [s1, s2,. . . , sm] is an ordered list of
itemsets, where si ⊆ A. The size of the itemset-sequence is denoted by |s| and
|si|. An itemset-sequence
the length of itemset-sequence s is deﬁned as l =
(cid:3)
(cid:3)
(cid:3) = [s
s = [s1, s2,. . . , sm] is a subsequence of itemset-sequence s
m(cid:2)], de-
2,. . . , s
noted s (cid:3) s
(cid:3), if there exist integers 1 ≤ i1 ≤ i2 . . . ≤ im such that s1 ⊆ s
(cid:3)
i1,
s2 ⊆ s
(cid:3) is a supersequence of s and s is
(cid:3). Given a collection of itemset-sequences S = {s(1), s(2), . . . , s(|S|)}
contained in s
the support (frequency) of an itemset-sequence s, denoted by supS(s), is deﬁned
as the number of itemset-sequences s(i) ∈ S that contain s as a subsequence.
The relative support (relative frequency) rsupS(s) = supS(s)
is the fraction of
itemset-sequences that contain s as a subsequence. Given a relative support
threshold minRelSup an itemset-sequence s is called a frequent sequential pat-
tern if rsupS(s) ≥ minRelSup. The problem of mining sequential patterns is to
ﬁnd all frequent sequential patterns in S given minRelSup. The support has an
anti-monotonic property meaning that supS(s) ≥ supS(s
(cid:3). A pattern s
is called a closed frequent sequential pattern if none of its frequent supersequences
has the same support. A pattern s is called a maximal frequent sequential pattern
if none of its frequent supersequences is frequent. Table 1 presents an example
collection of itemset-sequences, where itemset-sequence id = 1 has size s = 3,
length l = 4 and consists of three elements (itemsets): (1, 3), 1 and 1. Given
minRelSup = 0.5, s = [(1, 3), 1] is a frequent sequential pattern that is con-
tained in itemset-sequences: id = 1, 3, where rsupS(s) = 0.5.

(cid:3)) if s (cid:3) s

290

R. Gwadera and F. Crestani

Table 1. A collection of itemset-sequences

id itemset-sequence
0 [2, 0]
1 [(1, 3), 1, 1]
2 [2, 3, (0, 2)]
3 [(1, 3), (2, 3), 0, 1]

2.2 Signiﬁcance of Subsequence Patterns

(cid:2)

n

∃(e|w), where P

In this section we review the framework introduced in [6]. Let e = [e1, e2, . . . , em]
be a sequence of symbols. Let Ωn(e|w) =
i=1 Ii be a random variable that rep-
resent the actual frequency (support) of size w windows containing at least one
occurrence of e as a subsequence in an event sequence of size n + w − 1 (n shifts
of the window), where Ii is an indicator function equal to 1 if the i-th shift con-
tains e. Clearly E[Ωn(e|w)] = nP
∃(e|w) is the probability that a
window ending at a given position in the event sequence contains at least one oc-
currence of e as a subsequence. The superscript ∃ means “at least one occurrence
as a subsequence” and is used to distinguish this probability from a probability
of e as a string. Clearly, I1, I2, . . . , In is a sequence of dependent random vari-
ables because a given subsequence pattern occurring in the input sequence may
occur in many consecutive windows depending on its span. Therefore, because
of the sliding window overlap Ωn(e|w) does not have a Binomial distribution
meaning Var[Ωn(e|w)] (cid:8)= nP
be a
random variable that represents the actual relative frequency of size w windows
containing at least one occurrence of e as a subsequence in an event sequence,
where E[Ωn(e|w)] = P
Let W∃(e|w) be the set of all distinct windows of length w containing at least
one occurrence of pattern e as a subsequence. Then P
x∈W∃(e|w) P (x),
where P (x) is the probability of string x in a given Markov model. W∃(e|w)
can be enumerated using an enumeration graph. The enumeration graph for a
subsequence pattern e = [e1, e2, . . . , em] is shown in Figure 1. In particular for
the 0-order Markov reference model P

∃(e|w) and Var[Ωn(e|w)] ≤ 1
n P
∃(e|w) =

∃(e|w)). Let Ωn(e|w) = Ωn(e|w)

∃(e|w)(1 − P

∃(e|w)).

∃(e|w)(1 − P

(cid:2)

n

∃(e|w) can be expressed as follows
(cid:3)

m(cid:4)

(1 − P (ek))nk ,

(1)

∃

(e|w) = P (e)

P

w−m(cid:3)

(cid:2)

i=0

m
k=1 nk=i

k=1

n1

e1

0

n2

e2

1

e1

nm

em

Anm+1

e2

2

em

m

Fig. 1. Enumeration graph for a subsequence pattern e = [e1, e2, . . . , em], where e =
A − e and A is the alphabet. The exponents n1, . . . , nm+1 above the self-loops denote
the number of times the corresponding self-loops are selected.

Ranking Sequential Patterns with Respect to Signiﬁcance

291

(cid:5)

m

i=1 P (ei) and P (ei) is the probability of symbol ei in the refer-
where P (e) =
∃(e|w) is the probability of getting from state 0 to m in w
ence model. Then P
steps. Paper [6] also presented an eﬃcient O(w2) dynamic programming algo-
∃(e|w) from (1). It was shown that if Var[Ωn(e|w)] > 0
rithm for computing P
then Ωn(e|w) satisﬁes the Central limit theorem (CLT) and this fact was used
to set a lower and upper signiﬁcance thresholds for Ωn(e|w).

3 Problem Deﬁnition

The problem of ranking sequential patterns (itemset-sequences) with respect to
signiﬁcance can be deﬁned as follows.
Given: (I) collection of itemset-sequences S = {s(1), s(2),. . . , s(n)}, where
⊆A = {a1, a2,. . . , a|A|} and M = max1≤i≤n |s(i)|
s(i) = [s(i)
and (II) minimum relative support threshold minRelSup for sequential patterns.

2 ,. . . , s(i)|s(i)|], s(i)

1 , s(i)

t

Task: rank the discovered sequential patterns with respect to signiﬁcance.
Note that in our method the main purpose of the support threshold for se-

quential patterns is to limit the search space of possible signiﬁcant patterns.

4 Sequence-Wise Reference Model

The sequence-wise reference model treats itemsets as alphabet symbols and rep-
resents an independence model where itemsets occur independently of their order
in an itemset-sequence. In order to present the sequence-wise reference model we
introduce the element-wise representation of a collection of itemset-sequences,
that is a sequence of itemset-sequences R = [r(1), r(2), . . . , r(|R|)] over an item-
set alphabet Ξ = {ξ1, ξ2, . . . , ξ|Ξ|}, where r(i) is the i-th itemset-sequence and
∈ Ξ is the element (itemset) at time point t. As an example, Figure 2
r(i)
t
presents the element-wise sequence of itemset-sequences for the collection from
Table 1, where Ξ = {0, 1, 2, 3, (1, 3), (2, 3)} and the itemsets are represented as
decimal numbers. Note that for the sequence-wise reference model Ξ is provided
by the itemset-wise reference model and includes all non-empty subsets of A.

4.1 Generative Process

Now consider the sequence-wise reference model as a generative process, that
generates itemset-sequences in R as follows:
1. it ﬁrst generates the size of the itemset-sequence from a distribution α =
[α1, α2, . . . , αM ], where αm is the probability of generating an itemset-
sequence of size m and

2. it generates a sequence of itemsets r(i) of size m from distribution θ =
[θ1, θ2, . . . , θ|Ξ|], provided by the itemset-wise reference model, where θj =
P (r(i)

t = ξj), for ξj ∈ Ξ.

292

R. Gwadera and F. Crestani

Elements in itemset-sequences

s
e
c
n
e
u
q
e
s
-
t
e
s
m
e
t
I

1

2

3

4

4

10

4

1

2

8

10

12

2

5

1

2

t

1
0
0
1
0

2
1
0
0
0

s

m
e
t
I

0
1
2
3

Elements in itemset-sequences
1
0
1
0
1

1
0
1
0
1

2
0
0
1
1

2
0
1
0
0

3
0
1
0
0

3
1
0
1
0

1
0
0
1
0

2
0
0
0
1

3
1
0
0
0

4
0
1
0
0

t

sequence

representing

Fig. 2. Element-wise
of
the
itemset-sequences
collection of
itemset-sequences from
Table 1. The streams correspond to
itemset-sequences, where a decimal
number at a given time point corre-
sponds to an itemset (e.g., 10 = 21+3
for itemset (1, 3)).

Fig. 3. Item-wise multi-attribute se-
quence representing the collection from
Table 1. The streams correspond to
items, every time point corresponds to
an itemset, where digit 1 in the i-th
stream means a presence of the i-th
item.

Let P (r(i), m) be the joint probability of a particular itemset sequence r(i) of
size m to be generated by the process. Then given the independence assumption
of the presented generative process we factorize P (r(i), m) as follows

P (r(i), m) = αm · P (r(i)|m),

(2)

m

(cid:5)

t=1 P (r(i)

t ) is the probability of a particular itemset-sequence

where P (r(i)|m) =
r(i) to be generated given the size m.
We compute the parameters of the sequence-wise reference model from S
(ML estimator), where Nn(|s(i)| = m) is the
as follows: (I) αm = Nn(|s(i)|=m)
number of occurrences of itemset-sequences of size m in S and (II) θ is computed
form the itemset-wise reference model, that is presented in Section 5 and whose
purpose is to provide decorrelated frequencies of itemsets. Note that we could
compute θj as θj = Nn(ξj )
(ML estimator), where nΞ is the number of itemsets
in S and Nn(ξj) is the number of occurrences of itemset ξj in S. However the
ML estimator for the itemsets does not provide decorrelated frequencies.

nΞ

n

4.2 Relative Frequency

In this section we consider occurrences of a sequential pattern as a subsequence
(gaps between elements of the sequential pattern are allowed) in the sequence-
wise reference model represented by its element-wise representation R. Let Ωn(s)
be a random variable representing the actual relative frequency of a sequential
pattern s occurring as a subsequence in R. Recall that the relative frequency of
a sequential pattern s is equal to the fraction of itemset-sequences in R that
contain it as a subsequence. This means that even if s occurs many times
(cid:3) ∈ R we count it only as one occurrence. Let
in a given itemset-sequence s
i=1 Ii be a random variable that represent the actual frequency of
Ωn(s) =
s occurring as a subsequence in R (supR(s)), where Ii is an indicator function
equal to 1 if the i-th itemset-sequence contains s. Then clearly, I1, I2, . . . , In is

(cid:2)
n

Ranking Sequential Patterns with Respect to Signiﬁcance

293

a sequence of independent random variables because occurrences of a pattern
as a subsequence in itemset-sequences are independent of each other. Therefore,
Ωn(s) has the Binomial distribution and Ωn(s) = Ωn(s)
is a Binomial propor-
∃(s)(1−P
∃(s)) and P
∃(s) is the
tion, where E[Ωn(s)] = P
probability that s exists as a subsequence in an itemset-sequence in R. Thus,
clearly Ωn(s) and Ωn(s) both satisfy CLT. However, since itemset-sequences
∃(s) depends on the distribution of the sizes
have variable sizes, for a given s, P
of itemset-sequences in R.
∃(s, m) be the joint probability that an itemset-sequence s of size |s|
(cid:3) of size m ≥ |s| in R. Then

∃(s), Var[Ωn(s)] = 1
n P

exists as a subsequence in another itemset-sequence s
following (2) we factorize P

Let P

n

P

(3)
∃(s|m) is the probability that s occurs given an itemset-sequence of size
∃(s) we marginalize from (3) as

where P
m in R. In order to obtain the formula for P
follows:

∃(s, m) as follows
(s, m) = αm · P
∃

∃

(s|m),

∃

P

(s) =

M(cid:3)

m=|s|

αm · P

∃

(s|m).

(4)

Finally, the formula for P

(cid:3) ∈ R for which |s

(cid:3) depends on the size of s

(cid:3)| ≥ |s|. In other words, P

∃(s) is expressed as a discrete mixture model, where the mixing coeﬃ-
Thus, P
cients (α1, α2,. . . , αM ) model the fact that an occurrence of s as a subsequence
(cid:3) and may possibly occur in any
in an itemset-sequence s
∃(s) is a weighted
itemset-sequence s
combination of contributions from itemset-sequences of all possible relevant sizes
in R.
∃(s|m) for an itemset-sequence s = [s1, s2,. . . , s|s|]
given an itemset-sequence of size m in R can be obtained as follows. Let Xi =
(cid:6)
ξj be the set of all supersets of itemset si in itemset alphabet Ξ.
ξj∈Ξ,si⊆ξj
Then clearly, the enumeration graph for W∃(s|m) can be obtained from the
enumeration graph for a sequence of items e = [e1, e2, . . . , em] by substitut-
ing X1,X2, . . . ,X|s| for items e1, e2, . . . , em in Figure 1. Then the formula for
∃(s|m) can be obtained from (1) by substituting marginal probabilities of item-
sets PM(si) = P (Xi) for probabilities of items in (1). Thus, P
∃(s|m) in a 0-order
Markov sequence-wise reference model, can be obtained from (1) as follows:

P

∃

(s|m) = PM(s)

P

m−|s|(cid:3)

(cid:3)

|s|(cid:4)

i=0

(cid:2) |s|

k=1 nk=i

k=1

(1 − PM(sk))nk ,

(5)

(cid:5)|s|

where PM(s) =
i=1 PM(si) and PM(si) is the marginal probability computed
from the itemset-wise reference model. Formula (5) can be computed in O(m2)
∃(s)
using the dynamic programming algorithm given in [6]. Thus computing P
from (4) takes O(M 3) time.

The presented sequence-wise reference model can be easily extended to
more application speciﬁc models. As a ﬁrst extension, we could assume that

294

R. Gwadera and F. Crestani

itemset-sequences are generated using a Markov model and use the algorithm
∃(s|m) from [7] for Markov models. As another extension, we
for computing P
could assume that the distribution of itemsets θ depends on the size of an
itemset-sequence (e.g., itemsets having large cardinality are more likely to occur
in shorter itemset-sequences).

5 Itemset-Wise Reference Model

The itemset-wise reference model treats itemsets as binary vectors and provides
decorrelated frequencies of itemsets. In order to present the itemset-wise refer-
ence model we introduce the item-wise representation, that is a multi-attribute
binary sequence B = {b(1), b(2), . . . , b(|A|)} of size |A|, where: b(i) is a binary
sequence corresponding to attribute (item) ai ∈ A and b(j)
∈ {0, 1} is the value
at time point t. Thus, B represents S as a sequence of time ordered itemsets.
Figure 3 presents the item-wise multi-attribute sequence for the collection from
Table 1.
Note that we do not consider empty itemsets (binary vectors consisting of all
zeros in B) because a lack of attributes is meaningless in our framework. There-
) (cid:8)=
fore the streams in B are inherently dependent, i.e., P (b(1)
(cid:5)
(|A|)
j=1 P (b(j)
t ) and the independence model is inappropriate in our framework.
Therefore we build a maximum entropy model of the form [4]

, . . . , b(|A|)

, b(2)

t

t

t

t

P (b(1)

t

, b(2)

t

, . . . , b

|A|
t

) = Z

⎛

⎝

|A|(cid:4)

i=1

μI(i)

i

⎞

⎠ μ

|A|−(cid:2) |A|
c

i=0 I(i)

,

(6)

where Z is the normalizing constant, I (i) is an indicator function equal to one
if b(i)
t = 1. We build (6) using Generalized Iterative Scaling (GIS) algorithm
by ﬁnding μi for i = 1, . . .|A|, μc and Z under constraints that empirical
)I (i)=
marginal probabilities of items are preserved, i.e,
P (b(i)
t = 1), where μc is the correction feature that ensures that the num-
ber of parameters (features) for every binary vector in (6) is constant. Let
t = 1) and
Sum =
let p(i)M = Z · ui
be the marginal proba-
bility of attribute i computed from the model given the current estimates of the
parameters.

), let p(i) = P (b(i)
j(cid:4)=i n(j)

, . . . , b(|A|)
|A|−1−(cid:2)
μ
c

t >0P (b(1)

∈{0,1},
(cid:2)

nj∈{0,1}

j(cid:8)=i μnj

, b(2)
(cid:12)

P (b(1)

, . . . , b

, b(2)

|A|
t

(cid:11)(cid:5)

(i)
b
t

(cid:2)

(cid:2)

(cid:2)

(i)

i b

t

t

t

t

t

j

(cid:13)

μc = 1, Z = 1

The iterative scaling algorithm proceeds as follows: (I) initialization: μi = 1,
Sum; and (II) iteration: repeat for i = 1 to |A| do begin
(cid:14) 1|A|
Sum end until for i = 1 to

p(i)
μn+1
(i)M
p
|A| |p
< ε. Thus, the maximum entropy model satisﬁes our requirements:
(I) it preserves empirical marginal probabilities; (II) it is deﬁned only for all

= μn
i
(i)M−p(i)|
p(i)

(cid:16) 1|A| , Z = 1

c = μn
c

Z·Sum

, μn+1

(cid:15)

1

i

Ranking Sequential Patterns with Respect to Signiﬁcance

295

Table 2. Comparison of marginal probabilities of the itemsets of size two and the
probability of the empty itemset for the collection from Table 1 obtained using the
independence model and the maximum entropy model

Itemset Independence model Maximum Entropy model
(0,2)
(1,3)
(2,3)
()

2.35e-01
3.81e-01
3.08e-01
0

8.33e-02
1.39e-01
1.11e-01
1.94e-01

non-empty subsets of A and (III) it gives as much independence to the attribute
streams as possible given the constraints.

Table 2 presents marginal probabilities of the itemsets of size two from Figure
3 obtained using the independence model and the maximum entropy model.
Thus, Table 2 shows the following facts: (I) although the empty itemset does not
occur in Figure 3 the independence model assigns a bigger probability (1.94e−01)
to the empty itemset than to the occurring itemsets of size two; and (II) the ME
model, as expected, assigns greater probabilities to the occurring itemsets than
the independence model.

6 Ranking Algorithm
Given a collection of itemset-sequences S = {s(1), s(2),. . . , s(n)}, where
s(i) = [s(i)
2 ,. . . , s(i)|s(i)|], the ranking algorithm proceeds as follows:

1 , s(i)

sequential patterns F.

1. run PreﬁxSpan for a given value of minRelSup to obtain a set of frequent
and Nn(|s(i)| = m)
2. compute α = [α1, α2, . . . , αM ], where αm = Nn(|s(i)|=m)
3. for every frequent sequential pattern s =[s1, s2,. . . , s|s|], where s ∈ F and

n
is the number of itemset-sequences of size m in S.
rsupS(s) ≥ minRelSup do the following:
(a) compute the marginal probability vector [θ1, θ2,. . . , θ|s|] (θi = PM(si))

for elements of s from the itemset-wise reference model.

∃(s) from (4) and compute the signiﬁcance rank as follows

(b) compute P

sigRank(s) =

√
n(rsupS(s)−P
√

(s))
P ∃(s)(1−P ∃(s))

∃

.

The reference model will be violated in S in two cases: (I) the sequence-wise
reference model is violated by correlated itemsets and (II) the itemset-wise ref-
erence model is violated by correlated items in itemsets.

7 Experiments

In this section we present our experimental results on the multi-stream of news
stories of size 224062 stories that have been retrieved, via RSS feeds, from

296

R. Gwadera and F. Crestani

the following thirteen news sources: ABC news (ABC), Aljazeera (ALJ), As-
sociated Press (AP), British Broadcast Co. (BBC), Canadian Broadcast Co.
(CBC), Xinhua News Agency (CHV), Central News Agency Taiwan (CNE),
CNN, MSNBC, Reuters (REU), United Press International (UPI), RIA Novosti
(RIA) and Deutsche Welle (DW). The stories were retrieved over a period of
thirteen months from the 12-th of November 2008 to the 3rd of January 2010.
We implemented a clustering algorithm that uses a time-window of a given dura-
tion (e.g., 24 hours) and is an incremental variant of a non-hierarchical document
clustering algorithm using a similarity measure based on nearest neighbors. We
ran the algorithm for the following parameters: (I) the time-window size w = 24
hours; (II) the document similarity threshold τd = 0.5 that is used to identify
nearest neighbors for a new arriving document to the window and (III) the time
quantization step size Qt = 1 hour. As a result we obtained a collection of
itemset-sequences S of size |S| = 32464, where there are 109964 itemsets, the
maximum itemset-sequence size M = 25, the average itemset-sequences size is
3.5 and the average itemset size is 1.2.

.content. The publishing timestamp d(i)
t

7.1 From Clusters to Itemset-Sequences
Let D = {d(1), d(2), . . . , d(|D|)} be a multi-stream of news stories (documents),
where d(i)
is a document in stream i at a time point t and has three attributes: (I)
t
the exact publishing timestamp d(i)
.timestamp; (II) stream identiﬁer d(i)
.stream
t
t
= i; and (III) text content d(i)
.timestamp
is unique in each stream d(i). Let C = [d1, d2, . . . , d|C|] be a cluster of documents
t
(reporting the same event in our case) deﬁned as a sequence of documents or-
dered with respect to publishing timestamp di.timestamp. We convert C to an
itemset-sequence s = [s1, s2, . . . , s|s|], where si ⊆ A and A = {0, 1, . . . ,|D| − 1}
is the set of all stream identiﬁers of the news sources in D. As a result of the con-
version each itemset si contains stream identiﬁers of documents with the same
timestamp (di.timestamp) and the itemset-sequence s is ordered with respect
to the timestamps of the itemsets. As an example consider itemset-sequence
[(1, 3), 1, 1] in Table 1, where s1 = (1, 3) means that two documents: the ﬁrst
from source 1 and the second from source 3 were published (within the time
granularity Qt) before a document from streams 1 and 1 respectively. Further-
more, for every itemset-sequence, we recorded content similarity between the
stories corresponding to its elements in terms of the cosine similarity measure.
In order to asses the nature of content similarity between documents in a given
itemset-sequence s we deﬁne the average content similarity AvgSimS(s) and the
variance of the content similarity V arSimS(s) between documents in an itemset-
sequence s of length l occurring as a subsequence over the whole collection of
itemset-sequences S are expressed as follows

AvgSimS(s) =

2 · supS(s)

l2 − l

(cid:3)

jk−1(cid:3)

l(cid:3)

s(cid:2)∈S,s(cid:10)s(cid:2)

k=1

i=j1

sim(s

(cid:3)
i, s

(cid:3)
jk

)

(7)

Ranking Sequential Patterns with Respect to Signiﬁcance

297

Table 3. Baseline: Top-20 most
frequent sequential patterns of size
greater than one

Table 4. Top-20 most signiﬁcant se-
quential patterns for minRelSup =
0.01

Pattern

rsupS
1 [AP, MSNBC]
1.78e-01
2 [MSNBC, UPI] 1.20e-01
1.06e-01
3 [BBC, UPI]
1.06e-01
4 [AP, UPI]
1.06e-01
5 [REU, UPI]
6 [AP, ABC]
9.03e-02
8.78e-02
7 [BBC, ALJ]
8.71e-02
8 [REU, BBC]
8.56e-02
9 [CNN, UPI]
10 [REU, CNE]
8.55e-02
11 [REU, MSNBC] 8.41e-02
12 [BBC, CNE]
8.15e-02
13 [ABC, UPI]
8.09e-02
14 [ABC, MSNBC] 8.07e-02
7.87e-02
15 [CNE, UPI]
7.83e-02
16 [AP, REU]
17 [BBC, REU]
7.51e-02
18 [MSNBC, REU] 7.49e-02
19 [MSNBC, ABC] 7.20e-02
7.05e-02
20 [CNE, BBC]

Pattern

sigRank

1 [BBC, ALJ, ALJ, ALJ] 25.5
2 [ALJ, ALJ, ALJ, CNE] 19.1
3 [CNE, ALJ, ALJ, ALJ] 18.6
4 [BBC, CNE, ALJ, ALJ] 18.1
5 [ALJ, ALJ, CNE, ALJ] 17.7
6 [CNE, ALJ, ALJ, CNE] 16.4
7 [BBC, ALJ, ALJ, UPI] 16.3
8 [BBC, CNE, ALJ, ALJ] 16.1
9 [AP, MSNBC]
15.7
10 [ALJ, ALJ, BBC, ALJ] 15.1
11 [ALJ, CNE, ALJ, ALJ] 14.5
12 [CNE, BBC, ALJ, ALJ] 14.2
13 [BBC, ALJ, ALJ, BBC] 14.1
14 [ALJ, BBC, ALJ, ALJ] 13.9
15 [BBC, ALJ, ALJ, CNN] 13.2
13.1
16 [ALJ, ALJ, CBS]
17 [ALJ, ALJ, ALJ, UPI]
12.9
18 [REU, ALJ, ALJ, ALJ] 12.8
19 [ALJ, ALJ, ALJ, BBC] 12.7
20 [BBC, ALJ, CNE, ALJ] 12.4

and

V arSimS(s) =

2 · supS(s)

l2 − l

(cid:3)

l(cid:3)

s(cid:2)∈S,s(cid:10)s(cid:2)

k=1

i=j1

jk−1(cid:3)

(AvgSimS(s) − sim(s

(cid:3)
i, s

(cid:3)
jk))2, (8)

where j1 ≤ j2 . . . ≤ jl are the positions where s occurs in s
(cid:3) as a subsequence
and sim(di, dj) is the cosine similarity or content similarity between documents
i and j. Thus, (7) computes the average content similarity over all itemset-
sequences containing s as a subsequence. We also use StdDevSimS(s) to denote
(cid:17)

V arSimS(s).

7.2 Baseline: Most Frequent Patterns

As a baseline against which we compare the performance of the ranking algo-
rithm we use the top-20 most frequent sequential patterns of size greater than
one, where we also removed patterns containing the same symbol, which corre-
sponds to frequent updates of the same news event. Table 3 presents the top-20
most frequent sequential patterns of size greater than one.

7.3 Signiﬁcant Patterns

In the ﬁrst experiment we rank the top-20 most frequent patterns from Table
3 with respect to signiﬁcance. Figure 4 presents the results. As it turns out the
most frequent pattern in Table 3 is also the most signiﬁcant one but for the
following patterns there is not any obvious relationship between the signiﬁcance
rank and the frequency rank. The dependency between AP and MSNBC can be
explained by the fact that as we saw in the recorded stories MSNBC is reusing
some content from AP.

 

298

R. Gwadera and F. Crestani

Top−20 most frequent sequential patterns and their significance rank values

Top−20 most significant sequential patterns and their content similarity

30

25

20

15

10

5

l

e
u
a
v
 
k
n
a
r
 
e
c
n
a
c
i
f
i

n
g
s

i

0

0

2

4

6

8

10

12
frequency rank

14

16

18

20

Fig. 4. Frequency rank (x-axis) versus
signiﬁcance rank (y-axis) for the top-20
most frequent sequential patterns from
Table 3

1

0.9

0.8

0.7

0.6

0.5

AvgSimS
StdDevSimS

y
t
i
r
a

l
i

i

m
s
 
t

n
e

t

n
o
c

0.4

0.3

0.2

0.1

0

 
0

2

4

6

8

10

12
significance rank

14

16

18

20

Fig. 5. Signiﬁcance rank (x-axis) ver-
sus content similarity (the average and
the standard deviation) (y-axis) for the
top-20 most signiﬁcant sequential pat-
terns from Table 4

In the second experiment we set minRelSup = 0.01 and found the most
signiﬁcant over-represented (sigRank > 0) sequential patterns. Table 4 presets
the top-20 most signiﬁcant sequential patterns, where among the top patterns
we removed patterns containing the same symbol and patterns having signiﬁcant
supersequences. Note however that the top-20 most signiﬁcant patterns for the
whole collection may not be the same since the patterns in Table 4 were obtained
using minRelSup = 0.01. In general the lower the value of minRelSup the
higher the chance that the reference model will discover long signiﬁcant patterns
having low support. By comparing the results from Table 3 and from Table 4
we can make the following observations: (I) the most signiﬁcant patterns are
generally longer than the most frequent ones since the sequence-wise reference
model leverages rank of correlated longer patterns and (II) there is a prevalence
of patterns involving BBC in the ﬁrst position and ALJ in the following positions.
The dependency between BBC and ALJ may be related to the fact that, as we
found out from the BBC web site, BBC signed a news exchange agreement with
ALJ and as the pattern suggests this exchange seems to be really “one-way”
from BBC to ALJ. Furthermore, ALJ tends to provide many updates of the
same news event. Also, although [AP, MSNBC] is the most frequent pattern it
has signiﬁcance rank nine in Table 4 as a result of the reference model leveraging
rank of the longer patterns involving BBC and ALJ.

Figure 5 presents a graph of the signiﬁcance rank (x-axis) versus the average
content similarity AvgSimS and the standard deviation StdDevSimS (y-axis)
for the top-20 most signiﬁcant sequential patterns from Table 4. Figure 5 shows
two facts: (I) the average content similarity is above the document similarity
threshold τd = 0.5 and (II) the value of the standard deviation is relatively low
for all patterns. These results suggest that temporally correlated news streams
tend to be also correlated with respect to their content.

Ranking Sequential Patterns with Respect to Signiﬁcance

299

8 Conclusions

We presented a reliable general method for ranking frequent sequential patterns
(itemset-sequences) with respect to signiﬁcance. We demonstrated the appli-
cability of the presented method on a multi-stream of news stories that was
gathered from RSS feeds of the major world news agencies. In particular we
showed that there are strong dependencies between the news sources in terms of
temporal sequential patterns of reporting the same news events and content sim-
ilarity, where the frequency and signiﬁcance rank are correlated with the content
similarity.

References

1. Agrawal, R., Srikant, R.: Mining sequential patterns. In: ICDE, pp. 3–14 (1995)
2. Allan, J.: Topic Detection and Tracking: Event-Based Information Organization.

Kluwer Academic Publishers, Norwell (2002)

3. Brin, S., Motwani, R., Silverstein, C.: Beyond market baskets: Generalizing associa-
tion rules to correlations. In: Proceedings ACM SIGMOD International Conference
on Management of Data, May 1997, pp. 265–276 (1997)

4. Darroch, J., Ratcliﬀ, D.: Generalized iterative scaling for log-linear models. The

Annals of Mathematical Statistics 43(5), 1470–1480 (1972)

5. Guan, E., Chang, X., Wang, Z., Zhou, C.: Mining maximal sequential patterns. In:
2005 International Conference on Neural Networks and Brain, pp. 525–528 (2005)
6. Gwadera, R., Atallah, M., Szpankowski, W.: Reliable detection of episodes in event
sequences. In: Third IEEE International Conference on Data Mining, November
2003, pp. 67–74 (2003)

7. Gwadera, R., Atallah, M., Szpankowski, W.: Markov models for discovering signif-
icant episodes. In: SIAM International Conference on Data Mining, pp. 404–414
(2005)

8. Gwadera, R., Crestani, F.: Mining and ranking streams of news stories using cross-
stream sequential patterns. In: CIKM 2009: Proceedings of the 18th International
Conference on Information and Knowledge Management, Hong Kong, October
2009, pp. 1709–1712 (2009)

9. Han, J., Cheng, H., Xin, D., Yan, X.: Frequent pattern mining: current status and

future directions. Data Mining and Knowledge Discovery 15(1) (2007)

10. Pei, J., Han, J., Mortazavi-Asl, B., Wang, J., Pinto, H., Chen, Q.: Mining sequential
patterns by pattern-growth: The preﬁxspan approach. TKDE 16(11) (November
2004)

11. Tatti, N.: Maximum entropy based signiﬁcance of itemsets. KAIS 17(1), 57–77

(2007)

12. Yan, X., Han, J., Afshar, R.: Clospan: Mining closed sequential patterns in large

datasets. In: SDM, pp. 166–177 (2003)


