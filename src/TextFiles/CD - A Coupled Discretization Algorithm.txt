CD: A Coupled Discretization Algorithm

Can Wang1, Mingchun Wang2, Zhong She1, and Longbing Cao1

1 Centre for Quantum Computation and Intelligent Systems

Advanced Analytics Institute, University of Technology, Sydney, Australia

{canwang613,zhong2024,longbing.cao}@gmail.com

2 School of Science, Tianjin University of Technology and Education, China

mchwang123@163.com

Abstract. Discretization technique plays an important role in data min-
ing and machine learning. While numeric data is predominant in the real
world, many algorithms in supervised learning are restricted to discrete
variables. Thus, a variety of research has been conducted on discretiza-
tion, which is a process of converting the continuous attribute values into
limited intervals. Recent work derived from entropy-based discretization
methods, which has produced impressive results, introduces information
attribute dependency to reduce the uncertainty level of a decision ta-
ble; but no attention is given to the increment of certainty degree from
the aspect of positive domain ratio. This paper proposes a discretization
algorithm based on both positive domain and its coupling with informa-
tion entropy, which not only considers information attribute dependency
but also concerns deterministic feature relationship. Substantial exper-
iments on extensive UCI data sets provide evidence that our proposed
coupled discretization algorithm generally outperforms other seven ex-
isting methods and the positive domain based algorithm proposed in this
paper, in terms of simplicity, stability, consistency, and accuracy.

1

Introduction

Discretization is probably one of the most broadly used pre-processing techniques
in machine learning and data mining [6, 13] with various applications, such as
solar images [2] and mobile market [14]. By using discretization algorithms on
continuous variables, it replaces the real distribution of the data with a mixture
of uniform distributions. Generally, discretization is a process that transforms
the values of continuous attributes into a (cid:12)nite number of intervals, where each
interval is associated with a discrete value. Alternatively, this process can be also
viewed as a method to reduce data size from huge spectrum of numeric variables
to a much smaller subset of discrete values.

The necessity of applying discretization on the input data can be due to dif-
ferent reasons. The most critical one is that many machine learning and data
mining algorithms are known to produce better models by discretizing continu-
ous attributes, or only applicable to discrete data. For instance, rule extraction
techniques with numeric attributes often lead to build rather poor sets of rules
[1]; it is not always realistic to presume normal distribution for the continuous

2

C. Wang et al.

values to enable the Naive Bayes classi(cid:12)er to estimate the frequency probabilities
[13]; decision tree algorithms cannot handle numeric features in tolerable time
directly, and only carry out a selection of nominal attributes [9]; and attribute
reduction algorithms in rough set theory can only apply to the categorical val-
ues [10]. However, real-world data sets predominantly consist of continuous or
quantitative attributes. One solution to this problem is to partition numeric do-
mains into a number of intervals with corresponding breakpoints. As we know,
the number of di(cid:11)erent ways to discretize a continuous feature is huge [6], in-
cluding binning-based, chi-based, fuzzy-based [2], and entropy-based methods
[13], etc. But in general, the goal of discretization is to (cid:12)nd a set of breakpoints
to partition the continuous range into a small number of intervals with high
distribution stability and consistency, and then to obtain a high classi(cid:12)cation
accuracy. Thus, di(cid:11)erent discretization algorithms are evaluated in terms of four
measures: simplicity [5], stability [3], consistency [6], and accuracy [5, 6].

Of all the discretization methods, the entropy-based algorithms are the most
popular due to both their high e(cid:14)ciency and e(cid:11)ectiveness [1, 6], including ID3,
D2, and MDLP, etc. However, this group of algorithms only concern the decrease
of uncertainty level by means of information attribute dependency in a decision
table [5], which is not rather convincing. From an alternative perspective, we
propose to improve the discretization quality by increasing the certainty degree
of a decision table in terms of deterministic attribute relationship, which is re-
vealed by the positive domain ratio in rough set theory [10]. Furthermore, based
on the rationales presented in [8, 12], we take into account both the decrement
of uncertainty level and increment of certainty degree to induce a Coupled Dis-
cretization (CD) algorithm. This algorithm selects the best breakpoint according
to the importance function composed of the information entropy and positive
domain ratio in each run. The key contributions are as follows:

- Consider the information and deterministic feature dependencies to induce
the coupled discretization algorithm in a comprehensive and reasonable way.
- Evaluate our proposed algorithm with existing classical discretization meth-
ods on a variety of benchmark data sets from internal and external criteria.
- Develop a way to de(cid:12)ne the importance of breakpoints (cid:13)exibly with our

fundamental building blocks according to speci(cid:12)c requirements.

- Summarize a measurement system, including simplicity, stability, consis-

tency, and accuracy, to evaluate discretization algorithm completely.

The paper is organized as follows. Section 2 brie(cid:13)y reviews the related work.
In Section 3, we describe the problem of discretization within a decision table.
Discretization algorithm based on information entropy is speci(cid:12)ed in Section 4.
In Section 5, we propose the discretization algorithm based on positive domain.
Coupled discretization algorithm is presented in Section 6. We conduct extensive
experiments in Section 7. Finally, we end this paper in Section 8.

2 Related Work

In earlier days, simple methods such as Equal Width (EW ) and Equal Frequency
(EF ) [6] are used to discretize continuous values. Afterwards, the technology for

CD: A Coupled Discretization Algorithm

3

discretization develops rapidly due to the great need for e(cid:11)ective and e(cid:14)cient
machine learning and data mining methods. From di(cid:11)erent perspectives, dis-
cretization methods can be classi(cid:12)ed into distinct categories. A global method
uses the entire instance space to discretize, including Chi2 and ChiM [6], etc.;
while a local one partitions the localized region of the instance space [5], for
instance, 1R. Supervised discretization considers label information such as 1R
and MDLP [1]; however, unsupervised method does not, e.g., EW, EF. Splitting
method such as MDLP proceeds by keeping on adding breakpoints, whereas
the merging approach by removing breakpoints obtains bigger intervals, e.g.,
Chi2 and ChiM. The discretization method can also be viewed as dynamic or
static by considering whether a classi(cid:12)er is incorporated during discretization,
for example, C4.5 [6] is a dynamic way to discretize continuous values when
building the classi(cid:12)er. The last dichotomy is direct vs. incremental, while di-
rect method needs the pre-de(cid:12)ned number of intervals, including EW and EF ;
incremental approach requires an additional criterion to stop the discretization
process, such as MDLP and ChiM [3]. In fact, our proposed method CD is
a global-supervised-splitting-incremental algorithm, and comparisons with the
aforementioned classical methods are conducted in Section 7.

3 Problem Statement

∪

of four tuples (U; C

In this section, we formalize the discretization problem within a decision table, in
which a large number of data objects with the same feature set can be organized.
A Decision Table is an information and knowledge system which consists

D; V; f ). U = {u1;··· ; um} is a collection of m objects.
C = {c1;··· ; cn} and D are condition attribute set and decision attribute set,
respectively. VC is a set of condition feature values, VD is a set of decision
D) → V
attribute values, and the whole value set is V = VC
is an information function which assigns every attribute value to each object.
D ̸= ∅ if there is at least one decision feature d ∈ D. The entry xij is the value of
continuous feature cj (1 ≤ j ≤ n) for object ui (1 ≤ i ≤ m). If all the condition

VD. f : U × (C

∪

∪

attributes are continuous, then we call it a Continuous Decision Table.

∗

∗
∗

D; V; f ) be a continuous decision table, S(P ) = (U; C

Let S = (U; C
; f
is the discretized condition attribute, V

D;
) is the Discretized Decision Table when adding breakpoint set P , where
V
is the attribute value set composed
C
of discretized values V
is
the discretized information function. For simplicity, we consider only one decision
attribute d ∈ D. Below, a consistent discrete decision table is de(cid:12)ned:

∗
C and decision value VD, and f

: U × (C

D) → V

∗∪

∗

∗

∗∪

∗

∪

∗∪

∗

∗

) is consis-
De(cid:12)nition 1 A discrete decision table S(P ) = (U; C
tent if and only if any two objects have identical decision attribute value when
they have the same condition attribute values.

D; V

; f

In fact, the discretization of a continuous decision table S is the search of
a proper breakpoint set P , which makes discretized decision table S(P ) consis-
tent. In this process, di(cid:11)erent algorithms result in distinct breakpoint sets, thus

4

C. Wang et al.

correspond to various discretization results. Chmielewski and Grzymala-Busse
[5] suggest three guidelines to ensure successful discretization, that is complete
process, simplest result and high consistency. Thus, among all the breakpoints,
we strive to obtain the smallest set of breakpoints which make the least loss on
information during discretization.

4 Discretization Algorithm based on Information Entropy

In this section, we present a discretization method which uses class information
entropy to evaluate candidate breakpoints in order to select boundaries [6]. The
discretization algorithm based on entropy (IE ) is associated with the information
gain of objects divided by breakpoints to measure the importance of them.
De(cid:12)nition 2 Let W ⊆ U be the subset of objects which contains |W| ob-
jects. kt denotes the number of the objects whose decision attribute values are
yt(1 ≤ t ≤ |d|), where |d| is the number of distinct decision values. Then the

class information entropy of W is de(cid:12)ned as follows:

|d|∑

H(W ) = −

kt
|W|

t=1

pt log2 pt; where pt =

(4.1)
Note that H(W ) ≥ 0. Smaller H(X) corresponds to lower uncertainty level
of the decision table [5, 6], since some certain decision attribute values play the
leading role in object subset W . In particular, H(W ) = 0 if and only if all the
objects in subset W have the same decision attribute value.

For a discretized decision table S(P ), let W1; W2;··· ; Wr be the sets of

equivalence classes based on the identical condition attribute values. Then, the
class information entropy of the discretized decision table S(P ) is de(cid:12)ned as
|Wi|
|U| H(Wi). Based on De(cid:12)nition 1, we obtain the relationship
H(S(P )) =
between entropy and consistency as follows. The proof is shown in the Appendix.

r
i=1

∑

Theorem 1 A discretized decision table S(P ) is consistent if H(S(P )) = 0.

After the initial partition, H(S(P )) is usually not equal to 0, which means
S(P ) is not consistent. Accordingly, we need to select breakpoints from candidate

set Q = {q1; q2;··· ; ql}, and it is necessary to measure the importance of every

∪{qi})
∪{qi} to
∪{qi})). The existing standard [6] to measure the importance of

element of Q to determine which one to choose in the next step. Let S(P
be the discretized decision table when inserting the breakpoint set P
the continuous decision table S, and the corresponding class information en-
tropy is H(S(P
breakpoint qi is de(cid:12)ned as:

∪

H(qi) = H(S(P )) − H(S(P

{qi})):

(4.2)

Note that the greater the decrease H(qi) of entropy, the more important the
breakpoint qi. Since H(S(P )) is a constant value for every qi(1 ≤ i ≤ l), then

∪{qi})), the larger probable the breakpoint qi

the smaller the entropy H(S(P
will be chosen.

CD: A Coupled Discretization Algorithm

5

5 Discretization Algorithm based on Positive Domain

Alternatively, we propose another discretization method incorporated with rough
set theory to select breakpoints to partition the continuous values. The dis-
cretization algorithm based on positive domain (PD) is built upon the indiscerni-
bility relations induced by the equivalence classes to evaluate the signi(cid:12)cance of
the breakpoints. Firstly, we recall the relevant concept in rough set theory [10].

De(cid:12)nition 3 Let U be a universe, P; Q are the equivalence relations over set
U , then the Q positive domain (or positive region) of P is de(cid:12)ned as:

P OSP (Q) =

W∈U=Q

{u : u ∈ U ∧ [u]P ⊆ W};

(5.1)

∪

∗∪

where W ∈ U=Q is the equivalence class based on relation Q, [u]P is the equiva-

lence class of u based on relation P .

∗

∗

∗

D; V

In the discretized decision table S(P ) = (U; C

be the
equivalence relation of \two objects have the same condition attribute values",
let D denote the equivalence relation of \two object have the same decision
attribute value". Then, the positive domain ratio of the decision table S(P )
is R(S(P )) = |P OSC∗(D)|=|U|. Note that |U| is the number of objects, and
0 ≤ R(S(P )) ≤ 1. The greater the ratio R(S(P )), the higher the certainty level
of discretized decision table [8, 10]. Below, we reveal the consistency condition
for the PD algorithm. The proof is also shown in the Appendix.

), let C

; f

Theorem 2 A discretized decision table S(P ) is consistent if R(S(P )) = 1.

Similarly, we usually have R(S(P )) ̸= 1, that is to say, S(P ) is not consistent
after initialization. Thus, it is necessary to choose breakpoints from candidate
set Q = {q1; q2;··· ; ql} according to the signi(cid:12)cance order of all the candidate

breakpoints for the next insertion. Let R(S(P
main ratio of the discretized decision table S(P
the importance of breakpoint qi as:

R(qi) = R(S(P

∪{qi})) denote the positive do-
∪{qi}). We could then de(cid:12)ne

{qi})) − R(S(P )):

∪
∪{qi})), the more important this break-

(5.2)

Note that the larger the increase R(qi) of ratio, the greater importance of the
breakpoint qi. Since R(S(P )) is a constant for each candidate qi(1 ≤ i ≤ l),

therefore, the larger the ratio R(S(P
point qi.

6 Discretization Algorithm based on the Coupling

Discretization algorithms are considered in terms of information entropy and
positive domain in Section 4 and Section 5, respectively. In a discretized de-
cision table, the information entropy measures the uncertainty degree from the

6

C. Wang et al.

perspective of information attribute relationship, while the positive domain ratio
reveals the certainty level with respect to the deterministic feature dependency
[8]. In this Section, we focus on both the information and deterministic attribute
dependencies to derive the coupled discretization (CD) algorithm.

Theoretically, Wang et. al [12] compared algebra viewpoint in rough set and
information viewpoint in entropy theory. Later on, Chen and Wang [4] applied
the aggregation of them to the hybrid space clustering. Similarly, by taking into
account both the increment of certainty level and the decrement of uncertainty
degree in a decision table, we consider to combine the PD and IE based methods
together to get the CD algorithm. This algorithm measures the importance of
breakpoints comprehensively and reasonably by aggregating the positive domain
ratio function R(·) and the class information entropy function H(·) together.
Alternatively, we propose one option to quantify the coupled importance:

De(cid:12)nition 4 For a discretized decision table S(P ), we have the coupled im-
portance of breakpoint set P be:

RH(P; qi) = k1R(qi) + k2H(qi);

(6.1)

(U; C

where R(qi) and H(qi) are the importance functions of breakpoint qi according
to (5.2) and (4.2), respectively; k1; k2 ∈ [0; 1] are the corresponding weights.

∪
For every condition attribute cj ∈ C in the continuous decision table S =
1j < ··· < x
′
′
mj = rci .Then, we
D; V; f ), its values are ordered as lcj = x
(1 ≤ i ≤ m − 1; 1 ≤ j ≤ n).
The process of the discretization algorithm based on the coupling of positive
domain and information entropy is designed as follows. The algorithm below
clearly shows that its computational complexity is O(m2n2) based on the loops.

de(cid:12)ne the candidate breakpoint as: qij =

′
i+1;j

x

′
ij +x
2

7 Experiment and Evaluation

In this section, several experiments are performed on extensive UCI data sets
to show the e(cid:11)ectiveness of our proposed coupled discretization algorithm. All
the experiments are conducted on a Dell Optiplex 960 equipped with an Intel
Core 2 Duo CPU with a clock speed of 2.99 GHz and 3.25 GB of RAM running
Microsoft Windows XP. For simplicity, we just assign the weights k1 = k2 = 0:5
in De(cid:12)nition 4 and Algorithm 1.

To the best of our knowledge, there are mainly four dimensions to evaluate

the quality [3, 5, 6] of discretization algorithms as follows:

- Stability: How to measure the overall spread of the values in each interval.
- Simplicity: The fewer the break points, the better the discretization result.
- Consistency: The inconsistencies caused by discretization should not be large.
- Accuracy: How discretization helps improve the classi(cid:12)cation accuracy.

CD: A Coupled Discretization Algorithm

7

Algorithm 1: Coupled Algorithm for Discretization

Data: Decision table S with m objects and n attributes (value xij), and k1; k2.
Result: breakpoint set P .
begin

breakpoint set P = ∅, candidate breakpoint set Q = ∅;
for j = 1 : n do

, Q = {q};

∪{qij}));

{x
ij} ← sort({xij});
′
for i = 1 : (m − 1) do

for j = 1 : n do

candidate breakpoint qij ← x

′
i+1;j

′
ij +x
2

Fix the (cid:12)rst breakpoint p1 ← argminqH(S(P
while H(S(P )) ̸= 0 ∧ R(S(P )) ̸= 1 do

calculate RH(P; qk) according to (6.1);

for candidate k = 1 : |Q| do
qmax ← argmaxqRH(P; qk);
P ← P ∪ {qmax}, Q ← Q\{qmax};

Output breakpoint set P ;
end

Discretization methods that adhere to internal criterion assign the best score to
the algorithm that produces break points with high stability and low simplicity;
while discretization approaches that adhere to external criterion compare the
results of the algorithm against some external benchmark, such as prede(cid:12)ned
classes or labels indicated by consistency and accuracy. From these two perspec-
tives, the experiments here are divided into two categories according to di(cid:11)erent
evaluation standards: internal criteria (stability, simplicity) and external criteria
(consistency, accuracy), as shown in Section 7.1 and Section 7.2, respectively.

7.1

Internal Criteria Comparison

With respect to the internal criterion, i.e., stability and simplicity, the goal in this
set of experiments is to show the superiority of our proposed coupled discretiza-
tion (CD) algorithm against some classic methods [6] such as Equal Frequency
(EF ), 1R, MDLP, Chi2, and Information Entropy-based (IE ) algorithms.

Speci(cid:12)cally, simplicity measure is described as the total number of intervals
(NOI ) for all the discretized attributes. More complicatedly, the stability mea-
sures are constructed from a series of estimated probability distributions for the
individual intervals constructed by incorporating the method of Parzen windows
[3]. As one of the induced measure, Attribute Stability Index (ASIj) is con-
structed from the weighted sum of the Stability Index (SIjk), which describes
the value distribution for each interval Ik of attribute cj. The measure SIjk fol-
lows 0 < SIjk < 1, if SIjk is near 0 then its values are next to the break points of
the interval Ik, while SIjk is close to 1 when its values are near the center of the
interval Ik. Furthermore, we have 0 < ASIj < 1, and the larger the ASIj value,

8

C. Wang et al.

∑

the more stable and better the discretization method. Here, we adapt this mea-
sure to be the Average Attribute Stability Index (AASI ), which is the weighted

sum of ASIj for all the attributes cj(1 ≤ j ≤ n): AASI =

n
j=1 ASIj=n.

The break points and intervals produced by the aforementioned six discretiza-
tion methods are then analyzed on 15 UCI data sets in di(cid:11)erent scales, ranging
from 106 to 1484 (number of objects). The results are reported in Table 1. As
discussed, larger AASI, smaller NOI indicate more stable and simpler character-
ization of the interval partition capability, which further corresponds to a better
discretization algorithm. The values in bold are the best relevant indexes for
each data. From Table 1, we observe that with the exception of only few items
(in italic), the other indexes all show that our proposed CD algorithm is better
than the other (cid:12)ve classical approaches (EF, 1R, MDLP, Chi2, IE ) in most cases
from the perspectives of stability and simplicity. It is also worth noting that our
proposed CD always outperforms the IE algorithm presented in Section 4 in
terms of stability, which veri(cid:12)es the bene(cid:12)t of aggregating the positive domain.

Table 1. Discretization Comparison with Stability and Simplicity

Average Attribute Stability Index

Number of Intervals

Data set

Tissue
Echo
Iris
Hepa
Wine
Glass
Heart
Ecoli
Liver
Auto

EF
0:57 0:56
0:44 0:52
0:33 0:28
0:16 0:21
0:59 0:59
0:63 0:50
0:25 0:25
0:51 0:29
0:66 0:24
0:58 0:35
Housing 0:50 0:64
Austra
0:28 0:15
0:17 0:13
Cancer
0:55 0:32
Pima
Yeast
0:47 0:17

IE
1R MDLP Chi2
0:15
0:64
0:32
0:50
0:39
0:67
0:18
0:19
0.83 0:60
0:75
0:56
0:31
0:34
0:51
0:54
0:74
0:69
0:67
0:65
0:61
0:56
0:32
0:36
0:22
0:22
0:20
0:60
0:55
0:30

0:27
0.67
0:66
0:21
0:65
0:80
0:40
0:62
0:78
0:69
0:72
0:39
0.27
0.73
0:62

CD
0.68
0:65
0.72
0.28
0:80
0.82
0.51
0.72
0.79
0.73
0.78
0.41
0:26
0:70
0.70

EF 1R MDLP Chi2
96
38
81
44
21
70
11
17
16
34
118
54
24
169 130
27
86
46
70
61
43
33
76
36
74
70
30
67
73
47
340
32
142
83
102
98
40
31
44
35
161
48
45
47
51

48
17
12
18
16
50
42
27
68
39
29
21
29
24
55

IE CD
24
26
14
19
14
14
19
21
13 13
20
34
28
26
28
30
22
24
31
39
25
13
26
17
18 18
29
33
51
49

7.2 External Criterion Comparison

In this part of our experiments, we focus on the other two aspects of evaluation
measures: consistency and accuracy. Two independent groups of experiments are
conducted with extensive data sets based on machine learning applications.

According to Liu et al. [6], consistency is de(cid:12)ned by having the least pattern
inconsistency count which is calculated as the number of times this pattern
appears in the data minus the largest number of corresponding class labels. Thus,
the fewer the inconsistency count, the better the discretization quality. Based on
the discretization results in Section 7.1, we compute the sum of all the pattern

CD: A Coupled Discretization Algorithm

9

inconsistency counts for all possible patterns of the original continuous feature
subset. Consistency evaluation is conducted on nine data sets with di(cid:11)erent
number of objects, ranging from 132 (Echo) to 768 (Pima) in an increasing
order. We also consider the other seven discretization methods for comparison,
i.e., Equal Frequency (EW ), EF, 1R, MDLP, ChiM, Chi2, and IE.

As shown in Fig. 1, the total inconsistency counts of IE and our proposed CD
are always 0 on all the data sets, because the stopping criteria are the consistency
conditions presented in Theorem 1 and Theorem 2. However, MDLP seems to
perform the worst in terms of the consistency index, and the inconsistency counts
of the other (cid:12)ve algorithms fall in the intervals between those of MDLP and CD
for all the data sets. These observations reveal the fact that algorithms IE and
CD are the most consistent candidates for discretization. While IE and AD both
indicate a surprisingly high consistency, in general, CD produces higher stability
(larger AASI ) and lower simplicity (smaller NOI ), as presented in Table 1.

Fig. 1. Discretization Comparison with Consistency.

How does discretization a(cid:11)ect the classi(cid:12)cation learning accuracy? As Liu
et al. [6] indicate, accuracy is usually obtained by running a classi(cid:12)er in cross
validation mode. In this group of experiments, two classi(cid:12)cation algorithms are
taken into account. i.e., Naive-Bayes, and Decision Tree (C4.5). A Naive Bayes
(NB ) classi(cid:12)er is a simple probabilistic classi(cid:12)er based on applying Bayes’ the-
orem with strong (naive) independence assumptions [13]. C4.5 is an algorithm
used to generate a decision tree (DT ) for classi(cid:12)cation. As pointed out in Section
1, the continuous attributes take too many di(cid:11)erent values for the NB classi(cid:12)er
to estimate frequencies; DT algorithm can only carry out a selection process
of nominal features [9]. Thus, discretization is rather critical for the task of
classi(cid:12)cation learning. Here, we evaluate the discretization methods with the
classi(cid:12)cation accuracies induced by NB and DT (C4.5 ), respectively.

Fig. 2 reports the results on 9 data sets with distinct data sizes, which vary
from 150 to 1484 in terms of the number of objects. As can be clearly seen from

EchoIrisHepaGlassEcoliLiverAutoHousingPima020406080100120140160180200Discretization Comparison with InconsistencyInconsistencyData Set  EWEF1RMDLPchiMchi2IE & CD10

C. Wang et al.

Fig. 2. Discretization Comparison with Accuracy.

this (cid:12)gure, the classi(cid:12)cation algorithms with CD, whether NB or DT, mostly
outperform those with other discretization methods (i.e., EW, EF, 1R, IE ) from
the perspective of average accuracy. That is to say, discretization algorithm CD
is better than others on classi(cid:12)cation qualities. Though for the data set Yeast,
the average accuracy measures induced by NB with CD are slightly smaller than
that with IE, the stability measures shown in Table 1 indicate that CD is better
than IE. Therefore, our proposed discretization algorithm CD is better than
other candidates with respect to the classi(cid:12)cation accuracy measure.

Besides, we lead a comparison among the algorithms presented in Section 4
(IE ), Section 5 (PD), and Section 6 (CD). Due to space limitations, only simplic-
ity and accuracy measures are considered to evaluate these three discretization
algorithms. Here, we take advantage of the k-nearest neighbor algorithm (k-NN )
[7], which is a method for classifying objects based on closest training examples
in the feature space. After discretization, (cid:12)ve data sets are used for classi(cid:12)cation
with both 1-NN and 3-NN, in which 70% of the data is randomly chosen for
training with the rest 30% for testing. As indicated in Table 2, our proposed
CD method generally outperforms the existing IE algorithm and proposed PD
algorithm. Speci(cid:12)cally for 3-NN, the average accuracy improving rate ranges
from 2:35% (Iris) to 27:06% (Glass) when compared CD with IE. With regard
to 1-NN, this rate falls within −1:58% (Glass) and 1:96% (Austra) between CD
and PD. However, by considering both simplicity and accuracy, we (cid:12)nd out that
CD is the best one since it takes the aggregation of the other two candidates.

Consequently, we draw the following conclusion: our proposed Coupled Dis-
cretization algorithm generally outperforms the other classical candidates in
terms of all the four measures: stability, simplicity, consistency, and accuracy.

Iris (150)Glass (214)Heart (303)Liver (345)Pima (768)Yeast (1484)0.30.40.50.60.70.80.91Discretization Comparison with Naive−Bayes AccuracyData SetNaive−Bayes AccuracyGlass (214)Heart (303)Ecoli (336)Liver (345)Austra (690)Cancer (699)Yeast (1484)0.30.40.50.60.70.80.91Discretization Comparison with Decision Tree AccuracyDecision Tree AccuracyData Set  EWEF1RIECDCD: A Coupled Discretization Algorithm

11

Table 2. Comparison between IE & PD & CD

Accuracy by 1-NN
IE
CD

Accuracy by 3-NN
IE
CD

Dataset

Iris (150)
Glass (214)
Heart (303)
Austra (690)
Pima (768)

Number of Intervals

IE
14
34
28
26
33

PD
10
79
45
78
74

CD
14
20
26
17
29

8 Conclusion

PD
95:24
96:95
61:60 79.53
73:33
63:28
76:60
70:14
67:10
70:74

PD
94:10
97.48 94:48
66:67
78:27
57:73
74.29 62:86
75:87
78.10 73:17 80.54
71.04 69:33
73:09

95.54
67.12
77.04
79:96
73.12

Discretization algorithm plays an important role in the applications of machine
learning and data mining. In this paper, we propose a new global-supervised-
splitting-incremental algorithm CD based on the coupling of positive domain
and information entropy. This method measures the importance of breakpoints
in a comprehensive and reasonable way. Experimental results show that our
proposed algorithm can e(cid:11)ectively improve the distribution stability and clas-
si(cid:12)cation accuracy, optimize the simplicity and reduce the total inconsistency
counts. We are currently applying the CD algorithm to the estimation of web
site quality with (cid:13)exible weights k1; k2 and stopping criteria, and we also con-
sider the aggregation of the CD algorithm with coupled nominal similarity [11]
to induce coupled numeric similarity and clustering ensemble applications.

9 Acknowledgment

This work is sponsored by Australian Research Council Grants (DP1096218,
DP0988016, LP100200774, LP0989721), and Tianjin Research Project (10JCY-
BJC07500).

References

1. An, A., Cercone, N.: Discretization of continuous attributes for learning classi(cid:12)ca-

tion rules. In: PAKDD 1999. pp. 509{514 (1999)

2. Banda, J.M., Angryk, R.A.: On the e(cid:11)ectiveness of fuzzy clustering as a data
discretization technique for large-scale classi(cid:12)cation of solar images. In: FUZZ-
IEEE 2009. pp. 2019{2024 (2009)

3. Beynon, M.J.: Stability of continuous value discretisation: an application within
rough set theory. International Journal of Approximate Reasoning 35, 29{53 (2004)
4. Chen, C., Wang, L.: Rough set-based clustering with re(cid:12)nement using Shannon’s
entropy theory. Computers and Mathematics with Applications 52(10-11), 1563{
1576 (2006)

5. Chmielewski, M.R., Grzymala-Busse, J.W.: Global discretization of continuous at-
tributes as preprocessing for machine learning. International Journal of Approxi-
mate Reasoning 15, 319{331 (1996)

12

C. Wang et al.

6. Liu, H., Hussain, F., Tan, C.L., Dash, M.: Discretization: an enabling technique.

Data Mining and Knowledge Discovery 6, 393{423 (2002)

7. Liu, W., Chawla, S.: Class con(cid:12)dence weighted kNN algorithms for imbalanced

data sets. In: PAKDD 2011. pp. 345{356 (2011)

8. Pawlak, Z., Wong, S.K.M., Ziarko, W.: Rough sets: probabilistic versus determin-

istic approach. International Journal of Man-Machine Studies 29, 81{95 (1988)

9. Qin, B., Xia, Y., Li, F.: DTU: a decision tree for uncertain data. In: PAKDD 2009.

pp. 4{15 (2009)

10. Son, N.H., Szczuka, M.: Rough sets in KDD. In: PAKDD 2005. pp. 1{91 (2005)
11. Wang, C., Cao, L., Wang, M., Li, J., Wei, W., Ou, Y.: Coupled nominal similarity

in unsupervised learning. In: CIKM2011. pp. 973{978 (2011)

12. Wang, G., Zhao, J., An, J., Wu, Y.: A comparative study of algebra viewpoint
and information viewpoint in attribute reduction. Fundamenta Informaticae 68,
289{301 (2005)

13. Yang, Y., Webb, G.I.: Discretization for Naive-Bayes learning: managing discretiza-

tion bias and variance. Machine Learning 74, 39{74 (2009)

14. Zhang, X., Wu, J., Yang, X., Lu, T.: Estimation of market share by using dis-
cretization technology: an application in China mobile. In: ICCS 2008. pp. 466{475
(2008)

Appendix: Theorem Proof

Proof. { [Theorem 1] Since H(S(P )) = 0 , then

|W1|
|U| H(W1) +

|W2|
|U| H(W2) + ··· +

|Wr|
|U| H(|Wr|) = 0:

H(Wi) = −∑

Because we have H(W ) ≥ 0 , then H(W1) = H(W2) = ··· = H(Wr) = 0:

According to the de(cid:12)nition of class information entropy of Wi(i = 1; 2;··· ; r),
j=1 pj log2 pj: Since 0 ≤ pj ≤ 1; log2 pj ≤ 0; H(Wi) = 0, then
pj = kj|Wi| = 0 or pj = kj|Wi| = 1, that is kj = 0 or kj = |Wi| respectively, which
indicates that the decision attribute values of Wi(i = 1; 2;··· ; r) are all equal.

r(d)

That is to say, the discretized decision table is consistent.

Proof. { [Theorem 2] Let the equivalence class of the objects that have the

equivalence class of the objects that have identical condition attribute value be

same decision attribute value be denoted as Y = {Y1; Y2;··· ; Ys} , and the
denoted as X = {X1; X2;··· ; Xt}.
Since we have R(S(P )) = 1, then |P OCC∗| = |U| holds. As we know
P OCC∗ (D) ⊆ U , then we further obtain that P OSC∗ (D) = U . According to
∩
∪
∩
the De(cid:12)nition 6, for each Yj ∈ Y , we then have at least one Xi ∈ X , to satisfy
Xi ⊆ Yj , and Yj = Xi1
∪ ··· ∪ Xij ; (Xi1;··· ; Xij
∈ X). As it is the fact that
Yj = ∅, then for each Xi ∈ X, there exists only
Xi =
one Yj ∈ Y , so that Xi ⊆ Yj. Hence, when the objects have identical condition

Yj = U ,

∪

Xi =

attribute value, their decision attribute values are the same, which means the
objects are consistent if R(S(P )) = 1.


