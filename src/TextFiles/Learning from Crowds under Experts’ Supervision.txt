Learning from Crowds under Experts’ Supervision

Qingyang Hu1, Qinming He1, Hao Huang2, Kevin Chiew3, and Zhenguang Liu1

1 College of Computer Science and Technology,

Zhejiang University, Hangzhou, China

{huqingyang,hqm,zhenguangliu}@zju.edu.cn

2 School of Computing, National University of Singapore, Singapore

huanghao@comp.nus.edu.sg

3 Provident Technology Pte. Ltd., Singapore

kev.chiew@gmail.com

Abstract. Crowdsourcing services have been proven efﬁcient in collecting large
amount of labeled data for supervised learning, but low cost of crowd work-
ers leads to unreliable labels. Various methods have been proposed to infer the
ground truth or learn from crowd data directly though, there is no guarantee that
these methods work well for highly biased or noisy crowd labels. Motivated by
this limitation of crowd data, we propose to improve the performance of crowd-
sourcing learning tasks with some additional expert labels by treating each labeler
as a personal classiﬁer and combining all labelers’ opinions from a model combi-
nation perspective. Experiments show that our method can signiﬁcantly improve
the learning quality as compared with those methods solely using crowd labels.

Keywords: Crowdsourcing, multiple annotators, model combination, classiﬁca-
tion.

1 Introduction

Crowdsourcing services such as Amazon Mechanical Turk have made it possible to col-
lect large amount of labels at relatively low cost. Nonetheless, since the reward is small
and the ability of workers is not certiﬁed, the labeling quality of crowd labelers is often
much lower than that of an expert. In the worst case, some workers just submit ran-
dom answers to get the fee deviously. One approach to dealing with low quality labels
is repeated-labeling. Sheng et al. [16] empirically showed that under certain assump-
tions, repeated-labeling can improve the label quality. Thus in crowdsourcing, people
may collect multiple labels y1
i from L different labelers for one instance xi,
while in traditional supervised learning, one instance xi corresponds to one label yi.

i , . . . , yL

i , y2

The problem remains as how to learn a reliable predictive model with the unreliable
crowd labels. Various methods have been proposed to infer the ground truth [4, 10] or
learn from crowd labels directly [8, 15]. The basic idea is employing generative models
for the labeling processes of crowd labelers. While these models are useful under certain
conditions, their assumptions on labelers are not easy to verify for a certain task.

This situation motivates us to investigate making full use of opinions collected from
crowds by incorporating some expert labels, which seems more sensible than trying
to verify the behavior of each labeler. Intuitively, combining expert labels with crowd

V.S. Tseng et al. (Eds.): PAKDD 2014, Part I, LNAI 8443, pp. 200–211, 2014.
c(cid:2) Springer International Publishing Switzerland 2014

Learning from Crowds under Experts’ Supervision

201

labels is expected to achieve higher learning quality than solely using crowd labels
though, little work has been done under this conﬁguration since most of the existing
work has focused on crowd labels.

This paper proposes to improve the performance of crowdsourcing learning tasks
with a minimum number of expert labels by maximizing the utilization of both the
crowd and expert labels1. Our major contribution is a formalized framework for utiliz-
ing expert labels in crowdsourcing. Following a series of existing work [8, 15, 19], our
work focuses on supervised classiﬁcation problems.

Some existing models [8, 13, 15] are capable of combining expert labels by straight-
forward extensions. The major difference between our method and these models is that
we use prior beliefs on experts much more explicitly.

1.1 An Illustrative Example

In what follows, we illustrate the limitation of crowd data with an example and explain
the idea which forms the basis of our framework. Fig. 1(a) shows a synthetic dataset for
binary classiﬁcation. For each class, we sample 100 points respectively from two differ-
ent Gaussian distributions, and get four underlying clusters. We simulate two labelers
whose opinions differ in one cluster as shown in Figs. 1(b) & 1(c). Here no model that
uses the crowd labels without extra information can weight one labeler over the other
since there is simply not enough evidence. Nonetheless, these two labelers provide very
informative labels. Labeler 1 actually gave all correct labels. If we can identify this fact
by a few expert labels, we achieve an efﬁcient method.

However, the problem is not trivial even for this toy data set. Supposing that we
choose a controversial point and let an expert label it, we will ﬁnd that Labeler 1 gave
the correct answer. This is far from enough to conclude that Labeler 1 gave true labels
for all controversial points given that in practice we only have crowd labels and are not
aware of the underlying data distribution. Adding more expert labels may increase our
conﬁdence on Labeler 1, still a formalized mechanism is needed to combine the ground
truth with crowd data.

We address the problem by a model combination process. We train a logistic re-
gression classiﬁer for each labeler separately with the labels provided by that labeler,
thus get 2 classiﬁers. A data instance xi will then get 2 predictions {f1(xi), f2(xi)}
from the 2 classiﬁers, where f(cid:2)(xi) ((cid:2) ∈ {1, 2}) is the posterior probability of the class
colored in blue. We treat the values of f(cid:2)(xi) as features in a new space, shown in Fig-
ure 1(d). This is referred to as the intermediate feature space [11]. The ﬁnal prediction
is made by another classiﬁer in this intermediate feature space.

By summarizing the opinions of labelers using personal classiﬁers, the separation
between classes becomes clearer and the controversial area is projected to the bottom
right in the new space and becomes more compact. Incorporating expert label evidence
in this space is much easier compared with the crowd labels in the original space. A
few ground truth labels in the controversial area will enables most classiﬁers built in

1 We assume that an expert always gives true labels and use the two terms ‘expert labels’ and
‘ground truth’ interchangeably. As experts can also make mistakes, this assumption is a sim-
pliﬁcation and may be relaxed in future work.

202

Q. Hu et al.

(a) True Labels

(b) Labeler 1

(c) Labeler 2 (biased)

(d) Intermediate feature space

8

6

4

2

0

−2

−4

−6

−8

−5

0

5

8

6

4

2

0

−2

−4

−6

−8

−5

0

5

8

6

4

2

0

−2

−4

−6

−8

2

l

 
r
e
e
b
a
L

1

0.8

0.6

0.4

0.2

0

−5

0

5

−0.2

0

0.5

Labeler 1

1

Fig. 1. An illustrative example. Instances labeled with cross(+) in (b)(c)(d) are controversial be-
tween the two labelers. These controversial data instances are gathered at the bottom right in the
intermediate feature space as shown in (d).

this space to favor Labeler 1 over Labeler 2 naturally. We leave the the crucial step of
combining expert evidence to the experiment section after we formalize our framework.

2 Related Work

With the arising of crowdsourcing services, crowd workers have shown their power in
applications such as sentiment tracking [3], machine translation [1] and name entity
annotating [5]. A key problem in crowdsourcing research is modeling data from multi-
ple unreliable sources for inferring the ground truth. The problem has its origin in the
early work [4] for combining multiple diagnostic test results. Recent work addressed
problems with the same formulation by methods such as message transferring [10] and
graphical models [13].

Our framework adopts the idea of learning a classiﬁer from crowd data directly.
Raykar et al. [15] and Yan et al. [19] treat true labels as hidden variables which are
inferred by the EM algorithm. Kajino et al. [8] infer only the true classiﬁer by personal
classiﬁers without considering true labels explicitly. The nature of our method is similar
to that of Kajino et al. [8], focusing on the ﬁnal learning tasks and not being tangled
with the correctness of a certain label.

To the best of our knowledge, very little work considered the case of learning from
crowd and expert data simultaneously. Kajino et al. [9] addressed this problem by ex-
tending some existing models straightforwardly. Wauthier and Jordan [18] also used
some expert labels. In their model crowd labels only make effects through the shared
latent factors which express labelers. Our method differs from these work in both moti-
vation and formulation.

We treat combining opinions of labelers as model combination. Getting the opti-
mal combination of a group of pattern classiﬁers has been studied thoroughly for a long
time and various methods have been proposed to employ the intermediate feature space.
Merz [14] proposed to do feature extraction using singular decomposition in this space
and Kuncheva et al. [12] proposed to combine classiﬁers giving soft labels using de-
cision templates. In traditional model combination framework, multiple classiﬁers are
obtained by different models trained on the same data set. Here the scenario is different,

Learning from Crowds under Experts’ Supervision

203

i.e., we have multiple unreliable label sets to train multiple classiﬁers, and we propose
to use some reliable labels to combine them. Under the crowdsourcing setting the idea
of absorbing the evidence of true labels in the intermediate feature space is also original.

3 Learning from Crowds and Experts

In this paper we focus on binary classiﬁcation problems with crowdsourcing training
data. The extension to multi-class cases is conceptually straightforward.

3.1 Problem Formulation
Formally, a crowdsourcing training set is denoted as D = {(xi, yi)}N
i=1, where instance
xi ∈ R
D is a D-dimensional feature vector. We have L distinct labelers each of which
gives labels to all N data instances.2 The label given by the (cid:2)th labeler for instance xi
i ∈ {−1, 1}. All labels corresponding to xi are collected in the
is denoted as y(cid:2)
i where y(cid:2)
L-dimensional vector yi.

j )}N0

Different from most of the existing methods, we use some additional expert-labeled
instances to improve the model quality. If there are N0 expert labels, then the expert
training set is D0 = {(xj, y0
j=1 where xj is again a D-dimensional feature vector
and y0
j is the true label provided by the expert. Note that an expert-labeled instance
xj in D0 is not necessarily in D. The task is to learn a reliable predictive function
D → [0, 1] for unseen data by taking both training sets D and D0 as inputs where
f : R
f (x) = p(y = 1|x) is the posterior probability of the positive class. We denote the
predictive function in this way for the convenience of the following steps.

3.2 Building Intermediate Feature Space

We extract the crowd opinions by treating labelers as personal classiﬁers. For the (cid:2)th
labeler, we use the personal training set D(cid:2) = {(xi, y(cid:2)
i=1 to learn a classiﬁer. Any
classiﬁcation model that expresses predictions as posterior probabilities of classes is
compatible with our approach. Here we follow the work [8] and use a logistic regression
model for each labeler, which is given by

i )}N

Pr[y = 1|x, w] = σ(wTx)

(1)

where w is the model parameter and the logistic sigmoid function is deﬁned as σ(a) =
1/(1 + e−a). We express all prediction functions of classiﬁers as an ensemble F =
{f1, f2, . . . , fL} where f(cid:2)(x) is the prediction of the classiﬁer obtained from labeler (cid:2)
on instance x. The outputs of all L classiﬁers for a particular instance xi is organized
in an L-dimensional vector [f1(xi), f2(xi), . . . , fL(xi)]T, which is referred to as a
decision proﬁle [11]. In what follows, we denote this vector as dpi with the (cid:2)th element
dp(cid:2)
i as features in a new feature space, namely the
intermediate feature space, and use another classiﬁer taking these values as inputs for
making the ﬁnal prediction.

i = f(cid:2)(xi). We treat values of dp(cid:2)

2 We assume at this point that all labelers give full labels to keep the notations simple. We will

discuss the case of missing labels in Section 3.5.

204

Q. Hu et al.

3.3 Combination of Evidence from Crowds and Experts

The next step is to train a classiﬁer in the intermediate feature space by utilizing expert
labels. As expert labels are much more reliable than crowd labels, we should put more
weights on them. However, if we discard crowd labels and use expert labels solely,
building a stable model can be costly even in the more compact and representative
intermediate feature space. Thus a balance has to be made between the crowd opinions
and expert evidence.

We address the problem by imposing a Bayesian treatment on the model parame-
ters of the classiﬁer in the intermediate feature space. We use some straightforward
combination of personal classiﬁers as the prior distribution of model parameters, and
absorb expert label evidence by updating the posterior distribution sequentially. We be-
lieve that a fully Bayesian method is essential here for utilizing the prior distribution on
parameters, which is informative in our framework as we will show later.

Speciﬁcally, we use the Bayesian logistic regression model [7] as our classiﬁer in
the intermediate feature space. The model achieved a tractable approximation of the
posterior distribution over parameter w in Equation (1) by using accurate variational
techniques. In our problem, the decision proﬁle dpi in the new space corresponding
to the instance xi in the original space is an (L + 1)-dimensional vector consisting of
all values of dp(cid:2)
i , (cid:2) = 1, . . . , L and an additional constant 1 corresponding to the bias
in parameter w. The corresponding true label is y0
i . The model assumes that the prior
distribution over w is Gaussian with mean μ and covariance matrix Σ. Absorbing the
evidence of expert-labeled instance dp and the true label y amounts to updating the
mean and covariance matrix by
post = Σ−1 + 2|λ(ξ)|dp · dpT
Σ−1

(2)

μpost = Σpost[Σ−1μ + (y/2)dp]

(3)
where λ(ξ) = [1/2− σ(ξ)]/2ξ and ξ = [dpTΣpostdp + (dpTμpost)2]0.5. The update
process is iterative and converges very fast (about two iterations) [7].

While one common criticism of the Bayesian approach is that the prior distribution
is often selected on the basis of mathematical convenience rather than as a reﬂection of
any prior beliefs [2], the prior distribution here is informative with a speciﬁc mean and
an isotropic covariance matrix given by
μ = [− 1
2
Σ = α−1I

, . . . , 1

L ]T

, 1
L

(4)

(5)

The mean is chosen such that all personal classiﬁers are combined by weighting them
equally, and the bias is −0.5 to ﬁt the shape of the logistic sigmoid function which is
equal to 0.5 for a = 0.

There is a single precision parameter α governing the covariance matrix. We can
interpret α as our conﬁdence on the crowds. A large α will cause the prior distribution
over w to peak steeply on the mean, thus the affect of absorbing one expert label will

Learning from Crowds under Experts’ Supervision

205

Algorithm 1. Learning from crowd labelers and experts

where (cid:2) = 1, . . . , L;

1. Input: Crowd and expert training sets D and D0;
2. Train the ensemble F of logistic regression classiﬁers deﬁned by Equation (1) using D(cid:2)
3. Use F to get predictions of data instances in D0, collect results in DP;
4. Initialize μ and Σ by Equations (4) & (5);
5. for j=1 to N0 do
6.
7.
8. end for
9. Output: Personal classiﬁer ensemble F, mean μ and covariance matrix Σ;

Calculate μpost and Σpost by Equations (2) & (3) using the evidence from dpj and y0
j ;
Set μ = μpost, Σ = Σpost;

be relatively small, leading to a ﬁnal classiﬁer depending heavily on the mean of prior,
which is the simple combination of personal classiﬁers. On the other hand, a small α
means that the prior is close to uniform, causing the ﬁnal classiﬁer to make predictions
mainly based on expert labels.

Intuitively, we should use a large α when personal classiﬁers are generally good,
and use a small one when crowd labels are inaccurate. In a crowdsourcing scenario
however, we usually do not have such knowledge. One alternative is to let α be related
to the number of expert labels N0 given by α = 1/N0. As this number increases, we
decrease the conﬁdence on crowds to let the ﬁnal model put more weight on expert
labels. Experiments show that with such selection of α, our model achieves relatively
stable performance under various values of N0.

Once the prior over w is chosen, we update its posterior distribution sequentially
with Equations (2) & (3) by adding one expert label each time. If an instance xj labeled
by the expert is not in D, we should ﬁrst calculate its predictions dpj by personal
classiﬁers and use these values to update the model. We collect all dpj in a set DP =
{dpj}N0
j=1. The complete steps of learning our model are summarized in Algorithm 1.

3.4 Classiﬁcation

To classify a new coming instance xk using the above results, we ﬁrstly get the predic-
tions dpk of personal classiﬁers on xk, and calculate the predictive distribution of the
true label y0
k in the intermediate feature space by marginalizing w.r.t. the ﬁnal distribu-
tion N (w|μ, Σ). The predictive likelihood is given by
k|xk,D,D0) = log σ(ξk) − 1
2
− 1
2

ξk − λ(ξk)ξ2
|Σk|
1
|Σ|
2

μTΣ−1μ +

k Σ−1
μT

log P (y0

(6)

k

1
2

k μk +

log

where subscript k assigned to μ and Σ refers to the posterior distribution over w after
absorbing the evidence of dpk and y0
k.

206

Q. Hu et al.

2

l

 
r
e
e
b
a
L

1

0.8

0.6

0.4

0.2

0

2

l

 
r
e
e
b
a
L

1

0.8

0.6

0.4

0.2

0

−0.2

−0.2

0

0.2

0.6
0.4
Labeler 1

0.8

1

−0.2

−0.2

0

0.2

0.6
0.4
Labeler 1

0.8

1

Fig. 2. Decision boundaries before and after absorbing expert label evidence. Dotted lines are
means of prior distributions over w, and solid lines are means of posterior distributions respec-
tively after absorbing the label information of the circled instance(s) .

3.5 Missing Labels

In real crowdsourcing tasks, workers may label part of the instances instead of the whole
set. Our model handles this problem naturally by training multiple personal classiﬁers
independently. A worker only labels a few instances may lead to a pool personal classi-
ﬁer. But this is not fatal as he uses only a tiny proportion of the whole budget. Also in
practice, we can avoid such cases simply by designing HITs with a moderate size.

4 Experiments

We use synthetic data to illustrate the process of absorbing expert evidence, and evaluate
the performance of our method on both UCI benchmark data and real crowdsourcing
data.

4.1 Synthetic Data

We complete our example in Figure 1 by illustrating the process of absorbing expert
labels, shown in Figure 2. For clarity, we only show the decision boundaries given
by means of the distributions over model parameter w. Dotted lines are priors before
adding expert labels. This prior is given by weighting each labeler equally following
our framework.

In the left sub-ﬁgure, we add one expert label and get the posterior. Since the true
label is blue, the decision boundary moves downward to suggest that data points near
this labeled instance is more likely to be blue. In the right sub-ﬁgure, we add four expert
labels for each class. The ﬁnal decision boundary separates the actual class very well
using merely eight expert labels. In this experiment we adjusted the model parameter α
to get the best illustrative effect.

Learning from Crowds under Experts’ Supervision

207

Table 1. Results on Waveform 1

Table 2. Results on Spambase

Classiﬁer

A1

A2

A3

GT

0.853± 0.010
0.408± 0.123 0.638± 0.074 0.831± 0.007
MV
0.490± 0.153 0.547± 0.101 0.831± 0.006
AOC
0.437± 0.190 0.743± 0.063 0.842± 0.009
ML
0.718± 0.046 0.740± 0.045 0.740± 0.059
EL-10
PCE-10 0.737± 0.051 0.742± 0.043 0.732± 0.051
CCE-10 0.725± 0.075 0.740± 0.068 0.816± 0.032
0.756± 0.037 0.759± 0.034 0.783± 0.034
EL-20
PCE-20 0.755± 0.033 0.768± 0.037 0.773± 0.048
CCE-20 0.801± 0.056 0.812± 0.057 0.822± 0.023
0.792± 0.028 0.788± 0.033 0.798± 0.014
EL-50
PCE-50 0.799± 0.025 0.796± 0.037 0.805± 0.016
CCE-50 0.816± 0.027 0.780± 0.039 0.833± 0.007
0.797± 0.023 0.782± 0.023 0.767± 0.046
EL-100
PCE-100 0.803± 0.008 0.811± 0.010 0.807± 0.015
CCE-100 0.831± 0.017 0.830± 0.024 0.829± 0.014

4.2 UCI Data

Classiﬁer

A1

A2

A3

GT

0.924± 0.008
0.477± 0.327 0.641± 0.228 0.885± 0.013
MV
0.535± 0.302 0.578± 0.208 0.879± 0.013
AOC
0.510± 0.357 0.711± 0.302 0.925± 0.007
ML
0.672± 0.057 0.606± 0.113 0.665± 0.069
EL-10
PCE-10 0.592± 0.095 0.641± 0.083 0.770± 0.049
CCE-10 0.857± 0.035 0.755± 0.165 0.890± 0.022
0.860± 0.025 0.758± 0.033 0.755± 0.047
EL-20
PCE-20 0.764± 0.080 0.708± 0.062 0.799± 0.046
CCE-20 0.891± 0.025 0.802± 0.087 0.894± 0.016
0.830± 0.041 0.826± 0.032 0.831± 0.051
EL-50
PCE-50 0.820± 0.053 0.803± 0.028 0.850± 0.017
CCE-50 0.900± 0.017 0.860± 0.053 0.895± 0.013
0.860± 0.025 0.859± 0.025 0.858± 0.034
EL-100
PCE-100 0.856± 0.025 0.861± 0.017 0.883± 0.010
CCE-100 0.891± 0.025 0.879± 0.031 0.903± 0.010

We test our method on three data sets from UCI Machine Learning Repository [6],
Waveform 1(5000 points, 21 dimensions), Wine Quality(6497 points, 12 dimensions)
and Spambase(4601 points, 57 dimensions). These data sets have moderate sizes which
enable us to perform experiments when number of crowd labels varies.

Since multiple labelers for these UCI datasets are unavailable, we simulate L label-
ers for each dataset. We ﬁrstly cluster the data into L clusters using k-means and assign
some labeling accuracy to each cluster for every labeler. Thus each labeler can have
different labeling qualities for different clusters. We use an L× L matrix A = [aij ]L×L
to express the simulation process, in which aij is the probability that labeler i gives the
true label for an instance in the jth cluster, thus a row corresponds to a labeler and a
column to a cluster. We set L = 5 and use three different accuracy matrices A1, A2,
and A3 to simulate different situations of labelers as follows.
⎤
⎡
⎥⎥⎦.
⎢⎢⎣

⎤
⎥⎥⎦, A2 =

⎤
⎥⎥⎦, A3 =

⎡
⎢⎢⎣

⎡
⎢⎢⎣

A1 =

0 1 0 1 0
1 1 0 0 1
0 0 1 1 0
1 0 0 1 1
1 0 1 1 0

0.3 0.1 0.8 0.8 0.8
0.3 0.8 0.1 0.8 0.8
0.3 0.8 0.8 0.1 0.8
0.3 0.8 0.8 0.8 0.1
0.8 0.1 0.1 0.1 0.1

0.55 0.55 0.55 0.55 0.55
0.65 0.65 0.65 0.65 0.65
0.75 0.75 0.75 0.75 0.75
0.68 0.68 0.68 0.68 0.68
0.95 0.95 0.95 0.95 0.95

A1 simulates severely biased labelers. A2 simulates labelers whose labels are both
noisy and biased. A3 simulates simply noisy labels. Note that A3 satisﬁes the model
assumption in the work by Raykar et al. [15].

We choose three baseline methods that learn with crowd data solely for comparison.
To verify the ability of our method to utilize the crowd labels, we compare the results
trained on expert labels solely. For comparison with existing methods we use the model
proposed by Kajino et al. [9], which is a state-of-art model that addresses the same
problem. We use the results trained on the original datasets which have all ground truth
labels as the approximate upper bounds of the classiﬁcation performance. Methods used
in experiments are summarized as follows.

208

Q. Hu et al.

Table 3. Results on Wine Quality

Classiﬁer

A1

A2

A3

0.743± 0.010
GT
0.424± 0.119 0.571± 0.110 0.739± 0.007
MV
0.582± 0.118 0.500± 0.109 0.740± 0.004
AOC
0.417± 0.133 0.701± 0.020 0.739± 0.004
ML
0.550± 0.042 0.583± 0.047 0.582± 0.083
EL-10
PCE-10 0.591± 0.035 0.578± 0.063 0.613± 0.033
CCE-10 0.634± 0.078 0.651± 0.092 0.715± 0.022
0.623± 0.064 0.575± 0.075 0.623± 0.063
EL-20
PCE-20 0.629± 0.019 0.604± 0.047 0.642± 0.041
CCE-20 0.679± 0.042 0.688± 0.047 0.720± 0.022
0.666± 0.036 0.675± 0.040 0.682± 0.024
EL-50
PCE-50 0.648± 0.019 0.644± 0.011 0.662± 0.022
CCE-50 0.687± 0.038 0.707± 0.025 0.722± 0.019
EL-100 0.707± 0.017 0.706± 0.017 0.711± 0.016
PCE-100 0.666± 0.011 0.665± 0.009 0.685± 0.020
CCE-100 0.718± 0.017 0.720± 0.010 0.733± 0.006

Table 4. Results under the variation of crowd label numbers on Spambase

Num.

50

100

200

500

1000

0.826± 0.037 0.855± 0.023 0.878± 0.022 0.900± 0.009 0.913± 0.003

GT
EL-50 0.835± 0.032
0.757± 0.057 0.734± 0.039 0.770± 0.021 0.849± 0.012 0.884± 0.012
MV
0.587± 0.045 0.727± 0.025 0.798± 0.025 0.852± 0.010 0.880± 0.010
AOC
0.807± 0.065 0.822± 0.025 0.876± 0.012 0.905± 0.005 0.921± 0.005
ML
PCE-50 0.838± 0.025 0.839± 0.024 0.842± 0.016 0.837± 0.018 0.861± 0.026
CCE-50 0.792± 0.045 0.807± 0.045 0.820± 0.018 0.873± 0.008 0.903± 0.006

– Majority Voting (MV) method learns from the single-labeled training set esti-

mated by majority voting.

– All-in-One-Classiﬁer (AOC) treats all labels as in one training set.
– Multiple Labelers (ML) method [15] learns from crowd labels directly.
– Kajino et al. [9] extended their personal classiﬁer model [8] to incorporate expert
labels, which we refer to as Personal Classiﬁers with Experts (PCE). PCE-N0 is
the results trained with N0 expert labels.

– We refer to our method as Classiﬁer Combination with Experts (CCE). CCE-N0

is the results after absorbing the evidence of N0 expert labels.

– Training with expert labels solely is referred as Expert Labels (EL) classiﬁers.

EL-N0 is the results trained with N0 expert labels.

– Ground Truth (GT) classiﬁer uses the original datasets for training.

For MV, AOC, GT, and EL, we use a logistic regression model respectively to train
the classiﬁers. For PCE, CCE and EL, the set of expert labels are randomly chosen from
the original datasets given the number of expert labels N0 which is restricted to a small
proportion of N . We divide each dataset into a 70% training set and a 30% test set and
each result is averaged on 10 runs.

Learning from Crowds under Experts’ Supervision

209

Tables 1–3 show the results for different datasets respectively. Results are in the form
of classiﬁcation accuracy and averaged on 10 trials. The GT classiﬁer is independent of
crowd labels thus it has only one result on each dataset. Our CCE outperforms EL, and
also outperforms MV, ACL and ML in most cases. This validates the ability of CCE
for combining crowd and expert labels. The only exception appears in ML under A3
where labelers are not biased. CCE outperforms PCE with clear advantages. There are
a number of cases that PCE performs worse than EL, which suggests that in the PCE
model expert evidences are easily disturbed by inaccurate crowd labels.

Table 4 shows the results under the variation of numbers of crowd labels. We show
the results on Spambase data under A3 since under this situation all methods seem to
work well. The top number of each column represents the number of labels provided
by each labeler. This is also the number of expert labels used for GT. We use 50 expert
labels for EL, PCE and CCE. EL has only one result as it is independent of crowd labels.
There is no surprise that ML performs very well in this experiment as the conﬁgura-
tion here meets ML’s model assumption. Yet we should not forget that ML fails in many
cases as shown in Tables 1–3. We do not choose those cases because showing groups
of failed results does not make any sense. Generally PCE and CCE outperform MV and
AOC by using extra expert labels. CCE performs slightly worse than PCE when the
number of crowd labels is small, while the performance raise of PCE is quite limited
when using more crowd labels.

In summary, our method CCE achieved reasonable performance on different data
sets with various labeler properties. The accuracy and stability of our CCE increase as
we use more expert labels. On the other hand, learning solely from crowd labels is risky,
especially when crowd labels are biased. PCE’s performance is limited compared with
CCE when we have enough crowd labels.

4.3 Affective Text Analysis Data

In this section we show results on the data for affective text analysis collected by Snow
et al. [17]. The data is collected from Amazon Mechanical Turk. Annotators are asked
to rate the emotions of a list of short headlines. The emotions include anger, disgust,
fear, joy, sadness, surprise and the overall positive or negative valence. The former six
are expressed with an interval [0, 100] respectively while valence is in [−100, 100].
There is a total number of 100 headlines labeled by 38 workers. For each headline 10
workers rated for each of the seven emotions. Most workers labeled 20 or 40 instances
thus more than one half labels are missing. All 100 instances are also labeled by the
experts and have an average rating for each emotion, which we treat as ground truth.

We design the classiﬁcation task which predicts the surprising level of a headline
using other emotion ratings as features. We deﬁne that a headline of which the surprise
rating is above 20 is a surprise, while others not, and use ratings of other six emotions
provided by the experts to express a headline. Thus we get a binary classiﬁcation task
in a 6-dimensional feature space.

Figure 3 shows classiﬁcation accuracy when continually adding expert labels. Re-
sults of MV, AOC and ML are not shown in this ﬁgure, which are three horizontal lines
below GT and stay close to each other. PCE only performs similarly with EL, which
collapses to GT when using all expert labels.

210

Q. Hu et al.

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55

0.5

y
c
a
r
u
c
c
A

0.45

 
0

10

20

30

 

CCE
PCE
LC
GT

80

90

100

110

40

50

Exper Label Number

60

70

Fig. 3. Results on Affective Text Analysis data. The x-axis is the number of expert labels used
while the y-axis is the classiﬁcation accuracy.

The result of CCE is promising. The value of GT is 0.65, which suggests that accord-
ing to the experts, there is no strong correlation between the surprising level and other
emotions. However, CCE only uses about 20 expert labels to get a similar performance
level with GT, and when adding more expert labels, CCE outperforms GT and achieves
an accuracy up to 0.8. We attribute this fact to the power of our CCE model as a ‘feature
extractor’. Among the 38 workers, one or more of them did give ratings in manners that
relate surprising levels to other emotions even if experts did not do so. Personal clas-
siﬁers trained from these workers will then be able to predict the target and our model
identiﬁes these classiﬁers successfully using expert labels.

5 Conclusion and Future Work

In this paper, we have proposed a framework for improving the performance of crowd-
sourcing learning tasks by incorporating the evidence of expert labels with a Bayesian
logistic regression classiﬁer in the intermediate feature space. Experimental results have
veriﬁed that by combining crowd and expert labels, our method has achieved better
performance as compared with some existing methods, and has been stable under the
variation of the number of expert labels and crowd labeler properties.

A promising direction of future work is to investigate actively querying for the expert
labels, for which we can develop models by adopting basic ideas from active learning
and considering the particular situation of crowdsourcing.

Acknowledgment. This work is supported by the National Key Technology R&D
Program of the Chinese Ministry of Science and Technology under Grant No.
2012BAH94F03.

Learning from Crowds under Experts’ Supervision

211

References

1. Ambati, V., Vogel, S., Carbonell, J.: Active learning and crowd-sourcing for machine trans-

lation. Language Resources and Evaluation (LREC) 7, 2169–2174 (2010)

2. Bishop, C.M., et al.: Pattern recognition and machine learning, vol. 4. Springer, New York

(2006)

3. Brew, A., Greene, D., Cunningham, P.: Using crowdsourcing and active learning to track

sentiment in online media. In: ECAI 2010, pp. 145–150 (2010)

4. Dawid, A.P., Skene, A.M.: Maximum likelihood estimation of observer error-rates using the

EM algorithm. Applied Statistics, 20–28 (1979)

5. Finin, T., Murnane, W., Karandikar, A., Keller, N., Martineau, J., Dredze, M.: Annotating
named entities in twitter data with crowdsourcing. In: Proceedings of the NAACL HLT 2010
Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pp.
80–88. Association for Computational Linguistics (2010)

6. Frank, A., Asuncion, A.: UCI machine learning repository (2010)
7. Jaakkola, T., Jordan, M.: A variational approach to Bayesian logistic regression models and
their extensions. In: Proceedings of the 6th International Workshop on Artiﬁcial Intelligence
and Statistics (1997)

8. Kajino, H., Tsuboi, Y., Kashima, H.: A convex formulation for learning from crowds. In:

Proceedings of the 26th AAAI Conference on Artiﬁcial Intelligence (2012) (to appear)

9. Kajino, H., Tsuboi, Y., Sato, I., Kashima, H.: Learning from crowds and experts. In: Pro-

ceedings of the 4th Human Computation Workshop, HCOMP 2012 (2012)

10. Karger, D.R., Oh, S., Shah, D.: Iterative learning for reliable crowdsourcing systems. In:

Advances in Neural Information Processing Systems (NIPS 2011), pp. 1953–1961 (2011)
11. Kuncheva, L.I.: Combining pattern classiﬁers: Methods and algorithms (kuncheva,

li;

2004)[book review]. IEEE Transactions on Neural Networks 18(3), 964 (2007)

12. Kuncheva, L.I., Bezdek, J.C., Duin, R.P.W.: Decision templates for multiple classiﬁer fusion:

an experimental comparison. Pattern Recognition 34(2), 299–314 (2001)

13. Liu, Q., Peng, J., Ihler, A.: Variational inference for crowdsourcing. In: Advances in Neural

Information Processing Systems (NIPS 2012), pp. 701–709 (2012)

14. Merz, C.J.: Using correspondence analysis to combine classiﬁers. Machine Learning 36(1),

33–58 (1999)

15. Raykar, V.C., Yu, S., Zhao, L.H., Valadez, G.H., Florin, C., Bogoni, L., Moy, L.: Learning

from crowds. The Journal of Machine Learning Research 11, 1297–1322 (2010)

16. Sheng, V.S., Provost, F., Ipeirotis, P.G.: Get another label? Improving data quality and data
mining using multiple, noisy labelers. In: Proceeding of the 14th ACM SIGKDD Interna-
tional Conference on Knowledge Discovery and Data Mining, pp. 614–622 (2008)

17. Snow, R., O’Connor, B., Jurafsky, D., Ng, A.Y.: Cheap and fast—but is it good? evaluat-
ing non-expert annotations for natural language tasks. In: Proceedings of the Conference on
Empirical Methods in Natural Language Processing, pp. 254–263. Association for Compu-
tational Linguistics (2008)

18. Wauthier, F.L., Jordan, M.I.: Bayesian bias mitigation for crowdsourcing. In: Advances in

Neural Information Processing Systems (NIPS 2011), pp. 1800–1808 (2011)

19. Yan, Y., et al.: Modeling annotator expertise: Learning when everybody knows a bit of some-
thing. In: Proceedings of 13th International Conference on Artiﬁcial Intelligence and Statis-
tics (AISTATS 2010), vol. 9, pp. 932–939 (2010)


