Satisfying Privacy Requirements: One Step

before Anonymization

Xiaoxun Sun1, Hua Wang1, and Jiuyong Li2

1 Department of Mathematics & Computing
University of Southern Queensland, Australia

{sunx,wang}@usq.edu.au

2 School of Computer and Information Science

University of South Australia, Australia

jiuyong.li@unisa.edu.au

Abstract. In this paper, we study a problem of privacy protection in
large survey rating data. The rating data usually contains both ratings of
sensitive and non-sensitive issues, and the ratings of sensitive issues in-
clude personal information. Even when survey participants do not reveal
any of their ratings, their survey records are potentially identiﬁable by
using information from other public sources. We propose a new (k, , l)-
anonymity model, in which each record is required to be similar with at
least k− 1 others based on the non-sensitive ratings, where the similarity
is controlled by , and the standard deviation of sensitive ratings is at
least l. We study an interesting yet nontrivial satisfaction problem of
the (k, , l)-anonymity, which is to decide whether a survey rating data
set satisﬁes the privacy requirements given by users. We develop a slice
technique for the satisfaction problem and the experimental results show
that the slicing technique is fast, scalable and much more eﬃcient in
terms of execution time than the heuristic pairwise method.

1 Introduction

The problem of privacy-preserving data publishing has received a lot of attention
in recent years. Privacy preservation on tabular data has been studied exten-
sively. A major category of privacy attacks on relational data is to re-identify
individuals by joining a published table containing sensitive information with
some external tables. Most of existing work can be formulated in the follow-
ing context: several organizations publish detailed data about individuals (e.g.
medical records) for research or statistical purposes [14,10,9,13].

Privacy risks of publishing microdata are well-known. Famous attacks include
de-anonymisation of the Massachusetts hospital discharge database by joining it
with a public voter database [14] and privacy breaches caused by AOL search
data [5]. Even if identiﬁers such as names and social security numbers have
been removed, the adversary can use linking [12], homogeneity and background
attacks [10] to re-identify individual data records or sensitive information of in-
dividuals. To overcome the re-identiﬁcation attacks, k-anonymity was proposed

M.J. Zaki et al. (Eds.): PAKDD 2010, Part I, LNAI 6118, pp. 181–188, 2010.
c(cid:2) Springer-Verlag Berlin Heidelberg 2010

182

X. Sun, H. Wang, and J. Li

[12,14]. Speciﬁcally, a data set is said to be k-anonymous (k ≥ 1) if, on the quasi-
identiﬁer attributes, each record is identical with at least k − 1 other records.
The larger the value of k, the better the privacy is protected. Several algo-
rithms are proposed to enforce this principle [2,6,7,8]. Machanavajjhala et al.
[10] showed that a k-anonymous table may lack of diversity in the sensitive at-
tributes. To overcome this weakness, they propose the l-diversity [10]. However,
even l-diversity is insuﬃcient to prevent attribute disclosure due to the skewness
and the similarity attack. To amend this problem, t-closeness [9] was proposed
to solve the attribute disclosure vulnerabilities inherent to previous models.

Recently, a new privacy concern has emerged in privacy preservation research:
how to protect individuals’ privacy in large survey rating data. Though several
models and many algorithms have been proposed to preserve privacy in relational
data (e.g., k-anonymity [12,14], l-diversity [10], t-closeness [9], etc.), most of the
existing studies are incapable of handling rating data, since the survey rating
data normally does not have a ﬁxed set of personal identiﬁable attributes as
relational data, and it is characterized by high dimensionality and sparseness.
The survey rating data shares the similar format with transactional data. The
privacy preserving research of transactional data has recently been acknowledged
as an important problem in the data mining literature [3,15,16]. To our best
knowledge, there is no current research addressing the issue of how to eﬃciently
determine whether the survey rating data satisﬁes the privacy requirement.

2 Motivation

On October 2, 2006, Netﬂix, the world’s largest online DVD rental service, an-
nounced the $1-million Netﬂix Prize to improve their movie recommendation
service [4]. To aid contestants, Netﬂix publicly released a data set containing
100,480,507 movie ratings, created by 480,189 Netﬂix subscribers between De-
cember 1999 and December 2005. Narayanan and Shmatikov shown in their
recent work [11] that an attacker only needs a little information to identify the
anonymized movie rating transaction of the individual. They re-identiﬁed Netﬂix
movie ratings using the Internet Movie Database (IMDb) as a source of auxil-
iary information and successfully identiﬁed the Netﬂix records of known users,
uncovering their political preferences and other potentially sensitive information.
We consider the privacy risk in publishing survey rating data. For example, in
a life style survey, ratings to some issues are non-sensitive, such as the likeness
of a book. Ratings to some issues are sensitive, such as the income level. Assume
that each survey participant is cautious about his/her privacy and does not
reveal his/her ratings. However, it is easy to ﬁnd his/her preferences on non-
sensitive issues from publicly available information sources, such as personal
weblog. An attacker can use these preferences to re-identify an individual and
consequently ﬁnd sensitive ratings of a victim. An example is given in the Table
1. In a social network, people make comments on various issues, which are not
considered sensitive. Some comments can be summarized as in Table 1(b). We
assume that people are aware of their privacy and do not reveal their ratings,

Satisfying Privacy Requirements: One Step before Anonymization

183

Table 1. (a) A published survey rating data (b) Public comments on some non-sensitive
issues of some survey participants

non-sensitive

sensitive
ID issue 1 issue 2 issue 3 issue 4
t1
t2
t3
t4
t5

null
null
null

null
null

1
6
5

6
1
1
1
5

6
1
2
1
2

5
6

non-sensitive issues

name issue 1 issue 2 issue 3
Alice excellent so bad
Bob
Jack

awful
bad

good

top

-
-

-

(a)

(b)

either non-sensitive or sensitive ones. However, individuals in the supposedly
anonymized survey rating data are potentially identiﬁable based on their public
comments from other sources. For example, Alice is at risk of being identiﬁed,
since the attacker knows Alice’s preference on issue 1 is ‘excellent’, by cross-
checking Table 1(a) and (b), s/he will deduce that t1 in Table 1(a) is linked to
Alice, the sensitive rating on issue 4 of Alice will be disclosed. This example
motivates us the following research question: Given a survey rating data set T
with the privacy requirements, how to eﬃciently determine whether T satisﬁes
the given privacy requirements?

3 (k, , l)-Anonymity

We assume that a survey rating data set publishes people’s ratings on a range of
issues. Each survey participant is cautious about his/her privacy and does not
reveal his/her ratings. However, an attacker may ﬁnd a victim’s preference (not
exact rating scores) by personal familiarity or by reading the victim’s comments
on some issues from personal weblog. We consider that attackers know prefer-
ences of non-sensitive issues of a victim but do not know exact ratings and want
to ﬁnd out the victim’s ratings on some sensitive issues.

The auxiliary information of an attacker includes: (i) the knowledge of a victim
being in the survey rating data; (ii) preferences of the victims on some non-
sensitive issues. The attacker wants to ﬁnd ratings on sensitive issues of the
victim. In practice, knowledge of Types (i) and (ii) can be gleaned from an
external database [11]. For example, in the context of Table 1(b), an external
database may be the IMDb. By examining the anonymized data in Table 1(a),
the adversary can identify a small number of candidate groups that contain the
record of the victim. It will be the unfortunate scenario where there is only one
record in the candidate group. For example, since t1 is unique in Table 1(a),
Alice is at risk of being identiﬁed. If the candidate group contains not only the
victim but other records, an adversary may use this group to infer the sensitive
value of the victim. For example, although it is diﬃcult to identify whether t2
or t3 in Table 1(a) belongs to Bob, since both records have the same sensitive
value, Bob’s private information is identiﬁed.

184

X. Sun, H. Wang, and J. Li

In order to avoid such attack, we propose a (k, , l)-anonymity model. The ﬁrst
step is to require that in the released data, every transaction should be similar
with at least k − 1 other records based on the non-sensitive ratings so that no
survey participants are identiﬁable. For example, t1 in Table 1(a) is unique, and
based on the preference of Alice in Table 1(b), her sensitive issues can be re-
identiﬁed. Jack’s sensitive issues, on the other hand, is much safer. Since t4 and
t5 in Table 1(a) form a similar group based on their non-sensitive rating. Second
is to prevent the sensitive rating from being inferred by requiring the sensitive
ratings in a similar group be diverse. For instance, although t2 and t3 in Table
1(a) form a similar group, their sensitive ratings are identical. Therefore, an
attacker can immediately infer Bob’s preference on the sensitive issue without
identifying which transaction belongs to Bob. In contrast, Jack’s preference on
the sensitive issue is much safer than both Alice and Bob.
Given a rating data set T , each transaction contains a set of numbers indicate
the ratings on some issues. Let (o1,··· , op, s1,··· , sq) be a transaction, oi ∈
{1 : r, null}, i = 1, 2,··· , p and sj ∈ {1 : r, null}, j = 1, 2,··· , q, where r is
the maximum rating and null indicates that a survey participant did not rate.
o1,··· , op stand for non-sensitive ratings and s1,··· , sq denote sensitive ratings.
Let TA = {oA1,··· , oAp, sA1 ,··· , sAq}, TB = {oB1,··· , oBp , sB1,··· , sBq} be
the ratings for participants A and B, then dissimilarity of non-sensitive ratings
(Dis(oAi , oBi) and sensitive ratings (Dis(sAi , sBi)) between TA and TB is deﬁned
as follows:

Dis(oAi , oBi) =

Dis(sAi , sBi) =

⎧
⎨

⎩

⎧
⎨

⎩

|oAi − oBi| oAi , oBi ∈ {1 : r}
0
oAi = oBi = null
otherwise
r

|sAi − sBi| sAi, sBi ∈ {1 : r}
sAi = sBi = null
otherwise

r
r

(1)

(2)

Deﬁnition 1 (-proximate). Given a survey rating data set T with a small
positive number , two transactions TA = {oA1,··· , oAp , sA1,··· , sAq} ∈ T and
TB = {oB1 ,··· , oBp, sB1 ,··· , sBq} ∈ T . TA and TB are -proximate, if ∀1 ≤
i ≤ p, Dis(oAi, oBi) ≤ . The set of transactions T is -proximate, if every two
transactions in T are -proximate.

If two transactions are -proximate, the dissimilarity between their non-sensitive
ratings is bound by .

Deﬁnition 2 ((k, )-anonymity). A survey rating data set T is said to be
(k, )-anonymous if and only if every transaction is -proximate with at least
k − 1 other transactions. The transaction t with all the other transactions that
are -proximate with t in T form a (k, )-anonymous group.

Although (k, )-anonymity can protect identity, it fails to protect sensitive infor-
mation. For example, in Table 1(a), t2 and t3 are in a (2, 1)-anonymous group,

Satisfying Privacy Requirements: One Step before Anonymization

185

but they have the same rating on the sensitive issue, thus Bob’s private informa-
tion is breaching. This example reﬂects the shortcoming of the (k, )-anonymity.
To mitigate this limitation, suﬃcient diversity of the sensitive values in each
(k, )-anonymous group should be allowed.
For a sensitive issue s, let the vector of ratings of the group be (s1,··· , sg),
where si ∈ {1 : r, null}. The mean of the ratings is ¯s = 1
i=1 si, where Q is
the number of non-null values, and si ± null = si. The standard deviation of
i=1(si − ¯s)2.
the rating is then deﬁned as SD(s) =

(cid:5)

(cid:5)

(cid:6)

Q

g

g

1
g

Deﬁnition 3 ((k, , l)-anonymity). A survey rating data set is said to be (k, , l)-
anonymous if and only if the standard deviation of sensitive ratings is at least l in
each (k, )-anonymous group.

4 The Algorithm

In this section, we formulate the satisfaction problem and develop a slicing tech-
nique to determine the following Satisfaction Problem.
Satisfaction problem: Given a survey rating data T and k, , l, the satisfaction
problem of (k, , l)-anonymity is to decide whether T satisﬁes k, , l requirements.
The satisfaction problem is to determine whether the user’s given privacy re-
quirement is satisﬁed by the given data set. If the data set has already met the
requirements, it is not necessary to make any modiﬁcations before publishing.
As follows, we propose a novel slice technique to solve the satisfaction problem.

then Cand ← Cand ∪ {tj}

Algorithm 1: Slicing(, T, t0)(C)
1 Can ← {t0}; S ← ∅
2 / ∗ To slice out the -proximate of t0 ∗ /
3 for j ← 1 to n
4 do if |tj − t0| ≤ 
5
6
7 / ∗ To trim the -proximate of t0 ∗ /
8 P Cand ← Cand
9 for i ← 1 to |S|
10 do for j ← 1 to |S|
11
12
13 return P Cand

do if |tS(i) − tS(j)| > 

S ← S ∪ {j}

then P Cand ← P Cand \ {tS(i)}

We illustrate the slicing technique using an example in 3-D space. Given
t = (t1, t2, t3) ∈ T , our goal is to slice out a set of transactions T (t ∈ T ) that are
-proximate. Our approach is ﬁrst to ﬁnd the -proximate of t, which is the set of
transactions that lie inside a cube Ct of side 2 centered at t. Since  is typically

186

X. Sun, H. Wang, and J. Li

small, the number of points inside the cube is also small. The -proximate of
(cid:3)
t can be found by an exhaustive comparison within the -proximate of t. If
C
there are no transactions inside the cube Ct, we know the -proximate of t is
(cid:3)
t. The transactions within the cube can
empty, so as the -proximate of the set C
be found as follows. First we ﬁnd the transactions that are sandwiched between
a pair of parallel planes X1, X2 and add them to a candidate set. The planes
are perpendicular to the ﬁrst axis of coordinate frame and are located on either
side of the transaction t at a distance of . Next, we trim the candidate set by
disregarding transactions that are not also sandwiched between the parallel pair
of Y1 and Y2, that are perpendicular to X1 and X2. This procedure is repeated
for Z1 and Z2 at the end of which, the candidate set contains only transactions
within the cube of size 2 centered at t. Slicing(, T, t0) (Algorithm 1) describes
how to ﬁnd the -proximate of the set Ct0 with t0 ∈ Ct0.

5 Experiments

deploys

downloadable

experimentation

at
Our
http://www.grouplens.org/taxonomy/term/14, which was made available by
the GroupLens Research Project at the University of Minnesota. In the data set,
a user is considered as an object while a movie is regarded as an attribute and
many entries are empty since a user only rated a small number of movies.

the MovieLens data

Data used for Fig. 1(a) is generated by re-sampling the Movielens data set
while varying the percentage of data from 10% to 100%. We evaluate the running
time for the (k, , l)-anonymity model with default setting k = 20,  = 1, l = 2.
The execution time for (k, , l)-anonymity is increasing with the increased data
percentage. This is because as the percentage of data increases, the computation
cost increases too, since the overhead is increased with the more dimensions.
Next, we evaluate how the parameters aﬀect the cost of computing. In these
experiments, we use the whole MovieLens data and evaluate by varying . With
k = 20, l = 2, Fig. 1(b) shows the computational cost as a function of , in
determining (k, , l)-anonymity. At the initial stage, when  is small, more com-
putation eﬀorts are put into ﬁnding -proximate of the transactions, but less
used in exhaustive search for -proximate of the set, and this explains the initial
decent of overall cost. As  grows, the searching time for -proximate is reduced,
but the number of transactions in the -proximate is increased, which results in
huge exhaustive search eﬀort and this causes the eventual cost increase.

In addition to the scalability, we experimented the comparison between the
slicing algorithm (Slicing) and the heuristic pairwise algorithm (Pairwise), which
works by computing all the pairwise distances to construct the dissimilarity ma-
trix and identify the violation of privacy requirements. We implemented both
algorithms and studied the impact of the execution time on the data percentage
and . Fig. 2(a) describe the trend of the algorithms by varying the percentage
of the data set. From the graph, the slicing algorithm is far more eﬃcient than
the heuristic pairwise algorithm especially when the volume of the data becomes
larger. This is because, when the dimension of the data increases, the disadvan-
tage of the heuristic pairwise algorithm, which is to compute all the dissimilarity

Satisfying Privacy Requirements: One Step before Anonymization

187

Movielens
Netflix

 

Movielens
Netflix

800

700

600

)
c
e
S

(
 

 

800

600

400

200

)
c
e
S

(
 

i

 

e
m
T
g
n
n
n
u
R

i

 

0
10%

20%

30%

40%

50%

60%

70%

80%

90% 100%

Data Percentage

(a)

i

 

e
m
T
g
n
n
n
u
R

i

500

400

300

200

 
1

2

3

The value of ε

4

5

(b)

Fig. 1. Running time comparison on Movielens data set vs. (a) data percentage varies;
(b)  varies

)
c
e
S

i

 

(
 
e
m
T
g
n
n
n
u
R

i

Slicing
Pairwise

 

)
c
e
S

i

 

(
 
e
m
T
g
n
n
n
u
R

i

1400

1200

1000

800

600

400

200

 

0
10%

20%

30%

50%

40%
70%
Data percentage of Movielens

60%

80%

90% 100%

(a)

1600

1400

1200

1000

800

600

400

200

 
1

 

Slicing
Pairwise

2

3

The value of ε

4

5

(b)

Fig. 2. Running time comparison of Slicing and Pairwise methods (a) data percentage
varies; (b)  varies

distance, dominates the execution time. On the other hand, the smarter group-
ing technique used in the slicing process makes less computation cost for the
slicing algorithm. The similar trend is shown in Fig. 2(b) by varying .

6 Conclusion

We have studied the problem of protecting individuals’ sensitive ratings in the
large survey rating data. We proposed a novel (k, , l)-anonymity model and
studied the satisfaction problem. A novel slicing technique was proposed to solve
the satisfaction problem by searching closest neighbors in large, sparse and high
dimensional survey rating data. The experimental results conﬁrm the slicing
technique is fast and scalable in practical.

Acknowledgement

Thanks for the reviewers’ valuable comments. The research is supported by Aus-
tralian Research Council (ARC) discovery grants DP0774450 and DP0663414.

188

X. Sun, H. Wang, and J. Li

References

1. Backstrom, L., Dwork, C., Kleinberg, J.: Wherefore Art Thou R3579x?:
Anonymized Social Networks, Hidden Patterns, and Structural Steganography. In:
WWW 2007, pp. 181–190 (2007)

2. Fung, B., Wang, K., Yu, P.: Top-down specialization for information and privacy

preservation. In: ICDE 2005, pp. 205–216 (2005)

3. Ghinita, G., Tao, Y., Kalnis, P.: On the Anonymisation of Sparse High-Dimensional

Data. In: ICDE 2008, pp. 715–724 (2008)

4. Hafner, K.: If you liked the movie, a Netﬂix contest may reward you handsomely.

New York Times (2006)

5. Hansell, S.: AOL removes search data on vast group of web users. New York Times

(2006)

6. Le Fevre, K., De Witt, D., Ramakrishnan, R.: Incognito: eﬃcient full-domain k-

anonymity. In: SIGMOD 2005, pp. 49–60 (2005)

7. Le Fevre, K., De Witt, D., Ramakrishnan, R.: Mondrian multidimensional k-

anonymity. In: ICDE 2006, pp. 25–26 (2006)

8. Li, J., Tao, Y., Xiao, X.: Preservation of Proximity Privacy in Publishing Numerical

Sensitive Data. In: SIGMOD 2008, pp. 473–486 (2008)

9. Li, N., Li, T., Venkatasubramanian, S.: t-Closeness: Privacy Beyond k-anonymity

and l-diversity. In: ICDE 2007, pp. 106–115 (2007)

10. Machanavajjhala, A., Gehrke, J., Kifer, D., Venkitasubramaniam, M.: l-Diversity:

Privacy beyond k-anonymity. In: ICDE 2006, pp. 24–25 (2006)

11. Narayanan, A., Shmatikov, V.: Robust De-anonymisation of Large Sparse Datasets.

In: IEEE Security & Privacy, 111–125 (2008)

12. Samarati, P.: Protecting respondents’ identities in microdata release. IEEE Trans-

actions on Knowledge and Data Engineering 13(6), 1010–1027 (2001)

13. Sun, X., Wang, H., Li, J.: Injecting purposes and trust into data anonymization.

In: CIKM 2009, pp. 1541–1544 (2009)

14. Sweeney, L.: k-Anonymity: A Model for Protecting Privacy. International Journal

on Uncertainty Fuzziness Knowledge-based Systems 10(5), 557–570 (2002)

15. Xu, Y., Wang, K., Fu, A., Yu, P.S.: Anonymizing Transaction Databases for Pub-

lication. In: KDD 2008, pp. 767–775 (2008)

16. Xu, Y., Fung, B., Wang, K., Fu, A., Pei, J.: Publishing Sensitive Transactions for

Itemset Utility. In: ICDM 2008, pp. 1109–1114 (2008)


