Constrained Least Squares Regression

for Semi-Supervised Learning

Bo Liu1,2, Liping Jing1,(cid:2), Jian Yu1, and Jia Li1

1 Beijing Key Lab of Traﬃc Data Analysis and Mining,

Beijing Jiaotong University, Beijing, 100044, China
2 College of Information Science and Technology,

liubohbu@126.com, {lpjing,jianyu}@bjtu.edu.cn, jiali.gm@gmail.com

Agricultural University of Hebei, Hebei, 071000, China

Abstract. The core tasks of graph based semi-supervised learning (GSSL)
are constructing a proper graph and selecting suitable supervisory infor-
mation. The ideal graph is able to outline the intrinsic data structure, and
the ideal supervisory information could represent the whole data. In this
paper, we propose a new graph learning method, called constrained least
squares regression (CLSR), which integrates the supervisory information
into graph learning process. To learn a more adaptive graph, regression
coeﬃcients and neighbor relations are combined in CLSR to capture the
global and local data structures respectively. Moreover, as byproduct of
CLSR, a new strategy is presented to select the high-quality data points as
labeled samples, which is practical in real applications. Experimental re-
sults on diﬀerent real world datasets demonstrate the eﬀectiveness of CLSR
and the sample selection strategy.

Keywords: graph based semi-supervised learning, graph construction,
constrained least squares regression, labeled sample selection.

1

Introduction

Lack of suﬃciently labeled data is a big problem when building supervised
learner in real applications. Semi-supervised learning (SSL) can bridge the gap
between labeled and unlabeled data, as it combines limited labeled samples with
rich unlabeled samples to enhance the learner’s ability [20]. As an important
branch of SSL, graph based semi-supervised learning (GSSL) propagates the su-
pervisory information (class labels) on a pre-deﬁned graph and aims to make the
similar samples share the common labels [12]. Under the cluster assumption [4]
or the manifold assumption [8], there are many GSSL methods have been pro-
posed, including Gaussian ﬁelds and harmonic functions (GFHF) [21], local and
global consistency (LGC) [19], manifold regularization [2], and etc. For GSSL,
there are several key issues to be solved including graph construction, labeled
sample selection, learning model formulation, parameter adjustment and etc. In
this paper, we will limit to highlight the former two issues.

(cid:2) Corresponding Author.

V.S. Tseng et al. (Eds.): PAKDD 2014, Part II, LNAI 8444, pp. 110–121, 2014.
c(cid:2) Springer International Publishing Switzerland 2014

Constrained Least Squares Regression for Semi-Supervised Learning

111

An adaptive graph construction is a main challenge of GSSL. Neighborhood-
driven methods (e.g., k -nearest-neighbors (k -NN) [16], -ball neighborhood [1]
and b-matching graph [7]) are unable to reﬂect the overall views of data and sen-
sitive to noise. Recently, some researchers formulate the graph building process
into a subspace learning problem. Under the subspace assumption, each sample
can be represented as a linear combination of other samples, and intuitively, the
representation coeﬃcients could be accepted as a proper surrogate of similar-
ity metric. In the literature, this measurement is referred to as self-expressive
similarity [6]. There are several methods such as sparse representation (SR) [6],
low-rank representation (LRR) [10], least squares regression (LSR) [13] to obtain
the representation coeﬃcients.

Although these approaches have gained great eﬀects in some domains, there
are still some drawbacks. First, labeled samples only work at propagating stage,
so the supervisory information cannot directly inﬂuence the aﬃnity learning
process. Second, regardless of noise and outliers, data points may not strictly
lie in a union of subspaces, which indicates that the graph’s adaptability is
restricted owing to the utilization of a single metric. Third, in the context of
the subspace assumption, when we have to select some samples as a labeled set,
however, the existing method, random sampling, does not leverage the structural
characteristic of the original dataset.

Inspired by the work [13], we propose an eﬀective graph construction frame-
work, called constrained least squares regression (CLSR), and try to improve
GSSL from three perspectives:

– The labeled samples are eﬀectively integrated into the graph learning process

of GSSL by representing them as additional pairwise constraints.

– Both local and global data structures are considered to build a more ﬂexible

graph via self-expressive similarity metric and k -NN.

– A greedy-like strategy is designed to pick out more representative samples

as the labeled set.

i=1 with cardinality |Xl| = l contains labeled points and Xu = {xi}n

2 Preliminaries and Related Works
Given a data set X = [x1, x2, . . . , xl, xl+1, . . . , xn] ∈ IRm×n, the subset Xl =
{xi}l
with cardinality |Xu| = n − l contains unlabeled points. The target of graph
learning is to generate a proper graph or weight matrix W ∈ IRn×n and its
element Wij denotes the similarity between the ith point and the j th point under
some measurement. SR [6] and LRR [10] are two popular aﬃnity representation
techniques. SR aims to construct a sparse graph or (cid:3)1-graph [17], where each
point could be reconstructed by a combination of other limited points, and thus
the sparse coeﬃcients correspond to a kind of similarity. Basic SR is formulated
as the following optimization problem:

i=l+1

(cid:3)Z(cid:3)1

min

Z

s.t. X = XZ, diag(Z) = 0

(1)

112

B. Liu et al.

where Z = [z1, z2, . . . , zn] ∈ IRn×n denotes the coeﬃcient matrix, (cid:3)Z(cid:3)1 is the
(cid:3)1-norm of Z which can promote sparse solution, (cid:3)Z(cid:3)1 =
|zij|. Then,
the graph weight matrix W could be easily obtained by W = (|Z| + |Z|T )/2

(cid:2)n

(cid:2)n

j=1

i=1

Compared with the k -NN graph, the (cid:3)1-graph avoids evaluating the hyper-
parameter k and therefore it outputs more robust result. Nevertheless, both
(cid:3)1-graph and k -NN graph are lack of the global views of data, so their perfor-
mance would be degenerated when there is no ”clean” data available [22]. In
order to capture the global data structure, Liu et al. [10] proposed LRR method
which enforces a rank minimization constraint on the coeﬃcient matrix. The
basic LRR problem can be formulated as:

(cid:3)Z(cid:3)∗

Z

min

s.t. X = XZ

(2)
where (cid:3)Z(cid:3)∗ denotes the nuclear norm of Z, which is a usual surrogate of rank
function, i.e., the sum of the singular values. Since the sparseness and low-
rankness are merits of a graph, Zhuang et al. [22] presented a non-negative
low-rank and sparse graph (NNLRS) learning method. Recently, Lu et al. [13]
pointed out that, besides (cid:3)1-norm and nuclear norm, Frobenius norm is also
an appropriate constraint for the coeﬃcient matrix Z, and presented the LSR
model with noise as follows:

λ(cid:3)Z(cid:3)2

F + (cid:3)X − XZ(cid:3)2

F

min

Z

(3)

where λ > 0 is the regularization parameter. Note there are little diﬀerences in
(1), (2) and (3), but (3) has a close-form solution

∗

Z

= (X T X + λI)

−1X T X

(4)

In this case, LSR can be solved eﬃciently.

Even though all the above approaches could output suitable graphs for GSSL,
the graph learning itself is still unsupervised. In recent work [15], Shang et
al. presented an enhanced spectral kernel (ESK) model, which makes use of
pairwise constraints to favor graph learning, and is solved as a low-rank matrix
approximation problem [9]. The main diﬀerence between ESK and our approach
is as follows. ESK uses the Gaussian kernel to initialize the weight matrix, and
encodes the known labels as the pairwise constraints. While in CLSR, we adopt
the regression coeﬃcients to measure the correlations among data points, and
consider additional local constraints to promote the model’s ﬂexibility.

Additionally, the quality of labeled points play an important role in GSSL,
thus, it is necessary to select the samples with high representability and discrim-
inability as labeled set. In [5] and [11], k-means algorithm has been veriﬁed as
an eﬀective method for sample selection. But for GSSL, an extra step is needed
to estimate the labels of clustering centers. Recently, some researchers pointed
out that collaborative representation is a promising method for sample selection
[18], [14]. In this paper, we propose a simple and eﬀective method which applies
minimal reconstruction error criterion to labeled sample selection.

Constrained Least Squares Regression for Semi-Supervised Learning

113

3 Constrained Least Squares Regression for Graph

Learning

In this section, we ﬁrst introduce the label consistent penalty for encoding known
labels, then integrate it with original LSR, and ﬁnally design its optimization
algorithm.

3.1 Label Consistent Penalty
Given two sets for labeled points, M L = {(xi, xj)} includes must-link con-
straints, where xi and xj have the same label, and CL = {(xi, xj )} covers
cannot-link constraints, where xi and xj have diﬀerent labels. Let Ω be a set of
indices which correspond to all pairwise constraints. The label consistent penalty
is deﬁned as:

(5)
where ◦ denotes the element-wise product. The sampling matrix S ∈ IRn×n is
deﬁned as:

F

f (Z) = (cid:3)S ◦ Z − L(cid:3)2

(6)

(7)

The constraint matrix L ∈ IRn×n is deﬁned as:

(cid:3)

1
0

(i, j) ∈ Ω
otherwise

Sij =

Lij =

(cid:4) 1
0
0

(i, j) ∈ M L
(i, j) ∈ CL
otherwise

Equation (5) is a squared lose function to measure the consistency between the
predicted aﬃnity matrix induced by Z and the given pairwise constraints. Here,
the pairwise constraints are expected to reﬂect the data structure. However, the
number of labeled samples is usually few so that it is hard to suﬃciently capture
the essential structure of data with them. Thus, it is necessary to bring in more
local pairwise constraints which are encoded as L

(cid:4) ∈ IRn×n:

(cid:4)
ij =

L

(cid:3)

1
0

i ∈ Nj and j ∈ Ni
otherwise

(8)

(cid:4)

where Ni stands for the set of k -nearest neighbor of xi. Actually, L
employs
a k -NN graph to roughly recover the local relations among data points by 0/1
assignments, and thus it will result in some wrong assignments. One way to ﬁx
these incorrect assignments is to utilize the original L with correct assignments
from labeled samples, and the ﬁxed Lf ∈ IRn×n is deﬁned as:
(i, j) ∈ M L or (i, j) ∈ CL
(i, j) /∈ M L and (i, j) /∈ CL

f
ij =

(9)

Lij
(cid:4)
L
ij

(cid:4)

L

From the perspective of matrix approximation, these wrong assignments in
(5) can be taken as one kind of sparse noise. Therefore, the (cid:3)1-norm is used here
instead of the Frobenius norm and we have

f (Z) = (cid:3)S ◦ Z − L(cid:3)1

(10)

114

B. Liu et al.

3.2 Objection Function

After adding the label consistent penalty to the LSR model, the objective func-
tion of CLSR is written as:

λe
2

(cid:3)Z(cid:3)2
n×n

(cid:3)XZ − X(cid:3)2

F + λs(cid:3)E(cid:3)1

s.t. E = S ◦ Z − L

F +

min
Z,E
where E ∈ IR
denotes the sparse error, λe and λs are parameters to trade
oﬀ other terms. In (11), the ﬁrst two items are used to hold the global structure
of data by Z and the third item introduces the pairwise constraints by L which
is deﬁned in (9).

(11)

3.3 Optimization

Equation (11) could be solved by the alternating direction method of multipliers
(ADMM) [3] method. To start, we introduce an auxiliary matrix A ∈ IRn×n for
variables separation, then obtain
(cid:3)XA − X(cid:3)2

s.t. E = S ◦ Z − L, Z = A

F + λs(cid:3)E(cid:3)1

(cid:3)Z(cid:3)2

(12)

F +

min
Z,E

λe
2

The augmented Lagrangian function of (12) can be written as:

Z,E

F +

(cid:3)Z(cid:3)2

L = min
(cid:3)XA − X(cid:3)2
+ < Y2, E − S ◦ Z + L > +
μ
2

λe
2

F + λs(cid:3)E(cid:3)1+ < Y1, Z − A >
((cid:3)Z − A(cid:3)2

F + (cid:3)E − S ◦ Z + L(cid:3)2
F )

(13)

where Y1 ∈ IRn×n and Y2 ∈ IRn×n are two Lagrange multipliers. ADMM ap-

proach updates the variables Z, A and E alternately with other variables ﬁxed,
and we can get the updating rules as:

Zk+1 =arg min

Z

=(1/(

2
μk

F +

μk
2

(cid:3)Zk − Ak +

(cid:3)Zk(cid:3)2
+ 1 + S)) ◦ (Ak − Y1
μk

(cid:3)2
Y1
F +
μk
+ S ◦ (

Y2
μk

(cid:3)S ◦ ZK − Ek − L − Y2
μk

μk
2
) + S ◦ Ek + S ◦ L)

(cid:3)2

F

Ak+1 = arg min
A

λe
2

= (λeX T X + μkI)

(cid:3)XAk − X(cid:3)2

F +

(cid:3)Zk+1 − Ak +
−1(λeX T X + μkZk+1 + Y1)

μk
2

(cid:3)2

F

Y1
μk

Ek+1 =arg min

E

λs(cid:3)Ek(cid:3)1 +

μk
2

(cid:3)Ek − (S ◦ Zk+1 − L − Y2

μk

)(cid:3)2

F

=S λs

μk

(S ◦ Zk+1 − L − Y2

μk

)

(14)

(15)

(16)

Constrained Least Squares Regression for Semi-Supervised Learning

115

where 1 ∈ IRn×n stands for an all-one matrix, and Sμ(·) is the shrinkage-
thresholding operator [9] which is deﬁned as:

Sμ(ν) = sign(ν)(|ν| − μ)+

(17)

The complete algorithm is summarized in Algorithm 1.

Algorithm 1. Solving Problem (11) via ADMM
Input: data matrix X, sampling matrix S, constraint matrix L, and parameters λe, λs.
1. Initialize Z0 = A0 = E0 = Y1 = Y2 = 0, μ0 = 0.1, μmax = 104, ρ = 1.1,  = 10
2. while not converged do
3.
4.

−2

Update Z, A and E by (14-16).
Update the multipliers Y1, Y2 as:
Y1 = Y1 + μ(Z − A),
Y2 = Y2 + μ(E − S ◦ Z + L).
Update μ : μ = min(ρμ, μmax).
Check the convergence conditions:
||E − S ◦ Z + L||∞ <  and ||Z − A||∞ < .

5.
6.

7. end while
Output: Zk, Ek

4 Labeled Sample Selection via CLSR

In many real applications, we need select a small part of data set as a labeled set.
Usually, a natural and simple method, random sampling is adopted. However,
this method cannot guarantee the quality of labeled samples. Based on the sub-
space assumption, we could select a more representative data subset to upgrade
graph’s performance in GSSL.
In the CLSR framework, it is convenient to use the basic LSR model for
i=1 from X, Xi ∈
labeled sample selection. We randomly select c subsets {Xi}c
and each subset contains p samples, p (cid:5) n. We consider each subset
m×p
IR
as a tiny dictionary and use it to reconstruct the whole data set, consequently,
the representative ability of each subset could be ranked by the corresponding
reconstruction error, therefore, the smaller reconstruction error it has, the more
representative it is. The reconstruction error can be solved by
s.t. Ei = X − XiZi

(18)
where Xi ∈ IRm×p denotes the selected subset, Ei ∈ IRm×n is the reconstruction
error, and Zi ∈ IRp×n is the coeﬃcient matrix of Xi. Note problem (18) has a
close-form solution

F + (cid:3)Ei(cid:3)2

λ(cid:3)Zi(cid:3)2

min
Zi,Ei

F

Zi = (X T

i Xi + λI)

−1X T

i X

(19)

The labeled sample selection method is summarized in Algorithm 2.

116

B. Liu et al.

i=1, parameter λ.

Algorithm 2. Labeled Sample Selection via Minimal Reconstruction Error
Input: data matrix X, selected subset {Xi}c
1. Initialize λ = 10
2. for i = 1, . . . , c do
Get Zi by (19).
3.
Computer the reconstruction error ri(Xi) = (cid:3)X − XiZi(cid:3)2
F .
4.
5. end for
6. Find X∗
Output: X∗

i with minimal reconstruction error arg mini ri(Xi).
i

5 CLSR for Semi-Supervised Classiﬁcation

In this section, we integrate CLSR with a popular label propagation approach,
LGC [19], for semi-supervised classiﬁcation. Deﬁne a label set F = {1, . . . , k},
and an initial label matrix Y ∈ IRm×k with Yij = 1 for xi is labeled as j and
Yij = 0 otherwise. The iterative scheme for propagation is

Yk+1 = αW Yk + (1 − α)Y0

(20)
−1/2 and D is a
where W is a normalized aﬃnity matrix with W = D
diagonal matrix whose diagonal entries are equal to the sum of corresponding
rows. We ﬁx the parameter α to 0.01 in following experiments. The detail of the
algorithm is summarized in Algorithm 3.

−1/2W D

Algorithm 3. CLSR for Semi-Supervised Classiﬁcation
Input: data matrix X, initial label matrix Y , parameters λs, λe, λ.
1. Initialize λ = 10
2. Get the labeled subset Xi by Algorithm 2 or random sampling.
3. Generate the sampling matrix S by (6) and the constraint matrix L by (9).
4. Get the coeﬃcient matrix Z by Algorithm 1.
5. Normalize all column vectors of Z to unit-norm, zi = zi/(cid:3)zi(cid:3)2.
6. Get the weight matrix W by W = (|Z| + |Z|T )/2.
7. Compute the label matrix Y by (20).
Output: Y

6 Experimental Results and Analysis

In this section, we evaluate the performance of CLSR and other popular graph
construction methods on six public databases.

6.1 Datasets and Settings

We use two categories of public datasets in the experiments, including UCI data
and image data (see Table 1).

Constrained Least Squares Regression for Semi-Supervised Learning

117

1. UCI data1. We perform experiments on three UCI datasets including WDBC,

Sonar and Parkinsons.

2. Extended YaleB database2. This face database contains 38 individuals
under 9 poses and 64 illumination conditions. We choose the cropped images
of ﬁrst 10 individuals, and resize them to 48×42 pixels.

3. ORL database3. There are 40 distinct subjects and each of them has 10
diﬀerent images. For some subjects, the images were taken at diﬀerent times,
varying the lighting, facial expressions and facial details. We resize them to
32×32 pixels.
4. COIL20 database4. This database consists of a set of gray-scale images
with 20 objects. For each object, there are 72 images of size 32×32 pixels.

Table 1. Descriptions of datasets

Dataset
WDBC
Sonar

Parkinsons

YaleB
ORL

COIL20

label Size (cid:5) of Features (cid:5) of Classes

569
208
195
640
400
1440

30
60
21
2016
1024
1024

2
2
2
10
40
20

We compare following six graph construction algorithms. There are some param-
eters in each algorithm, and we tune the parameters on each dataset for every
algorithm and record the best results.

1. k -NN: the Euclidean distance is used as similarity metric, and the Gaussian
kernel is used to reweight the edges. The number of nearest neighbors is set
to 5 for k -NN5, and 15 for k -NN15, respectively. The scale parameter of
Gaussian kernel is set as [22]

2. ESK: Following the lines of [15], a low-rank kernel is learned as the aﬃnity
matrix. ESK model also use the Gaussian kernel to initialize the weight
matrix.

3. LSR: Compared with CLSR, LSR [13] does not consider the pairwise con-

straints in graph leaning process.

4. LRR: Following [10], we construct the low-rank graph and adopt (cid:3)2,1-norm

to model ”sample-speciﬁc” corruptions.

5. NNLRS: Following [22], we construct the non-negative low-rank and sparse

graph.

6. CLSR: In CLSR, the neighbor relations are encoded as the additional pair-
wise constraints for reﬂecting the local data structure. In the experiments,
the sizes of nearest neighbors are set to 0, 5 and 15, respectively.

1

2

3

4

http://archive.ics.uci.edu/ml/
http://vision.ucsd.edu/~leekc/ExtYaleDatabase/ExtYaleB.html
http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html
http://www.cs.columbia.edu/CAVE/software/softlib/coil-20.php

118

B. Liu et al.

6.2 Results and Discussions

All experiments are repeated 20 times, for each dataset, the label rate varies
from 10% to 40%. Table 2 lists the average accuracies.

From Table 2 we can get following observations.

1. LSR, LRR and NNLRS generally outperform k -NN and ESK on YaleB and
ORL datasets, as these datasets have roughly subspace structures. Corre-
spondingly, datasets WDBC, Parkinsons cater to Euclidean distance-based
measurement, so k -NN, ESK can work well on these datasets.

2. NNLRS usually achieves better performance than LSR and LRR, owing to

it considers both sparseness and low-rankness of the graph.

3. ESK generally outperforms k -NN with the increasing of the sampling per-
centage, which testiﬁes the eﬀectiveness of integrating pairwise constraints
into the graph learning process.

4. In most cases, CLSR outperforms other algorithms, since it takes advantage
of both the self-expressive similarity and local constraints to enhance the
model’s ﬂexibility and performance.

Table 2. Average accuracies (mean and standard deviation) of diﬀerent graphs inte-
grated with LGC label propagation strategy (The best results are highlighted in bold)

Dataset

k-NN5

k-NN15

ESK

LSR

LRR

NNLRS

CLSR

Sonar(10%)
Sonar(20%)
Sonar(30%)
Sonar(40%)

YaleB(10%)
YaleB(20%)
YaleB(30%)
YaleB(40%)

WDBC(10%)
WDBC(20%)
WDBC(30%)
WDBC(40%)

93.56±1.26 93.54±0.80 92.41±1.50 89.01±1.70 91.14±1.48 91.11±1.43 94.27±1.23
94.02±0.56 94.08±0.67 94.20±1.15 91.86±1.34 93.27±1.21 92.41±0.87 95.09±0.39
94.84±0.55 94.87±0.63 94.90±0.74 93.51±1.10 94.15±1.03 93.59±0.76 96.18±0.27
95.52±0.51 95.10±0.59 95.60±0.43 94.75±0.89 95.34±0.94 94.64±0.60 96.85±0.26
73.44±4.21 73.41±4.48 67.18±4.34 67.90±4.32 68.37±7.55 71.06±3.98 74.40±3.19
75.60±3.38 76.87±3.08 76.54±3.56 74.25±3.06 75.10±3.20 76.39±3.11 81.38±2.99
79.71±2.03 78.32±3.25 81.82±3.13 79.46±2.42 80.94±2.92 81.39±2.05 85.60±1.66
83.55±1.39 85.20±2.76 85.37±2.24 83.54±1.85 83.31±2.01 85.38±1.13 88.75±0.94
Parkinsons(10%) 75.82±6.23 72.66±3.93 75.27±6.18 67.12±3.54 74.11±3.33 76.10±3.48 77.95±3.26
Parkinsons(20%) 79.24±5.49 72.00±3.17 82.44±5.10 74.43±3.29 77.58±2.34 80.72±1.88 83.59±2.19
Parkinsons(30%) 80.57±4.66 72.29±2.93 85.91±4.78 79.24±2.80 81.66±1.86 82.87±1.55 87.44±1.06
Parkinsons(40%) 81.19±3.98 72.10±2.44 88.34±3.81 82.37±2.10 84.68±2.12 86.26±1.19 89.05±0.85
69.06±2.25 66.98±6.48 60.23±2.51 87.80±1.74 87.88±2.11 88.86±2.35 88.85±1.65
75.91±2.17 74.72±1.53 71.96±2.12 94.21±1.05 94.08±0.96 93.70±1.02 94.37±0.88
79.58±1.89 78.64±2.11 77.82±1.23 96.28±0.94 96.20±0.79 95.39±0.58 96.65±0.43
79.92±1.72 79.96±1.76 81.69±0.83 97.39±0.73 97.00±0.57 96.45±0.41 97.69±0.35
40.47±3.39 44.88±3.03 53.50±3.71 49.95±3.78 50.49±3.85 52.58±4.23 54.52±3.07
ORL(10%)
63.88±2.10 66.05±3.36 72.60±2.33 72.15±2.79 71.95±3.24 75.10±2.92 76.65±2.39
ORL(20%)
78.63±5.88 78.95±5.55 82.98±2.35 83.75±2.19 83.98±2.73 84.33±2.73 85.50±2.14
ORL(30%)
86.17±3.19 84.52±3.93 88.20±2.01 91.53±1.65 90.19±2.24 90.95±2.38 92.50±1.91
ORL(40%)
COIL20(10%) 86.12±0.81 85.80±1.01 86.24±1.21 80.10±1.52 79.38±2.54 81.39±1.55 88.12±1.07
COIL20(20%) 88.24±0.76 86.23±0.91 88.50±1.18 87.85±1.16 87.39±1.18 87.26±1.06 90.84±0.73
COIL20(30%) 89.88±0.85 87.82±1.02 90.69±0.77 91.35±0.92 90.57±1.11 89.96±0.81 92.93±0.69
COIL20(40%) 90.17±0.80 88.61±0.84 92.10±0.60 93.28±0.71 92.98±0.92 92.47±0.74 94.58±0.55

Next, we study the eﬀectiveness of sample selection strategy based on mini-
mal reconstruction error. We ﬁrst randomly select 50 labeled subsets from each
dataset, and then sort them in ascending order to form a subset-residual array
according to the representative residual of each subset. Secondly, these labeled
subsets are used as the supervisory information for classiﬁcation and the average
accuracies are recorded. Furthermore, another two results are listed for compari-
son, one is the average accuracy of the top 10% of the array (denoted as AT-10%),

Constrained Least Squares Regression for Semi-Supervised Learning

119

the other is the average accuracy of the lowest 10% of the array (denoted as AL-
10%). The percentage of labeled samples is 5% on WDBC, Parkinsons, YaleB,
COIL20 and Sonar, because the selection strategy could be useful in case that
there are only limited labeled samples available, especially, we select 20% of sam-
ples from ORL, since there are only 10 samples in each class of ORL.

The results are plotted in Fig. 1(a-f). It shows that our method is almost ef-
fective for all graph construction approaches on each dataset, except Parkinsons.
The result on Parkinsons is unstable. The reason is that there are two classes in
Parkinsons, but its imbalance ratio is nearly 3. In this case, our method tends to
select more samples from the majority class to minimize the total reconstruction
error, which leads to that the selected samples are incapable of capturing the
true geometric structure of the dataset. We balance the sizes of two classes by
randomly selecting some samples from the majority class, and the result shown
in Fig. 1(g) is consistent with the other datasets’.

AL(cid:882)10%

Average

AT(cid:882)10%

AL(cid:882)10%

Average

AT(cid:882)10%

k(cid:882)NN5 k(cid:882)NN15

ESK

LSR

LRR NNLRS CLSR

Different(cid:3)Algorithms
(a) WDBC 
Average

AL(cid:882)10%

AT(cid:882)10%

k(cid:882)NN5 k(cid:882)NN15

ESK

LSR

LRR NNLRS CLSR

Different(cid:3)Algorithms
(b) Sonar 
Average

AL(cid:882)10%

AT(cid:882)10%

k(cid:882)NN5 k(cid:882)NN15

ESK

LSR

LRR NNLRS CLSR

k(cid:882)NN5 k(cid:882)NN15

ESK

LSR

LRR NNLRS CLSR

Different(cid:3)Algorithms
(c) Parkinsons 
AL(cid:882)10%

Average

AT(cid:882)10%

Different(cid:3)Algorithms
(d) YaleB 
Average

AL(cid:882)10%

AT(cid:882)10%

k(cid:882)NN5 k(cid:882)NN15

ESK

LSR

LRR NNLRS CLSR

Different(cid:3)Algorithms
(e) ORL 

k(cid:882)NN5 k(cid:882)NN15

ESK

LSR

LRR NNLRS CLSR

Different(cid:3)Algorithms
(f) COIL20 

y
c
a
u
r
c
c
A

0.75

0.7

0.65

0.6

0.55

0.5

AL(cid:882)10%

Average

AT(cid:882)10%

k(cid:882)NN5 k(cid:882)NN15

ESK

LSR

LRR NNLRS CLSR

Different(cid:3)Algorithms

Fig. 1. Classiﬁcation results of all graph construction algorithms on each dataset after
applying sample selection strategy

(g) Parkinsons (balanced) 

y
c
a
u
r
c
c
A

y
c
a
u
r
c
c
A

1

0.95

0.9

0.85

0.8

0.75

0.8

0.75

0.7

0.65

0.6

0.55

0.8

0.75

y
c
a
u
r
c
c
A

0.7

0.65

0.6

0.55

0.75

0.7

y
c
a
u
r
c
c
A

0.65

0.6

0.55

0.5

y
c
a
u
r
c
c
A

0.8
0.7
0.6
0.5
0.4
0.3
0.2

y
c
a
u
r
c
c
A

0.9
0.85
0.8
0.75
0.7
0.65
0.6
0.55

120

B. Liu et al.

7 Conclusion

We propose a new graph based semi-supervised learning approach called CLSR,
which utilizes the pairwise constraints to guide the graph learning process. Beside
the labeled information, there constraints also bring in local neighbor relations
to enhance the graph’s ﬂexibility. In addition, based on CLSR, we design a
labeled sample selection strategy which is used to select more representative
points as a labeled set. Experimental results on real world datasets demonstrate
the eﬀectiveness of our method. Furthermore, given a small size of labeled set
(e.g., 5% of total samples), our sample selection strategy could generally improve
the performance of several state-of-the-art methods on most of the datasets used
in the experiments.

Acknowledgments. This work was supported in part by the National Natural
Science Foundation of China under Grant 61375062, Grant 61370129, and Grant
61033013, the Ph.D Programs Foundation of Ministry of Education of China un-
der Grant 20120009110006, the National 863 project under Grant 2012AA040912,
the Opening Project of State Key Laboratory of Digital Publishing Technology,
and the Fundamental Research Funds for the Central Universities.

References

1. Belkin, M., Niyogi, P.: Laplacian eigenmaps for dimensionality reduction and data

representation. Neural Computation 15(6), 1373–1396 (2003)

2. Belkin, M., Niyogi, P., Sindhwani, V.: Manifold regularization: A geometric frame-
work for learning from labeled and unlabeled examples. The Journal of Machine
Learning Research 7, 2399–2434 (2006)

3. Boyd, S., Parikh, N., Chu, E., Peleato, B., Eckstein, J.: Distributed optimization
and statistical learning via the alternating direction method of multipliers. Foun-
dations and Trends R(cid:4) in Machine Learning 3(1), 1–122 (2011)

4. Chapelle, O., Weston, J., Sch¨olkopf, B.: Cluster kernels for semi-supervised learn-

ing. In: Advances in Neural Information Processing Systems, pp. 585–592 (2002)

5. Chen, X., Cai, D.: Large scale spectral clustering with landmark-based represen-

tation. In: The 25th Conference on Artiﬁcial Intelligence, AAAI 2011 (2011)

6. Elhamifar, E., Vidal, R.: Sparse subspace clustering. In: Proceedings of the 22th
Conference on Computer Vision and Pattern Recognition, pp. 2790–2797. IEEE
(2009)

7. Jebara, T., Wang, J., Chang, S.F.: Graph construction and b-matching for semi-
supervised learning. In: Proceedings of the 26th International Conference on Ma-
chine Learning, pp. 441–448. ACM (2009)

8. Li, Z., Liu, J., Tang, X.: Pairwise constraint propagation by semideﬁnite program-
ming for semi-supervised classiﬁcation. In: Proceedings of the 25th International
Conference on Machine Learning, pp. 576–583 (2008)

9. Lin, Z., Chen, M., Ma, Y.: The augmented lagrange multiplier method for exact
recovery of corrupted low-rank matrices. UIUC Technical report UILU-ENG-09-
2215 (2010)

Constrained Least Squares Regression for Semi-Supervised Learning

121

10. Liu, G., Lin, Z., Yan, S., Sun, J., Yu, Y., Ma, Y.: Robust recovery of subspace
structures by low-rank representation. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 171–184 (2013)

11. Liu, W., He, J., Chang, S.F.: Large graph construction for scalable semi-supervised
learning. In: Proceedings of the 27th International Conference on Machine Learn-
ing, pp. 679–686 (2010)

12. Liu, W., Wang, J., Chang, S.F.: Robust and scalable graph-based semisupervised

learning. Proceedings of the IEEE 100(9), 2624–2638 (2012)

13. Lu, C.-Y., Min, H., Zhao, Z.-Q., Zhu, L., Huang, D.-S., Yan, S.: Robust and eﬃcient
subspace segmentation via least squares regression. In: Fitzgibbon, A., Lazebnik,
S., Perona, P., Sato, Y., Schmid, C. (eds.) ECCV 2012, Part VII. LNCS, vol. 7578,
pp. 347–360. Springer, Heidelberg (2012)

14. Peng, X., Zhang, L., Yi, Z.: Scalable sparse subspace clustering. In: IEEE Proceed-
ings of the 26th Conference on Computer Vision and Pattern Recognition (2013)
15. Shang, F., Jiao, L., Liu, Y., Tong, H.: Semi-supervised learning with nuclear norm

regularization. Pattern Recognition 46(8), 2323–2336 (2013)

16. Tenenbaum, J.B., De Silva, V., Langford, J.C.: A global geometric framework for

nonlinear dimensionality reduction. Science 290(5500), 2319–2323 (2000)

17. Yan, S., Wang, H.: Semi-supervised learning by sparse representation. In: SDM,

pp. 792–801 (2009)

18. Zhang, L., Yang, M., Feng, X.: Sparse representation or collaborative represen-
tation: Which helps face recognition? In: Proceedings of the 12th International
Conference on Computer Vision, pp. 471–478. IEEE (2011)

19. Zhou, D., Bousquet, O., Lal, T.N., Weston, J., Sch¨olkopf, B.: Learning with local
and global consistency. Advances in Neural Information Processing Systems 16(16),
321–328 (2004)

20. Zhu, X.: Semi-supervised learning literature survey. Technical report, Department

of Computer Science, University of Wisconsin-Madison (2006)

21. Zhu, X., Ghahramani, Z., Laﬀerty, J., et al.: Semi-supervised learning using gaus-
sian ﬁelds and harmonic functions. In: Proceedings of the 20th International Con-
ference on Machine Learning, vol. 3, pp. 912–919 (2003)

22. Zhuang, L., Gao, H., Lin, Z., Ma, Y., Zhang, X., Yu, N.: Non-negative low rank and
sparse graph for semi-supervised learning. In: Proceedings of the 25th Conference
on Computer Vision and Pattern Recognition, pp. 2328–2335. IEEE (2012)


