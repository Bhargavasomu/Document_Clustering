Generating Diverse Ensembles to Counter the Problem

of Class Imbalance

T. Ryan Hoens and Nitesh V. Chawla

The University of Notre Dame, Notre Dame, IN 46556, USA

{thoens,nchawla}@nd.edu

Abstract. One of the more challenging problems faced by the data mining
community is that of imbalanced datasets. In imbalanced datasets one class
(sometimes severely) outnumbers the other class, causing correct, and useful
predictions to be difﬁcult to achieve. In order to combat this, many techniques
have been proposed, especially centered around sampling methods. In this paper
we propose an ensemble framework that combines random subspaces with sam-
pling to overcome the class imbalance problem. We then experimentally verify
this technique on a wide variety of datasets. We conclude by analyzing the per-
formance of the ensembles, and showing that, overall, our technique provides a
signiﬁcant improvement.

1 Introduction

A common problem faced in data mining is dealing with “imbalanced datasets”, or
datasets in which one class vastly outnumbers the other in the training data. For most
classiﬁers this causes a problem, as merely classifying every instance as the majority
class present in the training data can result in very high accuracy. Therefore when deal-
ing with imbalanced datasets, sampling methods such as oversampling, undersampling,
and sampling by synthetically generating instances, SMOTE, have become standard
approaches for improving classiﬁcation performance [1]. Performance is improved as
the sampling methods alter the training data distribution, biasing the data towards the
minority class.

In addition to sampling methods, ensemble methods [2] have also been used to
improve performance on imbalanced datasets. They combine the power of multiple
(usually weak) classiﬁers trained on similar datasets to provide accurate predictions for
future instances. The training data is often varied in such a way as to give each classiﬁer
a (slightly) different dataset so as to avoid overﬁtting. The popular ensemble methods,
bagging [3] and boosting [4], have been adapted with sampling strategies to counter the
issue of high class imbalance [5–7]. These methods work on the entire feature space as
they focus on modifying the selection of instances in the ensemble training sets.

We posit that sampling methods, especially SMOTE, might stand to gain more when
working in a reduced feature space as doing so will not only inject more diversity into
the ensemble via the learning algorithm, but also via the bias of the sampling algorithm.
To this end, we propose an extension to the random subpace method [8] that integrates
SMOTE (in Section 2). We test on 21 widely available datasets with varying degrees

M.J. Zaki et al. (Eds.): PAKDD 2010, Part II, LNAI 6119, pp. 488–499, 2010.
c(cid:2) Springer-Verlag Berlin Heidelberg 2010

Generating Diverse Ensembles to Counter the Problem of Class Imbalance

489

of class imbalance (Section 4 includes the results). We comprehensively compare the
proposed random subspace and SMOTE combination with bagging, AdaBoost, random
subspaces, random forest, and the combination of random subspace and undersampling.
Using the statistical tests recommended by Demˇsar, we determine the statistical signif-
icance of results. We show that sampling methods, combined with random subspaces
are more effective than the other combinations of ensemble and sampling methods. We
explain this behavior by invoking the notion of diversity.

To summarize, the contributions of this paper are as follows:

1. An extension to the random subspace method to deal with the class imbalance prob-

lem.

2. Empirical evaluation on a wide variety of imbalanced datasets, and establishing the

superiority of the new ensemble framework.

3. Analyzing the performance of the methods using the measures of diversity.

2 Using Random Subspaces to Improve Performance on

Imbalanced Datasets

The random subspace method (RSM) [8], described in Algorithm 1, is an ensemble
method in which multiple classiﬁers are learned on the same dataset, but each classiﬁer
only actively uses a subset of the available features.

Because each classiﬁer learns on incomplete data, each individual classiﬁer is less
effective than a single classiﬁer trained on all of the data. However since the RSM
combines multiple classiﬁers of this type, each with its own bias based on the features
it sees, it sees an increase in performance over the base classiﬁer.

Algorithm 1. The random subspace method.
Require: Training set X with n instances and m features, number of features to consider 0 <

m(cid:2) < m, and number of classiﬁers to train e > 0.

Ensure: CLASSIFIER is the model trained on training set X, consisting of e classiﬁers.

for i = 1 to e do
Select F , a random subset of the features such that |F| = m(cid:2)
Let X(cid:2) ← X.
for all f such that f is a feature of X(cid:2)

do

if f /∈ F then

Remove feature f from X(cid:2)

.

end if
end for
Train CLASSIFIERi on dataset X(cid:2)

.

end for

2.1 Random Subspace Method + Sampling

Sampling is a popular methodology to counter the problem of class imbalance.
However sampling methods, especially SMOTE, work with the entire set of features
or dimensions and may not be as effective in reducing the sparsity of the data. High

490

T.R. Hoens and N.V. Chawla

Algorithm 2. The random subspace method with an added resampling step.
Require: Training set X with n instances and m features, number of features to consider 0 <
m(cid:2) < m, number of classiﬁers to train e > 0, and sampling function S which takes a dataset
as a parameter, and returns a dataset.

Ensure: CLASSIFIER is the model trained on training set X, consisting of e classiﬁers.

for i = 1 to e do
Select F , a random subset of the features such that |F| = m(cid:2)
Let X(cid:2) ← X.
for all f such that f is a feature of X(cid:2)

do

if f /∈ F then

Remove feature f from X(cid:2)

.

end if
end for
s ← S(X(cid:2)).
X(cid:2)
Train CLASSIFIERi on dataset X(cid:2)
s.

end for

dimensionality is an Achilles’ heel for sampling approaches like SMOTE, as they can
lead to a higher degree of variance given low density and separation among classes
because of the features’ spread. Intuitively, applying SMOTE to reduced subspaces at a
time will also control the amount of variance that SMOTE may introduce.

Sampling methods consider the class skew and properties of the dataset as a whole.
Datasets often exhibit characteristics and properties at a local, rather than global level.
Hence, it becomes important to analyze and consider the datasets in the reduced sub-
space. We also posit that by ﬁrst randomly selecting a reduced subspace, sampling along
that subspace, and then learning a classiﬁer will induce a higher diversity, which is a
necessary condition for improved performance of classiﬁers.

To this end we propose using SMOTE within each randomly selected subspace. Since
SMOTE creates synthetic instances by interpolating feature values based on neighbors,
we see that it is dependent upon the distance metric used to determine nearest neigh-
bors. Thus by removing features from the feature space, we see that we are altering the
distance between instances, and therefore (potentially) changing the way in which each
classiﬁer modiﬁes its training data. That is, we create a hybrid of the RSM by combining
it with SMOTE to create RSM+SMOTE. Just like in the RSM during the training phase
each classiﬁer is trained on a subset of the data in which some features are removed.
After removing a subset of the features, SMOTE is then applied to the dataset which is
subsequently used to train the classiﬁer. Since SMOTE is dependent upon the features,
and since in ensemble methods having classiﬁers with different biases is optimal, this
should provide better performance over other techniques.

A generalized version of this algorithm (RSM+sampling) is given in Algorithm 2.
In this algorithm as opposed to explicitly using SMOTE, a generic sampling method is
supplied as a parameter for use in the algorithm. That is, RSM+SMOTE is a special case
of RSM+sampling curried with SMOTE as the sampling method. In our tests we also
consider using random undersampling as the sampling method. This lacks the appeal
of SMOTE, however, as it does not depend upon the subspace chosen, and is thus more
like random forests.

Generating Diverse Ensembles to Counter the Problem of Class Imbalance

491

Note that while in principal any classiﬁer can be learned, we use C4.5 decision trees
as is common in the literature. In order to determine the predicted class of the ensem-
ble, we consider simple majority voting of the classiﬁers, similar to the other ensemble
methods discussed. As an area of future work Chawla and Sylvester [9] outline a pro-
cedure for weighted voting as applied to imbalance datasets. We will also be including
Hellinger Distance Decision Trees as part of future work [10].

Complexity Analysis. When discussing ensemble methods, an important consider-
ation is the method’s computational complexity. In this section we therefore give an
overview of the complexity of the scheme considered in the paper. That is we only
consider the case of RSM+SMOTE with C4.5 decision trees as the classiﬁer.

Let us now consider building an ensemble of e C4.5 decision trees using random
subspaces and SMOTE on a dataset D consisting of n instances and m features. From
the pseudo-code given in Algorithm 2, we see that each iteration of the for-loop ﬁrst
features O(m(cid:2)). The algorithm then (in our case) applies SMOTE which
selects m(cid:2)
has a complexity of O(n2
p), where np denotes the number of positive (minority) class
examples in D. Applying SMOTE to the dataset results in additional instances being
added to the dataset, leaving us with n(cid:2)
instances. Finally a C4.5 decision tree is learned
on the resulting dataset which has complexity O(n(cid:2)m(cid:2) log n(cid:2)). Combining this, we see
p + n(cid:2)m(cid:2) log n(cid:2)) = O(n2
that each iteration of the loop has a complexity of O(m(cid:2) + n2
p +
n(cid:2)m(cid:2) log n(cid:2)). Therefore the complexity of constructing RSM+SMOTE is O(e · (n2
p +
n(cid:2)m(cid:2) log n(cid:2))).

As with most ensembles, we can achieve an order e speedup to our method if we
instead build each tree in parallel. That is we break the task up into e jobs and distribute the
tree building process to e cores. Additionally since the voting function is simple majority
voting, the trees can remain distributed if the cost of transferring them is too high.

Table 1. Legend of abbreviations

Full Name
Bagging
AdaBoost

Random Subspace Method

Random Forest

Synthetic Minority

Over-sampling Technique

Abbreviation

BG
BT
RSM
RF

SMOTE

Undersampling

usamp

Random Subspace Method RSM+SMOTE

with SMOTE

Random Subspace Method RSM+usamp

with Undersampling

3 Experimental Design

We used the open source tool Weka, and implemented our classiﬁer RSM+sampling. In
order to test
the robustness of our method compared to existing methods, we

492

T.R. Hoens and N.V. Chawla

Table 2. Details of the datasets in this report

Dataset
boundary
breast-w
breast-y

cam

compustat
covtype
credit-g
estate

fourclass

germannumer

heart-v

Dataset
hypo
ism
oil
page

# Features # Examples CV
0.93
0.31
0.41
0.90
0.92
0.86
0.40
0.76
0.29 SVMguide1
0.40 tic-tac-toe
0.49

3505
699
286
18916
13657
38500
1000
5322
862
1000
200

175
9
9
132
20
10
20
12
2
24
13

phoneme

PhosS
pima

satimage

# Features # Examples CV
0.90
0.95
0.91
0.80
0.41
0.89
0.30
0.81
0.29
0.31

3163
11180
937
5473
5404
11411
768
6430
3089
958

25
6
49
10
5
480
8
36
4
9

included AdaBoostM1 (BT), bagging (BG), Random Forests (RF), and Random Sub-
spaces (RSM) in our experiments. We also applied SMOTE and undersampling (usamp)
to the entire dataset and then learned a single classiﬁer. For all of the methods we use
the J48 decision tree as the base classiﬁer with Laplace smoothing and no pruning. As
each of the ensemble methods requires a number of classiﬁers to train, we train 100
classiﬁers for each. See Table 1 for the entire set of classiﬁers used in this paper.

We evaluated each of the classiﬁers on the twenty-one datasets from a number of
different resources, including ﬁnance, biology, oil spill, medicine, UCI and LibSVM
data repositories (Table 2) [10–12]. Similar to [13], we use the coefﬁcient of variation
(“CV”) to measure imbalance. To determine the CV, we calculate the ratio of the mean
and standard deviation of the class counts in the dataset. As we seek to determine our
classiﬁers’ performance on imbalanced datasets, we choose to test on datasets with a CV
above 0.28. This corresponds to a dataset for which less than 35% of the instances are
minority class. In general, the larger the value of CV, the more imbalanced the dataset.

3.1 Experiments

In order to compare the classiﬁers, we use 10-fold cross validation. In 10-fold cross
validation, each dataset is broken into 10 disjoint sets such that each set has (roughly)
the same distribution. The classiﬁer is learned 10 times such that in each iteration a dif-
ferent set is withheld from the training phase, and used instead to test the classiﬁer. We
then compute the AUROC (Area Under the Receiver Operating Characteristic) value as
the average of each of these runs.

3.2 Statistical Tests

While many different techniques have been applied to attempt to compare classiﬁer
performance across multiple datasets, Demˇsar suggests comparisons based on ranks.
Following the strategy outlined in [14], we rank the performance of each classiﬁer by
its average AUROC, with 1 being the best. Since we seek to determine whether or not

Generating Diverse Ensembles to Counter the Problem of Class Imbalance

493

our new methods are statistically signiﬁcantly better than the existing methods, we use
the Friedman and Bonferroni-Dunn tests [14].

The Friedman test is ﬁrst applied to determine if there is a statistically signiﬁcant
difference between the rankings of the classiﬁers. That is, it tests to see if the rankings
are not merely randomly distributed. Next, as recommended by Demˇsar, we preform
the Bonferroni-Dunn test to compare each classiﬁer against the control classiﬁer.

4 Results

From Table 3 it is apparent that RSM+SMOTE performs signiﬁcantly better than the
other classiﬁers, achieving an average ranking that is over a full rank better than the next
best classiﬁer, RSM+usamp. This is further reinforced by noting that RSM+SMOTE is
the best classiﬁer in 12 of the 21 datasets, and second best on 5. RSM+SMOTE not
only outperforms bagging and boosting, but also applying SMOTE on the entire dataset
before learning a classiﬁer. The results of using SMOTE on the entire dataset are not
included in this paper due to space restrictions, as well as to keep the focus on ensemble
based methods.

Out of all of the datasets, RSM+SMOTE achieves its worst rank of 4 on the four-
class dataset. This is unsurprising, however, as fourclass is not a suitable dataset to use
RSM+SMOTE on, as it only has 2 features. Since RSM chooses a subspace of the fea-
tures to learn on, it is necessary that there are enough features to gain power from only

Table 3. Rank of each classiﬁer on the datasets

breast-w
breast-y

pima
hypo

phoneme

ism

SVMguide1

cam

credit-g

ion

covtype
satimage
PhosS
fourclass
tic-tac-toe
compustat

estate
heart-v
boundary

oil
page

Average

RSM+SMOTE RSM+usamp RF RSM BG BT
6.0
3.0
6.0
5.0
6.0
5.0
6.0
5.0
6.0
1.0
6.0
4.0
6.0
5.0
6.0
5.0
6.0
4.0
6.0
4.0
6.0
2.0
6.0
1.0
6.0
5.0
2.0
1.0
4.0
1.0
6.0
2.0
6.0
4.0
6.0
5.0
6.0
4.0
6.0
4.0
6.0
5.0
3.571 3.452 3.762 5.714

2.0
3.0
1.0
1.0
3.0
1.0
2.0
1.0
1.0
1.0
1.0
2.0
2.0
4.0
3.0
1.0
1.0
2.0
1.0
1.0
1.0
1.667

1.0
1.0
2.0
2.0
5.0
2.0
4.0
2.0
3.0
3.0
5.0
3.0
1.0
5.0
5.0
4.0
3.0
1.0
2.0
3.0
2.5
2.833

4.0
2.0
3.0
3.0
4.0
3.0
3.0
3.0
2.0
2.0
4.0
4.0
3.0
6.0
6.0
5.0
5.0
3.0
3.0
2.0
2.5

5.0
4.0
4.0
4.0
2.0
5.0
1.0
4.0
5.0
5.0
3.0
5.0
4.0
3.0
2.0
3.0
2.0
4.0
5.0
5.0
4.0

494

T.R. Hoens and N.V. Chawla

Table 4. Table denoting whether or not RSM+SMOTE is statistically signiﬁcantly better than all
other classiﬁers at various conﬁdence levels on the datasets

Conﬁdence Level RSM+usamp RF RSM BG BT
(cid:2) (cid:2) (cid:2) (cid:2)
(cid:2) (cid:2) (cid:2) (cid:2)
(cid:2) (cid:2) (cid:2) (cid:2)

(cid:2)
(cid:2)

99%
95%
90%

considering such a subset. Since fourclass only has 2 features, however, each classiﬁer
only classiﬁes based on one feature. Similarly when SMOTE is applied to the feature
there is only one axis of freedom, and the effect is that of placing instances somewhat
randomly on a line. Intuitively this does not seem like an optimal strategy, and explains
the poor performance of the classiﬁer on the dataset. This also leads us to not recom-
mend using RSM+SMOTE on such low dimensional datasets.

RSM+usamp also performs very well when compared to the other classiﬁers. On 10
of the 21 datasets it obtains the best or second best AUROC among the tested methods.
This points to the sampling of individual subspaces to be a robust technique, as the two
presented techniques offer a great advantage over the other classiﬁers.

In order to measure the statistical signiﬁcance of the results, we use the methods
outlined in Section 3.2. The results of the tests are presented in Table 4. In this ﬁg-
ure we show the results of the Friedman and Bonferroni-Dunn tests which show that
RSM+SMOTE performs statistically signiﬁcantly better than all classiﬁers at the 95%
conﬁdence level. At 99% conﬁdence it outperforms all of the classiﬁers except for RSM+
usamp. This is a strong result as it shows that the RSM can beneﬁt greatly from applying
sampling at the individual classiﬁer level in addition to using a subspace of the features.

5 Analysis of Ensembles Using Diversity

When considering an ensemble of classiﬁers, the ideal situation would be a case where
no two classiﬁers agree on instances on which they err. Given the nature of classiﬁers
and data, however, achieving this ideal is highly unlikely. In order to determine how
close an ensemble comes to the ideal, we measure the diversity by the degree to which
an ensemble of classiﬁers disagree on such instances. Kuncheva and Whitaker [15]
show that such diversity is an important property for achieving a good performance
from ensembles.

In order to measure the diversity of the ensembles, we use the κ metric deﬁned by

Dietterich [16] as:

Θ1 =

Θ2 =

κ =

(cid:2)T

i=1 Cii
m
⎛

T(cid:3)

T(cid:3)

⎝

i=1

Θ1 − Θ2
1 − Θ2

,

⎞

⎠ ,

Cij
m

T(cid:3)

j=1

Cji
m

j=1

,

Generating Diverse Ensembles to Counter the Problem of Class Imbalance

495
where T is the number of classes, C is a T × T matrix such that Cij denotes the number
of instances assigned to class i by the ﬁrst classiﬁer and class j by the second classiﬁer,
and m is the number of instances. Given this, Θ1 measures the degree of agreement, and
Θ2 is the degree of agreement expected at random. κ is then the measure of diversity,
where 0 denotes agreement purely by chance, and 1 (−1) denotes perfect agreement
(disagreement). The κ values can then be plotted against the accuracy for each pair of
classiﬁers in the ensemble in order to obtain a graphical representation of the diversity
of the ensemble. The x-axis is the κ value and the y-axis is the accuracy.

Fig. 1. κ plots for RSM+SMOTE (left) and AdaBoost (right) on the oil dataset

We now show some of the representative results explaining the behavior. We consider
three representative results. Figure 1 shows the κ plot of RSM+SMOTE and AdaBoost
on the oil dataset. Given the two plots, it becomes apparent why RSM+SMOTE outper-
forms AdaBoost on this dataset. While RSM+SMOTE shows some agreement among
the classiﬁers along with high accuracy, AdaBoost shows much lower agreement (in
fact almost completely random agreement) with similar accuracy. Since the classiﬁers
in AdaBoost are agreeing almost at random, and the problem is very imbalanced, this
is close to what we would expect of random classiﬁers which are biased towards the
majority class. Alternatively, RSM+SMOTE shows very high accuracy with non neg-
ligible agreement. This points to highly accurate classiﬁers which agree when they are
correct, and disagree on incorrectly classiﬁed instances, as desired.

While the previous example showed RSM+SMOTE performing well, Figure 2 de-
picts it being outperformed by RF. While AdaBoost underperformed for being too dis-
similar, RSM+SMOTE underperformed for being too similar. As can be seen in the
ﬁgure, RSM+SMOTE has a cluster of points at κ = 1. This means that the classiﬁers
were in perfect agreement on every example. This adversely affects the diversity of
the ensemble, effectively adding weighted classiﬁers to the ensemble. RF, on the other
hand, shows highly accurate, yet diverse, classiﬁers.

Finally we show an instance where RSM+SMOTE outperforms RSM+usamp in
Figure 3. This plot shows precisely the desired results for RSM+SMOTE. That is, since
the classiﬁers are highly accurate, they should have a high κ value which is approxi-
mately twice the error rate. While the κ values are in the higher strata, none of them
are actually 1, which means that classiﬁers never have a perfect agreement and are
diverse, despite high accuracies. RSM+usamp, on the other hand, shows similar high

496

T.R. Hoens and N.V. Chawla

Fig. 2. κ plots for RSM+SMOTE (left) and RF (right) on the phoneme dataset

Fig. 3. κ plots for RSM+SMOTE (left) and RSM+usamp (right) on the covtype dataset

accuracies, yet many classiﬁers overlap as demonstrated by the number of points at
κ = 1. As aforementioned this directly affects the diversity, and it is therefore unsur-
prising that RSM+SMOTE outperformed RSM+usamp.

6 Related Work

One popular approach towards improving performance on classiﬁcation problems is to
use ensembles. When using ensembles one attempts to leverage the classiﬁcation power
of multiple classiﬁers (learned on different subsets of the training data), to overcome
the downsides of traditional classiﬁcation algorithms, such as over ﬁtting. Dietterich
[2] provides a broad overview as to why ensemble methods often outperform a single
classiﬁer. In fact Hansen and Salamon [17] prove that under certain constraints (the
average error rate is less than 50% and each classiﬁer is erroneous independent of the
others), the expected error rate of an instance can go to zero as the number of classiﬁers
goes to inﬁnity. Thus when seeking to build multiple classiﬁers, it is optimal to ensure
that the classiﬁers are diverse. One way of ensuring this is by modifying the training
data each classiﬁer is learned on.

Two techniques for modifying the training data used by each classiﬁer are bagging
[3] and AdaBoost [4]. In bagging, introduced by Breiman, each training set is chosen by
sampling (with replacement) from the original training set T . In AdaBoost introduced

Generating Diverse Ensembles to Counter the Problem of Class Imbalance

497

by Freund and Schapire, however, each training set is iteratively chosen based on the
difﬁcult to classify instances. That is classiﬁers are iteratively learned, and misclassiﬁed
instances are more likely to be chosen in later training sets.

Both of the aforementioned techniques have their advantages and disadvantages.
Bagging is trivially parallelizeable, and thus more amenable to building a large en-
semble, however does not reduce the bias. AdaBoost, on the other hand reduces both
variance and bias and has theoretical guarantees, but is sensitive to noise. It is there-
fore dependent upon the dataset which technique will provide better results. Because of
this, Kotsiantis and Pintelas [18] devise a methodology of combining both bagging and
AdaBoost in order to create a better ensemble of classiﬁers.

While the previous techniques modiﬁed the training set by altering the distribution of
examples, the random subspace method [8] modiﬁes the examples themselves. That is,
when attempting to build an ensemble with n classiﬁers, each classiﬁer independently
chooses a subset of the features to use for training. Thus while most classiﬁers suffer
from the curse of dimensionality, the random subspace method mitigates this by pruning
the feature space.

Finally the random forest technique [19] combines the above techniques by using
bagging to create the training sets, and then (optionally) applying random subspaces to
each training set individually before learning the ﬁnal classiﬁer. This has the effect of
generally producing a highly effective ensemble on most datasets, as combining multi-
ple different sources of randomness leads increased diversity of the training data, and
thus increased diversity of the ensemble.

In addition to applying these traditional ensemble methods to the class imbalance
problem, modiﬁed versions have also been developed. Chawla et al. introduce SMOTE-
Boost [5], which combines boosting with SMOTE by, during each iteration of boosting,
using SMOTE on the hard to classify instances. Guo and Viktor develop another exten-
sion for boosting, DataBoost-IM, which identiﬁes hard instances in order to generate
similar synthetic examples, and then reweights the instances to prevent a bias towards
the majority class [7]. Liu, Wu, and Zhou propose two methods, EasyEnsemble and
BalanceCascade [6], which generate training sets by choosing an equal number of ma-
jority and minority class instances from a larger training set. Finally Hido and Kashima
introduce a variant of bagging, “Roughly Balanced Bagging” (RB bagging) [20] which
alters bagging to emphasize the minority class.

In addition to ensemble methods to deal with the class imbalance problem, sampling
techniques can also be employed. The two simplest sampling techniques are oversam-
pling and undersampling. In oversampling, the minority class instances are replicated
to create a larger minority class set. Conversely, in undersampling majority class in-
stances are removed in order to level the class distribution. Both of these techniques,
among others, have been widely employed [21–23]

A more sophisticated technique which seeks to combat class imbalance is known as
SMOTE (Synthetic Minority Over-sampling Technique) [24]. When applying SMOTE,
the training set is altered by adding more minority class examples in order to force
the class distribution to become more balanced. Instead of simply oversampling the
minority class (i.e., duplicating minority examples at random), SMOTE creates new
synthetic minority examples by ﬁrst selecting a minority example and its k nearest

498

T.R. Hoens and N.V. Chawla

neighbors. The synthetic example is then created by choosing a neighbor and an feature.
As the two examples form a line segment in the chosen feature space, the synthetic
example’s value for the feature is chosen to lie along that line segment.

7 Conclusion

We proposed an extension to the RSM to mitigate the effects of class imbalance, called
RSM+Sampling, proposing SMOTE be used as the default sampling method. We then
compared this method and RSM+usamp against bagging, AdaBoost, random forest,
and the random subspace method. We posited that by applying SMOTE to subspaces
and then learning classiﬁers will lead to an improved performance due to more diverse
classiﬁers as well as less noise imputation due to SMOTE. The latter arises as SMOTE
is only applied to a much reduced set of features at a time, and is thus a more controlled
phenomenon. It is also not affected by the data sparsity and high dimensionality. To test
this hypothesis we ran experiments on 21 widely available imbalanced datasets.

The results on the selected datasets showed RSM+SMOTE outperformed all other
classiﬁers tested at the 95%, conﬁdence level, and only did not outperform RSM+usamp
at the 99% conﬁdence levels. From these results it is apparent that RSM+SMOTE is
well suited to overcoming the class imbalance problem. We argue that this is due to
the diversity that SMOTE adds to the ensemble. That is SMOTE adds perturbations
to the training data based on the features which has a positive effect on the diversity
of the ensemble, and therefore increases performance. Based on this and the statistical
signiﬁcance tests, we recommend the use of RSM+SMOTE on imbalanced datasets.

Acknowledgements

Work was supported in part by the NSF Grant ECCS-0926170 and the Notebaert Pre-
mier Fellowship.

References

1. Chawla, N.V., Cieslak, D.A., Hall, L.O., Joshi, A.: Automatically countering imbalance and
its empirical relationship to cost. Data Mining and Knowledge Discovery 17(2), 225–252
(2008)

2. Dietterich, T.G.: Ensemble methods in machine learning. In: Kittler, J., Roli, F. (eds.) MCS

2000. LNCS, vol. 1857, pp. 1–15. Springer, Heidelberg (2000)

3. Breiman, L.: Bagging predictors. Machine Learning 24(2), 123–140 (1996)
4. Freund, Y., Schapire, R.: Experiments with a new boosting algorithm. In: Thirteenth Inter-

national Conference on Machine Learning (1996)

5. Chawla, N.V., Lazarevic, A., Hall, L.O., Bowyer, K.W.: Smoteboost: improving predic-
tion of the minority class in boosting. In: Lavraˇc, N., Gamberger, D., Todorovski, L.,
Blockeel, H. (eds.) PKDD 2003. LNCS (LNAI), vol. 2838, pp. 107–119. Springer,
Heidelberg (2003)

6. Liu, X.Y., Wu, J., Zhou, Z.H.: Exploratory under-sampling for class-imbalance learning. In:
ICDM ’06: Proceedings of the Sixth International Conference on Data Mining, Washington,
DC, USA, pp. 965–969. IEEE Computer Society, Los Alamitos (2006)

Generating Diverse Ensembles to Counter the Problem of Class Imbalance

499

7. Guo, H., Viktor, H.L.: Learning from imbalanced data sets with boosting and data generation:
the databoost-im approach. In: SIGKDD Explorations, pp. 30–39. ACM, New York (2004)
8. Ho, T.: The random subspace method for constructing decision forests. Pattern Analysis and

Machine Intelligence 20(8), 832–844 (1998)

9. Chawla, N.V., Sylvester, J.: Exploiting diversity in ensembles: Improving the performance on
unbalanced datasets. In: Haindl, M., Kittler, J., Roli, F. (eds.) MCS 2007. LNCS, vol. 4472,
pp. 397–406. Springer, Heidelberg (2007)

10. Cieslak, D.A., Chawla, N.V.: Learning decision trees for unbalanced data. In: Daelemans,
W., et al. (eds.) ECML PKDD 2008, Part I. LNCS (LNAI), vol. 5211, pp. 241–256. Springer,
Heidelberg (2008)

11. Asuncion, A., Newman, D.: UCI machine learning repository (2007)
12. Chang, C.C., Lin, C.J.: LIBSVM: a library for support vector machines (2001), Software

available at http://www.csie.ntu.edu.tw/˜cjlin/libsvm

13. DeGroot, M., Schervish, M.: Probability and Statistics, 3rd edn. Addison-Wesley, Reading

(2001)

14. Demsar, J.: Statistical comparisons of classiﬁers over multiple data sets. Journal of Machine

Learning Research 7, 1–30 (2006)

15. Kuncheva, L.I., Whitaker, C.J.: Measures of diversity in classiﬁer ensembles. Machine

Learning 51, 181–207 (2003)

16. Dietterich, T.G.: An experimental comparison of three methods for constructing ensembles
of decision trees: Bagging, boosting, and randomization. Machine Learning 40(19), 139–157
(2000)

17. Hansen, L.K., Salamon, P.: Neural network ensembles. IEEE Transactions on Pattern Anal-

ysis and Machine Intelligence 12(10), 993–1001 (1990)

18. Kotsiantis, S., Pintelas, P.: Combining bagging and boosting. International Journal of Com-

putational Intelligence 1(4), 324–333 (2004)

19. Breiman, L.: Random forests. Machine Learning 45(1), 5–32 (2001)
20. Hido, S., Kashima, H.: Roughly balanced bagging for imbalanced data. In: Statistical Anal-

ysis and Data Mining, pp. 143–152. SIAM, Philadelphia (2008)

21. Drummond, C., Holte, R.C.: C4.5, class imbalance, and cost sensitivity: Why under-
sampling beats over-sampling. In: ICML Workshop on Learning from Imbalanced Datasets
II (2003)

22. Weiss, G.M., Provost, F.: Learning when training data are costly: the effect of class distribu-

tion on tree induction. Journal of Artiﬁcal Intelligent Research 19, 315–354 (2003)

23. Batista, G.E.A.P.A., Prati, R.C., Monard, M.C.: A study of the behavior of several methods

for balancing machine learning training data. SIGKDD Explorations 6, 20–29 (2004)

24. Chawla, N.V., Bowyer, K.W., Hall, L.O., Kegelmeyer, W.P.: SMOTE: Synthetic minority

over-sampling technique. Journal of Artiﬁcial Intelligence Research 16, 321–357 (2002)


