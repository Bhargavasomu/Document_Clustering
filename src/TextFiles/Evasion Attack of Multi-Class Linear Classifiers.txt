Evasion Attack of Multi-Class Linear Classiﬁers

Han Xiao1,2, Thomas Stibor2, and Claudia Eckert2

1 CeDoSIA of TUM Graduate School

2 Chair for IT Security

Technische Universit¨at M¨unchen, Germany

{xiaoh,stibor,claudia.eckert}@in.tum.de

Abstract. Machine learning has yield signiﬁcant advances in decision-making
for complex systems, but are they robust against adversarial attacks? We gener-
alize the evasion attack problem to the multi-class linear classiﬁers, and present
an efﬁcient algorithm for approximating the optimal disguised instance. Experi-
ments on real-world data demonstrate the effectiveness of our method.

1 Introduction

Researchers and engineers of information security have successfully deployed systems
using machine learning and data mining for detecting suspicious activities, ﬁltering
spam, recognizing threats, etc. [2, 12]. These systems typically contain a classiﬁer that
ﬂags certain instances as malicious based on a set of features. Unfortunately, evaded
malicious instances that fail to be detected are inevitable for any known classiﬁer. To
make matters worse, there is evidence showing that adversaries have investigated sev-
eral approaches to evade the classiﬁer by disguising malicious instance as normal in-
stances. For example, spammers can add unrelated words, sentences or even paragraphs
to the junk mail for avoiding detection of the spam ﬁlter [11]. Furthermore, spammers
can embed the text message in an image. By adding varied background and distorting
the image, the generated junk message can be difﬁcult for OCR systems to identify but
easy for humans to interpret [7]. As a reaction to adversarial attempts, authors of [5]
employed a cost-sensitive game theoretic approach to preemptively adapt the decision
boundary of a classiﬁer by computing the adversary’s optimal strategy. Moreover, sev-
eral improved spam ﬁlters that are more effective in adversarial environments have been
proposed [7, 3].

The ongoing war between adversaries and classiﬁers pressures machine learning
researchers to reconsider the vulnerability of classiﬁer in adversarial environments. The
problem of evasion attack is posed and a query algorithm for evading linear classiﬁers
is presented [10]. Given a malicious instance, the goal of the adversary is ﬁnding a
disguised instance with the minimal cost to deceive the classiﬁer. Recently, the evasion
problem has been extended to the binary convex-inducing classiﬁers [13].

We continue investigate the vulnerability of classiﬁers to the evasion attack and
generalize this problem to the family of multi-class linear classiﬁers; e.g. linear support
vector machines [4, 6, 9]. Multi-class linear classiﬁers have become one of the most
promising learning techniques for large sparse data with a huge number of instances
and features. We propose an adversarial query algorithm for searching minimal-cost

disguised instances. We believe that revealing a scar on the multi-class classiﬁer is the
only way to ﬁx it in the future. The contributions of this paper are:
1. We generalize the problem of evasion attack to the multi-class linear classiﬁer,

where the instance space is divided into multiple convex sets.

2. We prove that effective evasion attack based on the linear probing is feasible under
certain assumption of the adversarial cost. A description of the vulnerability of
multi-class linear classiﬁers is presented.

3. We propose a query algorithm for disguising an adversarial instance as any other
classes with minimal cost. The experiment on two real-world data set shows the
effectiveness of our algorithm.

2 Problem Setup
Let X = {(x1, . . . , xD) ∈ RD | L ≤ xd ≤ U for all d} be the feature space. Each
component of an instance x ∈ X is a feature bounded by L and U which we denote
as xd. A basis vector of the form (0, . . . , 0, 1, 0, . . . , 0) with a 1 only at the dth feature
terms δd. We assume that the feature space representation is known to the adversary,
thus the adversary can query any point in X .
2.1 Multi-Class Linear Classiﬁer
The target classiﬁer f is a mapping from feature space X to its response space K;
i.e. f : X → K. We restrict our attention to multi-class linear classiﬁers and use
K = {1, . . . , K}, K ≥ 2 so that

(1)

f (x) = argmax

wk x

k

T + bk,

where k = 1, . . . , K and wk ∈ RD, bk ∈ R. Decision boundaries between class k and
other classes are characterized by wk and bk. We assume that w1, . . . , wK are linearly
independent. The classiﬁer f partitions X into K sets; i.e. Xk = {x ∈ X | f (x) = k}.
2.2 Attack of Adversary
As a motivating example, consider a text classiﬁer that categorizes incoming emails
into different topics; e.g. sports, politics, lifestyle, spam, etc. An advertiser of pharma-
cological products is more likely to disguise the spam as lifestyle rather than politics in
order to attract potential consumers while remaining inconspicuous.

We assume the adversary’s attack will be against a ﬁxed f so the learning method of
decision boundaries and the training data used to establish the classiﬁer are irrelevant.
The adversary does not know any parameter of f but can observe f (x) for any x by
issuing a membership query. In fact, there are a variety of domain speciﬁc mechanisms
that an adversary can employ to observe the classiﬁer’s response to a query. Moreover,
the adversary is only aware of an adversarial instance x
A in some class, and has no
information about instances in other classes. This differs from previous work which
require at least one instance in each binary class [10, 13]. In practice, x
A can be seen as
the most desired instance of adversary; e.g. the original spam. The adversary attempts
to disguise x

A so that it can be recognized as other classes.

2.3 Adversarial Cost

We assume that the adversary has the access to an adversarial cost function a(x, y) :
X × X → R0+. An adversarial cost function measures the distance between two in-
stances x, y in X from the adversary’s prospective. We focus on a linear cost function
which measures the weighted ℓ1 distance so that

a(x, y) =

ed|xd − yd|,

(2)

DXd=1

where 0 < ed < ∞ represents the cost coefﬁcient of the adversary associates with
the dth feature, allowing that some features may be more important than others. In
particular, given the adversarial instance x
A) measures different costs
of using some instances as compared to others. Moreover, we use B(y, C) = {x ∈
X | a(x, y) ≤ C} to denote the cost ball centered at y with cost no more than C.
In generalizing work [10], we alter the deﬁnition of minimal adversarial cost (MAC).
Given a ﬁxed classiﬁer f and an adversarial cost function a we deﬁne the MAC of class
k with respect to an instance y to be the value

A, function a(x, x

MAC(k, y) = min
x:x∈Xk

a(x, y), k 6= f (y).

2.4 Disguised Instances

We now introduce some instances with special adversarial cost that the adversary is
interested in. First of all, instances with cost of MAC(k, y) are termed instances of
minimal adversarial cost (IMAC), which is formally deﬁned as

IMAC(k, y) = {x ∈ Xk | a(x, y) = MAC(k, y), k 6= f (y)} .
A) for all k 6= f (x

Ideally, the adversary attempts to ﬁnd IMAC(k, x
A). The most
naive way for an adversary to ﬁnd the IMAC is performing a brute-force search. That
is, the adversary randomly samples points in X and updates the best found instance
repetitively. To formulate this idea, we further extend the deﬁnition of IMAC. Assume
eX is the set of adversary’s sampled or observed instances so far and eX ⊂ X , we deﬁne

instance of sample minimal adversarial cost (ISMAC) of class k with respect to an
instance y to be the value

ISMAC(k, y) = argmin
x:x∈ eX∩Xk

a(x, y), k 6= f (y).

Note, that in practice the exact decision boundary is unknown to the adversary, thus
ﬁnding exact value of IMAC becomes an infeasible task. Nonetheless, it is still tractable
to approximate IMAC by ﬁnding ǫ-IMAC, which is deﬁned as follows

ǫ-IMAC(k, y) = {x ∈ Xk | a(x, y) ≤ (1 + ǫ) · MAC(k, y), k 6= f (y), ǫ > 0} .

That is, every instance in ǫ-IMAC(k, y) has the adversarial cost no more than a fac-
tor of (1 + ǫ) of the MAC(k, y). The goal of the adversary now becomes ﬁnding
ǫ-IMAC(k, x

A) while keeping ǫ as small as possible.

A) for all classes k 6= f (x

3 Theory of Evasion Attack

We discuss the evasion attack from a theoretical point of view. Speciﬁcally, by describ-
ing the feature space as a set of convex polytopes, we show that IMAC must be attained
on the convex surface. Under a reasonable assumption of adversarial cost function, ef-
fective evasion attack can be performed by linear probing. Finally, we derive bounds
for quantitatively studying the vulnerability of multi-class linear classiﬁers to linear
probing.
Lemma 1. Let Xk = {x ∈ X | f (x) = k}, where the classiﬁer f is deﬁned in (1).
Then Xk is a closed convex polytope.
Proof. Let x be a point in Xk. As x ∈ X it follows that

x

T ≥ L · 1D

and

− x

T ≥ U · 1D,

(3)

(4)

14].

p=1 H +

1 , . . . , H +

1 , . . . , H +

wk − w1

...

b1 − bk

...

bK − bk

wk − wK

 .

i ; i.e. Hi = ∂H +

p , where {H +

where 1D is a D-dimensional unit vector (1, . . . , 1). Moreover, since f (x) = k, it
follows that


T ≥
 x
Thus, the foregoing linear inequalities deﬁne an intersection of at most (K + 2D − 1)
T ≥ ebi}, where 1 ≤ i ≤ (K + 2D − 1). We
half-spaces. Denote H +
i = {x ∈ X |fwix
have Xk =Ti H +
i , which establishes a half-space representation of convex polytope [8,
⊓⊔
Lemma 1 indicates that the classiﬁer f decomposes RD into K convex polytopes.
Following the notations and formulations introduced in [8], we represent a hyperplane
Hi as the boundary of a half-space ∂H +
T =
ebi}. Let Xk = TPk
Pk} is irredundant3 to Xk. Let Hk =
{H +
Pk} be an irredundant set that deﬁnes Xk, then Xk ⊂ intX provided that
none half-space in Hk is deﬁned by (3). Moreover, we deﬁne the pth facet of Xk as
Fkp = Hp ∩ Xk, and the convex surface of Xk as ∂Xk =SPk
Theorem 1. Let y be an instance in X and k ∈ K \ f (y). Let x be an instance in
IMAC(k, y) as deﬁned in Section 2.3. Then x must be attained on the convex surface
∂Xk.
Proof. We ﬁrst show the existence of IMAC(k, y). By Lemma 1, Xk deﬁnes a feasible
region. Thus minimizing a(x, y) on Xk is a solvable problem. Secondly, Xk is bounded
in each direction of the gradient of a(x, y), which implies that IMAC(k, y) exists.
We now prove that x must lie on ∂Xk by contrapositive. Assume that x is not on
∂Xk thus is an interior point; i.e. x ∈ intXk. Let B(y, C) denote the ball centered at
y with cost no more than a(x, y). Due to the convexity of Xk and B(y, C), we have
int Xk ∩ intB(y, C) 6= ∅. Therefore, there exists at least one instance in Xk with cost
less than a(x, y), which implies that x is not IMAC(k, y).
⊓⊔
3 Let C be a convex polytope such that C = Tn
n } is called
irredundant to C provided that T1≤j≤n,j6=i H +

i = {x ∈ X |fwix

i=1 H +

i . The family {H +

1 , . . . , H +

6= C for each j = 1, . . . , n.

j

p=1 Fkp.

Theorem 1 restricts the searching of IMAC to the convex surface. In particular,
when cost coefﬁcients are equal, e.g. e1 = ··· = eD, we can show that searching in all
axis-aligned directions gives at least one IMAC.
Theorem 2. Let y be an instance in X such that Xf (y) ⊂ intX . Let P be the number of
facets of Xf (y) and Fp be the pth facet, where p = {1, . . . , P}. Let Gd = {y+θδd | θ ∈
R}, where d ∈ {1, . . . , D}. Let Q = {Gd∩ Fp | d = 1, . . . , D, p = 1, . . . , P}, in which
each element differs from y on only one dimension. If the adversarial cost function
deﬁned in (2) has equal cost coefﬁcients, then there exists at least one x ∈ Q such that
x is IMAC(f (x), y).

Proof. Let Hp be the hyperplane deﬁning the pth facet Fp. Consider all the points
of intersection of the lines Gd with the hyperplanes Hp; i.e. I = {Gd ∩ Hp | d =
1, . . . , D, p = 1, . . . , P}. Let x = argminx∈I a(x, y). Then x is our desired instance.
We prove that x ∈ Q by contrapositive. Suppose x /∈ Q , due to the convexity of
Xf (y), the line segment [x, y] intersects ∂Xf (y) at a point on another facet. Denote this
point as z, then z differs from y on only one dimension and a(z, y) < a(x, y).
Next, we prove x is IMAC(f (x), y) by contrapositive. Let B(y, C) denote the reg-
ular cost ball centered at y with cost no more than a(x, y). That is, each vertex of the
cost ball has the same distance of C with y. Suppose x is not IMAC(f (x), y), then
there exists z ∈ Xf (x) ∩ intB(y, C). By Theorem 1, z and x must lie on the same
facet, which is deﬁned by a hyperplane H∗. Let Q∗ be intersection points of H∗ with
lines G1, . . . , GD; i.e. Q∗ = {Gd ∩ H∗ | d = 1, . . . , D}. Then there exists at least one
point v ∈ Q∗ such that v ∈ intB(y, C). Due to the regularity of B(y, C), we have
a(v, y) < a(x, y).
⊓⊔
We now deﬁne special convex sets for approximating ǫ-IMAC near the convex sur-
face. Given ǫ > 0, the interior parallel body of Xk is P−ǫ(k) = {x ∈ Xk |B(x, ǫ) ⊆ Xk}
and the corresponding exterior parallel body is deﬁned as P+ǫ(k) = Sx∈Xk B(x, ǫ).
Moreover, the interior margin of Xk is M−ǫ(k) = Xk \ P−ǫ(k) and the corresponding
exterior margin is M+ǫ(k) = P+ǫ(k) \ Xk. By relaxing the searching scope from the
convex surface to a margin in the distance ǫ, Theorem 1 and Theorem 2 immediately
imply the following results.
Corollary 1. Let y be an instance in X and k ∈ K \ f (y). For all ǫ > 0 such that
M−ǫ(k) 6= ∅, ǫ-IMAC(k, y) ⊆ M−ǫ(k).
Corollary 2. Let y be an instance in X and ǫ be a positive number such that P+ǫ(f (y)) ⊂
intX . Let P be the number of facets of P+ǫ(f (y)) and Fp be the pth facet, where
p = {1, . . . , P}. Let Gd = {y + θδd | θ ∈ R}, where d ∈ {1, . . . , D}. Let Q =
{Gd ∩ Fp | d = 1, . . . , D, p = 1, . . . , P}, in which each element differs from y on only
one dimension. If adversarial cost function deﬁned in (2) has equal cost coefﬁcients,
then there exists at least one x ∈ Q such that x is in ǫ-IMAC(f (x), y).

Corollary 1 and Corollary 2 point out an efﬁcient way of approximating ǫ-IMAC
with linear probing, which forms the backbone of our proposed algorithm in Section 4.
Finally, we consider the vulnerability of a multi-class linear classiﬁer to linear prob-
ing. The problem arises of detecting convex polytopes in X with a random line. As one

can easily scale any hypercube to a unit hypercube with edge length 1, our proof is
restricted to the unit hypercube in RD.

Deﬁnition 1 (Vulnerability to Linear Probing). Let X = [0, 1]D, and X1, . . . ,XK be
the sets that tile X according to the classiﬁer f : X → {1, . . . , K}, with K ≥ 2. Let G
be a random line in RD that intersects X . Denote Z the number of sets intersect G, the
vulnerability of classiﬁer f to linear probing is measured by the expectation of Z.

When E Z is small, a random line intersects small number of decision regions and
not much information is leaked to the adversary. Thus, a robust multi-class classiﬁer
that resists linear probing should have a small value of E Z.

Theorem 3. Let f be the multi-class linear classiﬁer deﬁned in (1), then the expectation
of Z is bounded by 1 < E Z < 1 +

√2(K−1)

.

2D

Proof. By Lemma 1, we have K convex polytopes X1, . . . ,XK. Let F be the union of
all facets of polytopes. Observe that each time the line touches a convex polytope, it
only touches its surface twice. The exit point is the entrance point for a new polytope,
except at the end-point. Thus, the variable that we are interested in can be represented
as

Z = |F ∩ G|,

where | · | represents the cardinality of a set. Obviously, E Z is bounded by 1 < E Z <
K. We will give a tighter bound in the sequel.
Let G be the class of all lines of RD, and µ be the measure of G. Following the
notation introduced in [15], we denote the measure of G that meet a ﬁxed bounded
convex set C as µ(G;G ∩ C 6= ∅). Considering an independent Poisson point process on
G intensity measure µ, let N be the number of lines intersecting X . We emphasize that
N is a ﬁnite number, so that one can label them independently G1, . . . , GN . It follows
that Gn, n = 1, . . . , N are i.i.d.. Given a ﬁxed classiﬁer f, we have

E

NXn=1

|F ∩ Gn| = E

|F ∩ Gi|#

nXi=1

NXn=1"P (N = n)
NXn=1

= E N · (E Z).

=

[P (N = n) · n · E|F ∩ G1|]

(5)

Remark that G1, . . . , GN follow the Possion point process, we have E N = µ(G;G ∩
X 6= ∅). Therefore we can rewrite (5) as,
EPN

n=1 |F ∩ Gn|
µ(G;G ∩ X 6= ∅)

E Z =

(6)

.

any given line can hit a facet no more than once. Therefore, we have

n=1 |F ∩ Gn|. Let M = |F|. Due to the convexity of Xk,

Next, we compute EPN

E

NXn=1

|F ∩ Gn| = E

=

=

|Fm ∩ Gn|

MXm=1
NXn=1
E(cid:12)(cid:12)(cid:12)nn ∈ {1, . . . , N}|Fm ∩ Gn 6= ∅o(cid:12)(cid:12)(cid:12)
MXm=1
MXm=1
E Z = PM

m=1 µ(G;G ∩ Fm 6= ∅)
µ(G;G ∩ X 6= ∅)

µ(G;G ∩ Fm 6= ∅).

.

By substituting (7) into (6) we obtain

(7)

(8)

Assume that µ is translation invariant, by Cauchy-Crofton formula we can rewrite (8)
as

E Z = PM

m=1 A(Fm)
A(X )

,

(9)

where A(·) denotes the surface area4. Note, that the numerator of (9) depends on the
shape of each polytope and relates to the training method of classiﬁer. Thus, it is difﬁcult
to compute the exact value of E Z. Nonetheless, we can bound the expectation by using
m=1 A(Fm) < A(X ) + √2(K − 1) (see [1] for the upper bound).
the fact A(X ) <PM
Remark that the surface area A(X ) of a unit hypercube is 2D. We yield

1 < E Z < 1 +

√2(K − 1)

,

2D

which concludes our proof.

⊓⊔
We remark that Theorem 3 implies a way to construct a robust classiﬁer that resists
evasion algorithm based on linear probing, e.g. by jointly minimizing (9) and the error
function in the training procedure.

4 Algorithm for Approximating ǫ-IMAC

Based on theoretical results, we present an algorithm for deceiving the multi-class linear
classiﬁer by disguising the adversarial instance x
A as other classes with approximately
minimal cost, while issuing polynomially many queries in: the number of features, the
range of feature, the number of classes and the number of iterations.

An outline of our searching approach is presented in Algorithms 1 to 3. We use
a K × D matrix Ψ for storing ISMAC of K classes and an array C of length K for
4 The surface area in RD is the (D − 1)-dimensional Lebesgue measure.

the corresponding adversarial cost of these instances. The scalar value W represents
the maximal cost of all optimum instances. Additionally, we need a K × I matrix T
for storing the searching path of optimum instances in each iteration. The kth row of
matrix Ψ is denoted as Ψ[k, :]. We consider Ψ, T, C, W as global variables so they are
accessible in every scope. After initializing variables, our main routine MLCEvading
(Algorithm 1 line 4) ﬁrst invokes MDSearch (Algorithm 2) to search instances that is
close to the starting point x
A in all classes and saves them to Ψ. Then it repetitively
selects instances from Ψ as new starting points and searches instances with lower ad-
versarial cost (Algorithm 3 line 6–7). The whole procedure iterates I times. Finally, we
obtain Ψ[k, :] as the approximation of ǫ-IMAC(k, x

A) .

u and x

l in the same class while x

We next describe Algorithm 2. Given x which is known as ISMAC(k, x

We begin by describing RBSearch in Algorithm 3, a subroutine for searching
instances near decision boundaries along dimension d. Essentially, given an instance x,
an upper bound u and a lower bound l, we perform a recursive binary search on the line
segment {x + θδd | l ≤ θ ≤ u} through x. The effectiveness of this recursive algorithm
relies on the fact that it is impossible to have x
m is in
another class. In particular, if the line segment meets an exterior margin M+ǫ(k) and
ǫ-IMAC(k, x) is the intersection, then RBSearch ﬁnds an ǫ-IMAC. Otherwise, when
the found instance y yields lower adversarial cost than instance in Ψ does, Algorithm 4
is invoked to update Ψ. The time complexity of RBSearch is O( u−l
ǫ ).
A) and the
current maximum cost W , the algorithm iterates (D − 1) times on P+ǫ(Xf (x)) for
ﬁnding instances with cost lower than W . Additionally, we introduce two heuristics to
prune unnecessary queries. First, the searched dimension in the previous iteration of x is
omitted. Second, we restrict the upper and lower bound of the searching scope on each
dimension. Speciﬁcally, knowing W and a(x, x
A) = c, we only allow RBSearch to
] since any instance lying out of this scope gives
ﬁnd instance in [xd − W−c
adversarial cost higher than W . This pruning is signiﬁcant when we have obtained
ISMAC for every class. Special attention must be paid to searched dimensions of x
(see Algorithm 2 line 5–7). Namely, if d is a searched dimension before the (i − 1)th
] so that no low-
iteration, then we relax the searching scope to [xA
cost instances will be missed.

, xA

d + W−c

d − W−c

ed

, xd + W−c
ed

ed

ed

Algorithm 1: Query algorithm for evasion of multi-class linear classiﬁers

(Ψ, C) ←MLCEvading(xA, e, D, L, U, K, I, ǫ):
for k ← 1 to K do

Ψ[k, :] ← 0, T [k, :] ← 0, C[k] ← +∞

C[1] ← 0
MDSearch(xA, xA, e, 1, 0, D, L, U, 1, ǫ)
for i ← 2 to I do

for k ← 2 to K do

MDSearch(Ψ[k, :], xA, e, k, C[k], D, L, U, i, ǫ)

1
2

3

4
5
6
7

Algorithm 2: Multi-dimensional search from ISMAC(k, x

A)

MDSearch(x, xA, e, k, c, D, L, U, i, ǫ):
for d ← 1 to D do

if d 6= T [k, i − 1] then

ed

δ ← W −c
u = min{U, xd + δ}, l = max{L, xd − δ}
if d ∈ {T [k, 1], . . . , T [k, i − 2]} then
d then l = max{L, xA

d − δ}

if xd > xA
else u = min{U, xA

d + δ}

xu ← x, xl ← x
d ← u, xl
xu
d ← l
if f (xu) 6= k then RBSearch(xd, u, x, d, i, ǫ)
if f (xl) 6= k then RBSearch(l, xd, x, d, i, ǫ)

1
2

3
4
5
6

7

8

9
10

11

Algorithm 3: Recursive binary search on dimension d

RBSearch(l, u, x, d, i, ǫ):
x∗ ← x
if u − l < ǫ then

d ← u

x∗
k ← f (x∗), c ← a(x∗)
if c < C[k] then Update(x∗, k, c, d, i)

xu ← x, xl ← x, xm ← x
d ← u, xl
xu
if f (xm) = f (xl) then

d ← l, xm

d ← u+l

2

RBSearch(m, u, x, d, i, ǫ)

else if f (xm) = f (xu) then

RBSearch(l, m, x, d, i, ǫ)

else

RBSearch(l, m, x, d, i, ǫ)
RBSearch(m, u, x, d, i, ǫ)

1
2
3
4
5

6

7

8
9
10
11
12
13
14

Algorithm 4: Update ISMAC(k, x

A)

(Ψ, C, T, W ) ←Update(x∗, k, c, d, i):
Ψ[k, :] ← x∗
C[k] ← c
T [k, i] ← d
W ← max{C[1], . . . , C[K]}

1
2
3
4

Theorem 4. The asymptotic time complexity of our algorithm is O( U−L

ǫ DKI).

Proof. Follows from the correctness of the algorithm and the fact that the time com-
plexity of RBSearch is O( u−l
ǫ ).
⊓⊔
5 Experiments

We demonstrate the algorithm5 on two real-world data sets, the 20-newsgroups6 and
the 10-Japanese female face7. On the newsgroups data set, the task of the adversary is
to evade a text classiﬁer by disguising a commercial spam as a message in other top-
ics. On the face data set, the task of adversary is to deceive the classiﬁer by disguising
a suspect’s face as an innocent. We employ LIBLINEAR [6] package to build target
multi-class linear classiﬁers, which return labels of queried instances. The cost coefﬁ-
cients are set to e1 = ··· = eD = 1 for both tasks. For the groundtruth solution, we
directly solve the optimization problem with linear constraints (3) and (4) by using the
models’ parameters. We then measure the average empirical ǫ for (K−1) classes, which
A) − 1i, where C[k] is the adversarial cost
is deﬁned asbǫ = 1
of disguised instance of class k. Evidently, smallbǫ indicates better approximation of

K−1Pk6=f (x

A)h

C[k]

MAC(k,x

IMAC.

5.1 Spam Disguising
The training data used to conﬁgure the newsletter classiﬁer consists of 7, 505 docu-
ments, which are partitioned evenly across 20 different newsgroups. Each document
is represented as a 61, 188-dimensional vector, where each component is the number
of occurrences of a word. The accuracy of the classiﬁer on training data is 100% for
every class. We set the category “misc.forsale” as the adversarial class. That is, given
a random document in “misc.forsale”, the adversary attempts to disguise this docu-
ment as from other category; e.g. “rec.sport.baseball”. Parameters of the algorithm are
K = 20, L = 0, U = 100, I = 10, ǫ = 1. The adversary is restricted to query at most
10, 000 times. The adversarial cost of each class is depicted in Fig. 1 (left).

5.2 Face Camouﬂage
The training data contains 210 gray-scaled images of 7 facial expressions (each with
3 images) posed by 10 Japanese female subjects. Each image is represented by a 100-
dimensional vector using principal components. The accuracy of the classiﬁer on train-
ing data is 100% for every class. We randomly pick a subject as an imaginary suspect.
Given a face image of the suspect, the adversary camouﬂage this face to make it be
classiﬁed as other subjects. Parameters of the algorithm are K = 10, L = −105, U =
105, I = 10, ǫ = 1. The adversary is restricted to query at most 10, 000 times. The
adversarial cost of each class is depicted in Fig. 1 (right). Moreover, we visualize dis-
guised faces in Fig. 2. Observe that many disguised faces are similar to the suspect’s
face by humans interpretation, yet they are deceptive for the classiﬁer. This visualization
directly demonstrates the effectiveness of our algorithm.

5 A Matlab implementation is available at http://home.in.tum.de/∼xiaoh/pakdd2012-code.zip
6 http://people.csail.mit.edu/jrennie/20Newsgroups/
7 http://www.kasrl.org/jaffe.html

t
s
o
c
 
l

a
i
r
a
s
r
e
v
d
A

50

40

30

20

10

0

m
s
e
h

i

t

a

.
t
l

a

h
p
a
r
g

.

p
m
o
c

i

n
w
s
m
p
m
o
c

.

c
p
m
b

i
.

p
m
o
c

.

c
a
m
p
m
o
c

i

n
w
x
.

p
m
o
c

s
o

t

u
a

.
c
e
r

r
o

t

o
m
.
c
e
r

l
l

a
b
b

.

o
p
s

y
e
k
c
o
h

.

o
p
s

t

p
y
r
c
.
i
c
s

c
e
e

l

.
i
c
s

d
e
m

.
i
c
s

e
c
a
p
s
.
i
c
s

l

g
e
r
.
c
o
s

s
n
u
g

.
l

o
p

t
s
a
e
m

.
l

o
p

c
s
m

i

.
l

o
p

c
s
m
g
e
r

.

i

l

  ×103

3

 

2

 

1

Fig. 1. Box plots for adversarial cost of disguised instance of each class. (Left) On the 20-
newsgroups data set, we consider “misc.forsale” as the adversarial class. Note, that feature values
of the instance are non-negative integers as they represent the number of words in the document.
Therefore, the adversarial cost can be interpreted as the number of modiﬁed words in the dis-
guised document comparing to the original document from “misc.forsale”. The value of bǫ for 19
classes is 0.79. (Right) On the 10-Japanese female faces data set, we randomly select a subject as
the suspect. The box plot shows that the adversarial cost of camouﬂage suspicious faces as other
subjects. The value of bǫ for 9 classes is 0.51. A more illustrative result is depicted in Fig. 2.

It has not escaped our notice that an experienced adversary with certain domain
knowledge can reduce the number of queries by careful selecting cost function and
employing heuristics. Nonetheless, the goal of this paper is not to design real attacks
but rather examine the correctness and effectiveness of our algorithm so as to understand
vulnerabilities of classiﬁers.

6 Conclusions

Adversary and classiﬁer are Yin and Yang of information security. We believe that un-
derstanding the vulnerability of classiﬁers is the only way to develop resistant classiﬁers
in the future. In this paper, we showed that multi-class linear classiﬁers are vulnerable
to the evasion attack and presented an algorithm for disguising the adversarial instance.
Future work includes generalizing the evasion attack problem to the family of general
multi-class classiﬁer with nonlinear decision boundaries.

References

1. Ball, K.: Cube slicing in Rn. Proc. American Mathematical Society 97(3), pp. 465–473

(1986)

2. Barbara, D., Jajodia, S.: Applications of data mining in computer security. Springer (2002)
3. Bratko, A., Filipiˇc, B., Cormack, G., Lynam, T., Zupan, B.: Spam ﬁltering using statistical

data compression models. JMLR 7, 2673–2698 (2006)

4. Crammer, K., Singer, Y.: On the learnability and design of output codes for multiclass prob-

lems. Machine Learning 47(2), 201–233 (2002)

5. Dalvi, N., Domingos, P., et al.: Adversarial classiﬁcation. In: Proc. 10th SIGKDD. pp. 99–

108. ACM (2004)

Suspect

Innocent

l

a
n
g
i
r

i

O

s
e
c
a

f
 

i

d
e
s
u
g
s
D

i

Fig. 2. Disguised faces given by our algorithm to defeat a multi-class face recognition system.
The original faces (with neutral expression) of 10 females are depicted in the ﬁrst row, where the
left most one is the imaginary suspect and the remaining 9 people are innocents. From the second
row to sixth row, faces of the suspect with different facial expressions are fed to the algorithm
(see the ﬁrst column). The output disguised faces from the algorithm are visualized in the right
hand image matrix. Each row corresponds to disguised faces of the input suspicious face on the
left. Each column corresponds to an innocent.

6. Fan, R.E., Chang, K.W., Hsieh, C.J., Wang, X.R., Lin, C.J.: LIBLINEAR: A library for large

linear classiﬁcation. JMLR 9, 1871–1874 (2008)

7. Fumera, G., Pillai, I., Roli, F.: Spam ﬁltering based on the analysis of text information em-

bedded into images. JMLR 7, 2699–2720 (2006)

8. Gr¨unbaum, B.: Convex polytopes, vol. 221. Springer (2003)
9. Keerthi, S., Sundararajan, S., Chang, K., Hsieh, C., Lin, C.: A sequential dual method for

large scale multi-class linear svms. In: Proc. 14th SIGKDD. pp. 408–416. ACM (2008)

10. Lowd, D., Meek, C.: Adversarial learning. In: Proc. 11th SIGKDD. pp. 641–647. ACM

(2005)

11. Lowd, D., Meek, C.: Good word attacks on statistical spam ﬁlters. In: Proc. 2nd Conference

on Email and Anti-Spam. pp. 125–132 (2005)

12. Maloof, M.: Machine learning and data mining for computer security: methods and applica-

tions. Springer (2006)

13. Nelson, B., Rubinstein, B.I.P., Huang, L., Joseph, A.D., hon Lau, S., Lee, S., Rao, S., Tran,
A., Tygar, J.D.: Near-optimal evasion of convex-inducing classiﬁers. In: Proc. 13th AISTATS
(2010)

14. Rockafellar, R.: Convex analysis, vol. 28. Princeton Univ Pr (1997)
15. Santal´o, L.: Integral geometry and geometric probability. Cambridge Univ Pr (2004)


