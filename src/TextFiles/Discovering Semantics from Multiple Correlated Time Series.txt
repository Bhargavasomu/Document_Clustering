Discovering Semantics from Multiple Correlated

Time Series Stream

Zhi Qiao1,2, Guangyan Huang1, Jing He1, Peng Zhang2, Li Guo2,

Jie Cao3, and Yanchun Zhang1

2 Institute of Information Engineering, Chinese Academy of Science, Beijing, China

1 Victoria University, Melbourne, Australia

3 Nanjing University of Finance and Economics, China
zhiqiao.ict@gmail.com, {zhangpeng,guoli}@iie.ac.cn,

{Guangyan.Huang,Jing.He,yanchun.zhang}@vu.edu.au, caojie690929@163.com

Abstract. In this paper, we study a challenging problem of mining data
generating rules and state transforming rules (i.e., semantics) underneath
multiple correlated time series streams. A novel Correlation ﬁeld-based
Semantics Learning Framework (CfSLF) is proposed to learn the seman-
tic. In the framework, we use Hidden Markov Random Field (HMRF)
method to model relationship between latent states and observations in
multiple correlated time series to learn data generating rules. The trans-
forming rules are learned from corresponding latent state sequence of
multiple time series based on Markov chain character. The reusable se-
mantics learned by CfSLF can be fed into various analysis tools, such as
prediction or anomaly detection. Moreover, we present two algorithms
based on the semantics, which can later be applied to next-n step predic-
tion and anomaly detection. Experiments on real world data sets demon-
strate the eﬃciency and eﬀectiveness of the proposed method.

Keywords: Semantics, correlated time series
anomaly detection.

streams, prediction,

1

Introduction

Time series data have emerged in a wide range of applications from almost
every domain. Examples include economic index data in stock markets, patient
medical observation data, experimental biological data, to name a few. As a
result, it is of utmost importance to ﬁnd inherent semantics from time series
data. Take the medical examination for example, doctors often estimate the
physical status of a patient by monitoring and collecting multiple correlated
time series data from electrocardiograms (ECG), electroencephalograms (EEG),
heart beat rate (HR), and blood pressure observations. Obviously, it is very hard
to make accurate estimation by only relying on a single time series data (e.g.,
HR increases does not mean the patient is suﬀering from severe illness, perhaps
she/he just had exercises). Hence, it is necessary to combine all these time series
data for estimation.

J. Pei et al. (Eds.): PAKDD 2013, Part II, LNAI 7819, pp. 509–520, 2013.
c(cid:2) Springer-Verlag Berlin Heidelberg 2013

510

Z. Qiao et al.

Many methods have been proposed to analyze multiple time series in [3][5][11].
In addition, there are some popular time series models for time series forecasting,
such as the Vector Auto-regression (VAR) and Linear-regression (LR) models.
These approaches focus on frequent patterns in the time series, which cannot
explain observations using the internal dynamics of systems. The dynamics of
a system can be considered as a mechanism of system-work as equivalent to
semantics. Here, the system is unseen and unknown, which determines observed
time series. It can help us to know more about observation generating rules and
state transforming rules underneath data by learning the mechanism. A previous
work [12] also studies the semantics detection problem from time series data. The
diﬀerence is that it uses the pattern-based Hidden Markov Model (pHMM) to
describe the univariate time series data where a line segmentation method is used
to obtain signiﬁcant segment patterns. In our work, the patterns are irregularly
summarized from multiple time series, and we use the Hidden Markov Random
Field (MRF) as the solution.

Fig. 1. Real Medical Case (All signals are sampled in a minute unit)

Supposing all time series are observed synchronously at each time point, ob-
servations from all time series compose of a tuple at each moment and each tuple
is regarded as an output generated by a certain latent state. Each state demon-
strates a certain pattern of ﬂuctuation. In this paper, the pattern of ﬂuctuation
is taken as a generating rule, conforming with generated observations. Semantics
learning basically learns both observation value generating rules and transfor-
mation rules among latent states. It is widely admitted that the Hidden Markov
Model (HMM) can be used to learn semantics. In HMM, state assignment is
mainly determined by aligning observation value production and state transmis-
sion. However, a single observation tuple value contains little information. We
illustrate this by a real medical example in Figure 1. Figure 1 demonstrates 6
vital body signals of a patient in an operating theatre. At the t0 time point,
the operation starts. Apparently, observation values at t1 are very similar with

Discovering Semantics from Multiple Correlated Time Series Stream

511

observation values at t2. Actually, they represent diﬀerent situations of the pa-
tient. At t1, the observation value represents a natural reaction of the patient to
an outside emergency which can be regarded as the operation. At t2, the obser-
vation value represents physical state of patient during the operation. If we only
consider observation values of one tuple, we cannot obtain signiﬁcant semantics
for a practical problem. Actually, what is diﬀerent from other common data is
that time series data has natural temporal ordering. The temporal ordering of
observation data is not directly considered in HMM. According to the temporal
ordering of time series, each observation tuple is strongly correlated with the
previous tuple. In Figure 1, it is obvious that correlation between t1 tuple and
t1 − 1 tuple is diﬀerent from correlation between t2 tuple and t2 − 1 tuple.

Hence, we propose Correlation ﬁeld-based Semantics Learning Framework
(CfSLF) to learn semantics underneath multiple correlated time series in this
paper. In the framework, generating rules and state transforming rules will be
learned. In order to comprehensively considering both temporal ordering and
observation value of tuple, we use hidden Markov Random Field (HMRF) to
obtain approximately optimal latent state assignment to learn generating rules.
Then, state transforming rules are learned from label sequence in the framework.
The rest of the paper is structured as follows. Section 2 introduces a math-
ematical description of the problem. Section 3 describes detailed proposed Cf-
SLF. Section 4 introduces two main applications: Observation Value Prediction
& Anomaly Detection. Section 5 reports experimental results to show the ad-
vantage of our CfSLF model compared with some other algorithms. Section 6
introduces related works. We conclude the paper in Section 7.

2 Problem Setting

In this paper, we propose a Correlation ﬁeld-based Semantics Learning Frame-
work (CfSLF) to learn latent semantics underlying multiple time series which
represent a mechanism of system work. It contains two parts: generating rules
learning and state transforming rules learning.

Given multiple time series X = {X1, ..., Xn}, Xi = {xi1, ..., xim}. Xi rep-
resents an observed tuple from the m time series at the ith moment and xij
represents observed data from the jth time series at ith moment. Assume that
there exists a state set S = {s1,..., sk}. Each tuple is produced by one state of S.
Z = {z1, z2, ..., zn} is a latent label set. zi is a label variable, which is discrete

and represents latent state of the ith tuple. The value of the variable is in the
range from 1 to k. Assume zi=j, which represents that the latent state of the
ith tuple is the jth state sj. In order to learn data generating rules, we need to
obtain optimal latent state assignment and the corresponding generating rules
to maximize production probability of observation tuples. The problem can be
described as follow:

ˆZ = argmaxZ P (X, Z) = argmaxZ P (X|Z)P (Z)

(1)

512

Z. Qiao et al.

which is maximized to obtain state assignment. According to the continuity of
tuples sequence, the label variable set constituted of ﬁnite states can be seen as
a latent state sequence. After we obtain optimal state assignment, transforming
rules among latent states are learned from the state sequence based on Markov
chain characteristics.

3 CfSLF Learning

In this section, we discuss how to learn semantics by our proposed CfSLF.

3.1 Construction of Dependence Relationship

In time series, temporal ordering can be seen as the relationship between the
current and previous tuples. In this paper, we simply consider temporal ordering
as changing the trend to represent the relationship. We introduce a deﬁnition of
a local trend as follow,

Deﬁnition 1. Local Trend: In time series, we consider the direction and volume
of changing from the last node to the current node as the local trend along time
axis.

Suppose that the current tuple is xt with observation value ot. We directly obtain

local trend of by Tdt = ot − ot−1. For the local trend of the tuple, we apply the

attribute to represent temporal ordering of the current tuple. Then, we simply
use the cosine distance to measure the similarity among local trends of diﬀerent
tuples. Intuitively, continuity of time series can be considered as integrating
temporal ordering of all tuples. We use the local trend of tuple to represent
temporal ordering, which can be seen as an independent attribute of each tuple.
In doing so, each tuple has two attributes: its observation value and its local
trend. For each tuple, the observation value represents its individual character
and local trend represents sequence character. As a result, the tuple series with
size n is divided into n independent observation tuples

3.2 Latent State Assignment

In the procedure of state assignment, tuples with similar local trends and ob-
servation values are more likely to have the same latent state. Therefore, we
use HMRF to learn the rules. A correlation ﬁeld is built based on local trend
similarity. In the correlation ﬁeld, the assignment of labels depends on corre-
sponding brotherhood set. In this paper, the similarity matrix is considered as
a correlation network describing dependency relationship among latent labels of
tuples in the correlation ﬁeld. In the network, similarity is seen as a weight of
dependency relationship.
Suppose that we have a correlation network denoted as M . Here, M is the
symmetric n × n matrix, where wij is the link weight between labels zi and zj.

Discovering Semantics from Multiple Correlated Time Series Stream

513

The links in M induce dependence relationships among latent labels, with the
rationale that if the link weight is higher between labels zi and zj, then they are
more likely to have the same value equivalent to the same state.
We deﬁne a brotherhood set of the ith label as a label set consisting of labels
which have the same latent state value, Bi={zj,i (cid:2)= j & zi = zj}. The random
ﬁeld deﬁned over hidden label variable Z is a Markov random ﬁeld, where the
Markov property is satisﬁed by p(zi|zBi ). It indicates the probability of zi de-
pending on zi’s brotherhood set. By introducing the HMRF model, latent labels
of tuples are mapped to a correlation ﬁeld, where assignment of labels depends
on a corresponding brotherhood set without considering tuple value.

Because the observation value of tuple is generated by the corresponding state,
it is irrelevant to other states and only depends on its latent state. Thus, the
values of tuples are conditional independent given their labels.

P (X|Z) =

n(cid:2)

i=1

p(xi = oi|zi)

(2)

(cid:3)

We assume that the observation value of the ith tuple generated by the kth latent
state is characterized by a set of parameters θk, i.e., as we mainly consider mul-
tiple time series as real data, we propose to model observation data by Gaussian
distribution, because of its ﬂexibility in approximating a wide range of continu-
ous distributions. Therefore, we use the parameter mean vector μ and variance
matrix
We ﬁrst assume model parameters λ={ Θi, i from 1 to k} are known a prior.
In order to obtain approximately optimal assignment of latent variables for each
observation tuple, we transform to ﬁnd the optimal conﬁguration that maximizes
the posterior distribution given λ.

to describe the kth latent state, θk = (μk, Σk).

As discussed in Eq. (1), the probability distribution of Z is given by P (Z) =

(cid:3)

ωijδ(zi − zj))/H.

exp(γ

In the above equation, H is a constant value, which can be neglected. We use
the Iterated Conditional Modes (ICM) algorithm [5] to estimate the maximum a
posteriori probability (MAP). The greedy algorithm can be used by calculating
local minimization iteratively, which converges after a few iterations. The basic
idea is to sequentially update the label of each object, keeping the labels of the
other objects ﬁxed. At each step, the algorithm updates zi given xi and the label
by maximizing the conditional posterior probability, p(zi|xi = oi, ZI−{i}).
zi=s ωijδ(zi − zj))

p(zi|xi = oi, ZI−{i}) = p(xi|zi = s) × exp(γ

(cid:3)

(3)

H2

Actually, H2 can be considered as a constant variable. In doing so, we take
the logarithm of the posterior probability, and transform the MAP estimation
problem into the minimization of the conditional posterior energy function as
shown in the following equation,

Ui(k) = −ln(p(xi|zi = k)) − γ

ωijδ(zi − zj)

(4)

(cid:4)

j∈Bi

514

Z. Qiao et al.

where γ is a predeﬁned parameter that represents the importance of the temporal
ordering correlation. γ > 0 represents the conﬁdence of the temporal ordering
correlation network. To minimize Ui(k), we ﬁnd the latent state k of the tuple i
by k = argmaxkUi(k).

3.3 Parameter Estimation

In this part, we consider the problem of estimating unknown λ in order to iter-
atively learn optimal state assignment. λ describes the pattern conformed with
the time stamp that x is generated. We ﬁrst seek to ﬁnd λ to maximize P (X|λ),
which can be considered as the maximal likelihood estimation for λ. However,
since both the hidden label and the parameter are unknown and inter-dependent,
it is intractable to directly maximize the data likelihood. We view it as an
”incomplete-data” estimation problem, and use the Expectation-Maximization
(EM) algorithm as the solution.

The basic procedure is as follows. We start with an initial estimate λ0. Assume
that there exist k latent states, where λ0 is obtained by a simple K-Means
algorithm. In the E-step, we calculate the conditional expectation Q(Θ|Θ(t)),

Q(λ|λt) = ElnP (X, Z|λt) =

{P (Z|X, λt) × ln(X, Z|λt)}

(5)

(cid:4)

Z

Next, in the M-step, we ﬁnd λt+1 by computing the derivation of the maximizing
function Q(λ|λt).

μt+1
j =
(cid:3)n
i=1

Z

(cid:3)

Σt+1

j =

{p(zi = sj)|xi, λt × xi}

(cid:3)

(cid:3)n
(cid:3)
(cid:3)n
i=1
i=1

Z

Z

{p(zi = sj|xi, λt)
{p(zi = sj)|xi, λt × (xi − μt+1
{p(zi = sj|xi, λt)

(cid:3)n
i=1

(cid:3)

Z

j

)(xi − μt+1

j

)T}

(6)

(7)

In each iteration, we have obtained the optimal latent state assignment from the
last step. Assume the latent state of tuple i is state j, given λt. Thus, p(zi|xi, λt)
is 1 when zi is at state j, else 0. As a result, the E-step and M-step can be
recursively computed until Q(λ|λt) converges to a local optimal solution.

3.4 State Transforming Learning

After we obtain the optimal latent state assignment, each observation tuple is
assigned a label corresponding to the latent state set of tuples. The labels can be
seen as a sequence consisting of limit states along the time axis. We then model
correlation among states to reveal system dynamics. Here we regard correlation
as the transforming probability p(si|sj) representing the probability from the
state j to state i. A Markov chain model can be used to approximately estimate
the transforming probability among states by using p(sj|si) = N (sisj)/N (si),
where N (sj) represents the amount of labels with sj value in label series and
N (sjsi) represents the amount of adjacent labels with sj and si values in label
series.

Discovering Semantics from Multiple Correlated Time Series Stream

515

4 Applications of the Model

Observation Value Prediction. Time series semantics can be used to make
the following value prediction. We ﬁrst introduce the next 1-step value predic-
tion. Assume that we have learned the semantics from the training data, we then
have λ and the state transformation rule. In test step, we consider time series X,
as we care about the local trend of tuples, let xt−1and xt are current tuples. Our
task is to predict xt+1. We ﬁrst compute the current latent state of xt. When
we assign a label to the current tuple, we need to predict the label of the next
tuple according to the Markov chain characteristics. Assume the current label

zt is sc, the next label zt+1 can be obtained by maximizing p(zt+1 = i|zt = sc).

Additionally, according to time series continuity, we can estimate that the next
tuple close to the current tuple with high probability. Therefore, it is safe to say
that the next state maintains the continuity with high probability. That is, next
state can be predicted by,

ˆZt+1 = argmaxip( ˙xt+1 = xt) × p(zt+1 = i|zt = sc)

(8)

Because observation tuples produced by a state have similar values and similar
trends, we approximately predict observation values of the state by,

ˆxt+1 = argmaxx|x − Ezt+1| + |x − xt| × | x − xt

|x − xt| − Vzt+1
|Vzt+1||

(9)

where EZt+1 represents the Expectation of the observation value of the predicted
state, and VZt+1 represents the exception of the local trend of the predicted
state. We use the Euclidean distance to measure similarity. By computing the
derivative of function, the prediction value x can be obtained. Then we extend
next 1-step value prediction to the next n-step value prediction. We iteratively
apply the predicted value as the new observation value to forecast the next value
until n steps have been performed.

Anomaly Detection. Our proposed model also can be used to detect data
anomaly. In time series, there is no apparent and deﬁnite label to represent
which observation is normal or abnormal. So, it is not a classiﬁcation problem.
Generally speaking, we only know that anomaly occurs in a certain period. Take
a ﬁnance application for example. The worldwide economical recessions have
occurred several times in history. The recession always lasts for a period of time,
which is regarded as recession date. It impacts all business activities. Compared
with economic aﬀairs in other periods, economical aﬀairs in recessions can be
seen as anomaly. Hence, we propose the method based on a semantics model
only qualitatively to indirectly reﬂect anomalies.

According to time series continuity and semantics rules, we know that the
current tuple is similar to the last tuple with high probability, and the current
state has high transformation probability from the last state. Considering both
rules, we can measure the probability with which the current tuple normally is

generated by the following equation: f (xt) = p(xt|zt−1)×p(zt|zt−1). A logarithm

function is generally used to obtain a degree of the energy. Thus, we compute the

516

Z. Qiao et al.

Table 1. Runtime and Accuracy Comparison

0.5

0.8

1

0.1

0.05

0.2
N
5.82 13.413 30.568 49.377
H-Runtime 2.223 4.727
H-Error
0.0259 0.0152 0.0129 0.0113 0.0112 0.0111
V-Runtime 1.915 4.9590 3.785 10.65 19.315 26.024
V-Error
0.115 0.0117 0.0092 0.0078 0.0064 0.0059
T-Runtime 0.845 16.433 25.635 73.180 136.077 180.367
T-Error
0.0389 0.0265 0.0211 0.0169 0.0163 0.0152

H(Humid data) V(Volt data) and T(Temp data)

anomaly score of the current tuple by −logf (xt), which measures the signiﬁcance
the current tuple deviates from that emanating from the normal producing rule.
Intuitively, high scores indicate anomalous data with high probability.

5 Empirical Evaluation

In this section, we present extensive experiments on real-world multiple time se-
ries data to validate performance of our proposed approach. All experiments are
conducted on a 3.0GHZ CPU with 2 GB RAM. The experimental environment
is windows XP with Matlab.

Benchmark Data. We adopt four multiple time series data as our test-bed: 1)

Price data1 2) Mote data sets2 3) Medical data3 4) Financial data4

Baseline Methods. As discussed above, our proposed semantics learning method
can be applied to value prediction. We compare our method with following
value prediction algorithms: (1) Multivariate Autoregression Model. (2) Hidden
Markov Model (HMM).

5.1 Experiment Results

We ﬁrst consider the time cost of model learning and hyper parameter λ sensi-
tivity for data. Then we compare our proposed model with benchmark method
on multiple step prediction accuracy. Additionally, our proposed model can be
used to indirectly reﬂect latent anomalies, which are hard to see from the original
data. Here we mainly analyze ﬁnancial data to discover ﬁnancial mark depres-
sion.
1 The data set consists of the Reference Price Data (RPD) for APX Power UK Spot

market, which can be downloaded from Website
http://www.apxendex.com/index.php?id=466.

2 Mote data sets are collected using Berkeley Mote sensors, at several diﬀerent loca-
tions in a lab, over a period of a month. For each category of data, we just select
four time series.

3 It consists of 11 medical time series of patient from an Australian Hospital.
4 It is constituted of 11 economical time series GS1, DTB3, TB3MS, WTB3MS, GS5,
GS10, MPRIME, WPRIME, FEDFUNDS, AAA and BAA respectively, which are
obtained from the Website of the Federal Reserve Bank of St. Louis.

Discovering Semantics from Multiple Correlated Time Series Stream

517

We use relative error to measure accuracy of prediction. We compute the
relative error by |ˆxi − xi|/|xi| where |xi| is the estimated value, and xi is the
real value. Thus, the lower relative error is, the higher the accuracy is.

Time Complexity. Suppose the number of tuples is N in multiple correlated
time series. In M-step,the time complexity is O(N). In E-step, the time complex-
ity is O(N2) because of aggregating the eﬀect of the labels of brotherhood set of
vi to compute P (Z). Actually, semantics rules seen as latent pattern repeatedly
exist in the multiple time series. Hence, we do not need to learn a model based
on entire training set. We can approximately obtain the semantics from part
of the data set. In the experiment, we conduct our proposed model on all of
benchmark data set, and compare the runtime and next 1-step value prediction
accuracy under diﬀerent N , the selection ratio of boundary points. For example,
assume size of train-ing is 2000, and 0.05 means we choose 0.05*2000 =100 time
points to train the model. The results are shown in Table 1. In Table 1, the
error is average relative error for applying learned CfSLF to 200 testing tuples
sampled from testing set. It can be seen that runtime gets longer and accuracy
gets higher when ratio N gets bigger. We can see that in every data set, when
the percentage is equal to or larger than a certain value, the accuracy is not
aﬀected much.

Fig. 2. Hyper parameter λ sensitive

Fig. 3. Next n-step prediction comparison

Hyper Parameter Sensitivity. In our proposed model, parameter λ represents
the conﬁdence of the temporal ordering correlation network. Diﬀerent values of
λ determine diﬀerent eﬀects of the corre-lation ﬁeld. We separately conduct ex-
periments on all of the data sets to demonstrate hyper parameter λ setting and

518

Z. Qiao et al.

eﬀect. In each experiment, we vary λ from 0.1 to 2 separately, and compute the
corresponding relative error of the next 1-step value prediction under the prede-
ﬁned λ. HMM is used as baseline method to compare with our proposed method.
The outcome is shown in Figure 2. It can be seen that there are slight changes
in performance when parameters are varied and CfSLF has better performance
than the HMM and Multivariate Autoregression models.

Application of the Model. In the former section, we have tested performance
on the next 1-step value prediction. In the following, we will discuss additional
next n-step value prediction and anomaly detection.

Next n-step Value Prediction. In this experiment, we test the accuracy of
CfSLF for the next n-step value prediction. The experiments are conducted
for all data sets. We select 10 points randomly. For each selected point, we
predict the values after 1, 2, 5, 10, 20, and 50 steps respectively. The CfSLF is
compared with a Hidden Markov Model (denoted by HMM). The HMM can be
used to learn system-work mechanisms underlying time series. In the experiment,
we suppose that each tuple is produced by a latent state and the producing
procedure conforms to Gaussian distribution. We ﬁrst ﬁnd latent state of the
tuple at a selected time point, then make state predictions at 6 future time
points. Prediction value is corresponding expectation of predicted state. We use
relative error as the measurement. The results are shown in Figure 3. It can be
seen that on all data sets, CfSLF is more accurate than HMM.

Anomaly Detection. In the experiment, we use CfSLF on ﬁnancial data to
verify its performance for anomaly detection. The experimental results indicate
that the proposed method detected deviations from that emanating from the
normal producing rule as anomalies and these corresponded to actual economic
events. The degree of anomaly in these time series is shown in Figure 4. In
Figure 4, we see that two apparently peak deviated from other scores. Each peak
corresponded to big economic events occurring in corresponding month. The ﬁrst
peak appeared on January 2008, where the Federal Reserve lowered its federal
funds rate, which impacts how much consumers pay on credit card debt, home
equity lines of credit and auto loans, to 3.5 percent from 4.25 percent, which was
the biggest rate cut by the Fed since October 1984. The second peak appeared on
September 2008, where Lehman Brothers announced its bankruptcy. The second
peak indicates that the proposed method detected the depression whictarted in
September 2008, as anomalies.

6 Related Works

There are many works on analysing time series, such as summary learning, time
series segmentation, forecasting and so on, which have always been popular top-
ics [8][9][11][12][13][14][15][16]. However, they just can be used to analyse sin-
gle time series. Additionally, pattern learning from time series based on sliding
windows has attracted more and more attention [1][2][3][4][5][6][7][17]. However,
these methods cannot reveal global system-work rules. In recent years, semantics
mining has been always a popular topic. In time series analysis, semantics can

Discovering Semantics from Multiple Correlated Time Series Stream

519

Fig. 4. Anomaly score time series of multiple correlated ﬁnancial time series data

mainly be seen as system-work mechanisms. While, there are few studies on it.
In [12], pHMM is proposed to learn time series semantics. However, it is just
used to analyze a single time series. Generally, a Hidden Markov Model can be
used to learn the semantics rules. Some other improved methods based on HMM
are applied to learn latent system rules [16]. However, in multiple time series, a
single observation value contains little information. Compared with these meth-
ods, our proposed model introduces local trends to extend information of tuple,
and learn semantics from multiple time series based on both observation values
and local trend correlation.

7 Conclusions

In this paper, we present a new Correlation ﬁeld-based Semantics Learning
Framework (CfSLF) to model multiple correlated time series. Our model aims
to ﬁnd semantics underneath multiple time series, by detecting data generating
rules and transforming rules. Experiments have demonstrated the utility of the
proposed method. The contribution of the study is three folder: (1) The Hid-
den Markov Random Field (HMRF) is used to model the data observations and
corresponding states, by which the irregular patterns can be summarized from
multiple correlated time series. (2) A value prediction method is presented based
on semantics learned by CfSLF. (3) An anomaly detection method is proposed
based on data semantics.

Acknowledgments. This research was supported by the ARC Linkage Project
(LP100200682) named ”Real-time and Self- Adaptive Stream Data Analyser
for Intensive Care Management”; Internation S&T Cooperation Program of
China No.2011DFA12910; National Science Foundation of China (NSFC) Grant
(71072172; 61103229; 61272480; 61003167); the ”Strategic Priority Research Pro-
gram” of the Chinese Academy of Sciences Grant (XDA06030200); 863 Program
Grant (2011AA01A103).

520

Z. Qiao et al.

References

1. Zhang, C., Weng, N., Chang, J., Zhou, A.: Detecting Abnormal Trend Evolu-
tion over Multiple Data Streams. In: Chen, L., Liu, C., Zhang, X., Wang, S.,
Strasunskas, D., Tomassen, S.L., Rao, J., Li, W.-S., Candan, K.S., Chiu, D.K.W.,
Zhuang, Y., Ellis, C.A., Kim, K.-H. (eds.) WCMT 2009. LNCS, vol. 5731, pp.
285–296. Springer, Heidelberg (2009)

2. Zhang, P., Gao, B.J., Liu, P., Shi, Y., Guo, L.: A framework for application-driven

classiﬁcation of data streams. Neurocomputing 92, 170–182 (2012)

3. Papadimitriou, S., Sun, J., Faloutsos, C.: Streaming Pattern Discovery in Multiple

Time-Series. In: Proceedings of VLDB 2005 (2005)

4. Chan, P.K., Mahoney, M.V.: Modeling Multiple Time Series for Anomaly Detec-

tion. In: Proceedings of ICDM

5. Hirose, S., Yamanishi, K., Nakata, T., Fujimaki, R.: ]Network Anomaly Detection

based on Eigen Equation Compression. In: Proceedings of SIGKDD 2009 (2009)

6. Qiao, Z., He, J., Cao, J., Huang, G., Zhang, P.: Multiple Time Series Anomaly
Detection Based on Compression and Correlation Analysis: A Medical Surveillance
Case Study. In: Sheng, Q.Z., Wang, G., Jensen, C.S., Xu, G. (eds.) APWeb 2012.
LNCS, vol. 7235, pp. 294–305. Springer, Heidelberg (2012)

7. Fujimaki, R., Nakata, T., Tsukahara, H., Sato, A., Yamanishi, K.: Mining Abnor-
mal Patterns from Heterogeneous Time-Series with Irrelevant Features for Fault
Event Detection. Statistical Analysis and Data Mining 2 (2009)

8. Zhang, P., Gao, B.J., Zhu, X., Guo, L.: Enabling Fast Lazy Learning for Data

Streams. In: Proceedings of ICDM (2011)

9. Zhang, P., Zhu, X., Shi, Y., Guo, L., Wu, X.: Robust ensemble learning for mining

noisy data streams. Decision Support Systems 50(2), 469–479 (2011)

10. Stock, J.H., Watson, M.W.: Vector Autoregressions. Journal of Economic Perspec-

tives 15(4), 101–115

11. Yves, N.: Total Least Squares: State-of-the-Art Regression in Numerical Analysis.

SIAM Review 36 (2), 258–264

12. Wang, P., Wang, H., Wang, W.: Finding Semantics in Time Series. In: Proceedings

of SIGMOD 2011 (2011)

13. Duncan, G., Gorr, W., Szczypula, J.: Forecasting Analogous Time Series, pp.

15213–13890. Carnegie Mellon University, Pittsburgh

14. Pang, C., Zhang, Q., Hansen, D.P., Maeder, A.J.: Unrestricted wavelet synopses

under maximum error bound. In: Proceedings of EDBT 2009 (2009)

15. Keogh, E., Chu, S., Hart, D., Pazzani, M.: An online algorithm for segmenting

time series. In: Proceedings of ICDM 2001 (2001)

16. Wang, Y., Zhou, L.: Mining complex time-series data by learning the temporal
structure using bayesian techniques and markovian models. In: Proceedings of
ICDM 2006 (2006)

17. Zhang, P., Li, J., Wang, P., Gao, B., Zhu, X., Guo, L.: Enabling Fast Prediction

for Ensemble Models on Data Streams. In: Proceedings of SIGKDD 2011 (2011)


