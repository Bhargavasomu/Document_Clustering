Cost-Sensitive Listwise Ranking Approach

Min Lu1, MaoQiang Xie2,(cid:2), Yang Wang1, Jie Liu1, and YaLou Huang1,2

1 College of Information Technology Science, Nankai University, Tianjin, China

2 College of Software, Nankai University, Tianjin, China

{lumin,wangyang022,jliu}@mail.nankai.edu.cn,

{xiemq,huangyl}@nankai.edu.cn

Abstract. This paper addresses listwise approaches in learning to rank
for Information Retrieval(IR). The listwise losses are built on the prob-
ability of ranking a document highest among the documents set. The
probability treats all the documents equally. However, the documents
with higher ranks should be emphasized in IR where the ranking order
on the top of the ranked list is crucial. In this paper, we establish a frame-
work for cost-sensitive listwise approaches. The framework redeﬁnes the
probability by imposing weights for the documents. The framework re-
duces the task of weighting the documents to the task of weighting the
document pairs. The weights of the document pairs are computed based
on Normalized Discounted Cumulative Gain(NDCG). It is proven that
the losses of cost-sensitive listwise approaches are the upper bound of
the NDCG loss. As an example, we propose a cost-sensitive ListMLE
method. Empirical results shows the advantage of the proposed method.

1 Introduction

Learning to rank is a popular research area due to its widespread applications.
This paper focuses on document retrieval. When learning to rank has been ap-
plied to document retrieval, it aims to learn a real-valued ranking function that
induces a ranking order over the documents set of a query.

The existing ranking approaches can be summarized into three categories:
pointwise, pairwise and listwise. The pointwise and pairwise methods[1,2] trans-
form ranking problem into regression or classiﬁcation problem. The listwise
approaches[3,4,5] minimize the loss functions deﬁned between the ranked list
and the ground truth list. Theoretical analysis about the properties of the list-
wise loss functions are also conducted. Fen Xia[3] studied the consistency and
soundness of the listwise loss functions, and Yanyan Lan[6] investigated the gen-
eralization bound of the listwise loss functions based on Rademacher Averages.
Many listwise algorithms are developed, but no research is conducted to elab-
orate the common characteristic of the listwise loss functions. What is more,
the listwise losses are inadequate for IR where the high performance of the top
documents in the ranked list is preferred, measure by NDCG. In this paper, we
propose a framework for cost-sensitive listwise approaches. The cost-sensitive

(cid:2) Corresponding author.

M.J. Zaki et al. (Eds.): PAKDD 2010, Part I, LNAI 6118, pp. 358–366, 2010.
c(cid:2) Springer-Verlag Berlin Heidelberg 2010

Cost-Sensitive Listwise Ranking Approach

359

listwise loss functions are constructed based on the documents with weights.
The framework reduces the task of setting weights for the documents to the task
of setting weights for the document pairs. The weights of the document pairs
are computed based on the NDCG. As an example, we develop a cost-sensitive
ListMLE algorithm. Experimental results show that the proposed method out-
performs ListMLE[3], RankCosine[5], AdaRank[7] and Ranking SVM[2].

2 Normalized Discount Cumulative Gain (NDCG)

NDCG[8] evaluates the performance of the top documents in the ranked list.
Suppose n candidate documents are retrieved for a query. Each candidate docu-
ment di is represented as a pair (xi, yi), where xi denotes the query-document
pair feature vector and yi is the relevance level. The NDCG@k is deﬁned as:

NDCG@k = DCGπ@k
DCGg@k

=

n(cid:2)
n(cid:2)

j=1

j=1

2yj − 1

log2 (1 + π(j)) I[π(j) ≤ k]
log2 (1 + g(j)) I[g(j) ≤ k]

2yj − 1

(1)

where k denotes the truncation level. π is the ranked list generated by a ranking
function. g denotes the ground truth list obtained in the document relevance
level descending order. g(j) and π(j) denote the ground truth ranking position
and the ranked position of the document dj respectively. I[x] yields one if x is
true and zero otherwise. Assume that the ranking function is f, then the ranked
position π(j) is computed from:

n(cid:2)

i=1

n(cid:2)

(cid:3)g(j) = 1 +

π(j) = 1 +

I[f(xi) > f(xj)]

(2)

where f(xi) denotes the rank score of the document di. If there are many doc-
uments sharing the same relevance level with the document dj, it is impossible

to state the deﬁnite value of g(j). So the approximate value (cid:3)g(j) is given.

I[yi > yj]

It is obvious that (cid:3)g(j) ≤ g(j) for each j , which implies the inequality.

i=1

DCGg@k ≤ DCG(cid:3)g@k

Then the NDCG@k loss, abbreviated Lndcg@k, has a upper bound.

Lndcg@k = 1 − N DCG@k ≤

1

DCG(cid:3)g@k

(DCG(cid:3)g@k − DCGπ@k)

(3)

(4)

(5)

360

M. Lu et al.

3 Listwise Approach

3.1 Existing Listwise Loss Function

The listwise approaches take documents lists as an instance, and minimize the
loss functions deﬁned between the ranked list and the ground truth list. Typical
methods include ListNet[4], RankCosine[5] and ListMLE[3]. We review their loss
functions. For the sake of describing simply, the documents have been ranked by
the relevance level in descending order, i.e., y1 (cid:3) y2 . . . (cid:3) yn.

RankCosine L =

1
2

⎛
⎝1 −
(cid:7)(cid:6)
n(cid:10)
(cid:6)
ListMLE L = − log
(cid:2)
n(cid:10)
(cid:6)

ListNet L = −

i=1

π∈Y

i=1

(cid:6)

n

j=1 φ(yj)φ(f(xj))

(cid:7)(cid:6)

n

j=1 φ(f(xj))2

n

j=1 φ(yj)2
exp (φ(f(xi)))
n
j=i exp (φ(f(xj)))

(cid:12)

(cid:11)

ψy(xπ(ti)

(cid:11)

ψy(xπ(tj ))

φ
n
j=i φ

(cid:12) log

n(cid:10)

i=1

(cid:6)

(cid:11)

φ
n
j=i φ

f(xπ(ti)

f(xπ(tj ))

(cid:12)

(cid:12)

⎞
⎠

(cid:11)

where φ is a positive and strictly increasing function. f(xi) is the rank score of
the document di computed by the ranking function f. ψy is a mapping function
deﬁned on the relevance level and preserves ground truth list, i.e., ψy(y1) ≥
ψy(y2) . . . ψy(yn). xπ(ti) denotes the query-document pair feature vector of the
document with ranked position being at i.

3.2 Analysis of Listwise Loss Function

Several listwise approaches are developed, but no analysis is studied to elaborate
the common characteristic of the listwise loss functions. In this study, we point
out that the listwise loss functions are in essence built upon the probability of
ranking a document highest among the documents set, deﬁned as

h(xi|x1, x2, . . . , xn; ψ) =

exp (ψ(f(xi)))
n
j=1 exp (ψ(f(xj)))

(6)

(cid:6)

where ψ denotes an increasing function. The existing listwise loss functions can
be expressed in terms of the function h.

(cid:13)

⎞
⎠
h (xj|x1, x2, . . . , xn ; 2 log φ)

RankCosine L =

1
2

ListMLE L = − log
(cid:2)

ListNet L =−

⎛
⎝1 − n(cid:2)
n(cid:10)
n(cid:10)

i=1

j=1

π∈Y

i=1

2

·

φ(yj)
i=1 φ(yi)

(cid:7)(cid:6)n
h(xi|xi, xi+1, . . . , xn; φ)
n(cid:10)
(cid:12) log
(cid:6)n

ψy(xπ (ti)

(cid:11)

(cid:11)

(cid:12)

ψy(xπ (tj ))

φ
j=i φ

i=1

h(xπ (ti)|xπ (ti+1). . . ,xπ (tn); φ)

Cost-Sensitive Listwise Ranking Approach

361

To minimize the listwise loss function is equivalent to minimizing the function
h. The reason for the goodness of these loss functions is that h is closely re-
lated to the pairwise classiﬁcation error. The conclusion is simply proved in the
following.

n(cid:2)

n(cid:2)
(cid:14)

j=i+1

1 +

j=i+1

j=i+1

n(cid:2)

1
n − i

1
n − i

I[φ(f (xj )) > φ(f (xi))] ≤ 1
n − i

log2 [1 + exp (φ(f (xj )) − φ(f (xi)))]
(cid:15)
≤ log2
≤ −log2 h(xi|xi, xi+1, . . . , xn ; φ)
exp(φ(f (xj )) − φ(f (xi)))
The I[φ(f(xj) > φ(f(xi)] is the pairwise classiﬁcation error because of yi (cid:3) yj.
(cid:6)
(cid:6)
The above inequalities can be veriﬁed with I[z > 0] ≤ log2 (1 + exp(z)) and
j=1 log2 (zj) ≤ log2 (
j=1 zj/n) when each zj is nonnegative. The [1] shows
1
n
that the pairwise classiﬁcation error is inadequate for IR where the ranking
order on the top of the ranked list is crucial. Hence, the listwise losses are also
inadequate for IR since their key component h is closely related to the pairwise
classiﬁcation error.

n

n

4 Cost-Sensitive Listwise Approach Framework

4.1 Cost-Sensitive Listwise Loss Function

To make the listwise losses focus on the ranking order on the top of the ranked
list, a good way is to take account of cost-sensitive in the listwise losses, more
precisely, to set diﬀerent weights for the documents. The function h is redeﬁned
by imposing weights for the documents:
h(xi|x1, x2, . . . , xn; ψ, αi) =

(cid:6)

(7)

αi,i exp (ψ(f(xi)))
n
j=1 αi,j exp (ψ(f(xj)))

where αi = (αi,1, . . . , αi,n) and its components are nonnegative. αi,j is the
weight of the document dj. The function h can be also expressed as in the form
based on the document pairs with weights.

h(xi|x1, x2, . . . , xn; ψ, αi) =

n

j=1 αi,j /αi,i exp (ψ(f(xj)) − ψ(f(xi)))

1

(8)

(cid:6)

where αi,j /αi,i is the weight of the document pair (di,dj). The cost-sensitive
listwise approaches focus on the performance of the top documents in the ranked
list, measured by NDCG. Therefore, one important issue is to relate the weights
of the documents with the NDCG. We formulate the problem of setting weights
for the documents into the problem of setting weights for the document pairs.
In this study, the weights of the document pairs are theoretically given.

362

M. Lu et al.

4.2 Bound NDCG Errors by Cost-Sensitive Listwise Loss Function

In this section, it is proven that the cost-sensitive listwise losses are the upper
bound of NDCG loss. The theorem states deﬁnite values of the document pairs’
weights. Theoretical proof is based on the lemma 1. For notational simplicity,
let a(i) = 2yi − 1, b(j) and its gradient ∇b(j) are deﬁned as:
− log 2

(cid:16)

∇b(j) =

(1 + j) [log (1 + j)]2 j ≤ k

(9)

1/log2 (1 + j) j ≤ k
1/log2 (1 + k) j > k

b(j) =

⎧⎨
⎩

0

j > k

It is simple to prove that the NDCG@k loss Lndcg@k has the following upper
bound on the basis of equation (5). Due to space limitation, we omit the proof.
Lndcg@k ≤

a(i) (b((cid:3)g(i)) − b(π(i)))

n(cid:2)

n(cid:2)

a(i)

+

1

1

(cid:15)

(cid:14)

DCG(cid:3)g@k

log2 (1 + k)

i=1

DCG(cid:3)g@k

i=1

(cid:2)

Lemma 1. The NDCG@k loss Lndcg@k is upper bounded by weighted pairwise
[−a(j)∇b((cid:3)g(j)) − a(i)∇b((cid:3)g(i))]I [f(xj) < f(xi)]+C2
classiﬁcation loss.
Lndcg@k ≤
1
query in the training set, the ranking position (cid:3)g(j) of the document dj is easily

where C2 denotes a constant. The proof is given in the Appendix A. For each

DCG(cid:3)g@k

computed. Therefore, the weights of the document pairs of each query can be
calculated at once in training. Based on the lemma, we can justify the correlation
between cost-sensitive listwise loss and NDCG@k loss1.

yj(cid:4)yi

n(cid:2)

j=1

1

DCG(cid:3)g@k

βj log2 h(xj|xj+1, xj+2, . . . , xn; ψ, αj) + C2
(cid:20)(cid:6)

Theorem 1. The cost-sensitive listwise loss is the upper bound of NDCG loss
Lndcg@k.
Lndcg@k ≤ −
(cid:6)
j=1 wj log2(zj) ≤

C2 denotes a constant that is same to the constant in the lemma 1. Based on
lemma 1, the above inequality is easily veriﬁed with I[z > 0] ≤ log2 (1 + exp(z))
and
when each
zj and wj are nonnegative. The theorem demonstrates that many cost-sensitive
listwise approaches can be proposed to directly optimize NDCG. For example, we
can transform the existing listwise approaches to cost-sensitive listwise methods.
Meanwhile, the theorem states deﬁnite values for all the weights βj and αj.

n
j=1 wj zj/

t=1 wt)

n
j=1 wj

log2 (

(cid:6)

(cid:6)

(10)

(cid:21)

(cid:20)

(cid:21)

·

n

n

4.3 Diﬀerences from Cost-Sensitive Ranking SVM

The cost-sensitive Ranking SVM[1] and the framework for cost-sensitive listwise
approaches both make use of cost-sensitive learning in learning to rank, but there
are several diﬀerences between them.
1 In this paper, we use NDCG loss and NDCG@k ranking loss exchangeably.

Cost-Sensitive Listwise Ranking Approach

363

First, the training instance assigned weights is diﬀerent. The cost-sensitive
Ranking SVM weights on the document pairs. The framework credits the weights
for the documents. To solve the problem of setting weights for the documents,
the framework reduces it into the task of setting weights for document pairs.

Second, the way to calculate the weights of the document pairs is diﬀerent.
The cost-sensitive Ranking SVM sets the weights by the heuristic method. The
framework directly computes the weights based on NDCG@k.

Third, the relationship between the loss functions and NDCG loss is also diﬀer-
ent. The cost-sensitive Ranking SVM do not solve the problem. The framework
proves that the listwise losses are the upper bound of the NDCG loss.

4.4 A Case: Cost-Sensitive ListMLE Ranking Approach

L =

βj log2

n(cid:2)

n(cid:2)

j=1

1

DCG(cid:3)g@k

⎛
⎝1 +

To verify the framework, we propose a novel cost-sensitive approach named CS-
ListMLE(cost-sensitive ListMLE). The loss function on a query is deﬁned as:

⎞
⎠ (11)
αj,t exp (f(xt) − f(xj))
The weights are computed according to Theorem 1. The ranking function of CS-
ListMLE is linear, i.e., f(x) =< w, x >, where < ·,· > is the inner product and
w denotes model parameters. We takes gradient descent method to optimize the
loss function. Since the loss function is convex, the model parameters converge
to a global optimal solution. The algorithm is provided in Figure 1.

t=j+1,yj(cid:4)yt

Input: training set, learning rate η, tolerance rate ε, NDCG@k
Initialize: w and compute the weights with respect to each query using Eq. (10)
Repeat: do gradient descent until the change of the loss function is below ε
Return w

Fig. 1. Cost-sensitive ListMLE Ranking Approach

5 Experiments

The experiments are conducted on three datasets OHSUMED, TD2003 and
TD2004 in Letor2.02. The experiments validate the two points. The one is that
whether CS-ListMLE outperforms the ListMLE. The other is that whether CS-
ListMLE can obtains higher performance than the other baselines on the top
documents of the ranked list. MAP and NDCG@k(N@k) are used as evaluation
measures. The truncation level k in NDCG@k is usually 1, 3, 5 and 10. The
performances of ListMLE and RankCosine are directly taken from [9], where
both approaches do not provide their performance at NDCG@5 and MAP.

2

https://research.microsoft.com/en-us/um/beijing/projects/letor/Letor2.0
/dataset.aspx

364

M. Lu et al.

To validate the eﬀectiveness of CS-ListMLE, we train the algorithm with dif-
ferent parameters. CS-ListMLE@k claims that cost-sensitive ListMLE focuses on
the top k documents rank order in the ranked list. It means that CS-ListMLE@k
directly optimizes NDCG@k. In the experiments, the k takes 1, 3, 5 and 10.

5.1 Ranking Accuracy of ListMLE and Cost-Sensitive ListMLE

We report the performance of cost-sensitive ListMLE and ListMLE on three
datasets in Table 1, 2 and 3. The cost-sensitive ListMLE signiﬁcantly outper-
forms ListMLE on the datasets TD2003 and TD2004 in terms of all evaluation
measures, while their performance is comparable on OHSUMED.

Table 1. Ranking accuracies on OS-
HUMED

Table 2. Ranking accuracies on TD2003

Methods

N@1 N@3 N@5 N@10 MAP

Methods

N@1 N@3 N@5 N@10 MAP

0.548 0.473 —–

ListMLE
CS-ListMLE@1 0.555 0.482 0.464 0.446 0.442
CS-ListMLE@3 0.539 0.480 0.458 0.446 0.444
CS-ListMLE@5 0.548 0.468 0.453 0.438 0.443
CS-ListMLE@10 0.536 0.471 0.452 0.439 0.443

0.446 —–

0.24 0.253 —–

ListMLE
CS-ListMLE@1 0.48 0.400 0.362 0.359 0.262
CS-ListMLE@3 0.48 0.400 0.364 0.358 0.253
CS-ListMLE@5 0.48 0.391 0.358 0.355 0.264
CS-ListMLE@10 0.48 0.398 0.362 0.350 0.262

0.261 —–

Table 3. Ranking accuracies on TD2004

Table 4. Test Results on OSHUMED

Methods

N@1 N@3 N@5 N@10 MAP

Methods

N@1 N@3 N@5 N@10 MAP

0.4

0.351 —–

ListMLE
CS-ListMLE@1 0.467 0.427 0.422 0.444 0.364
CS-ListMLE@3 0.467 0.429 0.419 0.449 0.366
CS-ListMLE@5 0.467 0.420 0.407 0.444 0.366
CS-ListMLE@10 0.453 0.409 0.406 0.445 0.364

0.356 —–

0.523 0.478 0.466 0.449 0.450
ListNet
0.523 0.475 —–
RankCosine
0.514 0.462 0.442 0.437 0.442
AdaRank
RSVM
0.495 0.465 0.458 0.441 0.447
CS-ListMLE@1 0.555 0.482 0.464 0.446 0.442

0.437 —–

Table 5. Test Results on TD2003

Table 6. Test Results on TD2004

Methods

N@1 N@3 N@5 N@10 MAP

Methods

N@1 N@3 N@5 N@10 MAP

0.46 0.408 0.382 0.374 0.273
ListNet
0.36 0.346 —–
RankCosine
0.42 0.291 0.242 0.194 0.137
AdaRank
RSVM
0.42 0.379 0.347 0.341 0.256
CS-ListMLE@1 0.48 0.400 0.362 0.359 0.262

0.322 —–

0.440 0.437 0.420 0.458 0.372
ListNet
0.439 0.397 —–
RankCosine
0.413 0.402 0.393 0.406 0.331
AdaRank
RSVM
0.44
0.410 0.393 0.420 0.350
CS-ListMLE@1 0.467 0.427 0.422 0.444 0.364

0.405 —–

We explain why CS-ListMLE remarkably outperforms ListMLE on datasets
TD2003 and TD2004. On one hand, each query in datasets TD2003 and TD2004
has 1000 documents. The ratio of the queries containing at most 15 relevant doc-
uments in all queries is 95% and 89.3% respectively. Since ListMLE considers
all the document pairs, 95% and 89.3% queries loss functions of ListMLE in-
duces the losses caused by 484620 irrelevant document pairs. As is well known,
the NDCG loss is not sensitive to the losses generated by the irrelevant document

Cost-Sensitive Listwise Ranking Approach

365

pairs. Thus, ListMLE introduces a large deviation from the NDCG loss. How-
ever, CS-ListMLE only cares about the document pairs composed of relevant
documents and irrelevant documents. On the other hand, ListMLE treats the all
documents equally. But CS-ListMLE assigns the weights for the document based
on NDCG@k. In summary, the loss of CS-ListMLE is more close to NDCG@k
loss than ListMLE on datasets TD2003 and TD2004.

As far as dataset OHSUMED, 81.1% queries have less than 200 documents,
while 85.89% queries contain at least 10% relevant documents. Under such situ-
ation, the weights of the documents does not aﬀect much in CS-ListMLE. The
loss of CS-ListMLE is approximate to the loss of ListMLE.

5.2 Comparison with the Other Baselines

We take CS-ListMLE@1 as an example to compare the performance with the
other baselines, including RankCosine[5], ListNet[4], RSVM[2] and AdaRank[7].
Experimental results are presented in Table 4, 5 and 6. CS-ListMLE almost out-
performs RankCosine, AdaRank and Ranking SVM on three datasets at all eval-
uation measures. Compared to ListNet, CS-ListMLE obtains higher performance
at NDCG@1. We conduct t-test on the improvement of CS-ListMLE over List-
Net, Ranking SVM and AdaRank on the three datasets in terms of NDCG@1.
The results indicate that the improvement of NDCG@1 over Ranking SVM and
AdaRank on dataset OHSUMED is statistically signiﬁcant(p-value<0.05). There
is no statistically signiﬁcant diﬀerence on dataset TD2003 in spite of rising 6%
over AdaRank and Ranking SVM.

Experiments results demonstrate that the CS-ListMLE can achieve high per-
formance on NDCG@1, which meets the goal that the CS-ListMLE focuses on
the top one documents ranking order of the ranked list. Meanwhile, the cost-
sensitive ListMLE obtains comparable performance to the baselines at MAP.

6 Conclusion

In this paper, we point out that the existing listwise losses are inadequate IR
where the documents with higher ranks should be emphasized. To address the
issue, we propose a framework for cost-sensitive listwise approaches. The frame-
work credits weights for the documents. The framework reduces the problem
of setting weights for the documents to the problem of setting weights for the
document pairs. The weights of the document pairs are computed based on the
NDCG. It is proven that the cost-sensitive listwise loss is the upper bound of
NDCG loss. As an example, we develop a cost-sensitive ListMLE approach. Ex-
perimental results show that the cost-sensitive ListMLE outperforms ListMLE
on two benchmark datasets in terms of all evaluation measures. In addition, the
cost-sensitive ListMLE almost outperforms the baselines, such as RankCosine,
AdaRank and Ranking SVM, on the three datasets at all evaluation measures.

366

M. Lu et al.

References

1. Xu, J., Cao, Y., Li, H., Huang, Y.: Cost-sensitive learning of svm for ranking. In:

ECML, pp. 833–840 (2006)

2. Joachims, T.: Optimizing search engines using clickthrough data. In: Proceedings
of the eighth ACM SIGKDD international conference on Knowledge discovery and
data mining, pp. 133–142. ACM, New York (2002)

3. Xia, F., Liu, T.Y., Wang, J., Zhang, W., Li, H.: Listwise approach to learning to
rank: theory and algorithm. In: Proceedings of the 25th international conference on
Machine learning, pp. 1192–1199. ACM, New York (2008)

4. Cao, Z., Qin, T., Liu, T.Y., Tsai, M.F., Li, H.: Learning to rank: from pairwise
approach to listwise approach. In: Proceedings of the 24th international conference
on Machine learning, pp. 129–136. ACM, New York (2007)

5. Qin, T., Zhang, X.D., Tsai, M.F., Wang, D.S., Liu, T.Y., Li, H.: Query-level loss

functions for information retrieval. Inf. Process. Manage. 44(2), 838–855 (2008)

6. Lan, Y., Liu, T.Y., Ma, Z., Li, H.: Generalization analysis of listwise learning-to-
rank algorithms. In: Proceedings of the 26th Annual International Conference on
Machine Learning, pp. 577–584. ACM, New York (2009)

7. Xu, J., Li, H.: Adarank: a boosting algorithm for information retrieval. In: Pro-
ceedings of the 30th annual international ACM SIGIR conference on Research and
development in information retrieval, pp. 391–398. ACM, New York (2007)

8. J¨arvelin, K., Kek¨al¨ainen, J.: Cumulated gain-based evaluation of ir techniques. ACM

Trans. Inf. Syst. 20(4), 422–446 (2002)

9. Xia, F., Liu, T.Y., Li, H.: Statistical consistency of top-k ranking. In: Advances in

Neural Information Processing Systems, pp. 2098–2106 (2009)

(cid:14)
n(cid:2)

i=1
a(i)

1

DCG(cid:3)g@k
n(cid:2)
1

i=1

(cid:15)

1

+

1

DCG(cid:3)g@k
(cid:2)

a(i)

n(cid:2)
[−a(j)∇b((cid:3)g(j)) − a(i)∇b((cid:3)g(i))]

log2 (1 + k)

i=1

A Theoretical Justiﬁcation of Lemma 1

Lndcg@k ≤

a(i) (b((cid:3)g(i)) − b(π(i)))

yj =yi

DCG(cid:3)g@k

DCG(cid:3)g@k
n(cid:2)
n(cid:2)

j=1

C1 =

DCG(cid:3)g@k
∵ b is convex function ⇐⇒ b(x) − b(y) ≤ ∇b(x)(x − y)

log2 (1 + k) C2 = C1 +
a(j) · ∇b((cid:3)g(j)) · ((cid:3)g(j) − π(j)) + C1
n(cid:2)
a(j) · (−∇b((cid:3)g(j))) ·
(cid:2)
[−a(j)∇b((cid:3)g(j)) − a(i)∇b((cid:3)g(i))] I[f(xj) < f(xi)] + C2

I[f(xi) > f(xj)]

(cid:22)(cid:14)

(cid:15)

∵ I[x > 0] − I[y > 0] ≤ I[xy < 0] + I[y = 0]

DCG(cid:3)g@k

≤

=

1

1

(cid:14)

1 +

j=1

i=1

≤

1

DCG(cid:3)g@k

yj (cid:2)yi

−

1 +

(cid:15)(cid:23)

I[yi > yj]

+ C1

n(cid:2)

i=1


