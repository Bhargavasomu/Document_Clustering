Knowl Inf Syst
DOI 10.1007/s10115-014-0765-8

REGULAR PAPER

A robust one-class transfer learning method
with uncertain data
Yanshan Xiao · Bo Liu · Philip S. Yu · Zhifeng Hao

Received: 14 July 2013 / Revised: 10 May 2014 / Accepted: 3 July 2014
© Springer-Verlag London 2014

Abstract One-class classiﬁcation aims at constructing a distinctive classiﬁer based on one
class of examples. Most of the existing one-class classiﬁcation methods are proposed based
on the assumptions that: (1) there are a large number of training examples available for
learning the classiﬁer; (2) the training examples can be explicitly collected and hence do not
contain any uncertain information. However, in real-world applications, these assumptions
are not always satisﬁed. In this paper, we propose a novel approach called uncertain one-class
transfer learning with support vector machine (UOCT-SVM), which is capable of constructing
an accurate classiﬁer on the target task by transferring knowledge from multiple source tasks
whose data may contain uncertain information. In UOCT-SVM, the optimization function is
formulated to deal with uncertain data and transfer learning based on one-class SVM. Then,
an iterative framework is proposed to solve the optimization function. Extensive experiments
have showed that UOCT-SVM can mitigate the effect of uncertain data on the decision
boundary and transfer knowledge from source tasks to help build an accurate classiﬁer on
the target task, compared with state-of-the-art one-class classiﬁcation methods.
Keywords Transfer learning · Uncertain data · One-class classiﬁcation

Y. Xiao · Z. Hao
School of Computers, Guangdong University of Technology, Guangzhou, China
e-mail: xiaoyanshan@gmail.com

Z. Hao
e-mail: mazfhao@scut.edu.cn

B. Liu (B)

School of Automation, Guangdong University of Technology, Guangzhou, China
e-mail: csbliu@gmail.com

P. S. Yu
Department of Computer Science, University of Illinois at Chicago, Chicago, IL, USA
e-mail: psyu@uic.edu

P. S. Yu
Computer Science Department, King Abdulaziz University, Jeddah, Saudi Arabia

123

X. Yanshan et al.

1 Introduction

One-class classiﬁcation [1,2] is an important research area in machine learning and data
mining, which addresses the learning problems where only one class of examples is available
for training the classiﬁer. In this case, the class available to learn the classiﬁer is called the
target class, and the other classes are the nontarget classes. The main task of one-class
classiﬁcation is to build a classiﬁer on the target class, and the obtained classiﬁer is then used
to classify a new example to the target class or nontarget class [3]. One-class classiﬁcation
has found a large variety of applications, such as anomaly detection [1], automatic image
annotation [4], sensor data drift detection [5] and remote sensing [6].

Depending on the availability of training data, the existing work on one-class classiﬁcation
can be classiﬁed into two broad categories. (1) The approaches for one-class classiﬁcation
with positive data and unlabeled data [7,8], where different algorithms are proposed to extract
the negative examples from the unlabeled data, and then, a binary classiﬁer is constructed
based on the positive examples and the extracted negative examples. For example, Yu et
al. [7] extracts negative documents by checking the frequency of features within the positive
and unlabeled training documents and trains SVM iteratively to build a binary classiﬁer. (2)
The approaches for one-class classiﬁcation with positive examples [1,9], where the classiﬁer
is trained on only the positive examples. For example, one-class SVM [1] is proposed to
construct a hyperplane for separating the positive examples from the origin, such that the
hyperplane can be utilized to differentiate the outliers (negative examples) from the positive
examples. Our approach belongs to the second category.

Despite much progress on one-class classiﬁcation, most of the existing work considers
the one-class classiﬁcation problem as a single learning task. However, in many real-world
applications, we expect to reduce the labeling effort of a new task (referred to as target
task) by transferring knowledge from the related task (source task), which is called transfer
learning [10]. Taking Web document classiﬁcation as an example, the user is interested in
football and plenty of Web documents are labeled on it. As time goes by, the user changes his
interest to basket ball, which has related, but different data distributions from football. For
this new task, we may not have many labeled documents since labeling a large number of
documents timely may be expensive for the user. Hence, we expect to transfer the knowledge
from previously labeled documents to help build the classiﬁer of the new task. Another
important observation is that, due to sampling error or instrument imperfection, the collected
data in real-world applications may be corrupted with noises and thereafter contain uncertain
information [11]. For example, in environmental monitoring applications, sensor networks
generate a large amount of uncertain data because of instrument errors, limited accuracy or
noise-prone wireless transmission [11]. Hence, how to build a classiﬁer on the target task
by transferring knowledge from the source task whose input data may contain uncertain
information remains a key challenge for real-world one-class applications.

In this paper, we address the one-class transfer learning problem with uncertain data. To
build the classiﬁer, we have two challenges. The ﬁrst one is how to construct the one-class
classiﬁer when the training examples on the target task are not sufﬁcient to build a precise
classiﬁer and may be corrupted by noises. The second one is how to solve the formulated
optimization problem effectively. To handle these challenges, we propose a novel approach,
called uncertain one-class transfer learning with support vector machine (UOCT-SVM). The
main characteristics of our approach can be viewed from the following aspects:

1. We propose an uncertain one-class transfer learning classiﬁer, which can improve the
one-class classiﬁer of the target task by transferring knowledge from source tasks which

123

A robust one-class transfer learning method with uncertain data

may contain uncertain data. At the same time, it can fulﬁll the knowledge transferring
from not only a single source task but also multiple source tasks.

2. We propose an iterative framework to mitigate the effect of noises on the one-class
classiﬁer and transfer knowledge from the source task to the target task. To the best of
our knowledge, this is the ﬁrst work to explicitly handle data uncertainty and knowledge
transfer in one-class classiﬁcation.

3. We conduct extensive experiments to evaluate the performance of our UOCT-SVM
approach. The experimental results show that UOCT-SVM learns a more accurate classi-
ﬁer for the target task by transferring the knowledge from the source task and meanwhile
mitigating the noises of the input data, compared with state-of-the-art one-class classiﬁ-
cation methods.

The rest of this paper is organized as follows. Section 2 discusses the existing work related
to our study. Section 3 introduces the preliminaries. Section 4 presents our proposed approach
in details. Section 5 extends our approach to knowledge transfer from multiple source tasks.
Section 6 reports the experimental results. Section 7 concludes the paper and offers the future
work.

2 Related work

2.1 Mining uncertain data

To deal with data uncertainty, many learning algorithms have been proposed. In the following,
we brieﬂy review the work on mining uncertain data in classiﬁcation, clustering and other
problems.

Some methods are proposed to deal with uncertain data in clustering problems. Kriegel
and Pfeiﬂe [12] adopts a fuzzy distance function to measure the similarity between uncertain
data on top of the hierarchical density-based clustering algorithm. Ngai et al. [13] studies the
problem of clustering data objects whose locations are uncertain and applies the UK-means
algorithm to cluster uncertain objects. Aggarwal [14] discusses how to use the density-based
approaches to handle error-prone and missing data.

Some other methods are devised to handle uncertain data in classiﬁcation problems. Bi and
Zhang [15] extends standard SVM to deal with uncertain data, which provides a geometric
algorithm by optimizing the probabilistic separation between the two classes on both sides
of the boundary. Gao and Wang [16] mines discriminative patterns from uncertain data as
classiﬁcation features/rules, to help train either SVM or rule-based classiﬁer. Tsang et al. [17]
modiﬁes classical decision tree building algorithms to handle data tuples with uncertain
values.

Recently, Murthy et al. [18] describes how aggregation is handled in the Trio system for
uncertain and probabilistic data. Yuen et al. [19] proposes a new problem, called superseding
nearest neighbor search, on uncertain spatial databases. Sun et al. [20] studies the discovery
of frequent patterns and association rules from probabilistic data under the possible world
semantics and proposes two efﬁcient algorithms to discover frequent patterns in bottom-up
and top-down manners.

Despite much progress on this area, most of the existing work considers uncertain data
mining as a single task learning problem. However, in real-world applications, it may be
expensive and time-consuming to label a large amount of data for a new learning task, and
we expect to reduce the labeling efforts of the new task by transferring knowledge from

123

X. Yanshan et al.

related tasks. In this paper, we propose a novel approach UOCT-SVM that can not only
handle uncertain data but also improve classiﬁcation accuracy of the new task’s classiﬁer by
transferring knowledge from related tasks.

2.2 Transfer learning

In transfer learning, algorithms are designed to transfer knowledge to the target task from
one or more source tasks that have similar, but not the same data distributions to the target
task, such that the knowledge from the source task can beneﬁt the learned classiﬁer for the
target task [10,21]. Transfer learning has been applied to solve the learning problems in
various areas, such as text categorization [22–24], WiFi localization [25,26] and computer
aided design [27].

According to Pan and Qiang [28],

the approaches for transfer learning can be
broadly categorized into four categories: instance-transfer, feature-representation-transfer,
parameter-transfer, and relational-knowledge-transfer. In instance-transfer [21–24,29], train-
ing instances in the source domain are re-weighted according to their impact on the learning in
the target task. In feature-representation-transfer [30–32], different algorithms are proposed
to learn a common feature representation across tasks that reduces the task divergence and
training error. In parameter-transfer [33–35], they attempt to discover the shared parameters
between the source task and target task, which beneﬁts for transfer learning. In relational-
knowledge-transfer [36–38], the source task and target task are assumed to be relational, and
the mapping of relational knowledge between the source task and target task is built. Our
approach falls into the parameter-transfer category.

Multitask learning [25] is closely related to transfer learning. In multitask learning, multi-
ple related tasks are learnt simultaneously to improve the predictive performance relative to
learning these tasks independently. Bonilla et al. [39] investigates the multitask learning prob-
lems when task-speciﬁc features are available. They consider the similarity between tasks
and construct a free-form kernel matrix to represent task relations. Lawrence and Platt [33]
extends the informative vector machine to handle multitask learning cases. Yu et al. [40] pro-
poses a hierarchical Gaussian process framework for multitask learning. Though multitask
learning is related to transfer learning, they focus on different learning objectives. Multitask
learning tries to improve the performances on all tasks, while transfer learning attempts to
transfer knowledge from the source task to the target task [41].

Most of the existing work on transfer learning assumes that the training data in the source
task and the target task can be precisely collected and does not contain any uncertain informa-
tion. However, in real-world applications, due to sampling error or instrument imperfection,
the collected data may be corrupted with noises and contain uncertain information. In this
case, how to train a classiﬁer that can fulﬁll the knowledge transferring, and meanwhile, deal
with data uncertainty effectively becomes a key challenge for real-world transfer learning
applications. This motivates the work in this paper.

3 Preliminary

3.1 One-class SVM
Suppose that the training target class is S = {x11, x12, . . . , x1|S|}, where x1i ∈ Rd is the ith
example in S. One-class SVM aims to determine a plane to separate the target class and the
origin of the space:

123

A robust one-class transfer learning method with uncertain data

Reachability
Area 

xt i 

xt i 

Origion

Fig. 1 Illustration of the reachability area of example xti

min

s.t. wT

|S|(cid:2)
i=1

ξi

1
2

(cid:3) w0 (cid:3)2 −ρ + C
0 x1i ≥ ρ − ξi
ξi ≥ 0, i = 1, 2, . . . ,|S|,

(1)

where w0 and ρ are the norm vector and bias, respectively; C is a parameter trading off the
margin and the errors. After solving problem (1), we can obtain the classiﬁer f (x) = wT
0 x−ρ.
0 x − ρ > 0, it is classiﬁed into the target class; otherwise, it
For a test example x, if it has wT
belongs to the nontarget class.

In this paper, we extend standard one-class SVM to one-class transfer learning with uncer-

tain data.

3.2 Uncertain data model

Since the collected example may deviate from the location that it should be, we assume that
the example is subject to an additive noise vector x. The original uncorrupted input xs can
thereafter be denoted as

xs = x + x.

(2)

In practice, we may not have any prior knowledge about the distribution of x. For this
reason, we assume that the noise vector x follows a particular distribution. The method of
bounded and ellipsoidal uncertainties has been widely investigated and successfully applied
in machine learning problems [9,42]. As in Liu et al. [9] and Huffel and Vandewalle [42],
we consider a simple bound score δ for each example as

(cid:3) x (cid:3)≤ δ,

(3)
where (cid:3) x (cid:3) represents the norm of example x. It is seen from (3) that the norm of x
is no less than a bound score δ. We let x + x ((cid:3) x (cid:3)≤ δ) denote the reachability area of
example x, as illustrated in Fig. 1. Then, it has

(cid:3) xs (cid:3)=(cid:3) x + x (cid:3)≤(cid:3) x (cid:3) + (cid:3) x (cid:3)≤ (cid:3) x (cid:3) +δ

(4)

123

In this way, xs falls into the reachability area of x. By using the bound score for each
example, we can convert the uncertain one-class transfer learning into standard one-class
learning problems with constraints.

X. Yanshan et al.

4 One-class transfer learning on uncertain data

In this section, we will present our proposed UOCT-SVM approach to handle uncertain data
in one-class transfer learning when a single source task is available. The extension to transfer
learning from more than one source task will be discussed in Sect. 5. Then, we will show
how to solve the learning problem.

4.1 Formulation

Suppose that there are two one-class classiﬁcation tasks—the source task S1 and the tar-
get task S2. The source task S1 consists of |S1| positive examples, i.e., x11, x12, . . . , x1|S1|,
where x1i (i = 1, . . . ,|S1|) denotes the ith examples in the source task S1, and |S1| is the
number of examples in S1. Likewise, the target task S2 contains |S2| positive examples, i.e.,
x21, x22, . . . , x2|S2|, where |S2| is the number of examples in S2. The main objective of one-
class transfer learning is to transfer the knowledge of the source task S1 to the target task S2.
Let f1(x) = wT
2 x − ρ2 be the classiﬁcation planes for S1 and S2,
respectively. To facilitate the transfer, we make
w1 = w0 + v1,
w2 = w0 + v2,

1 x − ρ1 and f2(x) = wT

(5)
(6)

where w0 can be considered as a bridge to transfer knowledge from the source task to the
target task; v1 and v2 represent the discrepancy between the globe optimal decision boundary
(w0) and the local optimal decision boundary (w0 + v1 for the source task S1 and w0 + v2
for the target task S2). By substituting Eqs. (5) and (6), the hyperplanes for S1 and S2 can
be rewritten as f1(x) = (w0 + v1)T x − ρ1 and f2(x) = (w0 + v2)T x − ρ2. Moreover, the
input examples in the source task and the target task may contain uncertain information. To
deal with the uncertainty, we represent each example of S1 and S2 as x + x, based on the
uncertain data model in Sect. 3.2. Hence, the learning problem for one-class transfer learning
with uncertain data can be formulated as

min (cid:3)w0(cid:3)2 + C1(cid:3)v1(cid:3)2 + C2(cid:3)v2(cid:3)2 − ρ1 − ρ2 + 2(cid:2)
t=1
s.t. (w0 + v1)T (x1i + x1i ) ≥ ρ1 − ξ1i ,
(w0 + v2)T (x2 j + x2 j ) ≥ ρ2 − ξ2 j ,
ξ1i ≥ 0,

|St|(cid:2)
ξti
i=1
i = 1, . . . ,|S1|
j = 1, . . . ,|S2|
ξ2 j ≥ 0, (cid:3)x1i(cid:3) ≤ δ1i , (cid:3)x2 j(cid:3) ≤ δ2 j .

Ct

(7)

where C1, C2 and Ct are regularized parameters; ξ1i and ξ2 j are training errors. From the
optimization problem (7), we can observe that:

1 x1i and x2 j are the imputed training examples, which may be corrupted by noises and
contain uncertain information. To reduce the effect of noises, we let each example in the
source task and the target task represented as x1i + x1i and x2 j + x2 j , respectively.

123

A robust one-class transfer learning method with uncertain data

By optimizing the values of x1i and x2 j , we can reﬁne the learnt one-class transfer
learning classiﬁer less sensitive to noises. (cid:3)x1i(cid:3) ≤ δ1i and (cid:3)x2 j(cid:3) ≤ δ2 j restrict the
range of uncertain information using bound scores δ1i and δ2 j .

2 We utilize the common variable w0 as a bridge to transfer the knowledge. Parameters C1
and C2 control the preference of the two tasks. If C1 > C2, task 1 is preferred to task 2;
otherwise, task 2 is preferred to task 1.

4.2 Solution to uncertain one-class transfer learning classiﬁer

The optimization function (7) is difﬁcult to solve since the variables w0, v1, v2, ρ1, ρ2, x1i ,
x2 j , ξ1i and ξ2 j are unknown to us. In this section, we will employ an iterative framework
to calculate these unknown variables and present a novel scheme to estimate the bound score
δ1i and δ2 j for the training examples.

Speciﬁcally, the iterative framework consists of two steps. In the ﬁrst step, we ﬁx x1i
and x2 j , and solve the learning problem (7) to obtain the values of w0, v1, v2, ρ1, ρ2, ξ1i
and ξ2 j . In the second step, we ﬁx w0, v1, v2, ρ1, ρ2, ξ1i and ξ2 j , and optimize the learning
problem (7) to get the values of x1i and x2 j . The above two steps repeat alternatively
until the termination criterion is met. In the following, we present the two steps in details.

4.2.1 Calculating the classiﬁer by ﬁxing x1i and x2 j

We initialize x1i and x2 j as x1i and x2 j , respectively, and let them satisfy the con-
straints(cid:3) x1i (cid:3)≤ δ1i and(cid:3) x2 j (cid:3)≤ δ2 j .1 Then, the constraints(cid:3) x1i (cid:3)≤ δ1i ,(cid:3) x2 j (cid:3)≤
δ2 j in problem (7) will not have effect on the solution, and we remove them from the objective
function. The objective function (7) is transformed into

min (cid:3)w0(cid:3)2 + C1(cid:3)v1(cid:3)2 + C2(cid:3)v2(cid:3)2 − ρ1 − ρ2 + 2(cid:2)
t=1
s.t. (w0 + v1)T (x1i + x1i ) ≥ ρ1 − ξ1i ,
(w0 + v2)T (x2 j + x2 j ) ≥ ρ2 − ξ2 j ,
ξ1i ≥ 0,

|St|(cid:2)
ξti
i=1
i = 1, . . . ,|S1|
j = 1, . . . ,|S2|

ξ2 j ≥ 0.

Ct

(8)

Since the values of x1i and x2 j are given, problem (8) is a QP problem, which can be
transformed into a standard one-class SVM and solved via the dual form. Hence, we give the
dual form of problem (8) in Theorem 1.

Theorem 1 By introducing the Lagrange function [43], the dual form of the optimization
problem (8) can be given by
|S1|(cid:2)
i=1

|S1|(cid:2)
h=1

|S1|(cid:2)
g=1

1hx1gα1g

α1hxT

min

1
2

|S2|(cid:2)
j=1
+ C2 + 1

1i x2 j α2 j + C1 + 1
α1i xT
|S2|(cid:2)
|S2|(cid:2)
p=1
k=1

2 px2k α2k

α2 pxT

4C1

4C2

1 In the experiments, we initialize x1i = 0 and x2 j = 0.

123

X. Yanshan et al.

α1i = 1, 0 ≤ α1i ≤ C1, i = 1, . . . ,|S1|

|S1|(cid:2)
s.t.
i=1
|S2|(cid:2)
α2 j = 1, 0 ≤ α2 j ≤ C2,
j=1

(9)
where α1i , α2 j , α1h , α1g, α2 p, α2k ≥ 0 are Lagrange multipliers; it has x1i = x1i + x1i ;
x2 j , x1g, x1h , x2 p and x2k are similar to x1i .

j = 1, . . . ,|S2|

The proof for obtaining Theorem 1 can refer to Sect. 8.1. After solving the dual form (9),
we can obtain the solutions of α1i , α2 j , α1h , α1g, α2 p, and α2k. Then, the values of w0, v1
and v2 can be calculated as

α1i x1i +

⎞
⎠ ,

|S2|(cid:2)
j=1

α2 j x2 j

⎛
|S1|(cid:2)
⎝
i=1
|S1|(cid:2)
i=1
|S2|(cid:2)
j=1

w0 = 1
2

v1 = 1
2C1
v2 = 1
2C2

α1i x1i ,

α2 j x2 j ,

∗
For the examples x1i in S1, we let subset S
1 contain those examples with 0 < α1i < C1.
∗
For the examples x2 j in S2, we let subset S
2 contain those examples with 0 < α2 j < C2.
According to the KKT conditions [43], the examples with 0 < αti < Ct (i = 1, . . . ,|St|, t =
1, 2) are support vectors (SVs) whose corresponding constraints become equation and it has
ξti = 0. Hence, we obtain

(10)

(11)

(12)

(13)
(14)

(15)

(16)

According to Eqs. (13) and (14), ρ1 and ρ2 can be computed as

(w0 + v1)T x1i = ρ1, x1i ∈ S
∗
1
(w0 + v2)T x2 j = ρ2, x2 j ∈ S
∗
2

ρ1 = 1|S
∗
1
ρ2 = 1|S
∗
2

|

|

(cid:2)
x1i∈S
∗
1
(cid:2)
x2 j∈S

∗
2

(w0 + v1)T x1i ,

(w0 + v2)T x2 j ,

| and |S

∗
2

∗
1

where |S
∗
2 . Based
on the values of w0, v1, v2, ρ1 and ρ2, the classiﬁer for the target task can be obtained as
f2(x) = (w0 + v2)T x − ρ2.

| represent the corresponding numbers of examples in S

∗
1 and S

4.2.2 Calculating x1i and x2 j by ﬁxing the classiﬁer
Supposing that f1(x) = (w0 + v1)T x − ρ1 and f2(x) = (w0 + v2)T x − ρ2 are the classiﬁer
obtained from the ﬁrst step. Here, we ﬁx the classiﬁers f1(x) and f2(x), i.e., making w0 =
w0, v1 = v1, v2 = v2, ρ1 = ρ1 and ρ2 = ρ2 and optimize problem (7) over x1i and x2 j .

To do this, we have Theorem 2 as follows.

123

A robust one-class transfer learning method with uncertain data

Algorithm 1 Uncertain one-class transfer learning with uncertain data
Input: Source task S1, target task S2 ; // Training set
........... C1, C2; // Regularization parameters
........... δ1i , δ2 j ; // bound scores for training examples.
Output: f1(x) and f2(x).
1: t=0;
2: Initialize Fval (t ) = ∞;
3: repeat
4:
5:
6:
7:
8:
9:
10:
11: Compute w0, v1 and v2 according to Equations (10)-(12);
12: Compute ρ1 and ρ2 based on Equations (15)-(16);
13: Let Fval (t ) be the decision function’s value of problem (8);
14: Let Fmax = max{|Fval (t − 1)|,|Fval (t )|}
15: until |Fval (t ) − Fval (t − 1)| < εFmax
16: Return f1(x) = (w0 + v1)T x − ρ1 and f2(x) = (w0 + v2)T x − ρ2.

t = t + 1;
if t=1 then
Initialize x1i = 0 and x2 j = 0;
else
Update x1i and x2 j based on (17) and (18), by ﬁxing w0, v1, v2, ρ1, ρ2;

end if
Substitute x1i and x2 j , and solve problem (8);

Theorem 2 By ﬁxing w0, v1, v2, ρ1 and ρ2 to be w0, v1, v2, ρ1 and ρ2, respectively, the
solutions of x1i and x2 j for optimizing problem (7) are

x1i = δ1i
x2 j = δ2 j

w0 + v1
(cid:3) w0 + v1 (cid:3) , i = 1, . . . ,|S1|,
w0 + v2
j = 1, . . . ,|S2|.
(cid:3) w0 + v2 (cid:3) ,

(17)

(18)

(cid:7)

(cid:7)|St|
i=1

It is seen from (7) that the objective function’s value is determined by w0, v1, v2, ρ1, ρ2 and
2
t=1
ξti . Considering that w0, v1, v2, ρ1 and ρ2 are ﬁxed to be w0, v1, v2, ρ1 and ρ2,
ξti , and the optimization
ξti . In
ξti . The

respectively, the objective function’s value is decided by
of the objective function (7) is transformed into the minimization of
Theorem 2, we try to optimize xti and xt j to minimize the value of
proof for Theorem 2 refers to Sect. 8.2.

(cid:7)
(cid:7)

(cid:7)|St|
2
t=1
i=1
(cid:7)|St|
2
t=1
i=1

(cid:7)
2
t=1

(cid:7)|St|
i=1

4.2.3 Iterative framework

We have introduced the details of how to train the classiﬁers f1(x) and f2(x) and update the
noise vectors x1i and x2 j . By referring to the alternating optimization method in [42], an
iterative framework is proposed to train the classiﬁer and update the noise vectors alternatively
until a termination criterion is met. Algorithm 1 illustrates the pseudo codes of our approach.
Here, we employ the stopping criterion as in [44] to determine the termination of UOCT-
SVM. When the proportion of |Fval (t ) − Fval (t − 1)| and Fmax is smaller than a threshold
ε, the algorithm stops.

Moreover, in problems (7) and (8), δ1i and δ2 j are parameters that we need to estimate.
For each example x1i in S1, we calculate the average distance between x1i and its k–nearest
neighbors—and assign this average distance to δ1i . The same operation is utilized to the
examples x2 j in S2. This setting has been successfully utilized in the previous work [9].

123

X. Yanshan et al.

It is noted that the problem settings of our approach and “Transfer Learning with One-Class
data” (TLOC) [45] are different. In our approach, the target task is a one-class classiﬁcation
problem, and the source tasks are also one-class classiﬁcation problems. Our approach aims
at describing the data distribution of the positive class. In TLOC, the target task is a one-class
classiﬁcation problem, but the source tasks are binary-class classiﬁcation problems. TLOC
attempts to depict the distributions of both the positive class and the negative class.

4.3 Kernelized uncertain one-class transfer learning classiﬁer

In the nonlinear classiﬁcation problems, the examples of the target class and the nontarget
class are difﬁcult to be separated by using a linear classiﬁcation plane. To make the data more
separable, we map the training examples into the feature space via a nonlinear mapping func-

tion φ (·). Hence, the examples in the source task are transformed into{φ (x11), . . . , φ (x1|S1|)},
and those in the target task are changed into{φ (x21), . . . , φ (x2|S2|)}, where φ (xti ) is the image
of example xti in the feature space. The inner product of φ (xti ) and φ (xh j ) can be calculated
using a kernel function K (xti , xh j ) = φ (xti )T φ (xh j ).
To build the nonlinear classiﬁer, we need to conduct some modiﬁcations on the two steps
presented in Sect. 4.2. In the dual form (9) of the ﬁrst step, we replace xT
1hx1g and
xT
2 px2k with K (x1i , x2 j ), K (x1h , x1g) and K (x2 p, x2k ), respectively. After solving the dual
form, the corresponding classiﬁers for the source task and the target task are obtained as (19)
and (20).

1i x2 j , xT

f1(φ (x)) = 1
2
f2(φ (x)) = 1
2

2(cid:2)
t=1
2(cid:2)
h=1

|St|(cid:2)
i=1
|Sh|(cid:2)
j=1

αti K (xti , x) + 1
2C1
αh j K (xh j , x) + 1
2C2

|S1|(cid:2)
i=1
|S2|(cid:2)
j=1

α1i K (x1i , x) − ρ1

α2 j K (x2 j , x) − ρ2

(19)

(20)

Moreover, in the input space, we estimate the uncertainties using bounded sphere as
xti ≤ δti . However, in the feature space, the bounded spheres correspond to irregular
shapes and it brings difﬁculty to solve the optimization problem. For this reason, we adopt
an approximation strategy based on the ﬁrst order Taylor expansion of the kernel function
K (·). The ﬁrst order Taylor expansion of K (·) with respect to x at point xti is

(21)

where K

K (xti + xti ,·) = K (xti ,·) + xT
ti K

(cid:8)(xti ,·),
(cid:8)(xti ,·) is the gradient of K (·) with respect to x at point xti .
(cid:7)
2
t=1

(cid:7)

xti∈St

(cid:7)|Sh|
j=1

2
h=1

ξti over x1i
For the second step in Sect. 4.2, we minimize the value of
and x2 j , which can be transformed into the minimization of each ξ = max{0, ρt −
(cid:7)
αt j K (xt j + xt j , x + x} (x ∈
St , t = 1, 2) over x. By applying the ﬁrst order Taylor expansion of K (·) in Eq. (21), we
have Theorem 3 in the following.

αh j K (xh j + xh j , x + x) − 1

(cid:7)|St|
j=1

2Ct

1
2

Theorem 3 Assuming that w0, v1, v2, ρ1 and ρ2 are ﬁxed to w0, v1, v2, ρ1 and ρ2, respec-
tively, the optimal values of x1i and x2 j are

x1i = δ1i

u1i

||u1i|| , x2 j = δ2 j

u2 j
||u2 j|| ,

(22)

123

A robust one-class transfer learning method with uncertain data

where it has
u1i = 1
2
u2 j = 1
2

2(cid:2)
h=1
2(cid:2)
h=1

(cid:8) (cid:8)

(cid:8) (cid:8)

αhg K

xhg + xhg, x1i

|Sh|(cid:2)
g=1
|Sh|(cid:2)
g=1
(cid:8)(xhg + xhg, x1i ) and K

xhg + xhg, x2 j

αhg K

(cid:9) + 1
2C1
(cid:9) + 1
2C2

(cid:8) (cid:8)

x1g + x1g, x1i

(cid:9)

,

α1g K

(cid:8) (cid:8)

x2g + x2g, x2 j

(cid:9)

.

α2g K

|S1|(cid:2)
g=1
|S2|(cid:2)
g=1

In Theorem 3, K

(cid:8)(xhg + xhg, x2 j ) are the gradient of K (xhg +
xhg, x) with respect to x at points x1i and x2 j , respectively. The proof for Theorem 3 can
refer to Sect. 8.3. Similar to the linear cases, for the nonlinear classiﬁcation problems, x1i
and x2 j are initialized to be zero vectors. In the ﬁrst step, we ﬁx x1i and x2 j to obtain the
nonlinear classiﬁer by replacing xT
2 px2k with K (x1i , x2 j ), K (x1h , x1g)
and K (x2 p, x2k ), respectively, in the dual form (9). In the second step, we ﬁx the classiﬁer
and optimize the value of xti as presented in (22). Since the initial values of x1i and x2 j
are given, x1i and x2 j in the current iteration can be obtained based on their values in the
previous iteration by using (22). The above two steps iterate until the termination criterion
is met.

1hx1g and xT

1i x2 j , xT

5 Extension to one-class transfer learning with uncertain data from multiple source

tasks

In this section, we extend the uncertain one-class transfer learning classiﬁer to the learning
problems where the knowledge from more than one source tasks is transferred to the target
task. Suppose that there are K−1 source tasks S1, S2, . . . , SK−1, and one target task SK . Each
task is one-class classiﬁcation problem, which consists of a number of positive examples,
i.e., xt1, xt2, . . . , xt|St|, where xti (i = 1, . . . ,|St|) is the ith example in task St , and |St| is
the number of examples in St . We aim at constructing an one-class transfer learning classiﬁer
that is capable of transferring the knowledge of the K − 1 source tasks to the target task with
uncertain data.
Let fi (x) = (w0 + vi )T x − ρi be the classiﬁcation plane for the ith source tasks (i =
1, . . . , K − 1), and f K (x) = (w0 + vK )T x − ρK denote the plane for the target task. Since
e ach example in the source and target tasks may contain uncertain information, similar to
Sect. 4, we introduce a noise vector xti for each example xti , and the corrected example is
represented as xti + xti . Based on this uncertain data representation, the learning problem
of the uncertain one-class transfer learning classiﬁer with multiple source tasks can be given
as

min ||w0||2 + 1
ξti
K
s.t. (w0 + vt )T (xti + xti ) ≥ ρt − ξti , i = 1, . . . ,|St|,

Ct||vt||2 − K(cid:2)
t=1

ρt + K(cid:2)
t=1

K(cid:2)
t=1

Ct

|St|(cid:2)
i=1

t = 1, . . . , K

||xti|| ≤ δti , ξti ≥ 0.

(23)
It is seen from problem (23) that for the corrected examples xti + xti in task St , the
corresponding classiﬁer ft (x) = (w0 + vt )T x − ρt separates them from the origin with a
margin. w0 is considered as a common variable to transfer the knowledge between the source
tasks and the target task.

123

X. Yanshan et al.

Similar to Sect. 4, an iterative framework is adopted to solve problem (23). In the ﬁrst
step, we initialize xti to be 0, and ﬁx them to train the uncertain one-class transfer learn-
ing classiﬁer. Since the values of xti are no larger than δti , we eliminate the constraints
||xti|| ≤ δti , and problem (23) is changed into
Ct||vt||2 − K(cid:2)
t=1

min ||w0||2 + 1
ξti
K
s.t. (w0 + vt )T (xti + xti ) ≥ ρt − ξti , i = 1, . . . ,|St|,

ρt + K(cid:2)
t=1

t = 1, . . . , K

|St|(cid:2)
i=1

K(cid:2)
t=1

Ct

(24)
To solve problem (24), we ﬁrst let e = (1, 1, . . . , 1)T be a K -dimensional column vector,

ρ = (ρ1, ρ2, . . . , ρK )T , and redeﬁne the following notations.
(cid:11)

(cid:11)

(cid:11)

(cid:10)

ξti ≥ 0.

w =

z(xti , t ) =

z(xti , t ) =

et =

C2
K
(cid:17)

K
Ct
(cid:17)

C1
K

v1,

t−1

w0,
⎛
⎝xti , 0, . . . , 0
,
(cid:13) (cid:14)(cid:15) (cid:16)
⎛
⎝xti , 0, . . . , 0
,
(cid:13) (cid:14)(cid:15) (cid:16)
⎛
⎝0, . . . , 0
(cid:13) (cid:14)(cid:15) (cid:16)

t−1

, 1, 0, . . . , 0
(cid:13) (cid:14)(cid:15) (cid:16)

K−t

t−1

(cid:12)T

CK
K

vK
⎞
⎠

T

v2, . . . ,

xti , 0, . . . , 0
(cid:13) (cid:14)(cid:15) (cid:16)

K−t

K
Ct

xti , 0, . . . , 0
(cid:13) (cid:14)(cid:15) (cid:16)

K−t

T

⎞
⎠

T

(25)

(26)

(27)

(28)

⎞
⎠

where 0 = {0, 0, . . . , 0}T and 1 = {1, 1, . . . , 1}T are d-dimensional column vectors with
all elements being 0 and 1, respectively. Based on the above notations, problem (24) can be
transformed into

min ||w||2 − ρT e + K(cid:2)
t=1
s.t. wT z(xti , t ) ≥ ρT et − ξti , i = 1, . . . ,|St|,

ξti

Ct

|St|(cid:2)
i=1

ξti ≥ 0.

t = 1, . . . , K

(29)
where it has z(xti , i ) = z(xti , t ) + z(xti , t ). Problem (29) is a standard one-class SVM,
which is solved via the dual from, as presented in Theorem 4.

Theorem 4 The dual form of problem (29) can be obtained as

αti z(xti , t )T z(xh j , h)αh j ,

|Sh|(cid:2)
j=1

|St|(cid:2)
K(cid:2)
K(cid:2)
max − 1
4
t=1
h=1
i=1
|St|(cid:2)
K(cid:2)
αti et = e,
t=1
i=1
0 ≤ αti ≤ Ct , i = 1, . . . ,|St|,

s.t.

t = 1, . . . , K .

(30)

123

A robust one-class transfer learning method with uncertain data

where αti ≥ 0 and αh j ≥ 0 are Lagrange multipliers. The proof for Theorem 4 can refer to
Sect. 8.4. Moreover, it has

w = 1
2

K(cid:2)
t=1

|St|(cid:2)
i=1

αti z(xti , t ).

(31)

By resolving the dual form (30), we can obtain αti and αh j , and thereafter the value
of w can be calculated according to Eq. (31). Let It ∈ R K+1 = (0, . . . , 0, 1, 0, . . . , 0)T
(t = 0, . . . , K ) be a column vector with the (t + 1)th elements being 1 and the other
elements being 0. Based on the deﬁned column vector It , it is easy to deduce that w0 = wT I0
and vt = (cid:18)

wT It .

In the second step, we ﬁx the values of w and ρ and optimize the learning problem (23)

K
Ct

over z(xh j , h). To do this, we have Theorem 5 as follow.

Theorem 5 Supposing that w and ρ are ﬁxed to be w and ρ, respectively, the solution of
z(xh j , h) for optimizing problem (23) is

z(xh j , h) = δh j

w0 + vh
(cid:3) w0 + vh (cid:3) = δh j

wT (I0 + (cid:18)
(cid:3) wT (I0 + (cid:18)

K
Ch

K
Ch

Ih )

Ih ) (cid:3) , j = 1, . . . ,|Sh|.

(32)

The above two steps repeat alternatively until the termination criterion is met. When the
optimization procedure stops, we can obtain the classiﬁer f K (x) = (w0 + vK )T x − ρK for
the target task, and the obtained classiﬁer is used to predict the unknown examples.
Let wφ be the weight vectors in the feature space. We can derive the objective function

for nonlinear problems from problem (29), as follows:

min ||wφ||2 − ρT e + K(cid:2)
t=1
s.t. (wφ )T φ (z(xti , t )) ≥ ρT et − ξti , i = 1, . . . ,|St|,

ξti

Ct

|St|(cid:2)
i=1

ξti ≥ 0.

t = 1, . . . , K

(33)

Considering that the noise vector xti is ﬁxed and z(xti , t ) is known, problem (33) is
representer theorem, the weight vector wφ can be expressed as wφ = (cid:7)
a standard one-class SVM, which satisﬁes the representer theorem [46]. By applying the
α j K (z(xh j , h),·)
and it has wφ · φ (z(xti , t )) = (cid:7)
α j K (z(xh j , h), z(xti , t )). The dual from can be obtained
by substituting wφ into problem (33). After solving the dual form, the values of αti , wφ and
ρ can be obtained.
ξti over xti . Since wφ and ρ are
Then, we ﬁx wφ and ρ, and optimize
known, problem (33) is transformed into problem (34) after substituting wφ · φ (z(xti , t ) +
z(xti , t )) = (cid:7)

(cid:7)|St|
i=1

K
t=1 Ct

n
j=1

n
j=1

(cid:7)

n
j=1

Ct

ξti

|St|(cid:2)
i=1
(cid:8)

α j K (z(xh j , h), z(xti , t ) + z(xti , t ))).
K(cid:2)
t=1
n(cid:2)
j=1
ξti ≥ 0, i = 1, . . . ,|St|,

(cid:9)
z(xh j , h), z(xti , t ) + z(xti , t )
t = 1, . . . , K .

α j K

min

s.t.

) ≥ ρT et − ξti ,

(34)

123

Theorem 6 In problem (34), the optimal z(xh j , h) is computed by

z(xh j , h) = δh j

(cid:19)uh j
||(cid:19)uh j|| ,

j = 1, . . . ,|Sh|, h = 1, . . . , K

(35)

X. Yanshan et al.

where it has

(cid:19)uh j = K(cid:2)
t=1

|St|(cid:2)
i=1

(cid:8)(z(xti , t ) + z(xti , t ), z(xh j , h)).

αti K

In Theorem 6, K

(cid:8)(z(xti , t ) + z(xti , t ), z(xh j , h)) is the gradient of K (z(xti , t ) +
z(xti , t ), z(x,·)) with respect to z(x,·) at points z(xh j , h). The proof for Theorem 6
can refer to Sect. 8.5. In the feature space, the nonlinear classiﬁer for the target task is
f K (φ (x)) = (wφ
)T φ (x)− ρK . By replacing z(xti , t ) with φ (z(xti , t )), we can get the
= (wφ )T IK , by
norm vector wφ in the feature space. Since it has wφ
0
substituting wφ
K into f K (φ (x)), the nonlinear classiﬁer f K (φ (x)) for the target task
can be computed as

= (wφ )T I0 and vφ

0 and vφ

+ vφ

K

0

K

(cid:10)

(cid:17)

f K (φ (x)) = 1
2

I0 +

(cid:12)T K(cid:2)
t=1

K
CK

IK

|St|(cid:2)
i=1

K (z(xti , t ), x) − ρK

(36)

Our approach satisﬁes the representer theorem [46]. Similar to the linear cases, our
approach for nonlinear classiﬁcation contains two alternative steps. The ﬁrst step is to ﬁx the
noise vector xti [i.e., z(xti , t )] and solve a QP problem (33) which is a standard one-class
SVM and meets the representer theorem [46]. The second step is to ﬁx wφ and ρ, and update
on wφ · φ (·) = (cid:7)
the noise vector xti (i.e., z(xti , t )), as shown in problem (34). This step is computed based
α j K (z(xh j , h),·), which has been obtained in the ﬁrst step. Hence,

n
j=1

thought our approach is non-convex, it satisﬁes the representer theorem.

6 Experiments

To investigate the effectiveness of our proposed UOCT-SVM approach, we conduct experi-
ments on several real-world datasets. All experiments run on a laptop with 2.8 GHz processor
and 3GB DRAM. The SVM-based algorithms are implemented based on LibSVM [47]. The
objectives of our experiments are: (1) to evaluate the effectiveness of UOCT-SVM on trans-
ferring knowledge to the target task from one or more than one source tasks; (2) to investigate
the sensitivity of UOCT-SVM to different percentages of data noise.

6.1 Baselines and metrics

We compare UOCT-SVM with the following baselines:

1. The ﬁrst baseline is standard one-class SVM (OC-SVM) [1], which uses a hyperplane to
separate the target class and the origin of the space. It is used to show the improvement
of our approach over OC-SVM.
2. The second baseline is transfer learning-based one-class SVM (TLOC-SVM), which is a
variant of our approach by excluding the uncertain data processing scheme. We set xi j =
0 and straightforwardly utilize problems (8) and (23) to train a transfer learning classiﬁer
with a single source task and multiple source tasks, respectively, without updating xi j .

123

A robust one-class transfer learning method with uncertain data

This baseline is utilized to evaluate the ability of our approach on dealing with data
uncertainty.

3. The third baseline is uncertain one-class SVM (UOC-SVM) [9], which builds one-class
classiﬁer to handle uncertain data. This baseline is used to investigate the capability of
our approach on transferring knowledge from the source task to beneﬁt the construction
of classiﬁers on the target task.
The performance of classiﬁcation systems is evaluated in terms of F-measure value [48].
We use it as the evaluation metric in the experiments. The F-measure value trades off the
precision p and recall r, and it has F = 2 pr /( p + r ). From this deﬁnition, we know that only
when both the precision p and recall r are large, the F-measure value will exhibit a large value.

6.2 Dataset description and experimental setting

6.2.1 One-class classiﬁcation datasets

To evaluate the effectiveness of our approach, we conduct experiments on three real-world
datasets—Reuters-21578,2 20 Newsgroup,3 and mushroom4 datasets. These datasets are
popularly used in the previous transfer learning work [21,28–30]. To fulﬁll the transfer
learning purpose, we split and reorganize each dataset to generate the source task, which has
the similar but different distribution to the test data, and the target task that has the same
distribution with the test data.

The 20 Newsgroup and Reuters-21578 datasets have hierarchical structures. Taking the
20 Newsgroup dataset as an example, it has 7 top categories. Under the top categories, there
are 20 sub-categories and each sub-category has 1,000 examples. Following the same routine
in previous work [49], we generate the one-class transfer learning datasets based on the top
categories. Speciﬁcally, we consider one sub-category as the target class in turn, and select
a number of examples from the other top categories as the non-target class. To do this, a
sub-category (a1) from a top category (A) is selected and considered as the target class. The
examples from the other top categories, i.e., those except for category (A), are treated as the
nontarget class. Based on this, we generate the target class and the nontarget class for the
target task. For the source task, we choose a sub-category (a2) from the same top category
(A), and consider this sub-category (a2) as the target class. The sub-datasets generated from
the 20 Newsgroup dataset are named as “NG.*”, as shown in Table 1. In this table, ”NG.os”
indicates that in the “NG.os” sub-dataset, the sub-category ”os” is considered as the target
class for the target task, while the other sub-categories in the same top category are regarded
as the target classes for the source tasks.

For the Reuters-21578 dataset, each top category has many sub-categories. For example,
the top category ”people” has 267 sub-categories and the size of each sub-category is not
always large. As in Pan and Qiang [28] and Dai et al. [21,29], we reorganize the sub-categories
within each top category. For a top category (A), all of the subcategories are reorganized into
two parts (denoted as a(1) and a(2)), and each part is approximately equal in sizes. Then, the
reorganized sub-categories a(1) and a(2) are regarded as the target classes for the target task
and the source task, respectively. Similar to the 20 Newsgroup dataset, the examples from the
other top categories, i.e., those except for the top category (A), are treated as the nontarget
class for the target task. The generated sub-datasets are named as “RT.*” in Table 1.

2 Available at http://www.daviddlewis.com/resources/testcollections/.
3 Available at http://people.csail.mit.edu/jrennie/20Newsgroups/.
4 Available at http://archive.ics.uci.edu/ml/datasets/Mushroom.

123

Table 1 The categories contained in the target and source tasks for each sub-dataset

Dataset

Target task

Source task

X. Yanshan et al.

NG.os
NG.ibm
NG.mac
NG.graphics
NG.autos
NG.baseball
NG.hokey
NG.crypt
NG.med
NG.space
NG.religion
NG.guns
NG.mideast
RT.orgs(1)
RT.orgs(2)
RT.people(1)
RT.people(2)
RT.place(1)
RT.place(2)
MR.edible(1)
MR.edible(2)
MR.poisonous(1)
MR.poisonous(2)

comp.os
comp.ibm
comp.mac
comp.graphics
rec.autos
rec.sport.baseball
rec.sport.hokey
sci.crypt
sci.med
sci.space
talk.religion
talk.politics.guns
talk.politics.mideast
orgs(1).{ ... }
orgs(2).{ ... }
people(1).{ ... }
people(2).{ ... }
place(1).{ ... }
place(2).{ ... }
edible(enlarging)
edible(tapering)
poisonous(enlarging)
poisonous(tapering)

comp.{graphics, ibm, mac }
comp.{graphics, os, mac }
comp.{graphics, os, ibm }
comp.{os, ibm, mac }
rec.{sport.baseball, sport.hokey }
rec.{autos, sport.hokey }
rec.{autos, sport.baseball }
sci.{med, space }
sci.{crypt, space }
sci.{crypt, med }
talk.politics.{guns, mideast }
talk.{politics.mideast, religion }
talk.{politics.guns, religion }
orgs(2).{ ... }
orgs(1).{ ... }
people(2).{ ... }
people(1).{ ... }
place(2).{ ... }
place(1).{ ... }
edible(tapering)
edible(enlarging)
poisonous(tapering)
poisonous(enlarging)

In the above operations, we generate the target classes, i.e., a(1) and a(2), from the same
top category (A) for the target task and the source task, respectively, which guarantees that
the two tasks are related. Otherwise, transfer learning may not improve, or may even hurt,
the performance of the target task, which can be referred to as negative transfer [50].

For the mushroom dataset, since it does not have hierarchy, we follow the same routine
in [29] to split the dataset based on the feature ”stalk-shape”. The mushroom dataset has two
categories: “edible” and ”poisonous”, and the feature ”stalk-shape” have two optional values:
”enlarging” and ”tapering”. As in Dai et al. [29], we generate four sub-datasets, as shown
in Table 1. For the MR.edible(1) sub-dataset, ”edible(enlarging)” in the target task column
and “edible(tapering)” in the source task column represent that for all the examples in the
”edible” category, those whose values in the “stalk-shape” feature are equal to ”enlarging”
are considered as the target task, and those equivalent to ”tapering” are regarded as the source
task. The other sub-datasets have similar meanings.

6.2.2 Uncertain information generation

The above datasets are deterministic, and we need to model and involve uncertainty to these
datasets. We follow the same operations in [51] to generate uncertain data, as follows.

123

A robust one-class transfer learning method with uncertain data

Fig. 2 Illustration of adding
noises to the data example x. x is
the original example. v is the
added noises. xv is the new
example with added noises. By
adding the noise vector v, the
new example xv has a certain
deviation from the original
example x

vx

v

x

i

1

, σ xt j
d

To include uncertain data in the training set, we ﬁrst compute the standard deviation σ 0

In the experiments, the RBF kernel K (xti , xh j ) = exp(−(cid:3) xti − xh j (cid:3)2

i of
the entire data along the ith dimension. In order to model the difference in noises on different
dimensions, we deﬁne the standard deviation σi along the ith dimension which is randomly
selected from the range [0, 2σ 0
]. Then, for the ith dimension, we add noises from a random
distribution with standard deviation σi . By doing this, an example xt j is added with noises,
which can be represented as a vector σ xt j = [σ xt j
]. Here, d denotes
, σ xt j
, . . . , σ xt j
d−1
, i = 1, . . . d represents the noises
2
the number of dimensions for a data example xt j , and σ xt j
i
added into the ith dimension of the data example. Figure 2 illustrates the basic idea of this
method. In this ﬁgure, x is the original example. v is the added noise. xv is the new example.
By adding the noise v, the new example xv has some deviations from the original example x.
/2τ 2)) is used.
−10 to 210. In our approach,
The parameter τ in the RBF kernel function is selected from 2
C1, C2, . . . , CK−1 are regularized parameters associated with the K −1 source tasks, and CK
is with the target task. By adjusting the values of these parameters, we can make a tradeoff
between the source tasks and the target task. If the regularized parameter of the target task
CK is larger than those of the source tasks Ci (i = 1, . . . , K − 1), it prefers the target
task to the source tasks. Otherwise, it prefers the source tasks to the target task. In transfer
learning setting, the target task attracts more attentions than the source tasks, and we let
CK > Ci (i = 1, . . . , K − 1). Moreover, for simplicity, we set C1 = ··· = CK−1 and let it
selected from 0 to 1,000. Likewise, CK is picked up from 0 to 1,000. For the bound score
δti of example xti , we compute it from the k-nearest neighbors of xti and set k equal to ten
percentages of the training target examples. For parameter ε, we set it to be 0.1.

2

6.2.3 Performance comparison

For each sub-dataset in Table 1, we form the training set by randomly selecting ten percentages
of the target examples from the target task and all the examples from the source tasks. This is
because transfer learning usually assumes that there are insufﬁcient training examples from
the target task to learn the classiﬁer. The remaining target examples and nontarget examples
from the target task are used as the testing set. To avoid sampling bias, we repeat the above
process ten times, and report the average F-measure values on the testing sets, as shown
in Tables 2 and 3. Here, the noise percentage is set to be 40 %, i.e., 40 % examples being
selected to add the noise.

123

Table 2 F-measure values for transfer learning problems with a single source task

Dataset

UOCT-SVM
87.51± 2.87
85.68± 2.14
82.85± 4.24
85.49± 3.32
79.63± 4.56
84.39± 2.54
82.84± 4.23
75.72± 3.46
70.74± 3.82
81.36± 2.79
The highest F-measure values are in bold

MR.edible(1)
MR.edible(2)
MR.poisonous(1)
MR.poisonous(2)
RT.orgs(1)
RT.orgs(2)
RT.people(1)
RT.people(2)
RT.place(1)
RT.place(2)

UOC-SVM
84.36± 3.28
81.39± 2.35
80.14± 4.52
80.76± 3.16
76.26± 4.95
78.66± 2.73
80.53± 3.58
71.05± 3.29
63.51± 4.39
76.12± 2.86

TLOC-SVM
84.09± 3.48
82.55± 2.16
80.62± 3.82
81.84± 3.65
74.01± 4.63
80.83± 2.77
76.98± 4.45
72.15± 3.52
66.78± 4.46
75.82± 3.24

Table 3 F-measure values for transfer learning problems with multiple source tasks

Dataset

NG.os
NG.ibm
NG.mac
NG.graphics
NG.hokey
NG.baseball
NG.autos
NG.crypt
NG.med
NG.space
NG.religion
NG.guns
NG.mideast

UOCT-SVM
76.63± 2.86
80.26± 3.44
79.58± 3.18
83.57± 3.87
84.81± 3.24
78.79± 4.34
71.34± 2.91
80.76± 3.18
78.74± 4.31
81.37± 2.39
79.06± 3.76
75.25± 3.81
81.31± 2.13

The highest F-measure values are in bold

UOC-SVM
72.16± 3.27
77.81± 3.19
75.77± 3.58
78.26± 4.24
79.58± 3.59
76.05± 4.71
72.96± 2.63
75.89± 3.35
73.59± 4.48
76.78± 2.92
74.72± 4.39
77.41± 3.35
78.94± 2.62

TLOC-SVM
70.95± 3.34
76.75± 3.51
75.43± 3.23
77.31± 4.54
81.19± 3.35
75.93± 4.62
68.32± 3.15
78.18± 3.42
76.28± 4.38
76.24± 2.69
75.33± 4.05
71.88± 4.28
77.97± 2.39

X. Yanshan et al.

OC-SVM
81.97± 3.65
78.11± 2.79
78.19± 4.64
78.22± 3.86
70.35± 5.17
73.92± 3.08
74.17± 4.75
68.08± 4.29
61.88± 4.67
71.73± 3.58

OC-SVM
67.57± 3.69
74.49± 3.95
72.21± 3.86
73.31± 4.65
76.65± 3.72
71.97± 4.46
69.35± 3.56
73.86± 4.03
69.88± 4.84
71.76± 3.34
72.23± 4.58
73.79± 4.51
75.92± 2.94

Tables 2 and 3 show the average F-measure values for transfer learning problems with a
single source task and multiple source tasks, respectively. It is observed that our proposed
approach UOCT-SVM delivers explicitly better classiﬁcation accuracy than UOC-SVM.
Although both of UOCT-SVM and UOC-SVM can deal with data uncertainty, UOCT-SVM
is capable of transferring knowledge from the source task to the target task such that a more
accurate classiﬁer can be built for the target task even when insufﬁcient training examples
from the target task are available. Moreover, UOCT-SVM outperforms TLOC-SVM and OC-
SVM. UOCT-SVM is able to reduce the effect of noises on the decision boundary. As a result,
the classiﬁer learnt by UOCT-SVM can be more robust to noises and obtains better classiﬁ-
cation accuracy than TLOC-SVM and OC-SVM. In addition, it is observed from Table 3 that
the classiﬁcation accuracy of UOCT-SVM is lower than UOC-SVM on the NG.autos and
NG.guns sub-datasets. This may be because the source tasks are not explicitly related to the
target task. As pointed out by [50], transfer learning does not always improve the accuracy;

123

A robust one-class transfer learning method with uncertain data

when the source tasks are irrelevant to the target task, it may lower the performance, which
is called negative transfer.

6.2.4 Performance on different noise levels

We investigate the sensitivity of UOCT-SVM, UOC-SVM, TLOC-SVM and OC-SVM to data
noise. Figure 3 illustrates the variation of F-measure values when the percentage of noises
increases from 20 to 100 % on part of the sub-datasets. The x-axis stands for the percentage
of noises added to the training data. The y-axis represents the average F-measure values. It is
seen that the F-measure values decrease with the increasing of noise percentages. This may
be due to the fact that when the percentage of noise increases, the target class potentially
becomes less distinguishable from the nontarget class. However, it is clearly to see that
UOCT-SVM can deliver consistently higher F-measure values than OC-SVM and UOC-
SVM with different percentages of noises. Compared to UOC-SVM, UOCT-SVM has the
lower decrease of F-measure values by transferring knowledge from the source tasks to the
target task. In contrast with TLOC-SVM and OC-SVM, UOCT-SVM still attains markedly
better F-measure values when the percentage of noises increases from 20 to 100 %, which
implies that UOCT-SVM is effective to reduce the effect of noises. The other sub-datasets
have similar observations.

6.2.5 Running time analysis

So far, we have investigated the classiﬁcation accuracy of UOCT-SVM and the baselines. It
is interesting to compare the running time. Figure 4 presents the training time of OC-SVM,
UOC-SVM, TLOC-SVM and UOCT-SVM on the experimental sub-datasets. It is observed
that OC-SVM is the most efﬁcient method. OC-SVM does not deal with data uncertainty and
transfer learning scenarios when training the classiﬁer. As a result, it trains faster, but has
lower classiﬁcation accuracy than UOCT-SVM. TLOC-SVM is the second efﬁcient method
of which the running time is slightly lower than OC-SVM. UOC-SVM is the third efﬁcient
method and UOCT-SVM is slower than UOC-SVM. UOCT-SVM redeﬁnes the training
examples as z(xti , t ), which has a larger number of dimensions than the original example
xti and takes up more computational time. However, the F-measure value of UOCT-SVM
is explicitly higher than UOC-SVM on most of the experimental sub-datasets, as shown in
Tables 2 and 3. For example, on the RT.orgs(2) sub-dataset, the F-measure value of UOCT-
SVM is 84.39 %, which is higher than UOC-SVM (78.66 %) at 5.73 %.

6.2.6 Sensitivity to regularization parameters

In problem (7), C1 and C2 are regularization parameters associated with the source task
and the target task, respectively. They control the discrepancies between the global optimal
boundary w and the local optimal boundaries w+ v1 (for the source task) and w+ v2 (for the
target task). When a relative lager value of C1 than C2 is set, the global optimal solution w
biases toward the source task, and vice versa. If we let C1 (cid:9) C2, e.g., C1
> 1,000, the value
of v1 approaches to zero and problem (7) degrades to a standard SVM. The global optimal
boundary approximates the classiﬁcation boundary of the source task. If we let C1 (cid:10) C2,
the global optimal boundary approaches to the boundary of the target task.

C2

Furthermore, we take the MR.edible(1) sub-dataset as an example and investigate the
performance variation in our approach with different values of C1 and C2. In Fig. 5a, we ﬁx

123

X. Yanshan et al.

e
r
u
s
a
e
m
−
F

e
r
u
s
a
e
m
−
F

e
r
u
s
a
e
m
−
F

e
r
u
s
a
e
m
−
F

85

80

75

70

65

60

55

80

75

70

65

60

55

50

45

85

80

75

70

65

90

85

80

75

70

65

60

UOCT−SVM
UOC−SVM
OCTL−SVM
OC−SVM

20%

60%

80%
40%
Percentage of Noise

RT.orgs(1)

100%

UOCT−SVM
UOC−SVM
OCTL−SVM
OC−SVM

20%

60%

40%
80%
Percentage of Noise

RT.place(1)

100%

UOCT−SVM
UOC−SVM
TLOC−SVM
OC−SVM

20%

60%

40%
80%
Percentage of Noise
MR.poisonous(2)

100%

UOCT−SVM
UOC−SVM
TLOC−SVM
OC−SVM

20%

60%

40%
80%
Percentage of Noise

100%

NG.hokey

e
r
u
s
a
e
m
−
F

e
r
u
s
a
e
m
−
F

e
r
u
s
a
e
m
−
F

e
r
u
s
a
e
m
−
F

90

85

80

75

70

65

60

55

95

90

85

80

75

70

65

85

80

75

70

65

60

55

50

90

85

80

75

70

65

60

UOCT−SVM
UOC−SVM
OCTL−SVM
OC−SVM

20%

60%

40%
80%
Percentage of Noise

RT.people(1)

100%

UOCT−SVM
UOC−SVM
TLOC−SVM
OC−SVM

20%

60%

40%
80%
Percentage of Noise

MR.edible(1)

100%

UOCT−SVM
UOC−SVM
TLOC−SVM
OC−SVM

20%

60%

40%
80%
Percentage of Noise

100%

NG.os

UOCT−SVM
UOC−SVM
TLOC−SVM
OC−SVM

20%

60%

40%
80%
Percentage of Noise

100%

NG.crypt

Fig. 3 Performance of OC-SVM, UOC-SVM and UOCT-SVM at different noise levels

123

A robust one-class transfer learning method with uncertain data

OC-SVM TLOC-SVM UOC-SVM UOCT-SVM

)
s
(
 
e
m

i
t
 

g
n
i
n
i
a
r
T

100
90
80
70
60
50
40
30
20
10
0

)
s
(
 
e
m

i
t
 

g
n
i
n
i
a
r
T

40
35
30
25
20
15
10
5
0

)
s
(
 
e
m

i
t
 

g
n
i
n
i
a
r
T

100

80

60

40

20

0

OC-SVM TLOC-SVM UOC-SVM UOCT-SVM

NG.os

NG.ibm

NG.mac

NG.graphics

Training time(1)

OC-SVM TLOC-SVM UOC-SVM UOCT-SVM

NG.med

NG.space

NG.guns

NG.mideast

)
s
(

e
m

i
t
g
n
i
n
i
a
r
T

)
s
(

e
m

i
t
g
n
i
n
i
a
r
T

50
45
40
35
30
25
20
15
10
5
0

90
80
70
60
50
40
30
20
10
0

NG.autos

NG.baseball

NG.hokey

NG.crypt

Training time(2)

OC-SVM TLOC-SVM UOC-SVM UOCT-SVM

NG.religion

RT.orgs(1)

RT.orgs(2)

RT.people(1)

Training time(3)

Training time(4)

OC-SVM TLOC-SVM UOC-SVM UOCT-SVM

OC-SVM TLOC-SVM UOC-SVM UOCT-SVM

)
s
(

e
m

i
t
g
n
i
n
i
a
r
T

0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0

MR.edible(1)

MR.edible(2) MR.poisonous(1) MR.poisonous(2)

RT.people(2)

RT.place(1)

RT.place(2)

Training time(5)

Training time(6)

Fig. 4 Training time of OC-SVM, UOC-SVM and UOCT-SVM

C2 = 1 and report the F-measure values when C1 increases from 0 to 1,000. It is observed
that the best F-measure value is achieved when C1 is equal to 0.1. As the increase of C1,
there is a relative obvious decrease when C1 is larger than 1 (namely C2). In Fig. 5b, we set
C1 = 0.1 and present the results with varying values of C2. Similarly, when C2 is equivalent
to 1, the highest F-measure value is obtained. When C2 is less than 0.1 (namely C1), a
noticeable decline is observed. This is because, when C2 is smaller than C1, the target task is
considered to be less important and the global optimal solution bias toward the source task.
Therefore, in order to let the global optimal solution bias toward the target task, a larger value
of C2 than C1 is usually set. In the experiments, we empirically ﬁnd that a relative satisfactory
result can be obtained when the value of C2
is around 10. In real-world applications, we need
C1
validation data to ﬁnd the optimal C1 and C2 for different datasets.

6.2.7 Performance comparison on real-world uncertain datasets

In the above sections, we artiﬁcially add the noises to the experimental datasets and evaluate
the sensitivity of our approach to different levels of noises. In the following, we will test our

123

X. Yanshan et al.

(a) Results with varying C1 (C2=1)

(b) Results with varying C2 (C1=0.1)

Fig. 5 Performance on different values of the regularization parameters C1 and C2

method on two real-world uncertain datasets: Isolet dataset5 for speech classiﬁcation and
Localization Data for Person Activity (LDPA) dataset6 for sensor-based abnormal activity
prediction.

The ﬁrst one is the Isolet dataset, which is a speech utterance classiﬁcation dataset and
naturally contains noisy information since the utterance data is collected from 150 different
speakers and they vary greatly in the way of utterances. The utterance for the same content
may be different. The speakers utter the characters in the English alphabet and the Isolet
dataset contains 7,797 examples in total. Each example contains 617 features extracted from
the utterance data, and the exact feature description can be found in [52]. The task is to
classify which English alphabet is uttered.

In the experiments, we ﬁrst select out the examples related to English alphabets “S”,
“X”, “M” and “N”, respectively, and form four sub-datasets SS, SX , SM and SN . Second,
SS, SX , SM and SN are divided into two groups: {SS, SX } and {SM , SN }. Third, we form
four one-class transfer learning problems, i.e., Isolet(S), Isolet(X), Isolet(M) and Isolet(N),
by treating one sub-dataset as the target task and the other sub-dataset as the source task in
turn for each group. For the Isolet(S) sub-dataset, we choose ten percentages of the examples
from SS as the target task and all the examples from SX as the source task to form the training
set. In the testing set, the remaining examples from SS are considered as the target class, and
three hundred examples randomly selected from the Isolet dataset, expect for those related
to “S” and “X”, are treated as the nontarget class. This process is repeated ten times, and
the average F-measure values are reported. The task of Isolet(S) is to predict whether a test
utterance is the English alphabet “S”. Isolet(S) is a one-class transfer learning problem by
considers SX as the source task and SS as the target task. On one hand, SX and SS are different
because they are related to two distinctive English alphabets “X” and S, respectively. On the
other hand, SX and SS are related to each other since the utterances of English alphabets
“X” and S are similar to some extent. Hence, we can build the one-class classiﬁer on SS
by transferring the knowledge from SX . Likewise, Isolet(X) treats SX as the target task and
SS as the source task. Isolet(M) considers SM as the target task and SN as the source task.
Isolet(N) treats SN as the target task and SM as the source task.

The second one is the Localization Data for Person Activity (LDPA) dataset, which is a
sensor-based dataset, aiming at determining the abnormal activities from the normal activi-
ties. It contains recordings of ﬁve people performing eleven activities: “walking”, “falling”,
“laundry”, “lying down”, “on all fours”, “standing up from sitting on the ground”, etc. Fol-
lowing the same operations in [53], the eleven activities are classiﬁed into two classes:

5 Available from http://archive.ics.uci.edu/ml/datasets/ISOLET.
6 Available from http://dis.ijs.si/conﬁdence/dataset.html.

123

A robust one-class transfer learning method with uncertain data

Table 4 F-measure values on the Isolet and LDPA sub-datasets

Dataset

Isolet(S)
Isolet(X)
Isolet(M)
Isolet(N)
LDPA(1)
LDPA(2)
LDPA(3)
LDPA(4)
LDPA(5)

UOCT-SVM
90.06± 2.14
88.82± 3.36
93.26± 1.86
91.32± 2.38
86.98± 3.14
77.84± 3.61
83.11± 2.54
84.46 ± 2.43
79.46 ± 3.17

The highest F-measure values are in bold

UOC-SVM
85.94± 2.51
87.27± 3.49
91.18± 1.97
89.29± 2.56
84.12± 3.42
74.75± 3.58
78.28± 2.58
79.48 ± 2.77
75.72 ± 3.23

TLOC-SVM
87.29± 2.26
85.75± 3.34
89.91± 2.25
88.69± 2.72
83.66± 3.53
73.86± 3.65
79.87± 2.87
81.82 ± 2.51
78.25 ± 3.28

OC-SVM
83.58± 2.47
83.64± 3.97
89.01± 2.62
85.99± 3.18
80.42± 3.76
71.53± 3.95
74.55± 3.07
78.64 ± 3.14
73.27 ± 3.76

We put the normal data and abnormal data related to the ith person into subset S

activities “falling”, “on all fours”, “sitting on the ground” and “standing up from sitting on
the ground” belonging to the nontarget class, and the other activities belonging to the target
class. Each person wears four localization sensors (ankle left, ankle right, belt and chest) and
performs different activities. The localization sensors record the x, y and z coordinates. The
LDPA dataset is relatively large, containing 164,860 examples, and we utilize around one
tenth of the dataset, i.e., 16,486 examples. Each example contains eight features, including
x coordinate of sensors, y coordinate of sensors, z coordinate of sensors, etc.
+
−
i and S
(i = 1, 2, . . . , 5), respectively. Considering that the ﬁve people may not perform the same
i
activity at exactly the same locations, the collected localization data can differ even for the
same activity. Moreover, the ﬁve people may in distinctive heights and body shapes, which
brings further difference to the localization data. Hence, we consider the data related to one
person as a learning task and obtain ﬁve tasks: one person for one task. Then, we treat each
task as the target task in turn, and acquire ﬁve one-class transfer learning problems, denoted
+
as “LDPA(1)”, “LDPA(2)”, “LDPA(3)”, “LDPA(4)” and “LDPA(5)”. In “LDPA(1)”, S
1 is
considered as the target task, and the source task is randomly selected from the remaining four
+
+
subsets S
i and all examples from the
2
source task are used to form the training set. In the testing set, the remaining examples from
−
+
i are treated as the target class, and S
is as the non-target class. “LDPA(2)”, “LDPA(3)”,
S
i
“LDPA(4)” and “LDPA(5)” are formed in a similar way to “LDPA(1)”.

+
5 . Ten percentages of examples from S

+
, S
3

+
4 and S

, S

Table 4 presents the F-measure values of our approach UOCT-SVM and the baselines on
the Isolet and LDPA sub-datasets. After investigating the details in Table 4, it is seen that
UOCT-SVM obtains the best F-measure values on all the sub-datasets. Taking the Isolet(S)
sub-dataset as an example, the F-measure value of UOCT-SVM is 90.06 %, which gains a min-
imum of 2.77 % and up to 6.48 % improvements, relative to UOC-SVM, TLOC-SVM and OC-
SVM. TLOC-SVM and OC-SVM do not take the data uncertainty into account, and the better
performance of UOCT-SVM over TLOC-SVM and OC-SVM indicates that the experimental
real-world datasets may contain uncertain information, and UOCT-SVM is capable of dealing
with data uncertainty to reﬁne the decision boundary and improve the learning accuracy.

7 Conclusions and future work

Most of the existing one-class classiﬁcation methods assume that there are sufﬁcient training
examples, and they do not contain any uncertain information. Nevertheless, these assumptions

123

X. Yanshan et al.

cannot always be met in real-world applications. In this paper, we propose a novel approach,
termed as uncertain one-class transfer learning with support vector machine (UOCT-SVM).
UOCT-SVM explicitly deals with uncertain data by introducing a boundary score for each
example. Then, the uncertain examples, together with their boundary scores, are incorporated
into an one-class transfer learning model. An iterative framework is put forward to solve the
optimization problem such that we can obtain an accurate classiﬁer for the target task by
transferring knowledge from the source task. Extensive experiments has demonstrated the
effectiveness of our approach.

In the future, we plan to exploit more optimization methods to accelerate the training
efﬁciency of UOCT-SVM. In the experiments, we utilize LibSVM [47] to solve the QP
problems (9) and (24), and the time complexity of solving a QP problem with n examples is
about O(n2). Assuming that our algorithm terminates after T iterations, the time complexity
is approximately O(T n2). When the dataset size is large, the time complexity may be high.
As the future work, we would like to speed up our approach by employing more efﬁcient
optimization methods [54–56], such as Pegasos [57] and NORMA [58] where the running
time of solving a QP problem is linearly with the number of nonzero features in each example,
and does not depend directly on the dataset size. They can be utilized to solve the QP problems
(9) and (24) on large-scale datasets.

Acknowledgments This work is supported by Natural Science Foundation of China (61070033, 61203280,
61202270), Guangdong Natural Science Funds for Distinguished Young Scholar (S2013050014133), Natural
Science Foundation of Guangdong province (9251009001000005, S2012040007078), Specialized Research
Fund for the Doctoral Program of Higher Education (20124420120004), Science and Technology Plan Project
of Guangzhou City(12C42111607, 201200000031,2012J5100054), Science and Technology Plan Project of
Panyu District Guangzhou (2012-Z-03-67), Scientiﬁc Research Foundation for the Returned Overseas Chi-
nese Scholars, State Education Ministry, GDUT Overseas Outstanding Doctoral Fund (405120095), US NSF
through Grants IIS-0905215, CNS-1115234, IIS-0914934, DBI-0960443, and OISE-1129076, US Department
of Army through Grant W911NF-12-1-0066, Google Mobile 2014 Program and KAU grant.

8 Appendix

8.1 Proof for Theorem 1
Assume that α1i ≥ 0, α2 j ≥ 0, β1i ≥ 0 and β2 j ≥ 0 are Lagrange multipliers. The Lagrange
function of problem (8) can be given as

|S1|(cid:2)
i=1

ξ1i + C2

|S2|(cid:2)
j=1

ξ2 j

α2 j[ρ2 − ξ2 j − (w0 + v2)T x2 j]

(37)

L = ||w0||2 + C1||v1||2 + C2||v2||2 − ρ1 − ρ2 + C1

+

−

|S1|(cid:2)
i=1
|S1|(cid:2)
i=1

α1i[ρ1 − ξ1i − (w0 + v1)T x1i] +

β1i ξ1i −

|S2|(cid:2)
j=1

β2 j ξ2 j

|S2|(cid:2)
j=1

where it has x1i = x1i + x1i and x2 j = x2 j + x2 j .

123

A robust one-class transfer learning method with uncertain data

By differentiating the Lagrange function (37) with w0, v1, v2, ρ1, ρ2, ξ1i and ξ2 j , respec-

tively, the following equations can be obtained.

α2 j x2 j = 0,

αi1xi1 −

|S2|(cid:2)
j=1
α1i x1i = 0,

= 2w0 −

|S1|(cid:2)
i=1
|S1|(cid:2)
= 2C1v1 −
i=1
|S2|(cid:2)
j=1
α1i = 0,

= 2C2v2 −

= −1 +

α2 j x2 j = 0,

= −1 +

α2 j = 0,

|S1|(cid:2)
i=1
|S2|(cid:2)
j=1

= C1 − α1i − β1i = 0, i = 1, . . . ,|S1|
j = 1, . . . ,|S2|
= C2 − α2 j − β2 j = 0,

∂ L
∂w0

∂ L
∂v1

∂ L
∂v2

∂ L
∂ρ1

∂ L
∂ρ2

∂ L
∂ξ1i
∂ L
∂ξ2 j

⎞
⎠ ,

α2 j x2 j

|S2|(cid:2)
j=1

(38)

(39)

(40)

(41)

(42)

(43)

(44)

(45)

(46)

(47)

(48)

(49)

(50)

(51)

(52)

123

From Eqs. (38)–(44), it is easy to deduce that

α1i x1i +

α1i x1i ,

α2 j x2 j ,

w0 = 1
2

⎛
|S1|(cid:2)
⎝
i=1
|S1|(cid:2)
i=1
|S2|(cid:2)
j=1
α1i = 1,

v1 = 1
2C1

v2 = 1
2C2

|S1|(cid:2)
i=1
|S2|(cid:2)
j=1
C1 = α1i + β1i , i = 1, . . . ,|S1|
j = 1, . . . ,|S2|
C2 = α2 j + β2 j ,

α2 j = 1,

Since it has β1i ≥ 0 and β2 j ≥ 0, from (50) and (51), we can obtain

0 ≤ α1i ≤ C1, i = 1, . . . ,|S1|

0 ≤ α2 j ≤ C2,

j = 1, . . . ,|S2|

X. Yanshan et al.

(53)

By substituting (38)–(53) into the Lagrange function (37), the dual form of problem (8)

can be written as

max − 1
2

|S1|(cid:2)
h=1

|S1|(cid:2)
g=1

α1hxT

1hx1gα1g

4C1

α2 pxT

|S2|(cid:2)
|S1|(cid:2)
1i x2 j α2 j − C1 + 1
α1i xT
j=1
i=1
|S2|(cid:2)
|S2|(cid:2)
− C2 + 1
p=1
k=1
|S1|(cid:2)
i=1
|S2|(cid:2)
j=1

α2 j = 1, 0 ≤ α2 j ≤ C2,

4C2
α1i = 1, 0 ≤ α1i ≤ C1, i = 1, . . . ,|S1|

j = 1, . . . ,|S2|

2 px2k α2k

s.t.

(cid:11)(cid:12)

8.2 Proof for Theorem 2

In Theorem 2, we ﬁx w0, v1, v2, ρ1 and ρ2 to be w0, v1, v2, ρ1 and ρ2, respectively, and
attempt to minimize the value of the objective function (7) by optimizing x1i and x2 j .
ξti since w0, v1, v2, ρ1
From (7), the objective function’s value is determined by
and ρ2 are ﬁxed. Hence, we need to optimize x1i and x2 j to minimize
ξti .
Each training example xti (i = 1, . . . ,|St|, t = 1, 2) is associated with an error term ξti
ξti can be decomposed into subproblems of minimizing

(cid:7)|St|
i=1

(cid:7)|St|
i=1

(cid:7)
2
t=1

2
t=1

(cid:7)

(cid:7)

and the minimization of
each error term ξti :

2
t=1
(cid:20)
ξti = max
= max

(cid:20)

(cid:7)|St|
i=1
(cid:21)
0, ρt − (w0 + vt )T (xti + xti )
0, ρt − (w0 + vt )T xti − (w0 + vt )T xti

(cid:21)

(54)
From Eq. (54), it is seen that we can minimize ξti by maximizing (w0 + vt )T xti .

According to the Cauchy–Schwarz inequality [59], it has

− ||w0 + vt|| · ||xti|| ≤ (w0 + vt )T xti ≤ ||w0 + vt|| · ||xti||

(55)
In Eq. (55) becomes equation if and only if xti = c(w0 + vt ), where c is a constant

number. Since xti is bounded by δti , the optimal value of xti is

w0 + vt
||w0 + vt|| , i = 1, . . . ,|St|,

t = 1, 2.

(56)
(cid:11)(cid:12)

xti = δti

8.3 Proof for Theorem 3

We ﬁx w0, v1, v2, ρ1 and ρ2, and focus on minimizing each ξ = max{0, ρt −
(cid:7)
αt j K (xt j + xt j , x + x) (x ∈
St , t = 1, 2) over x. According to the ﬁrst order Taylor expansion of K (·) in Eq. (21), it is
easy to deduce

αh j K (xh j + xh j , x + x) − 1

(cid:7)|St|
j=1

(cid:7)|Sh|
j=1

2
h=1

2Ct

1
2

123

1
2

αt j K (xt j + xt j , x + x)
|Sh|(cid:2)
j=1

(cid:8)(xh j + xh j , x)

αh j K

2(cid:2)
h=1
= 1
2

A robust one-class transfer learning method with uncertain data
|St|(cid:2)
j=1
2(cid:2)
h=1
|St|(cid:2)
j=1
αt j K (xt j + xt j , x)

αh j K (xh j + xh j , x + x) + 1
2Ct
|Sh|(cid:2)
αh j K (xh j + xh j , x) + xT 1
2
j=1
|St|(cid:2)
αt j K (xt j + xt j , x) + xT 1
2Ct
j=1
|Sh|(cid:2)
|St|(cid:2)
j=1
j=1
⎡
⎣ 1
2

|Sh|(cid:2)
j=1
2(cid:2)
h=1
+ 1
2Ct
2(cid:2)
h=1
+xT

αh j K (xh j + xh j , x) + 1
2Ct

(cid:8)(xh j + xh j , x) + 1
2Ct

= 1
2

(cid:8)(xt j + xt j , x)

αt j K

αh j K

2(cid:2)
h=1

|Sh|(cid:2)
j=1

⎤
(cid:8)(xt j + xt j , x)
⎦ (57)

αt j K

|St|(cid:2)
j=1

Similar to Sect. 8.2, by using the Cauchy–Schwarz inequality, the optimal value of xti

is as follows

xti = δti

uti
||uti|| ,

t = 1, 2

where it has

uti = 1
2

2(cid:2)
h=1

|Sh|(cid:2)
j=1

αh j K

(cid:8)(xh j + xh j , x) + 1
2Ct

|St|(cid:2)
j=1

(cid:8)(xt j + xt j , x).

αt j K

(cid:11)(cid:12)

8.4 Proof for Theorem 4
Let αti ≥ 0 and βti ≥ 0 be Lagrange multipliers. Based on the Lagrange multipliers, the
Lagrange function of problem (29) can be given as

L = ||w||2 − ρT e + K(cid:2)
t=1

|St|(cid:2)
j=1

ξti

Ct

αti (ρT et − ξti − wT z(xti , t )) − K(cid:2)
t=1
Differentiating the Lagrange function (58) with w, ρ, ξti leads to

+ K(cid:2)
t=1

|St|(cid:2)
i=1

αti z(xti , t ) = 0

|St|(cid:2)
i=1
|St|(cid:2)
i=1

= 2w − K(cid:2)
t=1
= −e + K(cid:2)
t=1
= Ct − αti − βti = 0.

αti et = 0,

∂ L
∂w

∂ L
∂ρ

∂ L
∂ξti

|St|(cid:2)
i=1

βti

(58)

(59)

(60)

(61)

123

According to Eqs. (59)–(61), we can obtain
|St|(cid:2)
i=1

αti z(xti , t ),

K(cid:2)
t=1
αti et = e,

w = 1
2
|St|(cid:2)
i=1

K(cid:2)
t=1
0 ≤ αti ≤ Ct .

By substituting (62)–(64) to problem (29), the dual form can be given as

αti z(xti , t )T z(xh j , h)αh j ,

|Sh|(cid:2)
|St|(cid:2)
K(cid:2)
j=1
h=1
i=1
αti et = e,

max − 1
4
K(cid:2)
t=1
0 ≤ αti ≤ Ct , i = 1, . . . ,|St|,

K(cid:2)
t=1
|St|(cid:2)
j=1

s.t.

t = 1, . . . , K .

X. Yanshan et al.

(62)

(63)

(64)

(65)
(cid:11)(cid:12)

8.5 Proof for Theorem 6
We ﬁx wφ and ρ to be wφ and ρ, respectively, and minimize each ξh j = max{0, ρT
h eh −
(wφ )T φ (z(xh j , h))} (xh j ∈ Sh , h = 1, . . . , K ) over xh j . Since ρT
h eh is known, we minimize
ξh j by maximizing (wφ )T φ (z(xh j , h)). Replacing z(xh j , h) with φ (z(xh j , h)) in Eq. (31)
leads to

wφ = 1
2

K(cid:2)
t=1

|St|(cid:2)
i=1

(66)
By employing the ﬁrst order Taylor expansion of K (·) in Eq. (21) and substituting Eq. (66)

αti φ (z(xti , t ))

into (wφ )T φ (z(xh j , h)), it has

αti φ (z(xti , t ))T φ (z(xh j , h))

(wφ )T φ (z(xh j , h))
= 1
2

K(cid:2)
t=1
K(cid:2)
t=1
K(cid:2)
t=1

|St|(cid:2)
i=1
|St|(cid:2)
i=1
|St|(cid:2)
i=1

= 1
2

= 1
2

αti K (z(xti , t ) + z(xti , t ), z(xh j , h) + z(xh j , h))

αti K (z(xti , t ) + z(xti , t ), z(xh j , h))

+ 1
2

z(xh j , h)T

K(cid:2)
t=1

|St|(cid:2)
i=1

(cid:8)(z(xti , t ) + z(xti , t ), z(xh j , h))

αti K

By utilizing the Cauchy–Schwarz inequality, the optimal value of xh j is
j = 1, . . . ,|Sh|, h = 1, . . . , K

z(xh j , h) = δh j

(cid:19)uh j
||(cid:19)uh j|| ,

123

(67)

(68)

A robust one-class transfer learning method with uncertain data

(cid:19)uh j = K(cid:2)
t=1

|St|(cid:2)
i=1

(cid:8) (cid:8)

(cid:9)
z(xti , t ) + z(xti , t ), z(xh j , h)

.

αti K

(cid:11)(cid:12)

where it has

References

1. Schölkopf B, Williamson RC, Smola A, Shawe-Taylor J (1999) Support vector method for novelty detec-

tion. In: Proceedings of neural information processing systems 1999, pp 582–588

2. Manevitz LM, Yousef M (2002) One-class SVMs for document classifﬁcation. J Mach Learn Res 2:139–

154

3. Ma J, Perkins S (2003) Time-series novelty detection using one-class support vector machines. In: Pro-

ceedings of international joint conference on neural networks 2003, pp 1741–1745

4. Li J, Su L, Cheng C (2011) Finding pre-images via evolution strategies. Appl Soft Comput 11(6):4183–

4194

5. Takruri M, Rajasegarar S, Challa S, Leckie C, Palaniswami M (2011) Spatio-temporal modelling-based

drift-aware wireless sensor networks. Wirel Sens Syst 1(2):110–122

6. Münoz-Marí J, Bovolo F, Gomez-Chova L, Bruzzone L, Camp-Valls G (2010) Semisupervised one-
class support vector machines for classiﬁcation of remote sensing data. IEEE Trans Geosci Remote Sens
48(8):3188–3197

7. Yu H, Han J, Chang KCC (2004) Pebl: web page classiﬁcation without negative examples. IEEE Trans

Knowl Data Eng 16(1):70–81

8. Fung GPC, Yu JX, Lu H, Yu PS (2006) Text classiﬁcation without negative examples revisit. IEEE Trans

Knowl Data Eng 18:6–20

9. Liu B, Xiao Y, Cao L, Yu PS (1995) One-class-based uncertain data stream learning. In: Proceedings of

SIAM international conference on data mining 2011, pp 992–1003

10. Pan SJ, Tsand IW, Kwok JT, Yang Q (2011) Domain adaptation via transfer component analysis. IEEE

Trans Neural Netw 22(2):199–210

11. Aggarwal CC, Yu PS (2009) A survey of uncertain data algorithms and applications. IEEE Trans Knowl

Data Eng 21(5):609–623

12. Kriegel HP, Pfeiﬂe M (2005) Hierarchical density based clustering of uncertain data. In: Proceedings of

international conference on data engineering 2005, pp 689–692

13. Ngai W, Kao B, Chui C, Cheng R, Chau M, Yip KY (2006) Efﬁcient clustering of uncertain data. In:

Proceedings of international conference on data mining 2006, pp 436–445

14. Aggarwal CC (2007) On density based transforms for uncertain data mining. In: Proceedings of interna-

tional conference on data engineering 2007, pp 866–875

15. Bi J, Zhang T (2004) Support vector classiﬁcation with input data uncertainty. In: Proceedings of neural

information processing systems, 2004

16. Gao C, Wang J (2010) Direct mining of discriminative patterns for classifying uncertain data. In: Pro-

ceedings of ACM SIGKDD conference on knowledge discovery and data mining 2010, pp 861–870

17. Tsang S, Kao B, Yip KY, Ho WS, Lee SD (2011) Decision trees for uncertain data. IEEE Trans Knowl

Data Eng 23(1):64–78

18. Murthy R, Ikeda R, Widom J (2011) Making aggregation work in uncertain and probabilistic databases.

IEEE Trans Knowl Data Eng 22(8):1261–1273

19. Yuen SM, Tao Y, Xiao X, Pei J, Zhang D (2010) Superseding nearest neighbor search on uncertain spatial

databases. IEEE Trans Knowl Data Eng 22(7):1041–1055

20. Sun L, Cheng R, Cheung DW, Cheng J (2010) Mining uncertain data with probabilistic guarantees. In:
Proceedings of the ACM SIGKDD conference on knowledge discovery and data mining 2010, pp 273–282
21. Dai W, Xue G, Yang Q, Yu Y (2007) Transferring naive bayes classiﬁers for text classiﬁcation. In:

Proceedings of the AAAI conference on artiﬁcial intelligence 2007, pp 540–545

22. Jiang J, Zhai C (2007) Instance weighting for domain adaptation in NLP. In: Proceedings of the association

for computational linguistics 2007, pp 264–271

23. Liao X, Xue Y, Carin L (2005) Logistic regression with an auxiliary data source. In: Proceedings of the

international conference on machine learning 2005, pp 505–512

24. Huang J, Smola A, Gretton A, Borgwardt KM, Schölkopf B (2007) Correcting sample selection bias by

unlabeled data. In: Proceedings of the neural information processing systems 2007, pp 601–608

123

X. Yanshan et al.

25. Zheng VW, Yang Q, Xiang W, Shen D (2008) Transferring localization models over time. In: Proceedings

of the AAAI conference on artiﬁcial intelligence 2008, pp 1421–1426

26. Pan SJ, Shen D, Yang Q, Kwok JT (2008) Transferring localization models across space. In: Proceedings

of the AAAI conference on artiﬁcial Intelligence 2008, pp 1383–1388

27. Raykar VC, Krishnapuram B, Bi J, Dundar M, Rao RB (2008) Bayesian multiple instance learning:
automatic feature selection and inductive transfer. In: Proceedings of the international conference on
machine learning 2008, pp 808–815

28. Pan SJ, Qiang Y (2010) A survey on transfer learning. IEEE Trans Knowl Data Eng 22(10):1345–1359
29. Dai W, Yang Q, Xue G, Yu Y (2007) Boosting for transfer learning. In: Proceedings of the international

conference on machine learning 2007, pp 193–200

30. Raina R, Battle A, Lee H, Packer B, Ng AY (2007) Self-taught learning: transfer learning from unlabeled

data. In: Proceedings of the international conference on machine learning 2007, pp 759–766

31. Dai W, Xue G, Yang Q, Yu Y (2007) Co-clustering based classiﬁcation for out-of-domain documents.
In: Proceedings of the ACM SIGKDD conference on knowledge discovery and data mining 2007,
pp 432–444

32. Ando RK, Zhang T (2005) A high-performance semi-supervised learning method for text chunking. In:

Proceedings of the association for computational linguistics 2005, pp 1–9

33. Lawrence ND, Platt JC (2004) Learning to learn with the informative vector machine. In: Proceedings of

the international conference on machine learning 2004, pp 432–444

34. Schwaighofer A, Tresp V, Yu K (2005) Learning gaussian process kernels via hierarchical bayes. In:

Proceedings of the neural information processing systems 2005, pp 1209–1216

35. Gao J, Fan W, Jiang J, Han J (2008) Knowledge transfer via multiple model local structure mapping.
In: Proceedings of the ACM SIGKDD conference on knowledge discovery and data mining 2008,
pp 283–291

36. Mihalkova L, Huynh T, Mooney RJ (2007) Mapping and revising markov logic networks for transfer

learning. In: Proceedings of the AAAI conference on artiﬁcial intelligence 2007, pp 608–614

37. Mihalkova L, Mooney RJ (2008) Transfer learning by mapping with minimal target data. In: Proceedings

of workshop transfer learning for complex tasks with AAAI, 2008

38. Davis J, Domingos P (2008) Deep transfer via second-order markov logic. In: Proceedings of workshop

transfer learning for complex tasks with AAAI, 2008

39. Bonilla EV, Agakov F, Williams C (2007) Kernel multi-task learning using task-speciﬁc features. In:

Proceedings of the international conference on artiﬁcial intelligence and statistics 2007, pp 43–50

40. Yu K, Tresp V, Schwaighofer A (2005) Learning gaussian processes from multiple tasks. In: Proceedings

of the international conference on machine learning 2005, pp 1012–1019

41. Bakker B, Heskes T (2003) Task clustering and gating for bayesian multitask learning. J Mach Learn Res

4:83–99

42. Huffel SV, Vandewalle J (1991) The total least squares problem: computational aspects and analysis.

Frontiers in applied mathematics. SIAM Press, Philadelphia

43. Vapnik V (1998) Statistical learning theory. Frontiers in applied mathematics. Springer, London
44. Wang F, Zhao B, Zhang CS (2010) Linear time maximum margin clustering. IEEE Trans Neural Netw

21(2):319–332

45. Chen J, Liu X (2014) Transfer learning with one-class data. Pattern Recognit Lett 37(1):32–40
46. Schölkopf B, Herbrich R, Smola AJ, Williamson RC (2001) A generalized representer theorem. In:

Proceedings of the annual conference on learning theory 2001, pp 416–426

47. Chang C-C, Lin C-J (2011) LIBSVM: a library for support vector machines. ACM Trans Intell Syst

Technol 2(3):1–27

48. William J, Shaw M (1986) On the foundation of evaluation. Am Soc Inf Sci 37(5):346–348
49. Tax DMJ, Duin RPW (2004) Support vector data description. Mach Learn 54(1):45–66
50. Cao B, Pan J, Zhang Y, Yeung DY, Yang Q (2010) Adaptive transfer learning. In: Proceedings of the

AAAI conference on artiﬁcial intelligence, 2010

51. Aggarwal CC, Yu PS (2008) A framework for clustering uncertain data streams. In: Proceedings of the

international conference on data engineering 2008, pp 150–159

52. Cole R, Fanty MA (1990) Spoken letter recognition. In: Proceedings of the workshop on speech and

natural language 1990, pp 385–390

53. Yin J, Yang Q, Pan JJ (2008) Sensor-based abnormal human-activity detection. IEEE Trans Knowl Data

Eng 20(8):1082–1090

54. Tsang IW, Kwok JT, Cheung PM (2005) Core vector machines: Fast SVM training on very large data

sets. J Mach Learn Res 6:363–392

55. Dong JX, Devroye L, Suen CY (2005) Core vector machines: fast SVM training algorithm with decom-

position on very large data sets. IEEE Trans Pattern Anal Mach Intell 27(4):603–618

123

A robust one-class transfer learning method with uncertain data

56. Tresp V (2000) A Bayesian committee machine. Neural Comput 12(11):2719–2741
57. Shalev-Shwartz S, Singer Y, Srebro N (2007) Pegasos: primal estimated sub-gradient solver for SVM.

In: Proceedings of the international conference on machine learning 2007, pp 807–814

58. Kivinen J, Smola AJ, Williamson RC (2004) Online learning with kernels. IEEE Trans Signal Process

52(8):1–12

59. Dragomir SS (2003) A survey on cauchy-bunyakovsky-schwarz type discrete inequalities. J Inequal Pure

Appl Math 4(3):1–142

Yanshan Xiao received the Ph.D. degree in computer science from
the Faculty of Engineering and Information Technology, University of
Technology, Sydney, Australia, in 2011. She is with the Faculty of
Computer, Guangdong University of Technology. Her research inter-
ests include multiple-instance learning, support vector machine, data
mining and machine learning.

Bo Liu is with the Faculty of Automation, Guangdong University of
Technology. His research interests include machine learning and data
mining. He has published papers on IEEE Transactions on Neural
Networks, IEEE Transactions on Knowledge and Data Engineering,
Knowledge and Information Systems, International Joint Conferences
on Artiﬁcial Intelligence (IJCAI), IEEE International Conference on
Data Mining (ICDM), SIAM International Conference on Data Mining
(SDM) and ACM International Conference on Information and Knowl-
edge Management (CIKM).

123

X. Yanshan et al.

Philip S. Yu received the B.S. Degree in E.E. from National Taiwan
University, the M.S. and Ph.D. degrees in E.E. from Stanford Univer-
sity, and the MBA degree from New York University. He is a Profes-
sor in the Department of Computer Science at the University of Illinois
at Chicago and also holds the Wexler Chair in Information Technol-
ogy. He spent most of his career at IBM Thomas J. Watson Research
Center and was manager of the Software Tools and Techniques group.
His research interests include data mining, privacy preserving data pub-
lishing, data stream, Internet applications and technologies, and data-
base systems. Dr. Yu has published more than 710 papers in refereed
journals and conferences. He holds or has applied for more than 300
US patents. Dr. Yu is a Fellow of the ACM and the IEEE. He is the
Editor-in-Chief of ACM Transactions on Knowledge Discovery from
Data. He is on the steering committee of the IEEE Conference on Data
Mining and ACM Conference on Information and Knowledge Manage-
ment and was a member of the IEEE Data Engineering steering com-
mittee. He was the Editor-in-Chief of IEEE Transactions on Knowl-
edge and Data Engineering (2001–2004). He had also served as an associate editor of ACM Transactions
on the Internet Technology (2000–2010) and Knowledge and Information Systems (1998–2004). In addition
to serving as program committee member on various conferences, he was the program chair or co-chairs of
the 2009 IEEE Intl. Conf. on Service-Oriented Computing and Applications, the IEEE Workshop of Scal-
able Stream Processing Systems (SSPS07), the IEEE Workshop on Mining Evolving and Streaming Data
(2006), the 2006 joint conferences of the 8th IEEE Conference on E-Commerce Technology (CEC 06) and
the 3rd IEEE Conference on Enterprise Computing, E-Commerce and E-Services (EEE 06), the 11th IEEE
Intl. Conference on Data Engineering, the 6th Paciﬁc Area Conference on Knowledge Discovery and Data
Mining, the 9th ACM SIGMOD Workshop on Research Issues in Data Mining and Knowledge Discovery,
the 2nd IEEE Intl. Workshop on Research Issues on Data Engineering: Transaction and Query Processing,
the PAKDD Workshop on Knowledge Discovery from Advanced Databases, and the 2nd IEEE Intl. Work-
shop on Advanced Issues of E-Commerce and Web-based Information Systems. He served as the general
chair or co-chairs of the 2009 IEEE Intl. Conf. on Data Mining, the 2009 IEEE Intl. Conf. on Data Engineer-
ing, the 2006 ACM Conference on Information and Knowledge Management, the 1998 IEEE Intl. Confer-
ence on Data Engineering, and the 2nd IEEE Intl. Conference on Data Mining. He had received several IBM
honors including 2 IBM Outstanding Innovation Awards, an Outstanding Technical Achievement Award, 2
Research Division Awards and the 94th plateau of Invention Achievement Awards. He was an IBM Master
Inventor. Dr. Yu received a Research Contributions Award from IEEE Intl. Conference on Data Mining in
2003 and also an IEEE Region 1 Award for promoting and perpetuating numerous new electrical engineering
concepts in 1999.

Zhifeng Hao received the B.S. degree from Sun Yat-Sen University,
Guangzhou, China, and the Ph.D. degree in mathematics from Nanjing
University, Nanjing, China, in 1990, and 1995, respectively. He is cur-
rently a Professor with the Faculty of Computer Science, Guangdong
University of Technology and School of Computer Science and Engi-
neering, South China University of Technology, Guangzhou. His cur-
rent research interests include algebra, machine learning, data mining,
and evolutionary algorithms.

123


