Highly Scalable Attribute Selection

for Averaged One-Dependence Estimators

Shenglei Chen1,2, Ana M. Martinez2, and Geoﬀrey I. Webb2

1 College of Information Science,

Nanjing Audit University, Nanjing, China

tristan chen@126.com

2 Faculty of Information Technology,

Monash University, VIC 3800, Australia

anam.martinezf@gmail.com, geoff.webb@monash.edu

Abstract. Averaged One-Dependence Estimators (AODE) is a popu-
lar and eﬀective approach to Bayesian learning. In this paper, a new
attribute selection approach is proposed for AODE. It can search in a
large model space, while it requires only a single extra pass through the
training data, resulting in a computationally eﬃcient two-pass learning
algorithm. The experimental results indicate that the new technique sig-
niﬁcantly reduces AODE’s bias at the cost of a modest increase in train-
ing time. Its low bias and computational eﬃciency make it an attractive
algorithm for learning from big data.

Keywords: Classiﬁcation, Naive Bayes, AODE, Semi-naive Bayes,
Attribute Selection.

1

Introduction

Naive Bayes (NB) [1] is a simple, computationally eﬃcient probabilistic approach
to classiﬁcation learning. It assumes that all attributes are conditionally inde-
pendent of each other given the class. As an improvement to NB, Averaged
One-Dependence Estimators (AODE) [2] relaxes the attribute independence as-
sumption by averaging all models that assume all attributes are conditionally
dependent on the class and one common attribute, known as the super-parent.
This often improves the classiﬁcation performance signiﬁcantly. An extensive
comparative study [3] shows that AODE obtains signiﬁcant lower error rates
than most alternative semi-naive Bayes algorithms with similar computational
complexity. One of the attractive features of AODE is that it has complexity
linear with respect to data quantity, making it a useful approach for big data.

Attribute selection has been demonstrated to be eﬀective at improving the
accuracy of AODE [4,5]. However, the most eﬀective conventional attribute se-
lection techniques have high computational complexity and hence are not feasible
in the context of big data. In this paper we develop an eﬃcient attribute selec-
tion algorithm for AODE that is linear with respect to data quantity, and of
low polynomial complexity in the number of attributes and hence well suited to

V.S. Tseng et al. (Eds.): PAKDD 2014, Part II, LNAI 8444, pp. 86–97, 2014.
c(cid:2) Springer International Publishing Switzerland 2014

Highly Scalable Attribute Selection

87

big data. The empirical results show that this technique obtains lower bias than
AODE, and thus usually achieves lower error on larger data sets, at the cost of
only a modest increase in training time.

2 Background
The classiﬁcation task can be described as follows, given a training sample T
of t classiﬁed objects, we are required to predict the probability P(y | x) that a
new example x = (cid:2)x1, . . . , xa(cid:3) belongs to some class y, where xi is the value of
the attribute xi and y ∈ {c1, . . . , ck}.

In the following sections, we describe AODE for this classiﬁcation task and a

number of its key variants.

2.1 AODE
From the deﬁnition of conditional probability, we have P(y | x) = P(y, x)/P(x) .
(cid:2)k
i=1 P(ci, x) and y ∈ {c1, . . . , ck}, it is reasonable to consider P(x)
As P(x) =
as the normalizing constant and estimate only the joint probability P(y, x) in
the remainder of this paper.

Since the example x does not appear frequently enough in the training data,
we cannot directly derive an accurate estimate of P(y, x) and must extrapolate
this estimate from observations of lower-dimensional probabilities in the data
[6]. Applying the deﬁnition of conditional probabilities again, we have P(y, x) =
P(y)P(x | y) . The ﬁrst term P(y) on the right side can be suﬃciently accurately
estimated from the sample frequencies, if the number of classes, k, is not too
large. For the second term P(x | y), AODE assumes every attribute depends
on the same parent attribute, the super-parent, thus obtains an one-dependence
estimator (ODE), and then averages all eligible ODEs [2]. The joint probability
P(y, x) is estimated as follows,

(cid:2)

i:1≤i≤a∧F(xi)≥m ˆP(y, xi)

(cid:3)a

j=1

ˆP(xj | y, xi)

ˆP(y, x) =

|{i : 1 ≤ i ≤ a ∧ F(xi) ≥ m}|

(1)
where |·| denotes the cardinality of a set, ˆP(·) represents an estimate of P(·),
F(xi) is the frequency of xi and m is the minimum frequency to accept xi as a
super parent. The current research uses m = 1 [7].

,

2.2 Weightily AODE

In the classiﬁcation of AODE, each ODE is treated equally, that is, all eligible
models are averaged and contribute uniformly to the classiﬁcation rule. How-
ever, in many real world applications, attributes do not play the same role in
classiﬁcation. This observation inspires the weightily AODE [8], in which the
joint probability is estimated as,

(cid:2)

ˆP(y, x) =

i:1≤i≤a∧F(xi)≥m Wi ˆP(y, xi)

(cid:2)

j=1
i:1≤i≤a∧F(xi)≥m Wi

(cid:3)a

ˆP(xj | y, xi)

.

(2)

88

S. Chen, A.M. Martinez, and G.I. Webb

In practice, mutual information between the super-parent and the class is often
used as the weight Wi.

2.3 AODE with Subsumption Resolution

One extreme type of inter-dependence between attributes results in a value of
one being a generalization of a value of the other. For example, consider Gender
and P regnant as two attributes, then P regnant = yes implies that Gender =
f emale. Therefore, Gender = f emale is a generalization of P regnant = yes.
Likewise, P regnant = no is a generalization of Gender = male. Where one value
xi is a generalization of another, xj, P (y|xi, xj) = P (y|xj). In consequence
dropping the more general value from any calculations should not harm any
posterior probability estimates, whereas assuming independence between them
may.

Motivated by this observation, Subsumption Resolution (SR) [9] identiﬁes
pairs of attribute values such that one appears to subsume the other and deletes
the generalization. Suppose that the set of indices of the resulting attribute
subset is denoted by R, the joint probability is estimated as,

ˆP(y, x) =

(cid:2)

i:i∈R∧F(xi)≥m ˆP(y, xi)

|{i : i ∈ R ∧ F(xi) ≥ m}|

(cid:3)

j∈R ˆP(xj | y, xi)

.

(3)

2.4 Forward and Backward Attribute Selection in AODE

In order to repair harmful inter-dependencies among highly correlated attributes,
Zheng et al [5] proposed to select an appropriate attribute subset by hill climbing
search. Two diﬀerent search strategies can be used: FSS begins with the empty
attribute set and successively adds attributes [10], while BSE starts with the
complete attribute set and successively removes attributes [11]. Both strategies
greedily select the attribute whose addition or elimination best reduces the leave-
one-out cross validation error on the training set. The process is terminated if
there is no error improvement.

To diﬀerentiate the selection of parent or child, they introduce the use of
a parent (p) and a child (c) set, each of which contains the set of indices of
attributes that can be employed in, respectively, a parent or a child role in
AODE. The joint probability is estimated as,
i:i∈p∧F(xi)≥m ˆP(y, xi)

j∈c ˆP(xj | y, xi)

(cid:2)

(cid:3)

.

(4)

ˆP(y, x) =

|{i : i ∈ p ∧ F(xi) ≥ m}|

As indicated in [5], the performance of BSE is better than FSS, so we focus
on BSE in this paper. Four types of attribute elimination are considered, parent
elimination (PE), child elimination (CE), parent and child elimination (P∧CE),
parent or child elimination (P∨CE) which performs the former three types of
attribute eliminations in each iteration, selecting the option that best reduces
the error.

The last strategy allows ﬂexible selection of parents and children, but comes
at a high cost, since it needs to scan the training data 2a times in the worst case.

Highly Scalable Attribute Selection

89

2.5 AnDE

The last extension to AODE we review here is AnDE [6], which allows children
to depend on not just one super-parent, but a combination of n parents. The
joint probability P(y, x) is estimated as follows,

(cid:2)

s:s∈(

ˆP(y, x) =

n)∧F(xs)≥m
A
|{s : s ∈ (cid:4)A

n

ˆP(y, xs)
(cid:5) ∧ F(xs) ≥ m}|

j=1

(cid:3)a

ˆP(xj | y, xs)

,

(5)

(cid:5)

(cid:4)A
n

indicates the set of all size-n subsets of {1,··· , a} and xs means the

where
set of attribute values indexed by the element in s.

Note that AnDE is in fact a superclass of AODE and NB. That is, AODE is

AnDE with n = 1 (A1DE) and NB is AnDE with n = 0 (A0DE).

3 Our Proposal: Attribute Selective AODE

Previous work on attribute selection for AODE through BSE and FSS [4,5] has
demonstrated attribute selection did succeed in reducing the harmful inﬂuence
of inter-dependencies among attributes. This success may be attributed to their
ability to search in a large model space. For P∨CE, the search space is of size
2a+1, as it includes all subsets of attributes in parent role coupled with all subsets
of attributes in child role.1
Nevertheless, this is achieved at a high computational overhead. The strategy
of P∨CE needs to scan the training data 2a times, as each time either one child or
one parent can be deleted. This is impractical for data sets with a large number
of attributes.

In order to explore a large space of models in a single additional pass through
the data, we propose a new attribute selection approach for AODE. Our proposal
is based on the observation that it is possible to nest a large space of alternative
models such that each is a trivial extension to another. Let p and c be the set of
indices of parent and child attributes, respectively. For every attribute xi, the
AODE models that use attributes in p as parents and attributes in c ∪ {i} as
children are minor extensions of a model that uses attributes in p as parents
and attributes in c as children. The same is true of models that use attributes in
p ∪ {i} as parents and attributes in c as children. Importantly, multiple models
that build upon one another in this way can be eﬃciently evaluated in a single
set of computations. Using this observation, we create a space of models that
are nested together, and then select the best model using leave-one-out cross
validation in single extra pass through the training data.

Step by step information of the algorithm is provided in the following sections.

1 Note that although the search space is of size 2a+1, the actual number of models
evaluated is O(a2), which is much smaller.

90

S. Chen, A.M. Martinez, and G.I. Webb

3.1 Ranking the Attributes

Our method for nesting models depends on a ranking of the attributes. Models
containing lower ranked attributes will be built upon models containing higher
ranked attributes. The mutual information between an attribute and the class
measures how informative this attribute is about the class [12], and thus it is a
suitable metric to rank the attributes.

The advantage of using mutual information is that it can be computed very
eﬃciently after one pass through the training data. Although the mutual infor-
mation between an attribute and the class can help to identify the attributes
that are individually most discriminative, it is important to note that it does not
directly assess the discriminative power of an attribute in combination with other
attributes. Nevertheless, the ranking of attributes based on mutual information
with the class will permit the search over a large space of possible models and
the deﬁciencies of this discriminative approach will be mitigated by the richness
of the search space that is evaluated in a discriminative fashion.

3.2 Building the Model Space

Without loss of generality, in the following we assume that the attributes are
ordered by mutual information. That is, xi represents the attribute with the ith
greatest mutual information with the class. As the attributes have been ranked,
we can create, in total, a2 nested submodels of attribute subsets. To be more
speciﬁc, suppose we select top r attributes as parents and top s attributes as
children, where 1 ≤ r, s ≤ a, the candidate AODE model would be,
ˆP(xj | y, xi)

i:1≤i≤r∧F(xi)≥m ˆP(y, xi)

(cid:3)s

(cid:2)

.

(6)

ˆP(y, x)r,s =

|{i : 1 ≤ i ≤ r ∧ F(xi) ≥ m}|

j=1

Figure 1 gives an example of the model space with 3 attributes. For instance,

model m21 considers the two attributes {x1, x2} as parents and a single attribute
{x1} as a child. Then, when the attribute x2 is considered to be added as a child,
we obtain a new model m22. When instead the attribute x3 is considered to be
added as a parent, we obtain a new model m31. Both of these models are minor
extensions to the existing model m21 and all three (and all their extensions) can
be applied to a test instance in a single nested computation. Consequently all
models can be eﬃciently evaluated in a single set of nested computations.

children

{x1} {x1, x2} {x1, x2, x3}
{x1}
m11 m12
{x1, x2} m21 m22
{x1, x2, x3} m31 m32

m13
m23
m33

s
t
n
e
r
a
p

Fig. 1. An example of the model space with 3 attributes

Highly Scalable Attribute Selection

91

3.3 Selecting the Best Model

Once we have built the model space, we can perform model selection within this
space. To evaluate the goodness of an alternative model, an evaluation function
is required, which commonly measures the discriminative ability of the model
among classes.

We use leave-one-out cross validation error to measure the performance of each
model. Rather than building a new model for every fold, we use incremental cross
validation [13], in which the contribution of the training example being left out
in each fold is simply subtracted from the count table, thus producing a model
without that training example. This method allows the model to be evaluated
quickly, whilst obtaining a good estimate of the generalization error.

There are several loss functions to measure model performance for leave-one-
out cross validation, zero-one loss and root mean squared error (RMSE) are
among the most common and eﬀective. Zero-one loss simply assigns a loss of ‘0’
to correct classiﬁcation, and ‘1’ to incorrect classiﬁcation, treating all misclassi-
ﬁcations as equally undesirable. RMSE, however, accumulates for each example
the squared error, which is the probability of incorrectly classifying the example,
and then computes the root mean of the sum. As RMSE gives a ﬁner grained
measure of the calibration of the probability estimates compared to zero-one loss,
with the error depending not just on which class is predicted, but also on the
probabilities estimated for each class, we use RMSE to evaluate the candidate
models in this research.

3.4 Algorithm and Analysis

Based on the methodology presented above, we develop the training algorithm
for attribute selective AODE shown in Algorithm 1.

Algorithm 1. Training algorithm for attribute selective AODE
1: Form the table of joint frequencies of pairwise attribute-values and class
2: Compute the mutual information
3: Rank the attributes
4: for all example in T do
5:
6:
7:
8: end for
9: Compute the root mean squared error for each model
10: Select the model with the lowest RMSE

Build all a2 models while leaving the current example out
Predict the current example using a2 models
Accumulate the squared error for each model

As in AODE, we need to form the table of joint frequencies of pairs of
attribute-values and class from which the probability estimates ˆP(y, xi), ˆP(xj |
y, xi) and the mutual information between the attributes and class are derived.

92

S. Chen, A.M. Martinez, and G.I. Webb

This is done in one pass through the training data (line 1). Note that this pro-
vides all of the information needed to create any selective AODE model with
any sets of parent and child attributes.

In the second pass through the training data (line 4-8), the squared error is
accumulated for each model. After this pass, the RMSE will be computed and
used to select the best model.
At training time, the space complexity of the table of joint frequencies of
attribute-values and class is O(k(av)2) as in AODE, where v is the average
number of values per attribute. Attribute selection will not require more memory.
Derivation of the frequencies required to populate this table is of time complexity
O(ta2). Attribute selection needs one more pass through the training data, the
time complexity of which is O(tka2), since for each example we need to compute
the joint probability in (1) for each class. So the overall time complexity is
O(t(k + 1)a2).
Classiﬁcation requires the table of probability estimates formed at training
time of space complexity O(k(av)2). The time complexity of classifying a single
example is O(ka2) in the worst-case scenario, because some attributes may be
omitted after attribute selection.

4 Empirical Comparisons

In this section, we compare the newly proposed attribute selective AODE (AS-
AODE) with AODE, weightily AODE (WAODE), AODE with subsumption
resolution (AODESR), BSE selective AODE (BSEAODE) and A2DE.

Zheng et al [9] discussed three diﬀerent subsumption resolution techniques,
Lazy SR, Eager SR and Near SR. Lazy SR is used in this paper, as it can improve
AODE with low training time and modest test time overheads. The minimum
frequency for identifying generalizations is set to 100. The results in [5] show that
BSE performs better than FSS, and the elimination of a child is more eﬀective
than the elimination of a parent. So we select only the children in BSEAODE.
However, we do not perform statistical tests in BSEAODE, as we do not do this
in ASAODE, either. We also include A2DE in the set of experiments so as to
provide a comprehensive comparison.

The experimental system is implemented in C++. In order to deal with nu-
merical data, Minimum Description Length (MDL) discretization [14] is imple-
mented. More speciﬁcally, the cut points are computed on 100,000 examples
randomly selected from training data or on all training examples if the train-
ing data is less than 100,000. These cut points are then used to discretize the
training and test data. The base probabilities are estimated using m-estimation
(m = 1) [15]. Missing values have been considered as a distinct value.

We run the above algorithms on 71 data sets from the UCI repository [16].
Table 1 presents the detailed characteristics of data sets in ascending order on
the number of instances. We run the experiments on a single CPU single core
virtual Linux machine running on a Sun grid node with dual 6 core Intel Xeon
L5640 processors running at 2.27 GHz with 96 GB RAM.

Highly Scalable Attribute Selection

93

Table 1. Data sets

No. Name

1 contact-lenses
2 lung-cancer
3 labor-negotiations
4 post-operative
5 zoo
6 promoters
7 echocardiogram 131
8 lymphography
9 iris

24
4
32 56
57 16
8
90
101 16
106 57
6
148 18
4
150
151
5
10 teaching-ae
155 19
11 hepatitis
178 13
12 wine
205 25
13 autos
208 60
14 sonar
9
214
15 glass-id
215
5
16 new-thyroid
226 69
17 audio
294 13
18 hungarian
303 13
19 heart-disease-c
306
3
20 haberman
339 17
21 primary-tumor
351 34
22 ionosphere
366 34
23 dermatology
368 21
24 horse-colic
435 16
25 house-votes-84
540 39
26 cylinder-bands
551 39
27 chess
600 60
28 syncon
625
4
29 balance-scale
683 35
30 soybean
690 15
31 credit-a
9
32 breast-cancer-w
699
8
33 pima-ind-diabetes 768
846 18
34 vehicle
898 38
35 anneal
36 tic-tac-toe
958
9

Inst Att Class
3
3
2
3
7
2
2
4
3
3
2
3
7
2
3
3
24
2
2
2
22
2
6
2
2
2
2
6
3
19
2
2
2
4
6
2

No. Name
37 vowel
38 german
39 led
40 contraceptive-mc
41 yeast
42 volcanoes
43 car
44 segment
45 hypothyroid
46 splice-c4.5
47 kr-vs-kp
48 abalone
49 spambase
50 phoneme
51 wall-following
52 page-blocks
53 optdigits
54 satellite
55 musk2
56 mushrooms
57 thyroid
58 pendigits
59 sign
60 nursery
61 magic
62 letter-recog
63 adult
64 shuttle
65 connect-4
66 ipums.la.99
67 waveform
68 localization
69 census-income
70 poker-hand
71 record-linkage

Inst Att Class
11
990 13
2
1000 20
10
7
1000
3
9
1473
1484
8
10
4
3
1520
4
1728
6
7
2310 19
2
3163 25
3
3177 60
3196 36
2
3
4177
8
2
4601 57
50
5438
7
4
5456 24
5
5473 10
5620 64
10
6
6435 36
2
6598 166
2
8124 22
20
9169 29
10
10992 16
12546
8
3
5
12960
8
2
19020 10
26
20000 16
2
48842 14
7
58000
9
3
67557 42
88443 60
19
3
100000 21
11
164860
5
2
299285 41
10
1025010 10
5749132 11
2

4.1 Bias, Variance and RMSE

Because ASAODE explores a larger space of models than AODE and BSEAODE
explores a larger space of models than ASAODE, we expect BSEAODE to have
the lowest bias, followed by ASAODE then AODE and this order to be reversed
for their relative variance. Hence we expect AODE to deliver the lowest error on
smaller datasets, ASAODE to dominate at some intermediate data size, and for
BSEAODE to deliver the lowest error on very large data. The bias and variance
of ASAODE relative to WAODE, AODESR and A2DE can be expected to vary
from dataset to dataset as these all embody diﬀerent learning biases and none
of their spaces of models subsumes the other.

In order to assess these expectations, we ﬁrst perform bias variance decompo-
sition using the experimental method proposed by Kohavi and Wolpert [17]. As
this study is more meaningful with more data, we run these experiments only
on the largest 28 data sets which have at least 2000 examples. For each data set,
1000 training examples and 1000 test examples are randomly selected. The bias
variance decomposition is calculated from the error on the test examples. This
process is repeated 10 times to obtain the mean bias and variance.

94

S. Chen, A.M. Martinez, and G.I. Webb

A summary of pairwise win/draw/loss records, which indicate the number of
data sets on which one algorithm has lower, equal or higher outcome relative
to the other, is presented in Table 2. Each entry in cell [i, j] compares the al-
gorithm in row i against the algorithm in column j. The p value following each
win/draw/loss record is the outcome of a binomial sign test and represents the
probability of observing the given number of wins and losses if each were equally
likely. The reported p value is the result of a two-tailed test. We consider a
diﬀerence to be signiﬁcant if p ≤ 0.05. All such p values have been changed to
boldface in the table.

Table 2 shows that all ﬁve variants to AODE achieve signiﬁcant reductions in
bias relative to AODE. While ASAODE achieves lower bias than WAODE and
AODESR more often than not, the reverse is true for BSEAODE and A2DE;
although these diﬀerences are not signiﬁcant.

Next, we conduct 10-fold cross validation experiments to obtain the error of
the alternative algorithms. As attribute selection is based on the RMSE metric,
we are inclined to evaluate the error by RMSE. The win/draw/loss records of
alternative algorithms for RMSE on 71 data sets are also presented in Table 2.
We can see that all ﬁve improvements to AODE have achieved signiﬁcant
reductions in RMSE relative to AODE. ASAODE has also achieved signif-
icant reductions in RMSE relative to WAODE and AODESR. The p value
(0.807)indicates that ASAODE and BSEAODE have achieved almost the same
performance. But the advantages of BSEAODE over WAODE and AODESR
are not as signiﬁcant as those of ASAODE over WAODE and AODESR. While
A2DE achieves signiﬁcant reductions in RMSE relative to AODE, WAODE,
AODESR and BSEAODE, its advantage over ASAODE is not signiﬁcant.

The fact that ASAODE obtains, in general, lower bias and higher variance
compared with WAODE and AODESR, indicates that it will perform better
on larger datasets, since it will be able to capture more complex relationships
from large amount of data [18]. In order to demonstrate this hypothesis, we also
compile the win/draw/loss results in terms of RMSE on the 43 smallest data
sets and the 28 largest data sets in Table 2. We can see that the performance
of ASAODE is better on large data sets than on small data sets. While for
even larger data sets BSEAODE and A2DE might outperform ASAODE for the
same reason, both have high computational complexity that can be prohibitive
for large data, since BSEAODE requires 2a pases on the whole training set
and A2DE’s memory requirements and classiﬁcation time are very high (see the
following Section 4.2).

4.2 Computation Time

The logarithmic means of training and classiﬁcation time on the 71 data sets for
all algorithms are shown in Fig. 2. We have added 1 to each mean before comput-
ing the logarithm to avoid negative bars. ASAODE requires more training time
than such one pass algorithms as AODE, WAODE and AODESR. This is because
ASAODE involves two passes through the training data. As BSEAODE needs
at most 2a passes, it requires signiﬁcantly more training time than ASAODE.

Highly Scalable Attribute Selection

95

Table 2. Win/draw/loss records of bias, variance and RMSE with binomial sign test

AODE

WAODE

AODESR

BSEAODE

ASAODE

p W/D/L

W/D/L
0.002
WAODE
21/2/5
0.041 12/1/15 0.701
AODESR 15/8/5
0.003 17/4/7
BSEAODE 19/5/4
0.064
ASAODE 21/3/4 <0.001 18/1/9
0.122
23/2/3 <0.001 21/1/6 0.006

A2DE

p

1

s
a
i
B

1 WAODE 13/1/14
e
c
AODESR 7/8/13
n
a
BSEAODE 11/5/12
i
r
ASAODE 9/1/18
a
V
14/1/13

A2DE

1

0.263 13/0/15 0.851
10/0/18 0.185
0.122 11/1/16 0.442

1

1

14/0/14

1

W/D/L

p W/D/L

p W/D/L

p

17/2/9
16/1/11
20/3/5

0.169
0.442 11/2/15 0.557
0.004 14/1/13

1

17/1/10 0.248

12/3/13
13/0/15
14/3/11

1

0.851 13/2/13
14/1/13
0.69

1
1

14/0/14

1

2 WAODE 45/5/21 0.004
E
S
M
R

AODESR 32/27/12 0.004 28/6/37 0.321
BSEAODE 40/20/11 <0.001 40/4/27 0.142 35/14/22 0.111
ASAODE 43/6/22 0.013 42/4/25 0.05

42/5/24 0.036 35/4/32 0.807

A2DE

52/4/15 <0.001 47/2/22 0.004 48/3/20 <0.001 42/4/25 0.05 43/2/26 0.053

0.081

3 WAODE 26/3/14
S
E
S
M
R

AODESR 20/19/4 0.002 18/3/22 0.636
BSEAODE 19/14/10 0.136 23/2/18 0.533 16/10/17
18/5/20
ASAODE 19/5/19
23/2/18

19/4/20

A2DE

1

1

1

27/3/13 0.038 24/1/18 0.441
19/2/7
AODESR 12/8/8
BSEAODE 21/6/1 <0.001 17/2/9
ASAODE 24/1/3 <0.001 23/0/5 <0.001 24/0/4 <0.001 15/0/13 0.851

0.029
0.503 10/3/15 0.424
0.169

4 WAODE
L
E
S
M
R

19/4/5

0.007

0.871 20/4/19
0.533 22/3/18 0.636 25/2/16 0.211

1

A2DE

25/1/2 <0.001 23/1/4 <0.001 25/1/2 <0.001 20/1/7 0.019 18/0/10 0.185

1 Bias and variance results on the 28 largest data sets.
2 RMSE results on all the 71 data sets.
3 RMSES: RMSE results on the 43 smallest data sets.
4 RMSEL: RMSE results on the 28 largest data sets.

e
m

i
t
 
n
a
e
m
 
c
m
h

i

t
i
r
a
g
o

l

6

5

4

3

2

1

0

A O D E

W A O D E

A O D ES R

BSEA O D E

ASA O D E

A2D E

e
m

i
t
 
n
a
e
m
 
c
m
h

i

t
i
r
a
g
o

l

3

2.5

2

1.5

1

0.5

0

A O D E

W A O D E

A O D ES R

BSEA O D E

ASA O D E

A2D E

(a) Training time

(b) Classiﬁcation time

Fig. 2. Computation time comparison of diﬀerent algorithms (seconds)

As for the classiﬁcation time, ASAODE, AODESR and BSEAODE require, in
general, less time than AODE and WAODE because they might eliminate some
attributes. Fig. 2 also shows that ASAODE requires even less classiﬁcation time
than AODESR and BSEAODE.

96

S. Chen, A.M. Martinez, and G.I. Webb

A2DE requires more training and classiﬁcation time than AODE, as it needs
to compile a more complicated table at training time and requires more compu-
tation at classiﬁcation time.

5 Conclusion

In this paper, a new attribute selection algorithm is proposed for AODE. It is
a two-pass algorithm, so compared to AODE, it just requires one more pass
through the training data. The alternative attribute selection methods, such as
FSA and BSE, need a number of passes that is linear to the number of attributes
to obtain similar results.

The empirical results show that the new algorithm is signiﬁcantly more accu-
rate than AODE, WAODE and AODESR, has comparable error to BSEAODE,
and as we expected, worse than A2DE. It requires signiﬁcantly less training time
than BSEAODE, and less classiﬁcation time than AODE and all other variants,
especially than A2DE.

It is worthwhile to note that the technique proposed in this paper is of squared
complexity in the number of attributes, so it is not scalable to high dimensional
data. On the other hand, it is compatible with weighting, subsumption resolution
and higher orders of AnDE. Consequently, it might be possible to further improve
the accuracy by combining it with weighting, subsumption resolution and A2DE.
This is a promising direction for future research.

Acknowledgments. This research has been supported by the Australian Re-
search Council under grant DP110101427, Asian Oﬃce of Aerospace Research
and Development, Air Force Oﬃce of Scientiﬁc Research under contract FA2386-
1214030, National Natural Science Foundation of China under grant 71271117,
61202135, Natural Science Foundation of Jiangsu, China under grant BK2011692,
BK2012472, Qinglan Project and Priority Academic Program of Audit Science
and Technology of Jiangsu, China, Jiangsu Government Scholarship for Overseas
Studies, Overseas Studying Scholarship of Nanjing Audit University.

This research has also been supported in part by the Monash e-Research
Center and eSolutions-Research Support Services through the use of the Monash
Campus HPC Cluster and the LIEF grant. This research was also undertaken
on the NCI National Facility in Canberra, Australia, which is supported by the
Australian Commonwealth Government.

References

1. Duda, R.O., Hart, P.E.: Pattern Classiﬁcation and Scene Analysis, 1st edn. John

Wiley & Sons Inc. (1973)

2. Webb, G.I., Boughton, J.R., Wang, Z.: Not so naive Bayes: Aggregating one-

dependence estimators. Machine Learning 58(1), 5–24 (2005)

3. Zheng, F., Webb, G.I.: A comparative study of semi-naive Bayes methods in clas-

siﬁcation learning. In: AusDM, pp. 141–156 (2005)

Highly Scalable Attribute Selection

97

4. Yang, Y., Webb, G.I., Cerquides, J., Korb, K.B., Boughton, J., Ting, K.M.:
To select or to weigh: A comparative study of linear combination schemes for
superparent-one-dependence estimators. IEEE Transactions on Knowledge and
Data Engineering 19(12), 1652–1665 (2007)

5. Zheng, F., Webb, G.I.: Finding the right family: Parent and child selection for
averaged one-dependence estimators. In: Kok, J.N., Koronacki, J., de Lopez Man-
taras, R., Matwin, S., Mladeniˇc, D., Skowron, A. (eds.) ECML 2007. LNCS (LNAI),
vol. 4701, pp. 490–501. Springer, Heidelberg (2007)

6. Webb, G.I., Boughton, J.R., Zheng, F., Ting, K.M., Salem, H.: Learning by extrap-
olation from marginal to full-multivariate probability distributions: Decreasingly
naive Bayesian classiﬁcation. Machine Learning 86(2), 233–272 (2012)

7. Cerquides, J., de M´antaras, R.L.: Robust Bayesian linear classiﬁer ensembles. In:
Gama, J., Camacho, R., Brazdil, P.B., Jorge, A.M., Torgo, L. (eds.) ECML 2005.
LNCS (LNAI), vol. 3720, pp. 72–83. Springer, Heidelberg (2005)

8. Jiang, L., Zhang, H.: Weightily averaged one-dependence estimators. In: Yang, Q.,
Webb, G. (eds.) PRICAI 2006. LNCS (LNAI), vol. 4099, pp. 970–974. Springer,
Heidelberg (2006)

9. Zheng, F., Webb, G.I., Suraweera, P., Zhu, L.: Subsumption resolution: An eﬃcient
and eﬀective technique for semi-naive Bayesian learning. Machine Learning 87(1),
93–125 (2012)

10. Langley, P., Sage, S.: Induction of selective Bayesian classiﬁers. In: Proceedings of
the Tenth International Conference on Uncertainty in Artiﬁcial Intelligence, pp.
399–406. Morgan Kaufmann Publishers Inc. (1994)

11. Kittler, J.: Feature selection and extraction. In: Handbook of Pattern Recognition

and Image Processing, pp. 59–83 (1986)

12. MacKay, D.J.: Information theory, inference and learning algorithms. Cambridge

university press (2003)

13. Kohavi, R.: The power of decision tables. In: Lavraˇc, N., Wrobel, S. (eds.) ECML

1995. LNCS, vol. 912, pp. 174–189. Springer, Heidelberg (1995)

14. Fayyad, U.M., Irani, K.B.: Multi-interval discretization of continuous-valued at-

tributes for classiﬁcation learning. In: IJCAI, pp. 1022–1027 (1993)

15. Cestnik, B.: Estimating probabilities: A crucial task in machine learning. In: ECAI,

vol. 90, pp. 147–149 (1990)

16. Bache, K., Lichman, M.: UCI machine learning repository (2013)
17. Kohavi, R., Wolpert, D.H.: Bias plus variance decomposition for zero-one loss func-

tions. In: ICML, pp. 275–283 (1996)

18. Brain, D., Webb, G.I.: The need for low bias algorithms in classiﬁcation learning
from large data sets. In: Elomaa, T., Mannila, H., Toivonen, H. (eds.) PKDD 2002.
LNCS (LNAI), vol. 2431, pp. 62–73. Springer, Heidelberg (2002)


