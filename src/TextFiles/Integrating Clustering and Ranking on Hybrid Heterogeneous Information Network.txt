 

Integrating Clustering and Ranking on Hybrid 

Heterogeneous Information Network 

Ran Wang1, Chuan Shi1, Philip S. Yu2,3, and Bin Wu1 

1 Beijing University of Posts and Telecommunications, Beijing, China 

{wangran51,shichuan,wubin}@bupt.edu.cn 

2 University of Illinois at Chicago, IL, USA 

3 King Abdulaziz University Jeddah, Saudi Arabia 

psyu@cs.uic.edu 

Abstract.  Recently,  ranking-based  clustering  on  heterogeneous  information 
network  has  emerged,  which  shows  its advantages on the mutual promotion of 
clustering and ranking. However, these algorithms are restricted to  information 
network  only  containing  heterogeneous  relations.  In  many  applications,  net-
worked data are more complex and they can be represented as a hybrid network 
which simultaneously includes heterogeneous and homogeneous relations. It is 
more promising to promote clustering and ranking performance  by  combining 
the  heterogeneous and homogeneous relations. This paper studied the ranking-
based clustering on  this  kind  of  hybrid network and proposed the ComClus al-
gorithm.  ComClus  applies  star  schema  with  self  loop to  organize  the  hybrid 
network  and uses a probability model to represent the generative probability of 
objects.  Experiments show that ComClus can achieve more accurate clustering 
results and do more reasonable ranking with quick and steady convergence. 

Keywords:  Clustering,  Ranking,  Heterogeneous 
Probability Model. 

Information  Network,   

1 

Introduction 

Information network analysis is an increasingly important direction in data mining in 
the  past  decade.  Many  analytical  techniques  have  been  developed  to  explore  struc-
tures and properties of information networks, among which clustering and ranking are 
two primary tasks. The clustering task [1] partitions  objects into different groups with 
similar  objects  gathered  and  dissimilar  objects  separated.  Spectral  method  [1,4]  is 
widely  used  in  graph  clustering.  The ranking task [6,10,12] evaluates the importance 
of objects based on some ranking function, such as PageRank [12] or MultiRank [10]. 
Clustering and ranking are often regarded as two independent tasks  and they are ap-
plied  separately  to  information  network  analysis.    However,  integrating  clustering 
and ranking makes more sense in many applications [2-3,11]. On one hand, the know-
ledge of important objects in a cluster  helps to  understand this cluster; on the other 
hand,  knowing clusters is benefited to  make  more elaborate ranking. Some prelimi-
nary works have explored this issue [11].   

J. Pei et al. (Eds.): PAKDD 2013, Part I, LNAI 7818, pp. 583–594, 2013. 
© Springer-Verlag Berlin Heidelberg 2013 

584 

R. Wang et al. 

Although it is a promising way to do clustering and ranking together, previous  ap-
proaches  confine  it to a  “pure”  heterogeneous  information  network  which  does  not 
consider  the  homogeneous  relations  among  same-typed  objects.  For  example,   
RankClus  [2]  only  considers  relations  between  two-typed  objects;  NetClus  [3]  just 
considers relations among center type and attribute types. However, in many applica-
tions,  the  networked  data  are  more  complex.  They  include  heterogeneous  relations 
among  different-typed objects  as  well  as homogeneous relations  among  same-typed 
objects. Taking bibliographic data as an example which is shown in Fig. 1(a), papers, 
venues,  authors  and  their  relations  construct  a  heterogeneous  information  network. 
Simultaneously, the network also includes the citation relations among papers and the 
social  network  among  authors.  It  is  important  to  cluster  on  such  a  hybrid  network 
which includes heterogeneous and homogeneous relations at the same time. The hybr-
id  network  can  more  authentically  represent  real  networked  data.  Moreover,  more 
information from heterogeneous and homogeneous relations is promising to promote 
the performance of clustering and ranking.   

Although it is important to integrate clustering and ranking on the  hybrid network, 
it  is  seldom  studied  due  to  the  following  challenges.  1)  It  is  difficult  to  effectively 
organize  networked data. The hybrid network is  more complex than either of them. 
The way to organize the network not only needs to effectively represent objects and 
their relations but also benefits for clustering and ranking analysis. 2) It is not easy to 
integrate  information  from  heterogeneous  and  homogeneous  relations  to  improve 
clustering and ranking performances. It is obvious that more information from differ-
ent sources can  help to obtain better performances. However,  we need to design an 
effective mechanism to make full use of information from these two networks.   

In this paper, we study the ranking based  clustering problem on a  hybrid network 
and propose a novel ComClus algorithm to solve it. A  star  schema  with  self  loop is 
applied to organize the hybrid network. The ComClus employs a probability model to 
represent the generative probability of objects and the experts model and generative 
method are used to effectively combine the information from heterogeneous and ho-
mogeneous  relations.  Moreover, 
the  probability information 
of objects,  we  propose  ComRank  to  identify  the  importance  of  objects  based  on 
ComClus. Experiments on DBLP  show that ComClus achieves better clustering and 
ranking accuracy compared to well-established algorithms.  In  addition, ComClus has 
better stability and quicker convergence. 
 

through  applying 

(a) Bibliographic data 

      (b)Hybrid network                    (c)Star schema with self loop        (d)Clusters on hybrid network 

 

Fig. 1. An example of clustering on bibliographic data 

 

 

Integrating Clustering and Ranking on Hybrid Heterogeneous Information Network 

585 

2 

Problem Formulation  

In this section, we give the problem definition and some important concepts used in 
this paper. 

Definition 1. Information Network. Given (cid:1837)(cid:3397)1   types of nodes, (cid:1848)(cid:3038)  is a vertex set, 
denoted by (cid:1848)(cid:3038)(cid:3404)(cid:4668)(cid:1874)(cid:2868)(cid:3038),(cid:1874)(cid:2869)(cid:3038),(cid:1874)(cid:2870)(cid:3038),…...,(cid:1874)(cid:3041)(cid:3038)(cid:4669), where (cid:1874)(cid:3041)(cid:3038)  represents the (cid:1866)-(cid:1872)(cid:1860)  node belong-
ing to the (cid:1863)-(cid:1872)(cid:1860)  type. An information network can be represented as a weighted net-
.  E  is  a  binary  relation  on (cid:1848),  and (cid:1849)  is  a 
work  G(cid:3404)(cid:3407)(cid:1848),(cid:1831),(cid:1849)(cid:3408),  if (cid:1848)(cid:3404)(cid:1666) (cid:1848)(cid:3038)
weight mapping from an edge e(cid:1488)E to a real number w(cid:1488)(cid:1844)(cid:2878).  If (cid:1837)(cid:3410)2  the informa-
(cid:3012)(cid:3038)(cid:2880)(cid:2868)
tion network when (cid:1837)(cid:3404)1.   

tion network G is heterogeneous information network;  and  homogeneous informa-

(cid:1848)(cid:3038)

  is called star schema  with self loop 

the links set  among the different-typed nodes  (called  hete-link).  Then  the hete-link 

For a network with multiple  types of nodes, K-partite network [7,9] and star schema 
[3]  are  widely  used.  These  network  structures  only  have  heterogeneous  relations 
among different-typed nodes, without considering the homogeneous relations among 
same-typed nodes. However, real networked data are more complex  hybrid  networks 
where links exist not only in heterogeneous nodes but also in homogeneous nodes. So 
we propose the star schema with self loop for this kind of networks.   

Definition  2.  Star  schema  with  self  loop  network. An information network (cid:1833)(cid:3404)(cid:3407)
(cid:1848),(cid:1831),(cid:1849)(cid:3408)  on K+1 types of nodes (cid:1848)(cid:3404)(cid:1666)
(cid:3012)(cid:3038)(cid:2880)(cid:2868)
network, (cid:1831)(cid:3404)(cid:1831)(cid:3035)(cid:3042)(cid:3040)(cid:3042)(cid:1666)(cid:1831)(cid:3035)(cid:3032)(cid:3047)(cid:3032)  and (cid:1831)(cid:3035)(cid:3042)(cid:3040)(cid:3042)(cid:1514)(cid:1831)(cid:3035)(cid:3032)(cid:3047)(cid:3032)(cid:3404)(cid:1486). If   (cid:1482)(cid:1857)(cid:3404) (cid:3407)(cid:1874)(cid:3036)(cid:2868),(cid:1874)(cid:3037)(cid:3038)(cid:3408) (cid:1488)(cid:1831)(cid:3035)(cid:3032)(cid:3047)(cid:3032), 
(cid:1874)(cid:3036)(cid:2868)(cid:1488)(cid:1848)(cid:2868)(cid:1663)(cid:1874)(cid:3037)(cid:3038)(cid:1488)(cid:1848)(cid:3038)(cid:4666)(cid:1863)(cid:3405)0(cid:4667) .  If (cid:1482)e(cid:3404)(cid:3407)(cid:1874)(cid:3036)(cid:3038),(cid:1874)(cid:3037)(cid:3038)(cid:3408) (cid:1488)(cid:1831)(cid:3035)(cid:3042)(cid:3040)(cid:3042) , (cid:1874)(cid:3036)(cid:2868)(cid:1488)(cid:1848)(cid:2868)(cid:1663)(cid:1874)(cid:3037)(cid:2868)(cid:1488)(cid:1848)(cid:2868) ((cid:1863) 
(cid:3404) 0)  or (cid:1874)(cid:3036)(cid:3038)(cid:1488)(cid:1848)(cid:3038)(cid:1663)(cid:1874)(cid:3037)(cid:3038)(cid:1488)(cid:1848)(cid:3038)((cid:1863)(cid:3405)0).  Type (cid:1848)(cid:2868)is  called  the  center  type  (denoted  as 
(cid:1848)(cid:3030)), and (cid:1848)(cid:3038)(cid:4666)(cid:1863)(cid:3405)0(cid:4667)  is called dependent types (denoted as (cid:1848)(cid:3031)).   
(cid:1831)(cid:3035)(cid:3042)(cid:3040)(cid:3042)  is  the links set among the same-typed nodes  (called  homo-link) and (cid:1831)(cid:3035)(cid:3032)(cid:3047)(cid:3032)  is 
can be written as e<(cid:1874)(cid:3036)(cid:3030),(cid:1874)(cid:3037)(cid:3031)>, representing the link between center node and dependent 
e<(cid:1874)(cid:3036)(cid:3030),(cid:1874)(cid:3037)(cid:3030)> or e<(cid:1874)(cid:3036)(cid:3031),(cid:1874)(cid:3037)(cid:3031)>. 
work (cid:1833)(cid:3404)(cid:3407)(cid:1848),(cid:1831),(cid:1849)(cid:3408), (cid:1848)(cid:3404)(cid:1666) (cid:1848)(cid:3038)
, where (cid:1829)(cid:3041)  is defined  as (cid:1829)(cid:3041)(cid:3404)(cid:3407)(cid:1833)(cid:4593),(cid:1842)(cid:3041)(cid:3408). (cid:1833)(cid:4593)is a  subnet of 
clusters set (cid:1829)(cid:3404)(cid:1666)
(cid:3012)(cid:3038)(cid:2880)(cid:2868)
G,  (cid:1831)(cid:4593)(cid:1603)(cid:1831),(cid:1848)(cid:4593)(cid:1603)(cid:1848)   and  (cid:1482)(cid:1857)(cid:3404)(cid:3407)(cid:1874)(cid:3036)(cid:3043),(cid:1874)(cid:3037)(cid:3044)(cid:3408)(cid:1488)(cid:1831)(cid:4593)
.  The  probability  function  (cid:1842)(cid:3041) 
  belongs  to  cluster(cid:1829)(cid:3041) , (cid:1842)(cid:3041)(cid:4666)(cid:1874)(cid:3036)(cid:3043)(cid:4667)(cid:1488)(cid:4670)0,1(cid:4671),  and 
represents  the  possibility  that  node (cid:1874)(cid:3036)(cid:3043)
∑
(cid:1842)(cid:3041)(cid:4666)(cid:1874)(cid:3036)(cid:3043)(cid:4667)
(cid:3015)(cid:3041)(cid:2880)(cid:2869)
(cid:1842)(cid:3041)(cid:4666)(cid:1874)(cid:3036)(cid:3043)(cid:4667)(cid:1488)(cid:4668)0,1(cid:4669), and for dependent node (cid:1874)(cid:3037)(cid:3031), (cid:1842)(cid:3041)  is the successive probability measure 

Fig. 1 shows such an example. For a complex bibliographical  data (see Fig. 1(a)), 
we can organize it as a hybrid network  which includes heterogeneous network among 
different  layers  and homogeneous network  on  the  same  layer in Fig.1 (b). As  shown 
in Fig. 1(c), the hybrid network can be represented with a star schema with self loop 
where “paper” is the center type, while “venue” and “author” are dependent types.   

Now, we can formulate the problem of clustering on  hybrid  network.  Given a net-
  and the cluster number N, our goal is to find a 

node. The  homo-link is the link  between  two same-typed nodes, which is denoted as 

  =  1.  In  our  solution,  we  restrict  probability  function  of  center  node 

(cid:1829)(cid:3041)

(cid:3015)(cid:3041)(cid:2880)(cid:2869)

from 0 to 1. 

 

586 

R. Wang et al. 

3 

The ComClus Algorithm   

After  introducing  the basic framework of ComClus,  this  section describes  the Com-
Clus in detail and then proposes ComRank for estimating the importance of objects. 

3.1  The Framework of ComClus 

The basic idea of ComClus is to determine the memberships of center nodes and then 
estimate the memberships of dependent nodes by center nodes. We consider that the 
probability of center node is estimated by two probabilities: homogeneous probability 
and heterogeneous probability. The homogeneous probability of center node depends 
on its homo-links. The heterogeneous probability of center node is generated by the 
dependent nodes that are correlated with it. In order to co-consider the heterogeneous 
and homogeneous probability for center nodes, generative method and experts model 
are used to mix these two types information. Finally, we estimate the posterior proba-
bility for center node according to the Bayesian rule and reassign the memberships of 
center  nodes.  The  ComClus  will  iteratively  calculate  posterior  probability  until  the 
memberships do not change. Algorithm 1 shows the basic framework of ComClus.   
 

repeat 

    end 

Randomly partition on network G 

Algorithm 1. ComClus: Detecting N clusters on hybrid information network 
Input: Cluster number N and hybrid network G 
Output: Membership of center node, the posterior probability of dependent node 
1:Begin: 
2: 
3: 
4: 
5: 
6: 
7: 
8: 
9: 
10: 

Calculate global probability of center node for smoothing:(cid:1868)(cid:4666)(cid:1874)(cid:3036)(cid:3030)|(cid:1833)(cid:4667) 
    foreach subnet Gn(cid:1603) G 
        Calculate the homogeneous probability of center node: (cid:1868)(cid:4666) (cid:1874)(cid:3036)(cid:3030)|(cid:1829),(cid:1833)(cid:3041)(cid:4667) 
        Calculate the conditional probability of dependent node: (cid:1868)(cid:3435)(cid:1874)(cid:3036)(cid:3031)(cid:3627)(cid:1833)(cid:3041)(cid:3439) 
        Calculate the heterogeneous probability of center node: (cid:1868)(cid:4666)(cid:1874)(cid:3036)(cid:3030)(cid:3627)(cid:1830)(cid:3036),(cid:1833)(cid:3041)(cid:4667) 
        Calculate the mixed probability: (cid:1868)(cid:4666)(cid:1874)(cid:3036)(cid:3030)|(cid:1833)(cid:3041)(cid:4667) 
   Calculate the center node posterior probability: (cid:1868)(cid:4666)(cid:1833)(cid:3041)|(cid:1874)(cid:3036)(cid:3030)(cid:4667)  and Reassign 
12: 
13:  until (cid:1830)(cid:4652)(cid:4652)(cid:1318)(cid:4666)(cid:1848)(cid:3036)(cid:3030)(cid:4667)  convergence obtained 
14:  Calculate the dependent node posterior probability:(cid:1868)(cid:4666)(cid:1833)(cid:3041)|(cid:1874)(cid:3036)(cid:3031)(cid:4667) 
The  homogeneous  probability  of (cid:1874)(cid:3036)(cid:3030) depends  on  its  homo-links  and  denotes  as 
(cid:1868)(cid:4666) (cid:1874)(cid:3036)(cid:3030)|(cid:1829),(cid:1833)(cid:4667). (cid:1868)(cid:4666) (cid:1874)(cid:3036)(cid:3030)|(cid:1829),(cid:1833)(cid:4667)   represents  the  fraction  of  links  that  the  center  node (cid:1874)(cid:3036)(cid:3030)   

3.2 

  Homogeneous Probability for Center Node 

15:End 
 

 

connects to other center nodes on G. This idea is inspired by a general phenomenon 
that  a  node  has  higher  probability  to  connect  with  nodes  within  the  same  cluster.   

587 

 

 

(1) 

(2) 

benefit from the homogeneous information. 

3.3  Conditional Probability for Dependent Node 

Integrating Clustering and Ranking on Hybrid Heterogeneous Information Network 

which will be used to rank (in  Sect.3.7  Eq.  (11))  and filter the unimportant nodes (in 

(cid:1868)(cid:4666) (cid:1874)(cid:3036)(cid:3030)|(cid:1829),(cid:1833)(cid:4667)(cid:3404) 
(cid:3035)(cid:3042)(cid:3031)(cid:3032)(cid:3034)(cid:4666) (cid:3049)(cid:3284)(cid:3278)|(cid:3008)(cid:4667)
 |(cid:3271)(cid:3278)|
∑
(cid:3035)(cid:3042)(cid:3031)(cid:3032)(cid:3034)(cid:4666) (cid:3049)(cid:3284)(cid:3278)|(cid:3008)(cid:4667)
(cid:3284)(cid:3128)(cid:3117)
(cid:1843)(cid:1873)(cid:1867)(cid:1872)(cid:1857)(cid:1856)(cid:1844)(cid:1853)(cid:1872)(cid:1857)(cid:4666) (cid:1874)(cid:3036)(cid:3030)|(cid:1833)(cid:4667)(cid:3404) (cid:3036)(cid:3041)(cid:3435) (cid:3049)(cid:3284)(cid:3278)|(cid:3008)(cid:3439)
|(cid:3271)(cid:3278)|
∑
(cid:3036)(cid:3041)(cid:3435) (cid:3049)(cid:3284)(cid:3278)|(cid:3008)(cid:3439)
(cid:3284)(cid:3128)(cid:3117)

For  convenience, (cid:1860)(cid:1867)(cid:1856)(cid:1857)(cid:1859)(cid:4666) (cid:1874)(cid:3036)(cid:3030)|(cid:1833)(cid:4667)  denotes  the  number  of  homo-links  of (cid:1874)(cid:3036)(cid:3030)   and  the 
number  of  in-degree  of  center  node (cid:1874)(cid:3036)(cid:3030)   on  homogeneous  network  is  denoted  as 
(cid:1861)(cid:1866)(cid:4666)(cid:1874)(cid:3036)(cid:3030)|(cid:1833)(cid:4667). 
The  value  of (cid:1843)(cid:1873)(cid:1867)(cid:1872)(cid:1857)(cid:1856)(cid:1844)(cid:1853)(cid:1872)(cid:1857)(cid:4666)(cid:1874)(cid:3036)(cid:3030)|(cid:1833)(cid:4667)  is  calculated  by  the  quoted  times  of (cid:1874)(cid:3036)(cid:3030)   on  G, 
Sect.3.3  Eq.  (3)) in our algorithm. The center node  (cid:1874)(cid:3036)(cid:3030)  has higher possibility to be 
assigned into a cluster  with  higher (cid:1868)(cid:4666) (cid:1874)(cid:3036)(cid:3030)|(cid:1829),(cid:1833)(cid:4667). Therefore, the clustering result  will 
We consider that the heterogeneous probability of center node (cid:1874)(cid:3036)(cid:3030)  is generated by its 
related dependent nodes (cid:1874)(cid:3036)(cid:3031).  Therefore,  we  need  to  estimate  the  probability of (cid:1874)(cid:3036)(cid:3031), 
which  can  be  represented  as (cid:1868)(cid:3435)(cid:1874)(cid:3036)(cid:3031)(cid:3627)(cid:1833)(cid:3439)(cid:3404)(cid:1868)(cid:4666)(cid:1856)|(cid:1833)(cid:4667)(cid:3400)(cid:1868)(cid:3435)(cid:1874)(cid:3036)(cid:3031)(cid:3627)(cid:1856) ,(cid:1833)(cid:3439).  The  probability  of 
dependent type (cid:1856)  being selected is (cid:1868)(cid:4666)(cid:1856) |(cid:1833)(cid:4667)(cid:3404)|(cid:3023)(cid:3279)|
|(cid:3023)| , where |(cid:1848)(cid:3031)| is the number of nodes 
in  dependent  type  d  layer,  and |(cid:1848)|  is  the  number  of  all  nodes  in  G.  After  the  type 
(cid:1856)  being  selected,  the  probability (cid:1868)(cid:3435)(cid:1874)(cid:3036)(cid:3031)(cid:3627)(cid:1856) ,(cid:1833)(cid:3439)  can  be  estimated.  We  utilize  the  two 
dependent  types  (cid:1856)(cid:3028),(cid:1856)(cid:3029)   to  mutually  estimate  the  probability  for (cid:1868)(cid:3435)(cid:1874)(cid:3036)(cid:3031)(cid:3276)(cid:3627)(cid:1856)(cid:3028) ,(cid:1833)(cid:3439) 
and (cid:1868)(cid:3435)(cid:1874)(cid:3036)(cid:3031)(cid:3277)(cid:3627)(cid:1856)(cid:3029) ,(cid:1833)(cid:3439). (cid:1830)(cid:3036)  is the related dependent type set of (cid:1874)(cid:3036)(cid:3030). Take (cid:1868)(cid:3435)(cid:1874)(cid:3036)(cid:3031)(cid:3276)(cid:3627)(cid:1856)(cid:3028) ,(cid:1833)(cid:3439)  as 
an  instance.  By  taking  advantage  of  the  homogeneous  information  of (cid:1874)(cid:3036)(cid:3031)(cid:3276),  we  set 
(cid:1868)(cid:3435)(cid:1874)(cid:3036)(cid:3031)(cid:3276)(cid:3627)(cid:1856)(cid:3028) ,(cid:1833)(cid:3439)(cid:3404)
 at  the  beginning  of  iteration.  We  consider  the 
(cid:3035)(cid:3042)(cid:3031)(cid:3032)(cid:3034)(cid:4666)(cid:3049)(cid:3284)(cid:3279)(cid:3276)|(cid:3008)(cid:4667)
center  node (cid:1874)(cid:3036)(cid:3030)  is  the  medium  between (cid:1874)(cid:3036)(cid:3031)(cid:3276) and (cid:1874)(cid:3036)(cid:3031)(cid:3160)  .  Naturally,  an  important  me-
 |(cid:3271)(cid:3279)(cid:3276)|
∑
(cid:3035)(cid:3042)(cid:3031)(cid:3032)(cid:3034)(cid:4666)(cid:3049)(cid:3284)(cid:3279)(cid:3276)|(cid:3008)(cid:4667)
(cid:3284)(cid:3128)(cid:3117)
dium  (cid:1874)(cid:3036)(cid:3030)  should have a higher (cid:1843)(cid:1873)(cid:1867)(cid:1872)(cid:1857)(cid:1856)(cid:1844)(cid:1853)(cid:1872)(cid:1857)(cid:4666) (cid:1874)(cid:3036)(cid:3030)|(cid:1833)(cid:4667)  than an  ordinary one.  Besides, 
we  use (cid:2016)   as  a  filter  factor  to  expand  the (cid:1843)(cid:1873)(cid:1867)(cid:1872)(cid:1857)(cid:1856)(cid:1844)(cid:1853)(cid:1872)(cid:1857)(cid:4666) (cid:1874)(cid:3036)(cid:3030)|(cid:1833)(cid:4667)   gap  among 
ent (cid:1874)(cid:3036)(cid:3030). Repeat calculating (4) and (5) until the convergence is obtained. 
(cid:2016)(cid:3404)(cid:3421)1 (cid:1861)(cid:1858) (cid:1843)(cid:1873)(cid:1867)(cid:1872)(cid:1857)(cid:1856)(cid:1844)(cid:1853)(cid:1872)(cid:1857)(cid:4666) (cid:1874)(cid:3036)(cid:3030)|(cid:1833)(cid:4667)(cid:3407)(cid:1853)(cid:1874)(cid:1859)(cid:1843)(cid:1873)(cid:1867)(cid:1872)(cid:1857)(cid:1856)(cid:1844)(cid:1853)(cid:1872)(cid:1857)(cid:4666)(cid:1848)(cid:3030)|(cid:1833)(cid:4667)
(cid:2016) (cid:1861)(cid:1858) (cid:1843)(cid:1873)(cid:1867)(cid:1872)(cid:1857)(cid:1856)(cid:1844)(cid:1853)(cid:1872)(cid:1857)(cid:4666) (cid:1874)(cid:3036)(cid:3030)|(cid:1833)(cid:4667)(cid:3410)(cid:1853)(cid:1874)(cid:1859)(cid:1843)(cid:1873)(cid:1867)(cid:1872)(cid:1857)(cid:1856)(cid:1844)(cid:1853)(cid:1872)(cid:1857)(cid:4666)(cid:1848)(cid:3030)|(cid:1833)(cid:4667)
0 (cid:1867)(cid:1872)(cid:1860)(cid:1857)(cid:1870)(cid:1875)(cid:1861)(cid:1871)(cid:1857)                                                                           
(cid:1871)(cid:1855)(cid:1867)(cid:1870)(cid:1857)(cid:4666) (cid:1874)(cid:3036)(cid:3030)|(cid:1833)(cid:4667)(cid:3404) (cid:2016)(cid:3400)(cid:1843)(cid:1873)(cid:1867)(cid:1872)(cid:1857)(cid:1856)(cid:1844)(cid:1853)(cid:1872)(cid:1857)(cid:4666) (cid:1874)(cid:3036)(cid:3030)|(cid:1833)(cid:4667)(cid:3400)∑
(cid:3032)(cid:2996)(cid:3049)(cid:3284)(cid:3278), (cid:3049)(cid:3284)(cid:3279)(cid:3277)(cid:2997)(cid:3400)(cid:3043)(cid:4672)(cid:3049)(cid:3284)(cid:3279)(cid:3277)(cid:4698)(cid:3031)(cid:3277) ,(cid:3008)(cid:4673)
|(cid:3023)(cid:3279)(cid:3277)|
(cid:3036)(cid:2880)(cid:2869)
(cid:3035)(cid:3032)(cid:3031)(cid:3032)(cid:3034)(cid:4666)(cid:3049)(cid:3284)(cid:3279)(cid:3277)(cid:4667)
(cid:1868)(cid:3435)(cid:1874)(cid:3036)(cid:3031)(cid:3276)(cid:3627)(cid:1856)(cid:3028) ,(cid:1833)(cid:3439)(cid:3404)∑
(cid:3032)(cid:2996)(cid:3049)(cid:3284)(cid:3278),(cid:3049)(cid:3284)(cid:3279)(cid:3276)(cid:2997)(cid:3400)(cid:3046)(cid:3030)(cid:3042)(cid:3045)(cid:3032)(cid:4666) (cid:3049)(cid:3284)(cid:3278)|(cid:3008)(cid:4667)
(cid:3035)(cid:3032)(cid:3031)(cid:3032)(cid:3034)(cid:4666)(cid:3049)(cid:3284)(cid:3279)(cid:3159)(cid:4667)

|(cid:3023)(cid:3278)|
(cid:3036)(cid:2880)(cid:2869)

 

(4) 

 

 

 

 

 

 

(3) 

(5) 

 

 

588 

R. Wang et al. 

dependent nodes and the “barren” nodes can  be distinguished obviously.  Normaliza-
tion method can be used when necessary. 

where (cid:1860)(cid:1857)(cid:1856)(cid:1857)(cid:1859)(cid:4666)(cid:1874)(cid:3036)(cid:3031)(cid:3276)(cid:4667)   is  the  number  of  hete-links  of (cid:1874)(cid:3036)(cid:3031)(cid:3276)on (cid:1833).   We  run  the  same 
process  for (cid:1874)(cid:3036)(cid:3031)(cid:3277)  to  get  the  probability (cid:1868)(cid:3435)(cid:1874)(cid:3036)(cid:3031)(cid:3276)(cid:3627)(cid:1856)(cid:3028) ,(cid:1833)(cid:3439).  As  a  result,  the  “productive” 
the  heterogeneous  probability  for (cid:1874)(cid:3036)(cid:3030).  Here,  we  make  an  independency  assumption 
pendently.  Given dependent node probabilities which are related to (cid:1874)(cid:3036)(cid:3030), the heteroge-
neous probability of center node (cid:1874)(cid:3036)(cid:3030)  can be denoted as (cid:1868)(cid:4666) (cid:1874)(cid:3036)(cid:3030)|(cid:1830)(cid:3036),(cid:1833)(cid:4667). 

After conditional probability of dependent nodes being figured out, we can estimate 

that the dependent nodes generate the heterogeneous probability of center node inde-

3.4  Heterogeneous Probability for Center Node 

(cid:1868)(cid:4666)(cid:1874)(cid:3036)(cid:3030)(cid:3627)(cid:1830)(cid:3036),(cid:1833)(cid:4667)(cid:3404)∏ ∏ (cid:1868)(cid:3435)(cid:1874)(cid:3036)(cid:3031)(cid:3627)(cid:1856) ,(cid:1833)(cid:3439)

|(cid:3023)(cid:3279)|
(cid:3036)(cid:2880)(cid:2869)

(cid:3005)(cid:3284)(cid:3031)

(6) 

 

 

 

on hybrid network G as follows: 

3.5  Mixed Probability for Center Node 

Until  now,  we  obtain  the  homogeneous  and  heterogeneous  probability  of  center 

jointly consider the homogeneous and heterogeneous distribution of center nodes. To 
mix  the  two  distributions,  we  employ  two  methods:  a  generative  method  of  center 
node and a mixture of experts model [5]. 

node (cid:1874)(cid:3036)(cid:3030).  Next,  the  major  difficulty  in  estimating  the  probability  measure  is  how  to 
In  the  generative  method,  we  consider  the  center  node (cid:1874)(cid:3036)(cid:3030)  is  generated  by  two 
parts:  the  homogeneous  and  heterogeneous  information  of (cid:1874)(cid:3036)(cid:3030) .  The  former  is 
(cid:1868)(cid:4666) (cid:1874)(cid:3036)(cid:3030)|(cid:1829),(cid:1833)(cid:4667)  and the latter is (cid:1868)(cid:4666)(cid:1874)(cid:3036)(cid:3030)(cid:3627)(cid:1830)(cid:3036),(cid:1833)(cid:4667). We can calculate the conditional probability 
(cid:1868)(cid:4666)(cid:1874)(cid:3036)(cid:3030)|(cid:1833)(cid:4667)  = (cid:1868)(cid:4666) (cid:1874)(cid:3036)(cid:3030)|(cid:1829),(cid:1833)(cid:4667)(cid:3400)(cid:1868)(cid:4666)(cid:1874)(cid:3036)(cid:3030)(cid:3627)(cid:1830)(cid:3036),(cid:1833)(cid:4667) 
In  experts  model, we regard the homogeneous and heterogeneous information of (cid:1874)(cid:3036)(cid:3030) 
(cid:1868)(cid:4666)(cid:1874)(cid:3036)(cid:3030)|(cid:1833)(cid:4667)(cid:3404)∑
(cid:2024)(cid:3040)(cid:1868)(cid:3040)(cid:4666)(cid:1874)(cid:3036)(cid:3030)|(cid:1833)(cid:4667)
where (cid:1839) (cid:3404)2  represents the number of experts. If (cid:1865)(cid:3404)1,  the homogeneous expert 
takes into effect: (cid:1868)(cid:2869)(cid:4666)(cid:1874)(cid:3036)(cid:3030)|(cid:1833)(cid:4667)= (cid:1868)(cid:4666) (cid:1874)(cid:3036)(cid:3030)|(cid:1829),(cid:1833)(cid:4667).  If (cid:1865)(cid:3404)2, the heterogeneous expert is acti-
vated  as: (cid:1868)(cid:2870)(cid:4666)(cid:1874)(cid:3036)(cid:3030)|(cid:1833)(cid:4667)(cid:3404) (cid:1868)(cid:4666)(cid:1874)(cid:3036)(cid:3030)(cid:3627)(cid:1830)(cid:3036),(cid:1833)(cid:4667) . (cid:2024)(cid:3040)(cid:3404) (cid:3043)(cid:3288)(cid:3435)(cid:3049)(cid:3284)(cid:3278)(cid:3627)(cid:3008)(cid:3439)(cid:3043)(cid:4666)(cid:3006)(cid:3288)(cid:4667)
weight of corresponding expert, and we adopt Softmax function to compute it. (cid:1868)(cid:4666)(cid:1831)(cid:3040)(cid:4667) 
∑
(cid:3043)(cid:3288)(cid:3435)(cid:3049)(cid:3284)(cid:3278)(cid:3627)(cid:3008)(cid:3439)(cid:3043)(cid:4666)(cid:3006)(cid:3288)(cid:4667)
(cid:3262)(cid:3288)(cid:3128)(cid:3117)
links of  (cid:1874)(cid:3036)(cid:3030).  For example, the weight of homogeneous expert of  (cid:1874)(cid:3036)(cid:3030)  is calculated by 
the  following formula: (cid:1868)(cid:4666)(cid:1831)(cid:2869)(cid:4667) = 
(cid:3035)(cid:3042)(cid:3031)(cid:3032)(cid:3034)(cid:3435)(cid:3049)(cid:3284)(cid:3278)|(cid:3008)(cid:3439)(cid:2878)(cid:2869)
|(cid:3253)(cid:3284)|(cid:3279)
(cid:3035)(cid:3042)(cid:3031)(cid:3032)(cid:3034)(cid:3435)(cid:3049)(cid:3284)(cid:3278)|(cid:3008)(cid:3439)(cid:2878)∑
(cid:3035)(cid:3032)(cid:3031)(cid:3032)(cid:3034)(cid:4666)(cid:3049)(cid:3284)(cid:3279)|(cid:3008)
(cid:4667)  .  Because  we only have 

as “homogeneous expert” and “heterogeneous expert”. Then  we can evaluate  mixed 
probability of center  node according to its own distribution. The  mixture of experts 
model is denoted as follows: 

is the weight of expert m, which is proportional to the number of heter-links or homo-

 can  be  seen  as  the 

(cid:3014)(cid:3040)(cid:2880)(cid:2869)

(7) 

(8) 

 

 

 

3.6 

Posterior Probability for Nodes   

 

Integrating Clustering and Ranking on Hybrid Heterogeneous Information Network 

589 

Besides,  to  avoid  zero  probabilities,  we smooth  the  distribution  by  the  following 

mixing  two  distributions.  Now,  we  need  to  calculate  the  posterior  probability 

result, the  method is more suitable for the hybrid network of  which the  homogenous 
and heterogeneous parts have different size. 

Both methods can evaluate the conditional probability of center node, which can be 
applied to different scenarios. The generative method equally treats the homogeneous 
and heterogeneous information, because it simply products homogeneous and hetero-
geneous probability.  Therefore, the generative method is suitable for the hybrid net-
work with  the  same scales of homogeneous and heterogeneous relations. The mixture 

two experts, the (cid:1868)(cid:4666)(cid:1831)(cid:2870)(cid:4667)  is simply  set as  1- (cid:1868)(cid:4666)(cid:1831)(cid:2869)(cid:4667). Obviously, the  weight is dynamic 
for each (cid:1874)(cid:3036)(cid:3030). 
of  experts  model  can dynamically adjust the weights of distributions  (by (cid:2024)(cid:3040)).  As a 
formula: (cid:1868)(cid:4666)(cid:1874)(cid:3036)(cid:3030)|(cid:1833)(cid:3041)(cid:4667)(cid:3404)(cid:2019)(cid:1868)(cid:4666)(cid:1874)(cid:3036)(cid:3030)|(cid:1833)(cid:3041)(cid:4667)(cid:3397) (cid:4666)1(cid:3398)(cid:2019)(cid:4667)(cid:1868)(cid:4666)(cid:1874)(cid:3036)(cid:3030)|(cid:1833)(cid:4667),  where (cid:2019)  is  a  smoothing  para-
meter. G is the whole hybrid network and (cid:1833)(cid:3041)  is the n-th subnet. 
In  the  previous  subsection,  we  get  the  conditional  probability  of  center  node (cid:1874)(cid:3036)(cid:3030)  by 
(cid:1868)(cid:4666)(cid:1833)(cid:3041)|(cid:1874)(cid:3036)(cid:3030)(cid:4667)  for each  (cid:1874)(cid:3036)(cid:3030), and reassign the memberships for center nodes. The posterior 
probability  of  center  node  can  be  calculated  by  Bayesian  rule: (cid:1868)(cid:4666)(cid:1833)(cid:3041)|(cid:1874)(cid:3036)(cid:3030)(cid:4667) (cid:1503)
(cid:1868)(cid:4666)(cid:1874)(cid:3036)(cid:3030)|(cid:1833)(cid:3041)(cid:4667)(cid:3400)(cid:1868)(cid:4666)(cid:1833)(cid:3041)(cid:4667),  where (cid:1868)(cid:4666)(cid:1874)(cid:3036)(cid:3030)|(cid:1833)(cid:3041)(cid:4667)  is  the  conditional  probability  in  cluster (cid:1833)(cid:3041)  and 
(cid:1868)(cid:4666)(cid:1833)(cid:3041)(cid:4667)  represents the cluster size. However, the size of cluster (cid:1833)(cid:3041)  is not fixed. For the 
purpose of getting the (cid:1868)(cid:4666)(cid:1833)(cid:3041)(cid:4667), the EM algorithm can be used to get the local optimum 
(cid:1868)(cid:4666)(cid:1833)(cid:3041)(cid:4667)  by maximizing the log likelihood of center nodes in different areas. 
(cid:1864)(cid:1867)(cid:1859)(cid:1842)(cid:3404)∑
where |(cid:1848)(cid:3030)|  is the size of (cid:1848)(cid:3030), and  N+1 represents the global distribution on G. The 
target is to maximize  (cid:1864)(cid:1867)(cid:1859)(cid:1842)  and two iterative steps can be set to optimize the value P. 
We set (cid:1868)(cid:2868)(cid:4666)(cid:1833)(cid:3041)(cid:4667)(cid:3404) (cid:2869)(cid:3015)(cid:2878)(cid:2869)  before the first iteration. The following two steps run iterative-
ly  until  the  convergence  is  obtained. (cid:1868)(cid:3047)(cid:4666)(cid:1833)(cid:3041)|(cid:1874)(cid:3036)(cid:3030)(cid:4667)(cid:1503)(cid:1868)(cid:4666)(cid:1874)(cid:3036)(cid:3030)|(cid:1833)(cid:3041)(cid:4667)(cid:3400)(cid:1868)(cid:4666)(cid:1833)(cid:3041)(cid:4667); (cid:1868)(cid:3047)(cid:2878)(cid:2869)(cid:4666)(cid:1833)(cid:3041)(cid:4667)(cid:3404)
.  Finally,  we will  have a (cid:1840)  dimensional indicator vector (cid:1830)(cid:4652)(cid:4652)(cid:1318)(cid:4666)(cid:1874)(cid:3036)(cid:3030)(cid:4667),  which 
∑ (cid:3043)(cid:3295)(cid:4666)(cid:3008)(cid:3289)|(cid:3049)(cid:3284)(cid:3278)(cid:4667)
|(cid:3023)|(cid:3036)(cid:2880)(cid:2869)
is  made  up  of  posterior  probability  of (cid:1874)(cid:3036)(cid:3030) .  Then  we  can  calculate  the  indicator  of 
|(cid:3023)|
(cid:1868)(cid:3435)(cid:1833)(cid:3041)(cid:3627)(cid:1874)(cid:3036)(cid:3031)(cid:3439)  can be evaluated by the average posterior probability of center nodes con-
necting  with (cid:1874)(cid:3036)(cid:3031).  The  notation (cid:1845)(cid:3036)  is  a  set  of  center  nodes  connecting  with (cid:1874)(cid:3036)(cid:3031)  and 
|(cid:1845)(cid:3036)| is the size of set (cid:1845)(cid:3036). 

(cid:1868)(cid:4666)(cid:1874)(cid:3036)(cid:3030)|(cid:1833)(cid:3041)(cid:4667)(cid:3400)(cid:1868)(cid:4666)(cid:1833)(cid:3041)(cid:4667)

(cid:4671)

After the iterative process is  finished,  the posterior probability of dependent node 

membership for each center node with K-means.   

log(cid:4670)∑

(cid:3015)(cid:2878)(cid:2869)(cid:3041)(cid:2880)(cid:2869)

|(cid:3023)(cid:3278)|
(cid:3036)(cid:2880)(cid:2869)

 

(9) 

 

 

 

(cid:1868)(cid:3435)(cid:1833)(cid:3041)(cid:3627)(cid:1874)(cid:3036)(cid:3031)(cid:3439)(cid:3404) ∑ (cid:3043)(cid:3435)(cid:3008)(cid:3289)(cid:3627)(cid:3049)(cid:3284)(cid:3278)(cid:3439)
|(cid:3020)(cid:3284)|

|(cid:3020)(cid:3284)|
(cid:3036)(cid:2880)(cid:2869)

 

(10) 

590 

R. Wang et al. 

3.7  Ranking for Nodes   

 

 

4 

As an additional benefit for ComClus, the posterior probabilities of nodes can be used 
for ranking nodes.  Once the cluster process is finished, we can further figure out the 
rank of nodes in their cluster. We proposed a function (called ComRank) to evaluate 
the importance of nodes. 

(cid:1844)(cid:1853)(cid:1866)(cid:1863)(cid:4666)(cid:1874)(cid:3036)(cid:3030)|(cid:1833)(cid:3041)(cid:4667)(cid:3404)(cid:1843)(cid:1873)(cid:1867)(cid:1872)(cid:1857)(cid:1856)(cid:1844)(cid:1853)(cid:1872)(cid:1857)(cid:4666)(cid:1874)(cid:3036)(cid:3030)|(cid:1833)(cid:4667)(cid:3400)(cid:1868)(cid:4666)(cid:1833)(cid:3041)|(cid:1874)(cid:3036)(cid:3030)(cid:4667) 

where (cid:1868)(cid:4666)(cid:1874)(cid:3036)(cid:3030)|(cid:1833)(cid:3041)(cid:4667)  is  the  probability  of  center  node (cid:1874)(cid:3036)(cid:3030).  Generally,  the  rank  of  center 
node  is  proportional  to  its (cid:1843)(cid:1873)(cid:1867)(cid:1872)(cid:1857)(cid:1856)(cid:1844)(cid:1853)(cid:1872)(cid:1857)(cid:4666)(cid:1874)(cid:3036)(cid:3030)|(cid:1833)(cid:4667).  It  is  natural  in  many  applications. 
of membership in that cluster.  The rank of dependent node (cid:1874)(cid:3036)(cid:3031)  can be computed ac-

Taking bibliographic network as an  example, the goodness of a paper is decided by 
the number of citations to a large extent. Another factor of rank function is the post-
erior probability, which can be seen as a cluster coefficient and represents the degree 

(11) 

cording to the rank of center nodes connecting with it.   

(cid:1844)(cid:1853)(cid:1866)(cid:1863)(cid:4666)(cid:1874)(cid:3036)(cid:3031)|(cid:1833)(cid:3041)(cid:4667)(cid:3404)∑ (cid:1844)(cid:1853)(cid:1866)(cid:1863)(cid:4666)(cid:1874)(cid:3036)(cid:3030)|(cid:1833)(cid:3041)(cid:4667)

(cid:3627)(cid:3020)(cid:3284)(cid:3627)
(cid:3036)(cid:2880)(cid:2869)

(cid:3400)(cid:1868)(cid:4666)(cid:1833)(cid:3041)|(cid:1874)(cid:3036)(cid:3031)(cid:4667) 

(12) 

Experiment   

In this section, we evaluate the effectiveness of our ComClus  algorithm, and compare 
it with the state-of-the-art methods on two data sets. 

4.1  Data Set 

The DBLP is a dataset of bibliographic information in computer  science domain. We 
use it to build  a hybrid network with three-typed  nodes:  papers  (center  type),  venues 
(dependent type) and authors (dependent type). Homo-links among authors form a co-
author network, and homo-links  among papers form a paper citation network.  Hete-
links are the writing relation between authors and papers and the publication relation 
between  venues and  papers. We  extract  venues  from  different  areas according to the 
categories  of  China  Computer  Federation  (http://www.ccf.org.cn).  Moreover,  CCF 
provides  three levels for ranking venues: A, B, C. The class A is top venues, such as 
KDD in data mining (DM). The class B is some famous venues such as SDM, ICDM. 
The class C is admitted venues such as WAIM.  In the  experiments, we extract two 
different-scaled subsets of the DBLP which are called DBLP-L and DBLP-S. 

The DBLP-S is a small size dataset and it includes three areas in computer domain: 
database, data mining, and information  retrieval. There are 21venues (7 venues for 
each area, covering three levels), 25,020 papers and 10,907 authors in DBLP-S. Two 
or three venues for each level are picked out. 

The DBLP-L is a large dataset. There are eight areas included, which are computer 
network, information security, computer architecture, theory, software engineering & 
programming  language,  artificial  intelligence&  pattern  recognition,  computer   
graphics,  data  mining&  information  retrieval  &database.  There  are  280  venues   

 

 

Integrating Clustering and Ranking on Hybrid Heterogeneous Information Network 

591 

(35 venues for each area), 275,649 papers, and 238,673  authors.  For  each  area, five 
venues are in A level and fifteen venues are selected in B or C level. 

In  these  two  datasets, venues are labeled  with their research areas. Moreover, in 
DBLP-S,  we randomly label 1031 papers and 1295 authors with three  research areas, 
which  are  used  to  evaluate the clustering  accuracy.  All the results are based on 20 
runnings, and average results are shown. 

4.2  Clustering Accuracy Comparison Experiments 

For accuracy evaluation, we apply our method to cluster on both DBLP-S and DBLP-
L. We compare ComClus with the representative ranking-based clustering algorithm 
NetClus  which can be applied in heterogeneous networks  organized as star schema. 

The smoothing parameter (cid:2019)  is fixed at 0.7 in both two algorithms.  The filter factor (cid:2016) 

in ComClus is 3.  The clustering  accuracy of paper is the fraction of nodes identified 
correctly. For author and venue nodes, the accuracy is the posterior probability frac-
tion  of  nodes  identified  correctly.  Results  are  shown  in  Table  1.  The  two  different 
mixture  methods  of  ComClus  both  have  higher  accuracy  than  NetClus.  The  lower 
deviation  of  ComClus  implies  that  ComClus  is  steadier  than  NetClus.  The  results 
show  that,  the  additional  homogeneous  relation  utilized  by  ComClus  is  helpful  for 
improving its accuracy as well as stability. In addition, ComClus with experts model 
achieves  better  performance  than  ComClus  with  generative  method.  We  think  the 
reason is that experts model considers the weight of heterogeneous and homogeneous 
information. In the following experiments, we use ComClus with experts model as the 
standard version of ComClus.   

Table 1. Clustering accuracy comparison for different-typed nodes    

Accuracy 

Paper(DBLP-S) 
Venue(DBLP-S) 
Author(DBLP-S) 
Venue(DBLP-L) 

ComClus(experts method) 

ComClus(generative method) 

NetClus 

Mean 
0.774 
0.855 
0.731 
0.681 

Dev. 
0.019 
0.018 
0.018 
0.041 

Mean 
0.766 
0.777 
0.680 
0.648 

Dev. 
0.021 
0.028 
0.016 
0.046 

Mean 
0.715 
0.739 
0.697 
0.579 

Dev. 
0.066 
0.067 
0.052 
0.084 

 
Since  the  hybrid  network  includes  homogeneous  network,  we  compare  ComClus 
with  those  clustering  algorithms  on  homogeneous  network,  where  a  representative 
spectral clustering algorithm Normalize Cut [4] is employed. We design the similarity 

of two nodes (cid:4666)(cid:1861),(cid:1862)(cid:4667)  as: (cid:1845)(cid:4666)(cid:1861),(cid:1862)(cid:4667)(cid:3404)(cid:1855)(cid:1867)(cid:1871)(cid:4666)(cid:1848)(cid:3036),(cid:1848)(cid:3037)(cid:4667), where (cid:1848)(cid:3036)  is the adjacent vector of node 

i. The result is shown in Table 2, which clearly illustrates that ComClus is better than 
Normalized Cut. ComClus combines the information from homogeneous and hetero-
geneous  relations.  It  makes  ComClus  outperform  Normalized  Cut  which  only  uses 
homogeneous network information.   

Table 2. Clustering accuracy comparison on homogeneous network 

Accuracy 

Paper Accuracy 

ComClus 

0.787 

Normalized Cut 

0.457 

 

592 

R. Wang et al. 

4.3  Ranking Accuracy Comparison Experiment 

On DBLP-L, we make a ranking accuracy comparison between ComRank and Autho-
rithyRank which is a rank method in NetClus[3]. In this application, it is hard to defi-
nitely compare the goodness of two venues, whereas we can roughly distinguish their 
levels. For example, it is difficult to compare the ranking of SDM and ICDM. But we 
can safely say that SDM and ICDM are on the same level and they are worse than the 
top level venues (e.g., KDD) and better than the common level venues (e.g., WAIM). 
Inspired  by  RankingLoss  measure  [8],  we  define  LevelRankingLoss  to  evaluate  the 
disorder ratio of object pairs on their levels  and it  is  abbreviated  as  LRLoss.  Without 
loss of generality, we define LRLoss on bibliographic data. First, we define a triple to 

represent  a  venue:(cid:1844)(cid:1853)(cid:1866)(cid:1863)(cid:1846)(cid:1873)(cid:1868)(cid:1864)(cid:1857)(cid:3036)(cid:3404)(cid:3407)(cid:1829)(cid:3036),(cid:1838)(cid:3036),(cid:1844)(cid:3036)(cid:3408),  where (cid:1829)(cid:3036)  represents  a  venue, (cid:1838)(cid:3036)  is 
the level of (cid:1829)(cid:3036), (cid:1838)(cid:3036)(cid:1488)(cid:4668)A,B,C(cid:4669)  (the recommended level of CCF). (cid:1844)(cid:3036)  is the rank num-
ber of (cid:1829)(cid:3036)  generated by the algorithms(the smaller, the better).  The LRLoss is  defined 
(cid:3019)(cid:3036)(cid:2880)(cid:2869)
where  R  is  the  size  of  Cartesian  product  of (cid:1844)(cid:1853)(cid:1866)(cid:1863)(cid:1846)(cid:1873)(cid:1868)(cid:1864)(cid:1857)   set  and  (cid:1838)(cid:1867)(cid:1871)(cid:1871)(cid:1842)(cid:1853)(cid:1861)(cid:1870)(cid:3036)(cid:3404)(cid:4668)(cid:3407)
(cid:1844)(cid:1853)(cid:1866)(cid:1863)(cid:1846)(cid:1873)(cid:1868)(cid:1864)(cid:1857)(cid:3036),(cid:1844)(cid:1853)(cid:1866)(cid:1863)(cid:1846)(cid:1873)(cid:1868)(cid:1864)(cid:1857)(cid:3037)(cid:3408)|(cid:1838)(cid:3036)(cid:3407)(cid:1838)(cid:3037),(cid:1844)(cid:3036)(cid:3408)(cid:1844)(cid:3037) (cid:1867)(cid:1870) (cid:1838)(cid:3036)(cid:3408)(cid:1838)(cid:3037),(cid:1844)(cid:3036)(cid:3407)(cid:1844)(cid:3037)(cid:4669). Here, (cid:1838)(cid:1867)(cid:1871)(cid:1871)(cid:1842)(cid:1853)(cid:1861)(cid:1870)(cid:3036) 
denotes  the  complementary  set. |(cid:1838)(cid:1867)(cid:1871)(cid:1871)(cid:1842)(cid:1853)(cid:1861)(cid:1870)(cid:3036)|   is  the  number  of  misordered  pairs  for 
(cid:1844)(cid:1853)(cid:1866)(cid:1863)(cid:1846)(cid:1873)(cid:1868)(cid:1864)(cid:1857)(cid:3036). For example, (cid:1844)(cid:1853)(cid:1866)(cid:1863)(cid:1846)(cid:1873)(cid:1868)(cid:1864)(cid:1857)(cid:2869)= <KDD, A, 2>, (cid:1844)(cid:1853)(cid:1866)(cid:1863)(cid:1846)(cid:1873)(cid:1868)(cid:1864)(cid:1857)(cid:2870)=<ICDM, B, 1> 
can be seen as one LossPair for (cid:1844)(cid:1853)(cid:1866)(cid:1863)(cid:1846)(cid:1873)(cid:1868)(cid:1864)(cid:1857)(cid:2869).   

|(cid:3013)(cid:3042)(cid:3046)(cid:3046)(cid:3017)(cid:3028)(cid:3036)(cid:3045)(cid:3284)|
(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)|
|(cid:3013)(cid:3042)(cid:3046)(cid:3046)(cid:3017)(cid:3028)(cid:3036)(cid:3045)(cid:3284)|(cid:2878)|(cid:3013)(cid:3042)(cid:3046)(cid:3046)(cid:3017)(cid:3028)(cid:3114)(cid:3045)(cid:3362)

(cid:1838)(cid:1844)(cid:1838)(cid:1867)(cid:1871)(cid:1871)(cid:3404)(cid:2869)(cid:3019)∑

as follows. 

(13) 

We select the top 5 and top 10 venues in different areas and then calculate LRLoss 
for  them.  Additionally,  we  also  compare  the  accuracy  of  the  global  rank  on  both 
ComRank and NetClus. Results are shown in Fig2. 
 

 

 

s
s
o
L
R
L

0.25

0.20

0.15

0.10

0.05

0.00

 NetClus(DBLP-S top5)
 ComClus(DBLP-S top5)

 

s
s
o
L
R
L

0.30

0.25

0.20

0.15

0.10

0.05

0.00

 NetClus(DBLP-L top10)
 ComClus(DBLP-L top10)

 NetClus(DBLP-L top10)
 Combine(DBLP-L top10)

0.5

0.4

0.3

0.2

0.1

s
s
o
L
R
L

0.0

Architecture

AI& P R
Graphics

Network

Security

PL&S E

Theory
Average

Global

IR 

DB 

Average Global 
Different areas , Average and Global

DM 

 

 
(a) 3 areas top5 venues on DBLP-S    (b) 3 areas top10 venues on DBLP-L (c) 8 areas top 10 venues on DBLP-L 

Different areas, Average and Global

DB 
Average Global 
Different areas , Average and Global

DM 

IR 

Fig. 2. Ranking accuracy comparison (The smaller LRLoss, the better) 

The results clearly show that ComRank better ranks these venues, since its LRLoss is 
lower than that of AuthorityRank on all research areas. We think the additional homoge-
neous information utilized by ComRank contributes to its better ranking performance.   

4.4  Case Study 

In this section, we further  show  the  performance  of  ComRank with a  ranking  case 
study.   

 

 

Integrating Clustering and Ranking on Hybrid Heterogeneous Information Network 

593 

Table 3. Top 15 venues with global rank on DBLP-S 

ComRank 

1 

Venue 
#Papers 
Level 

SIGMOD 

2428 

A 

AuthorityRank 

1 

2 

VLDB
2444 

A
2

3 

SIGIR 
2509 

A 
3 

4 

ICDE 
2832 

A 
4 

5 

KDD 
1531 

A 
5 

8 

7 

6 

9 

10 
PODS WWW CIKM ICDM EDBT
747 
940 
B
A
10
6

1436 

2204 

1501 

B
9

B
8

B
7

11 

12 

13 

14 

15 

PKDD WSDM PAKDD  WebDB  DEXA 
1731 

1030 

972 
C 
14 

C 
15 

680 
B
11

198 
B
12

B 
13 

Venue 
#Papers 
Level 

VLDB 
2444 

A 

ICDE  SIGMOD  SIGIR 
2509 
2832 

2428 

KDD  WWW CIKM 
2204 
1531 

1510 

A 

A 

A 

A 

B 

B 

ICDM 
1436 

B 

 

PODS  DEXA  PAKDD EDBT 
940 
747 
A 
B 

1731 

1030 

C 

B 

PKDD  WSDM  ECIR 
575 
C 

680 
B 

198 
B 

Table  3  shows  the  top  15  venues  ranked  by  ComRank  and  AuthorityRank  on 
DBLP-S.  The  results  show  that  the  ranks  of  venues  generated  by  ComRank  are  all 
consistent with the recommended level by CCF. However, there are some disordered 
venues in AuthorityRank, which implies that AuthorityRank is sensitive to the num-
ber of papers. That is, AuthorityRank tends to rank a venue publishing many papers 
with a higher value. For example, AuthorityRank ranks PODS with a low value and 
DEXA  with  a  relatively  high  value  because  PODS  published  not  many  papers  and 
DEXA published so many papers. In contrast, ComRank considers the citation infor-
mation from homogeneous network. So ComRank avoids these shortcomings.   

4.5  Convergence and Stability Experiments 

For observing the convergence, we compare each cluster probability distribution with 
global distribution by average KL divergence  [3].  Next,  we  use entropy to measure 
the unpredictability of cluster and prove the algorithm stability.   

(cid:1827)(cid:1874)(cid:1859)(cid:1837)(cid:1838)(cid:4666)(cid:1848)(cid:3031)(cid:4667)(cid:3404)(cid:2869)(cid:3015)∑
(cid:1830)(cid:3012)(cid:3013)(cid:4666)(cid:1868)(cid:4666)(cid:1874)(cid:3036)(cid:3031)|(cid:1833)(cid:3041)(cid:4667)||(cid:1868)(cid:4666)(cid:1874)(cid:3036)(cid:3031)|(cid:1833)(cid:4667)(cid:4667)
(cid:3015)(cid:3041)(cid:2880)(cid:2869)
∑
(cid:1827)(cid:1874)(cid:1859)(cid:1831)(cid:1866)(cid:1872)(cid:1870)(cid:1867)(cid:1868)(cid:1877)(cid:4666)(cid:1848)(cid:3043)(cid:4667)(cid:3404)(cid:3398)(cid:2869)(cid:3015)∑
(cid:1868)(cid:3435)(cid:1874)(cid:3036)(cid:3043)(cid:3627)(cid:1833)(cid:3041)(cid:3439)
|(cid:3023)(cid:3291)|
(cid:3015)(cid:3041)(cid:2880)(cid:2869)
(cid:3036)(cid:2880)(cid:2869)

(cid:3400)(cid:1864)(cid:1867)(cid:1859)(cid:1868)(cid:3435)(cid:1874)(cid:3036)(cid:3043)(cid:3627)(cid:1833)(cid:3041)(cid:3439) 

 

(14) 

(15) 

 ComClus
 NetClus

 

 

 

L
K
g
v
A
e
u
n
e
V

 

1.8

1.2

0.6

0.0

 ComClus
 NetClus

1.8

1.2

0.6

L
K
g
v
A

 
r
o
h

t

u
A

0.0

0

 ComClus
 NetClus

7.0

6.3

5.6

s
r
e
p
a
p

 
f

o
 
y
p
o
r
t

n
E

 ComClus
 NetClus

10.5

9.8

9.1

s
r
o
h
u
a

t

 
f

o

 
y
p
o
r
t
n
E

 ComClus
 NetClus

2.5

2.0

1.5

1.0

0

s
e
u
n
e
v
 
f

o

 
y
p
o
r
t
n
E

0

15

10

20

5
Iteration Number

 
(a)AvgKL of venues        (b) AvgKL of authors      (c)AvgEntropy of papers      (d)AvgEntropy of authors      (e)AvgEntropy of venues 

5
Iteration Number

5
Iteration Number

5
Iteration Number

5
Iteration Number

25

25

10

15

20

25

10

15

25

10

10

15

20

15

20

20

25

0

0

Fig. 3. The change of AvgKL and AvgEntropy of nodes with iteration number 

As shown in Fig. 3(a)  and  (b), the convergence of our algorithm is faster than Net-
Clus. From the results shown in Fig. 3(c),  (d)  and  (e), we can observe that ComClus 

achieves lower (cid:1859)(cid:1831)(cid:1866)(cid:1872)(cid:1870)(cid:1867)(cid:1868)(cid:1877)  . The reason is that ComClus prevents the negative effects 
of unimportant paper by the factor (cid:2016). Besides, in ComRank, the distribution informa-

tion of objects comes from heterogeneous and homogeneous relations.  However, the 
distribution  information  of  objects  in  NetClus  is  only  from  heterogeneous  network. 
More information helps ComClus fast converge and achieve steady solution.   

 

594 

R. Wang et al. 

5 

Conclusions 

In  this  paper,  we  proposed  a  new  ranking-based  clustering  algorithm  ComClus  on 
heterogeneous information networks. Different from conventional clustering methods, 
ComClus can  group different-typed objects on a hybrid network  which includes the 
homogeneous network and heterogeneous  relations together. Through applying prob-
ability  information  in  ComClus,  ComClus  can  also  rank  the  importance  of  objects. 
The experiments on real datasets have demonstrated that our algorithm can generate 
more accurate cluster and rank with quicker and steadier convergence.   

Acknowledgments.  It  is  supported  by  the  National  Natural  Science  Foundation  of 
China  (No.  60905025,  61074128,  71231002).  This  work  is  also  supported  by  the   
National  Basic  Research  Program  of  China  (2013CB329603)  and  the  Fundamental 
Research Funds for the Central Universities. 

References 

1.  Shen, H., Cheng, X.: Spectral Methods for the Detection of Network Community Struc-

ture: a Comparative Analysis. J. Stat. Mech., P10020 (2010) 

2.  Sun, Y., Han, J., Zhao, P., Yin, Z., Cheng, H., Wu, T.: Rankclus: Integrating Clustering 
with Ranking for Heterogeneous Information Network Analysis. In: EDBT, pp. 565–576 
(2009) 

3.  Sun,  Y.,  Yu,  Y.,  Han,  J.:  Ranking-based  Clustering  of  Heterogeneous  Information  Net-

works with Star Network Schema. In: KDD, pp. 797–806 (2009) 

4.  Shi,  J.,  Malik,  J.:  Normalized  Cuts  and  Image  Segmentation.  In:  CVPR,  pp.  731–737 

(1997) 

5.  Jacobs, R.A., Jordan, M.I., Nowlan, S., Hinton, G.E.: Adaptive Mixtures of Local Experts. 

Neural Computation 3, 79–87 (1991) 

6.  Zhou, D., Orshanskiy, S., Zha, H., Giles, C.: Co-ranking Authors and Documents in a He-

terogeneous Network. In: ICDM, pp. 739–744 (2007) 

7.  Liu,  X.,  Murata,  T.:  Detecting  Communities  in  K-partite  K-uniform  (Hyper)  Networks. 

JCST 26(5), 778–791 (2011) 

8.  Zhang, M.L., Zhang, K.: Multi-label Learning by Exploiting Label Dependency. In: KDD, 

pp. 999–1008 (2010) 

9.  Long, B., Wu, X., Zhang, Z.M., Yu, P.S.: Unsupervised Learning on K-partite Graphs. In: 

KDD, pp. 317–326 (2006) 

10.  Michael, K.N., Li, X., Ye, Y.: MultiRank: Co-ranking for Objects and Relations in Multi-

relational Data. In: KDD, pp. 1217–1225 (2011) 

11.  Ailon, N., Charikar, M., Newman, A.: Aggregating Inconsistent Information: Ranking and 

Clustering. J. ACM 55(5) (2008) 

12.  Brin,  S.,  Page,  L.:  The  Anatomy  of  a  Large-scale  Hyper  Textual  Web  Search  Engine. 

Comput. Netw. ISDN Syst. 30(1-7), 107–117 (1998) 

 

 


