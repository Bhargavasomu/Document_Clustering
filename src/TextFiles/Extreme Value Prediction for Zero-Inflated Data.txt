Extreme Value Prediction for Zero-Inﬂated Data

Fan Xin1 and Zubin Abraham2

1 Department of Statistics, Michigan State University

2 Department of Computer Science & Engineering, Michigan State University

{fanxin,abraha84}@msu.edu

Abstract. Depending on the domain, there may be signiﬁcant ramiﬁ-
cations associated with the occurrence of an extreme event (for e.g., the
occurrence of a ﬂood from a climatological perspective). However, due
to the relative low occurrence rate of extreme events, the accurate pre-
diction of extreme values is a challenging endeavor. When it comes to
zero-inﬂated time series, standard regression methods such as multiple
linear regression and generalized linear models, which emphasize esti-
mating the conditional expected value, are not best suited for inferring
extreme values. And so is the case when the the conditional distribution
of the data does not conform to the parametric distribution assumed by
the regression model. This paper presents a coupled classiﬁcation and re-
gression framework that focuses on reliable prediction of extreme value
events in a zero-inﬂated time series. The framework was evaluated by
applying it on a real-world problem of statistical downscaling of precipi-
tation for the purpose of climate impact assessment studies. The results
suggest that the proposed framework is capable of detecting the timing
and magnitude of extreme precipitation events eﬀectively compared with
several baseline methods.

1

Introduction

The notion behind being able to foretell the occurrence of an extreme event in a
time series is very appealing, especially in domains with signiﬁcant ramiﬁcations
associated with the occurrence of an extreme events. Predicting pandemics in
an epidemiological domain or forecasting natural disasters in a geological and
climatic environment are examples of applications that give importance to detec-
tion of extreme events. Unfortunately, the accurate prediction of the timing and
magnitude of such events is a challenge given their low occurrence rate. More so,
the prediction accuracy depends on the regression method used as well as char-
acteristics of the data. On the one hand, standard regression methods such as
generalized linear model (GLM) emphasize estimating the conditional expected
value, and thus, are not best suited for inferring extremal values. On the other
hand, methods such as quantile regression are focused towards estimating the
conﬁdence limits of the prediction, and thus, may overestimate the frequency
and magnitude of the extreme events. Though methods for inferring extreme
value distributions do exist, combining them with other predictor variables for
prediction purposes remains a challenging research problem.

P.-N. Tan et al. (Eds.): PAKDD 2012, Part I, LNAI 7301, pp. 318–329, 2012.
c(cid:2) Springer-Verlag Berlin Heidelberg 2012

Extreme Value Prediction for Zero-Inﬂated Data

319

Standard regression methods typically assume that the data conform to certain
parametric distributions (e.g., from an exponential family). Such methods are in-
eﬀective if the assumed distribution does not adequately model characteristics of
the real data. For example, a common problem encountered especially in model-
ing climate and ecological data is the excess probability mass at zero. Such zero-
inﬂated data, as they are commonly known, often lead to poor model ﬁtting using
standard regression methods as they tend to underestimate the frequency of ze-
ros and the magnitude of extreme values in the data. One way for handling such
type of data is to identify and remove the excess zeros and then ﬁt a regression
model to the non-zero values. Such an approach, can be used, for example, to pre-
dict future values of a precipitation time series [13], in which the occurrence of wet
or dry days is initially predicted using a classiﬁcation model prior to applying the
regression model to estimate the amount of rainfall for the predicted wet days. A
potential drawback of this approach is that the classiﬁcation and regressions mod-
els are often built independent of each other, preventing the models from gleaning
information from each other to potentially improve their predictive accuracy. Fur-
thermore, the regression methods used in modeling the zero-inﬂated data do not
emphasize accurate prediction of extreme values.

The paper presents an integrated framework that simultaneously classiﬁes
data points as zero-valued or not, and apply quantile regression to accurately
predict extreme values or the tail end of the non-zero values of the distribution
by focussing on particular quantiles.

We demonstrate the eﬃciency of the proposed approach on modeling climate
data (precipitation) obtained from the Canadian Climate Change Scenarios Net-
work website [1]. The performance of the approach is compared with four baseline
methods. The ﬁrst baseline is the general linear model (GLM) with a Poisson
distribution. The second baseline used is the general linear model using an ex-
ponential distribution coupled with a binomial distribution classiﬁer(GLM-C).
A zero-inﬂated Poisson was used as the third baseline method (ZIP). The fourth
basesline was quantile regression. Empirical results showed that our proposed
framework outperforms the baselines for majority of the weather stations inves-
tigated in this study.

In summary, the main contributions of this paper are as follows:

– We compare and analyze the performance of models created using variants
of GLM, quantile regression and ZIP approaches to accurately predict values
for extreme data points that belong to a zero-inﬂated distribution.

– We present a approach optimized for modeling zero-inﬂated data that out-
performs the baseline methods in predicting the value of extreme data points.
– We successfully demonstrated the proposed approach to the real-world prob-
lem of downscaling precipitation climate data with application to climate
impact assessment studies.

2 Related Work

The motivation behind the presented model is accurately predicting extreme
values in the presence of zero-inﬂated data. Previous studies have shown that

320

F. Xin and Z. Abraham

additional precautions must be taken to ensure that the excess zeros do not
lead to poor ﬁts [2] of the regression models. A typical approach to model a
zero-inﬂated data set is to use a mixture distribution of the form P (y|x) =
απ0(x) + (1 − α)π(x), where π0 and π are functions of the predictor variables

x and α is a mixing coeﬃcient that governs the probability an observation is
a zero or non-zero value. This approach assumes that the underlying data are
generated from known parametric distributions, for example, π may be Poisson
or negative binomial distribution (for discrete data) and lognormal or Gamma
(for continuous data).

Generally, simple modeling of zero values may not be suﬃcient, especially
in the case of zero-inﬂated climate data such as that of precipitation where
extreme value observations, (that could indicate ﬂoods, droughts, etc) need to be
accurately modeled. Due to the signiﬁcance of extreme values in climatology and
the increasing trend in extreme precipitation events over the past few decades, a
lot of work needs to be done in analysing the trends in precipitation, temperature,
etc., for regions in United states, Canada, among others [3]. Katz et al. introduces
the common approaches used in climate change research, especially with regard
to extreme values[4].

The common approaches to modeling extreme events are based on general
extreme value theory [5], Pareto distribution [10], generalized linear modeling
[6], hierarchical Bayesian approaches [9], etc. Gumbel [8] and Weibull [12] are the
more common variants of general extreme value distribution used. There are also
Bayesian models [11] that try augmenting the model with spatial information.
Watterson et al. propose a model that also deals with the skewness of non-
zero data/intermittency of precipitation using gamma distribution to interpret
changes in precipitation extremes [7]. In contrast, the framework presented in
this paper handles the intermittency of the data by coupling a logistic regression
classiﬁer to the quantile regression part of the model.

3 Preliminaries
Consider a multivariate time series L = (xt, yt), where t ∈ {1, 2,··· , n} is a
discrete-valued index for time, xt is a d-dimensional vector of predictor vari-
ables at time t, and yt is the corresponding value for the response (target)
variable. Given an unlabeled sequence of multivariate observations xτ , where
τ ∈ {n + 1,··· , n + m}, our goal is to learn a target function f (x, β) that best
estimates the values of the response variable by minimizing the expected loss
Ex,y[L(y, f (x, β))]. The weight vector β denotes the regression coeﬃcients to be
estimated from the training data L.

Multiple linear regression (MLR) is one of most widely used regression meth-
ods due to its simplicity. It assumes f (x, β) = βT x (where x is a (d + 1)-
dimensional vector whose ﬁrst element x0 = 1 and β ∈ (cid:3)d+1 is the weight
vector) and the response variable y is related to f (x, β) via the following equa-
tion:

y = βT

x + ,

 ∼ N (0, σ2).

Extreme Value Prediction for Zero-Inﬂated Data

321

(cid:2)

As a result, P (y|x) ∼ N (βT x, σ2) and Ey|x[y] =
yP (y|x)dy = βT x. Since the
predicted value of the response variable for a test data point xτ is βT xτ , this
implies that the predictions made by MLR focus primarily on the average value
of y given xτ . This explains the limitation of MLR in terms of inferring extreme
values in a given time series. The parameter vector β in MLR can be estimated
using the maximum likelihood (ML) approach to obtain

ˆβ = (XT X)

−1XT y,

where X is the n × (d + 1) design matrix and y is an n × 1 column vector for
the observed values of the response variable.

The drawback of simple linear regression is that it is built on a strong assump-
tion -namely, normality. Unfortunately, real world data may not always have a
normal distribution and may be skewed to one side or may not cover the whole
range of real numbers or may have a heavier tail than the normal distribution,
etc. Hence, alternative approaches that are not constrained by such assumptions
such as GLM may be used.

3.1 Generalized Linear Model(GLM) and 2-Step GLM (GLM-C)

The generalized linear model is one of most widely used regression methods due
to its simplicity. Generally, a GLM consists of three elements:

1. The response variable Y, which has a probability distribution from the

exponential family.

2. A linear predictor η = Xβ
3. A link function g(·) such that E(Y|X) = μ = g

−1(η)

where, Y ∈ Rn×1 is the response variables vector, X ∈ Rn×d is the design
matrix with all 1 in the last column. β ∈ Rp×1 is the parameter vector. Since
the link function shows the relationship between the linear predictor and the
mean of the distribution, it is very important to understand the detail about
the data before arbitrarily using the canonical link function. In our case, since
the precipitation data are always non-negative and values represented using a
millimeter scale, the non-zero data may be treated as count data allowing us
to use Poisson distribution or an exponential distribution to describe the data.
Hence, in our experiments we always choose log(·) as the link function and choose
to use Poisson distribution. We scale the Y used in the regression model to be
10 × Y :

(10 × Yi)|Xi ∼ P oi(λi)
−1(ηi) = g

E((10 × Yi)|Xi) = λi = g

−1(Xiβ);

The histogram in Figure 1 is a representation of the data belonging to station-
1. It is clear that the number of zero is too large. The second histogram which
is without zero looks similar to a kind of Poisson or exponential distribution.

Considering the large number of zeros, one is motivated to perform classiﬁ-
cation ﬁrst to eliminate the zero values before any regression. There are many

322

F. Xin and Z. Abraham

Precipitation Histogram for Site1

Nonzero Precipitation Histogram for Site1

y
t
i
s
n
e
D

0
.
1

8
.
0

6
.
0

4

.

0

2
0

.

0

.

0

3
.
0

2
.
0

y
t
i
s
n
e
D

1
0

.

0

.

0

0

20

40

Y

60

80

0

20

40

Y

60

80

Fig. 1. Comparison of the histogram of the original distribution of data at Station-1
with its truncated counterpart

classiﬁcation methods available. But for the purpose of our experiments, we use
logistic regression (which is also a variation of GLM) to do the classiﬁcation.
The response variable Y

of logistic regression is a binary variable deﬁned as:

∗

(cid:3)

∗

Y

=

1 Y > 0,
0 Y = 0

The detail of the model is as follows: The link function is a logit link g(p) =
log(

), such that,

p
1 − p

i |Xi ∼ Bin(pi)
∗
Y
−1(ηi) = g

i |Xi) = pi = g
∗

E(Y

−1(Xiβ);

When we derive the ﬁtted values, they will be transferred to be binary:

(cid:3)

∗

f

=

1 1 ≥ ˆY
∗
0 0.5 ≥ ˆY

> 0.5,
∗ ≥ 0

(cid:4)

The second part is a GLM with exponential distribution, the response variable
is just those non-zero data, and the link function is g(·) = log(·):

Y

i |Xi ∼ Exp(λi)
(cid:4)
Y
−1(ηi) = g

−1(Xiβ);

E(Y

i |Xi) = λi = g
(cid:4)
(cid:4)
for all Xi

Then, we got ﬁtted-value f
Finally, we report the product of those two ﬁtted-values ˆY = f

∗ × f

(cid:4)

To ﬁt the GLM model, we use iteratively reweighted least squares(IRLS)

method for maximum likelihood estimation of the model parameters.

Extreme Value Prediction for Zero-Inﬂated Data

323

3.2 Zero Inﬂated Poisson Regression(ZIP)

Diﬀering from the methods above, zero inﬂated poisson regression treats the
zero as a mixture of two distributions: a Bernoulli distribution with probability
πi to get 0, and a Poisson distribution with parameter μ (let P r(·; μ) denote the
probability density function). In fact, the ZIP regression model is deﬁned as:

P r(Y = yi|xi) =

(cid:3)

πi + (1 − πi)P r(Yi = 0; λi)
(1 − πi)P r(Y = yi; λi)

yi = 0,
yi > 0

where 0 < πi < 1, and

logit(πi) = log(

πi
1 − πi
log(μi) = xiβ

) = xiβ

1

2

1, β

where β
2 are all regression parameter. Both of them could be found by
maximizing the likelihood function. For the purpose of the experiments, we used
the R package ’pscl’ to ﬁt the model.

3.3 Quantile Linear Regression(QR) and 2-step QR(QR-C)

Quantile regression was used to estimate the speciﬁed quantile of a population.
Hence, if the objective of the regression is to estimate the conditional quan-
tile(e.g., median) of Y instead of a conditional mean like MLR and Ridge regres-
sion, one may use quantile regression. Its loss function for the linear regression
model is:

f (b) =

i=1

where

N(cid:4)

ρτ (Yi − XT

i b), and ˆβ = arg min
b

f (b),

(cid:3)

ρτ (u) =

τ u
u > 0
(τ − 1)u u ≤ 0

Let FY (y) = P (Y ≤ y) be the distribution function of a real valued random
variable Y. The τ th quantile of Y is given by:

QY (τ ) = F

−1(τ ) = inf{y : FY (y) ≥ τ}

It can be proved that the ˆy which minimizes Eρτ (y − ˆy) should satisfy that
FY (ˆy) = τ . Thus, quantile regression will ﬁnd the τ th quantile of a random
variable, for example:

Median(Y|X) = X ˆβ

qr

; ˆβ

qr

(cid:4)

ρ0.5(yi − XT

i b)

= arg min

b

324

F. Xin and Z. Abraham

For the purpose of the experiments conducted, we always used τ = 0.95 to rep-
resent extreme high value. Unlike the least squares methods mentioned above
which could be solved by numerical linear algebra, the solution to quantile re-
gression is relatively non-trivial. Linear programming is used to solve the loss
function by converting the problem to the following form.

{τ eT

Nu + (1 − τ )eT

Nv|Y − Xb = u − v; b ∈ Rp; u, v ∈ RN

}

+

min
u,v,b

For the same reason as mentioned in the Section 3.1, a classiﬁcation method
should be incorporated along with the regression model. We used logistic regres-
sion for classiﬁcation, and quantile regression on those nonzero Y . Finally, we
report the product of those two ﬁtted values. Quantile regression may return a
negative value, which we force to 0. We do this because precipitation is always
non-negative.

4 Framework for Integrated Classiﬁcation and Regression

Now that we have introduced quantile regression, which is an integral part
of our objective function we will elaborate the motivation behind the various
components of the proposed objective function. Since zero-inﬂated data is best
described with the help of a classiﬁer that help identify non-zero values and
a regression component to address non-zero values, our framework consists of
both components. For the classiﬁer component we use least square support vec-
tor machine and for the regression component, we use the intuition of quantile
regression to help focus the regression of extreme values. Since the ﬁnal predic-
tion of the data point using this framework is a product of the regression and
classiﬁcation component, the quantile regression component is built to work on
the eventual predicted return value, thereby integrating both the classiﬁer and
regression components.

4.1 Integrated Classiﬁer and Regression for Extreme Values(ICRE)

The classiﬁcation and regression models developed in this study are designed to
minimize the following objective function:

arg min
ω1,ω2

L(ω1, ω2) =

1
n

n(cid:4)

(1 − (2yi − 1)fi)2

(1)

i=1
n(cid:4)

i=1

+

1
n∗

yiρτ (y

i − f
(cid:4)

i × (fi + 1)/2) + λ(||ω1||2 + ||ω2||2)
(cid:4)

∗

where n

is the number of nonzero yi. Then it can be expanded as follows:

arg min
ω1,ω2

L(ω1, ω2) =

1
n

n(cid:4)

(1 − (2yi − 1)(xT

i ω2))2

(2)

i=1
n(cid:4)

+

1
n∗

yiρτ (y

i − (xT
(cid:4)
+ λ(||ω1||2 + ||ω2||2)

i=1

i ω1) × (sign((xT

i ω2 + 1)/2)))

Extreme Value Prediction for Zero-Inﬂated Data

325

The rationale for the design of our objective function is as follows. The ﬁrst
term which corresponds to the regression part of the equation represents quantile
regression performed for only the observed non-zero values in the time series.
The regression model is therefore biased towards estimating the non-zero extreme
values more accurately and not be adversely inﬂuenced by the over-abundance of
i × (fi + 1)/2 in the ﬁrst term, corresponds
(cid:4)
zeros in the time series. The product f
to the predicted output of our joint classiﬁcation and regression model. The sec-
ond term in the objective function, which is the main classiﬁcation component,
is equivalent to the least square support vector machine. And the last two terms
in the objective function are equivalent to the L2 norm used in ridge regression
models to shrink the coeﬃcients in ω1 and ω2.
We consider each data point to be a representative reading at an instance of
time t ∈ {1, 2,··· , n} in the time series. Each predictor variable is standardized
by subtracting its mean value and then dividing by its corresponding standard
deviation. The standardization of the variables is needed to account for the
varying scales.

The optimization method used while performing experiments is ’L-BFGS-
B’, described by Byrd et. al. (1995). It is a limited memory version of BFGS
methods. This method does not store a Hessian matrix, just a limited number
of update steps for it, and then it uses derivative information. Since our model
includes a quantile regression component, which is not diﬀerentiable, this method
of optimization is well suited to our objective function.

To solve the objective function, we used the inverse logistic function of xT

i ω2
instead of sign((xT
i ω2 + 1)/2)). The decision was motivated by the fact that the
optimizer tries to do a line search along the steepest descent direction and ﬁnds
the positive derivative along this line, which would result in a nearly ﬂat surface
for the binary component. Hence conversion of the binary report to an inverse
logistic function of xT
i ω2 was used to address this issue. During the prediction
stage, we use the binary-ﬁtted values from the SVM component.

5 Experimental Evaluation

In this section, the climate data that are used to downscale precipitation is de-
scribed. This is followed by the experiment setup. Once the dataset is introduced,
we analyzed the behavior of baseline models and contrasted them with ICRE
in terms of relative performance of the various models when applied to this real
world dataset to forecast future values of precipitation.

5.1 Data

All the algorithms were run on climate data obtained for 29 weather stations in
Canada, from the Canadian Climate Change Scenarios Network website [1]. The
response variable to be regressed (downscaled), corresponds to daily precipita-
tion values measured at each weather station. The predictor variables correspond
to 26 coarse-scale climate variables derived from the NCEP Reanalysis data set

326

F. Xin and Z. Abraham

and the H3A2a data set(computer generated simulations), which include mea-
surements of airﬂow strength, sea-level pressure, wind direction, vorticity, and
humidity. The predictor variables used for training were obtained from the NCEP
Reanalysis data set while the predictor variables used for the testing were ob-
tained from the H3A2a data set. The data span a 40-year period, 1961 to 2001.
The time series was truncated for each weather station to exclude days for which
temperature or any of the predictor values are missing.

5.2 Experimental setup

The ﬁrst step was to standardize the predictor variables by subtracting its mean
value and then dividing by its corresponding standard deviation to account for
their varying scales. The training size used was 10yrs worth of data and the
test size, 25yrs. During the validation process, the selection of the parameter λ
was done using the score returned by RMSE-95. Also, to ensure the experiments
replicated the real world scenario where the prediction for a future timeseries
needs to be performed using simulated values of the predictor variables for the
future time series, we used simulated values for the corresponding predictor
variables obtained from H3A2a climate scenario as XU , while XL are values
obtained from NCEP. All the experiments were run for 37 stations.

5.3 Baseline Algorithm

We compare the performance of ICRE with baseline models created using general
linear model(GLM), general linear model with classiﬁcation (GLM-C), quantile
regression(QR), quantile regression with classiﬁcation and zero-inﬂated Pois-
son(ZIP). Further details about the baselines are provided below.

General Linear Model (GLM). The baseline GLM refers to the generalized
linear model that uses a Poisson distribution as a link function, resulting in the
regression function log(λ) = Xβ, where E(Y |X) = λ

(cid:4)

General Linear Model with Classiﬁcation (GLM-C). Unlike the previous
baseline (GLM), GLM-C refers to a two step generalized linear model that uses
a Binomial distribution, for the classiﬁer with the model described as logit(p) =
= 0 when Y = 0
Xβ, and E(Y
and a second step that uses a generalized linear model with an exponential
distribution that is built only on non-zero response data points. The regression
function is log(λ) = Xβ, which E(Y |X) = λ. The eventual predicted value for
each data point is the product of the two respective ﬁtted values.

= 1|X) = p which Y

(cid:4)

= 1 when Y > 0 and Y

(cid:4)

Quantile Regression (QR). The baseline QR refers to the regular quantile
regression described earlier in the preliminary section 3

Extreme Value Prediction for Zero-Inﬂated Data

327

Quantile Regression with Classiﬁcation(QR-C). The baseline QR-C refers
to a two step model that has a GLM that uses a binomial distribution that acts
as a classiﬁer and a regular quantile regression model that is built on non-zero
valued data points as described earlier in the preliminary section. These two
models that comprise QR-C are built independent of each other and the eventual
predicted value for each data point is the product of the two respective ﬁtted
values.

Zero Inﬂated Poisson(ZIP). Zero Inﬂation Poisson model used as a baseline
and is similar to the ZIP model described in Section 3.

5.4 Evaluation Criteria

The motivation behind the selection of the various evaluation metrics was to
evaluate the diﬀerent algorithms in terms of predicting the magnitude and the
timing of the extreme events.The following criteria to evaluate the performance
of the models are used:

– Root Mean Square Error (RMSE), which measures the diﬀerence between

the actual and predicted values of the response variable, i.e.:

(cid:5) (cid:2)n

i−f(cid:2)

i=1(y(cid:2)
n

i fi)2

.

RMSE =

– RMSE-95, which we use to measure the diﬀerence between the actual and
predicted value of the response variable for only the extreme data points(j).
Extreme data points refer to the points whose actual value were 95 percentile
and above. The equation is with respect to 95 percentile, as throughout this
paper, we associate data points that are 95 percentile and above as extreme
values, i.e.:

(cid:6)

RMSE-95 =

(cid:2)n/20

j=1 (y(cid:2)
n/20

j−fif(cid:2)

j )2

.

– Confusion matrices will be computed to visualize the precision and recall of
extreme and non-extreme events. F-measure, which is the harmonic mean
between recall and precision values will be used as a score that evaluates the
precision and recall results.
F-measure = 2×Recall×P recision

Recall+P recision

To summarize, RMSE-95 is used for measuring magnitude and F-measure mea-
sures the correctness of the timing of the extreme events.

5.5 Experimental Results

The results section consists of two main sets of experiments. The ﬁrst set of
experiments evaluates the impact of zero-inﬂated data on modeling extreme
values. The second section compares the performance of ICRE with the baseline
methods which are followed .

328

F. Xin and Z. Abraham

Impact of Zero-Inﬂated Data on Extreme Value Prediction. Unlike
regular data which may be modeled using regression, modeling zero-inﬂated
data usually involves a classiﬁer and a regression component. The classiﬁer is
used to identify zero and non-zero values, which is followed by regression for
the non-zero values. But since the focus of the paper is on extreme data points
within zero-inﬂated data, the impact of the classiﬁer is unclear. In this section,
we compare the impact of including the classiﬁer in modeling extreme values of
zero-inﬂated data. We compared QR with QR-C and GCM with GCM-C and
show the results in Table 1. Note that the percentage of wins for F-measure,
recall, precision may not total to 100 in the case of a tie.

Table 1. Percentage of stations won

RMSE-95
F-Measure

QR-C
0
81.08

QR
100
18.92

GLM-C
67.57
18.92

GLM
32.43
35.13

As shown in the Table 1, it isn’t clear that using an independent classiﬁer
along with regression for modeling extreme values among zero inﬂated data is
preferred. But the results do indicate that the inclusion or exclusion of a classi-
ﬁer with the regression model built independent of each other may compromise
either RMSE-95 (by overestimating the magnitude) or F-measure (mistiming
predicting an extreme value), without necessarily compromising both together.

Comparison of ICRE to Baseline Methods. Table 2 shows the relative per-
formance of ICRE to all the baseline methods in terms of percentage of stations
outperformed against the baseline method in terms of RMSE-95 values calcu-
lated on extreme rain days. In terms of RMSE of extreme rain days, as shown in
Table 2, ICRE outperformed the baselines (except QR) in almost every one of
the 37 stations. But QR was the best across all methods for RMSE-95 of extreme
days. In terms of F-measure that was computed based on recall and precision of

Table 2. Percentage of stations ICRE outperformed the baseline

RMSE-95
F-Measure

QR-C
91.89
43.24

QR
0
62.16

GLM-C
97.3
89.19

GLM
97.3
89.19

ZIP
97.3
91.9

identifying extreme events, ICRE again outperformed the baselines(except QR-
C) in majority of the 37 stations. But ICRE was only able to outperform QR-C
in 16 or the 37 stations in terms of F-measure. Although QR performed the best
in terms of estimating magnitude for those extreme events, it over-estimate the
timing of the events as seen by the relatively lower F-measure score. QR-C did
the reverse, it did reasonably well in terms of modeling the timing, but performed
very poorly in terms of the magnitude of the events by overestimating.

Extreme Value Prediction for Zero-Inﬂated Data

329

6 Conclusions

This paper compare and analyze the performance of models created using vari-
ants of GLM, quantile regression and ZIP approaches to accurately predict values
for extreme data points that belong to a zero-inﬂated distribution. An alter-
nate framework(ICRE) was present that outperforms the baseline methods and
the eﬀectiveness of the model was demonstrated on climate data to predict the
amount of precipitation at a given station. For future work, we plan to extend
the framework to a semi-supervised setting.

References

1. Canadian Climate Change Scenarios Network, Environment Canada,

http://www.ccsn.ca/

2. Ancelet, S., Etienne, M.-P., Benot, H., Parent, E.: Modelling spatial zero-inﬂated
continuous data with an exponentially compound poisson process. Environmental
and Ecological Statistics (April 2009), doi:10.1007/s10651-009-0111-6

3. Kunkel, E.K., Andsager, K., Easterling, D.: Long-Term Trends in Extreme Pre-
cipitation Events over the Conterminous United States and Canada. J. Climate,
2515–2527 (1999)

4. Katz, R.: Statistics of extremes in climate change. Climatic Change, 71–76 (2010)
5. Gaetan, C., Grigoletto, M.: A hierarchical model for the analysis of spatial rainfall
extremes. Journal of Agricultural, Biological, and Environmental Statistics (2007)
6. Clarke, R.T.: Estimating trends in data from the Weibull and a generalized extreme

value distribution. Water Resources Research (2002)

7. Watterson, I.G., Dix, M.R.: Simulated changes due to global warming in daily
precipitation means and extremes and their interpretation using the gamma dis-
tribution. Journal of Geophysical Research (2003)

8. Booij, M.J.: Extreme daily precipitation in Western Europe with climate change

at appropriate spatial scales. International Journal of Climatology (2002)

9. Ghosh, S., Mallick, B.: A hierarchical Bayesian spatio-temporal model for extreme

precipitation events. Environmetrics (2010)

10. Dorland, C., Tol, R.S.J., Palutikof, J.P.: Vulnerability of the Netherlands and
Northwest Europe to storm damage under climate change. Climatic Change, 513–
535 (1999)

11. Cooley, D., Nychka, D., Naveau, P.: Bayesian spatial modeling of extreme proe-
cipitation return levels. Journal of the American Statistical Association, 824–840
(2007)

12. Clarke, R.T.: Estimating trends in data from the Weibull and a generalized extreme

value distribution. Water Resources Research (2002)

13. Wilby, R.L.: Statistical downscaling of daily precipitation using daily airﬂow and

seasonal teleconnection. Climate Research 10, 163–178 (1998)


