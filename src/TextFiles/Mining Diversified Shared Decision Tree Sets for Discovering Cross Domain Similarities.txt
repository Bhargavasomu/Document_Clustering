Mining Diversiﬁed Shared Decision Tree Sets

for Discovering Cross Domain Similarities

Guozhu Dong and Qian Han

Knoesis Center, and Department of Computer Science and Engineering,

Wright State University, Dayton, Ohio, USA

{guozhu.dong,han.6}@wright.edu

Abstract. This paper studies the problem of mining diversiﬁed sets of
shared decision trees (SDTs). Given two datasets representing two appli-
cation domains, an SDT is a decision tree that can perform classiﬁcation
on both datasets and it captures class-based population-structure simi-
larity between the two datasets. Previous studies considered mining just
one SDT. The present paper considers mining a small diversiﬁed set of
SDTs having two properties: (1) each SDT in the set has high qual-
ity with regard to “shared” accuracy and population-structure similarity
and (2) diﬀerent SDTs in the set are very diﬀerent from each other. A
diversiﬁed set of SDTs can serve as a concise representative of the huge
space of possible cross-domain similarities, thus oﬀering an eﬀective way
for users to examine/select informative SDTs from that huge space. The
diversity of an SDT set is measured in terms of the diﬀerence of the at-
tribute usage among the SDTs. The paper provides eﬀective algorithms
to mine diversiﬁed sets of SDTs. Experimental results show that the al-
gorithms are eﬀective and can ﬁnd diversiﬁed sets of high quality SDTs.

Keywords: Knowledge transfer oriented data mining, research by anal-
ogy, shared decision trees, cross dataset similarity, shared accuracy sim-
ilarity, matching data distribution similarity, tree set diversity.

1

Introduction

Shared knowledge structures across multiple domains play an essential role in
assisting users to transfer understanding between applications and to perform
analogy based reasoning and creative thinking [3,6,9,8,10], in supporting users
to perform research by analogy [4], and in assessing similarities between datasets
in order to avoid negative learning transfer [16]. Motivated by the above, Dong
and Han [5] studied the problem of mining knowledge structures shared by two
datasets, with a focus on mining a single shared decision tree. However, pro-
viding only one shared decision tree may present only a limited view of shared
knowledge structures that exist across multiple domains and does not oﬀer users
a concise representative of the space of possible shared knowledge structures.
Moreover, computing all possible shared decision trees is infeasible. The purpose
of this paper is to overcome the above limitations by studying the problem of
mining diversiﬁed sets of shared decision trees across two application domains.

V.S. Tseng et al. (Eds.): PAKDD 2014, Part II, LNAI 8444, pp. 534–547, 2014.
c(cid:2) Springer International Publishing Switzerland 2014

Mining Diversiﬁed Shared Decision Tree Sets

535

In a diversiﬁed set of shared decision trees, each individual tree is a high
quality shared decision tree in the sense that (a) the tree has high accuracy
in classifying data for each dataset and the tree has high cross-domain class-
distribution similarity at all tree nodes, and (b) diﬀerent trees are structurally
highly diﬀerent from each other in the sense that they use very diﬀerent sets
of attributes. The requirements in (a) will ensure that each shared decision
tree captures high quality shared knowledge structure between the two given
datasets, providing the beneﬁt that each root-to-leaf path in the tree corresponds
to a similar rule (having similar support and conﬁdence) for the two datasets
and the beneﬁt that the tree nodes describe similar data populations in the two
datasets connected by similar multi-node population relationships.

Presenting too many shared decision trees
imply that users will
to human users will
need to spend a lot of time to understand
those trees in order to select the ones most
appropriate for their application. Eﬃcient
algorithms solving the problem of mining di-
versiﬁed sets of shared decision trees meet-
ing the requirements in (b) can oﬀer a small
representative set of high quality shared de-
cision trees that can be understood without
spending a lot of time, hence allowing users
to more eﬀectively select the tree most ap-
propriate for their situation. Figure 1 illustrates the points given above, with
the six stars as the diversiﬁed representatives of all shared decision trees.

Fig. 1. Diversiﬁed Representatives
of SDT Space

The main contributions of this paper include the following: (1) The paper
motivates and formulates the diversiﬁed shared decision tree set problem. (2)
It presents two eﬀective algorithms to construct diversiﬁed high quality shared
decision tree sets. (3) It reports an extensive experimental evaluation on the
diversiﬁed shared decision tree set mining algorithms. (4) The shared decision
trees reported in the experiments are mined from high dimensional microarray
datasets for cancers, which can be useful to medical researchers.

The rest of the paper is organized as follows. Section 2 summarizes related
work. Section 3 formally introduces the diversiﬁed shared decision tree set prob-
lem and associated concepts. Sections 4 presents our algorithms for mining di-
versiﬁed shared decision tree sets. Section 5 reports our experimental evaluation.
Section 6 summarizes the results and discusses several future research problems.

2 Related Work

Limited by space, we focus on previous studies in four highly related areas.

Importance of Similarity/Analogy from Psychology and Cognitive
Science: Psychology/cognitive science studies indicate that analogy plays a vital
role in human thinking and reasoning, including creative thinking. For example,
Fauconnier [6] states that “Our conceptual networks are intricately structured by

536

G. Dong and Q. Han

analogical and metaphorical mappings, which play a key role in the synchronic
construction of meaning ...”. Gentner and Colhoun [9] state that “Much of hu-
mankind’s remarkable mental aptitude can be attributed to analogical ability.”
Gentner and Markman [10] suggest “that both similarity and analogy involve a
process of structural alignment and mapping.” Christie and Gentner [3] suggest,
based on psychological experiments, that “structural alignment processes are
crucial in developing new relational abstractions” and forming new hypothesis.

Learning Transfer: In learning transfer [16], it is typical to use available struc-
ture/knowledge of an auxiliary application domain to help build better classi-
ﬁers/clusters for a target domain where there is a lack of data with class label
or other domain knowledge. The constructed classiﬁers are not intended to cap-
ture cross-domain similarities. In contrast, our work focuses on mining (shared)
knowledge structures to capture cross domain similarity; this is a key diﬀerence
between learning transfer and our work. One of the intended uses of our mining
results is for direct human consumption. Our mining results can also be used
to assess cross domain similarity, to help avoid negative transfer where learning
transfer actually leads to poorer results, since learning transfer is a process that
is based on utilizing cross domain similarities that exist.

Shared Knowledge Structure Mining: Reference [4] deﬁned the cross do-
main similarity mining (CDSM) problem, and motivated CDSM with several
potential applications. CDSM has big potential in (1) supporting understand-
ing transfer and (2) supporting research by analogy, since similarity is vital to
understanding/meaning and to identifying analogy, and since analogy is a fun-
damental approach frequently used in hypothesis generation and in research.
CDSM also has big potential in (3) advancing learning transfer since cross do-
main similarities can shed light on how to best adapt classiﬁers/clusterings across
given domains and how to avoid negative transfer. CDSM can also be useful for
(4) solving the schema/ontology matching problem. Reference [5] motivated and
studied the shared decision tree mining problem, but that paper focused on min-
ing just one shared decision tree. Several concepts of this paper were borrowed
from [5], including shared accuracy, data distribution similarity, weight vector
pool (for two factors), and information gain for two datasets. We enhance that
paper by considering mining diversiﬁed sets of shared decision trees.

Ensemble Diversity: Much has been done on using ensemble diversity among
member classiﬁers [14] to improve ensemble accuracy, including data based di-
versity approaches such as Bagging [2] and Boosting [7]. However, most previ-
ous studies in this area focused on classiﬁcation behavior diversity, in the sense
that diﬀerent classiﬁers make highly diﬀerent classiﬁcation predictions. Several
studies used attribute usage diversity to optimize ensemble accuracy, in a less
systematic manner, including the random subspace method [13] and the distinct
tree root method [15]. Our work focuses on attribute usage diversity aimed at
providing diversiﬁed set of shared knowledge structures between datasets. The
concept of attribute usage diversity was previously used in [12], to improve clas-
siﬁer ensemble diversity and classiﬁcation accuracy for just one dataset.

Mining Diversiﬁed Shared Decision Tree Sets

537

3 Problem Deﬁnition

In this section, we introduce the problem of mining diversiﬁed sets of shared
decision trees. To that end, we also need to deﬁne a quality measure on diversiﬁed
sets of shared decision trees, which is based on the quality of individual shared
decision trees and on the diversity measure for sets of shared decision trees.

As mentioned earlier, a shared decision tree (SDT) for a given dataset pair

(D1 : D2) is a decision tree that can classify both data in D1 and data in D2.

We assume that D1 and D2 are datasets with (1) an identical set of class names
and (2) an identical set1 of attributes. Our aim is to mine a small diversiﬁed set
of high quality SDTs with these properties: (a) each tree in the set (1) is highly
accurate in each Di and (2) has highly similar data distribution in D1 and D2,
and (b) diﬀerent trees in the set are highly diﬀerent from each other.

3.1 Shared Accuracy and Data Distribution Similarity of SDT Set

The shared accuracy and data distribution similarity measures for shared deci-
sion tree (SDT) sets are based on similar measures deﬁned for individual SDTs
[5], which we will review below. Let T be an SDT for a dataset pair (D1 : D2),
and let AccDi (T ) denote T ’s accuracy2 on Di.

Deﬁnition 1. The shared accuracy of T (denoted by SA(T )) is deﬁned as the
minimum of T ’s accuracies on D1 and D2: SA(T ) = min(AccD1 (T ), AccD2 (T )).

The data distribution similarity of T reﬂects population-structure (or class
distribution) similarity between the two datasets across the nodes of T . The
class distribution vector of Di at a tree node V is deﬁned by

CDVi(V ) = (Cnt(C1, SD(Di, V )), Cnt(C2, SD(Di, V ))),

where Cnt(Cj , SD(Di, V )) = |{t ∈ SD(Di, V ) | t’s class is Cj}|, and SD(Di, V )
is the subset of Di for V (satisfying the conditions on the root-to-V path). The
distribution similarity (DSN) at V is deﬁned as DSN (V ) = CDV 1(V )·CDV 2(V )
(cid:3)CDV 1(V )(cid:3)·(cid:3)CDV 2(V )(cid:3) .

Deﬁnition 2. The data distribution similarity of an SDT T over (D1 : D2) is
deﬁned as DS(T ) = avgV DSN (V ), where V ranges over nodes of T .

We can now deﬁne SA and DS for shared decision tree sets.

Deﬁnition 3. Let T S be a set of SDTs over dataset pair (D1 : D2). The shared
classiﬁcation accuracy of T S is deﬁned as SA(T S) = avgT∈T SSA(T ), and the
data distribution similarity of T S is deﬁned as DS(T S) = avgT∈T SDS(T ).
1 If D1 and D2 do not have identical classes and attributes, one will need to identify
an 1-to-1 mapping between the classes of the two datasets, and an 1-to-1 mapping
between the attributes of the two datasets. The 1-to-1 mappings can be real or
hypothetical (for “what-if” analysis) equivalence relations on the classes/attributes.
2 When Di is small, one may estimate AccDi (T ) directly using Di. Holdout testing

can be used when the datasets are large.

538

G. Dong and Q. Han

3.2 Diversity of SDT Set

To deﬁne the diversity of SDT sets, we need to deﬁne tree-pair diﬀerence, and
we need a way to combine the tree-pair diﬀerences for all possible SDT pairs.3
We measure the diﬀerence between two SDTs in terms of their attribute us-
age summary (AUS). Let A1, A2, . . . , An be a ﬁxed list enumerating all shared
attributes of D1 and D2.
Deﬁnition 4. The level-normalized-count AUS (AUSLNC) of an SDT T over
(D1 : D2) is deﬁned as

AUSLNC(T ) = (

CntT (A1)

avgLvlT (A1)

, . . . ,

CntT (An)

avgLvlT (An)

),

where CntT (Ai) denotes the number of occurrences of attribute Ai in T , and
(cid:5)
avgLvlT (Ai) denotes the average level of A
is occurrences in T .

The root is at level 1, the children of the root are at level 2, and so on. In the
AUSLNC measure, nodes near the root have high impact since those nodes have
small level number and attributes used at those nodes often have small avgLvlT .
One can also use the level-listed count (AUSLLC) approach for AUS. Here we
use a matrix in which each row represents the attribute usage in one tree level:
Given an SDT T with L levels, for all attributes Ai and integers l satisfying 1 ≤
l ≤ L, the (l, i) component of AUSLLC has the value CntT (l, Ai) (the occurrence
frequency count for attribute Ai in the lth level of T ).
Remark: AUSLNC pays more attention to nodes near the root, while AUSLLC gives
more emphasis to levels near the leaves (there are many nodes at those levels).

Deﬁnition 5. Given an attribute usage summary measure AUSμ, the tree pair
diﬀerence (TPD) for two SDTs T1 and T2 is deﬁned as

TPDμ(T1, T2) = 1 − AUSμ(T1) · AUSμ(T2)

(cid:4)AUSμ(T1)(cid:4) · (cid:4)AUSμ(T2)(cid:4) .

We can now deﬁne the SDT set diversity concept.

Deﬁnition 6. Given an SDT set T S and an AUS measure AUSμ, the diversity
of T S is deﬁned as TDμ(T S) = avg{TPDμ(Ti, Tj) | Ti, Tj ∈ T S, and i (cid:5)= j}.

3.3 Diversiﬁed Shared Decision Tree Set Mining Problem

To mine desirable diversiﬁed SDT sets, we need an objective function. This
section deﬁnes our objective function, which combines the quality of the SDTs
and the diversity among the SDTs.
Deﬁnition 7. Given an attribute usage summary method AUSμ, the quality
score of an SDT set T S is deﬁned as:
SDTSQµ(T S) = min(SA(T S), DS(T S), TDµ(T S)) ∗ avg(SA(T S), DS(T S), TDµ(T S)).
3 We borrow the diversity concepts from [12], which considered mining diversiﬁed

decision tree ensembles for one dataset.

Mining Diversiﬁed Shared Decision Tree Sets

539

We also considered other deﬁnitions using e.g. average, weighted average, and
the harmonic mean of the three factors. They were not selected, since they give
smaller separation of quality scores or require parameters from users. The above
formula is chosen since it allows each of SA, DS, and TD to play a role, it is
simple to compute, and it does not require any parameters from users.

We now turn to deﬁning the diversiﬁed SDT set mining problem.

Deﬁnition 8 (Diversiﬁed Shared Decision Tree Set Mining Problem).
Given a dataset pair (D1:D2) and a positive integer k, the diversiﬁed shared
decision tree set mining problem ( KSDT) is to mine a diversiﬁed set of k SDTs
with high SDTSQ from the dataset pair.

An example SDT set mined from two cancer datasets will be given in § 5.

4 KSDT-Miner

This section presents two KSDT mining algorithms, for mining diversiﬁed high
quality SDT sets. One is the parallel KSDT-Miner (PKSDT-Miner), which builds
a tree set concurrently and splits one-node-per-tree in a round-robin fashion; the
other one is the sequential KSDT-Miner (SKSDT-Miner), which mines a tree set
by building one complete tree after another.

In comparison, PKSDT-Miner gives all SDTs almost equal opportunity4 in
selecting desirable attributes for use in high impact nodes near the roots of SDTs,
whereas SKSDT-Miner gives SDTs built earlier more possibilities in selecting
desirable attributes (even for use at low impact nodes near the leaves), which
deprives the chance of later SDTs in using those attributes at high impact nodes.
Limited by space and due to the similarity in most ideas except the node split

order, we present PKSDT-Miner and omit the details of SKSDT-Miner.

4.1 Overview of PKSDT-Miner

PKSDT-Miner builds a set of SDTs in parallel, in a node-based round-robin
manner. In each of the round-robin loop, the trees are processed in an ordered
manner; for each tree, one node is selected and spilt. Figure 2 illustrates with
two consecutive states in such a loop: 2(a) gives three (partial) trees (blank
rectangles are nodes to be split), and 2(b) gives those trees after splitting node
V2 of T2. Here, PKSDT-Miner splits node V2 in T2 even though V1 in T1 can be
split. PKSDT-Miner will select a node in T3 to split next.

4.2 Aggregate Tree Diﬀerence

To build highly diversiﬁed tree sets, the aggregate tree diﬀerence (ATD) is used
to measure the diﬀerences between a new/modiﬁed tree T and the set of other
trees T S. One promising approach is to deﬁne ATD as the average of (cid:2) smallest

4 An attribute may be highly desirable in more than one tree.

540

G. Dong and Q. Han

(a) State p

(b) State p+1

Fig. 2. PKSDT-Miner Builds Trees in Round-robin Manner

(cid:5)

) values (T

(cid:5) ∈ T S). We deﬁne a new aggregation function called
TPDμ(T, T
avgmin(cid:2), where avgmin(cid:2)(S) is the average of the (cid:2) smallest values in a set S
of numbers. (In experiments our best choice for (cid:2) was 3.) Then the μ-(cid:2)-minimal
aggregate tree diﬀerence (ATDμ,avgmin(cid:2)) is deﬁned as the average TPDμ between
T and the (cid:2) most similar trees in T S:

ATDμ,avgmin(cid:2)(T, T S) = a vgmin(cid:2)({TPDμ(T, T

(cid:5)

)|T

(cid:5) ∈ T S}).

PKSDT-Miner selects an attribute and a value to split a tree node by maximiz-
ing an objective function IDT that combines information gain (to be denoted
by IG and deﬁned below) and data distribution similarity (DS) on two datasets,
and aggregate tree diﬀerence (ATD) between the current tree and the other trees.
To tradeoﬀ the three factors, they are combined using a weighted sum based on a
weight vector w = (wIG, wDS, wAT D) whose three weights are required to satisfy
0 < wIG, wDS, wAT D < 1 and wIG + wDS + wAT D = 1.
Given an AUS measure μ, an aggregation method α ∈ {avg, min, avgmin(cid:2)},

a tree T and a tree set T S, the μ-α aggregate tree diﬀerence is deﬁned as

(cid:5)

ATDμ,α(T, T S) = α({TPDμ(T, T

) | T
For example, ATDμ,min(T, T S) = minT (cid:2)∈T S(TPDμ(T, T
)) when α is min. Each
variant of ATD can be used in our two SDT set mining algorithms, resulting a
number of variant algorithms. For instance, the standard version of the PKSDT-
Miner algorithm can be written as PKSDT-Miner(LNC,avgmin(cid:2)) and we can
replace LNC by LLC to get PKSDT-Miner(LLC,avgmin(cid:2)).

(cid:5) ∈ T S}).
(cid:5)

4.3 IG for Two Datasets

This paper uses the union-based deﬁnition of IG for two datasets of [5]. ([5]
discussed other choices and the union-based way was shown to be the best by
(cid:5)
experiments.) For each attribute A and split value a, and dataset pair (D
2)
(associated with a given tree node), the union-based information gain is deﬁned
(cid:5)
2). (The IG function is overloaded: IG in
as IG(A, a, D
the LHS is 4-ary while IG in the RHS is 3-ary.) IG(A, a, D
) is deﬁned in terms
of entropy, as used for decision tree node splitting.

(cid:5)
2) = IG(A, a, D

(cid:5)
1 : D

∪ D

(cid:5)
1, D

(cid:5)
1

(cid:5)

Mining Diversiﬁed Shared Decision Tree Sets

541

4.4 The Algorithm
PKSDT-Miner has six inputs: Two datasets D1 and D2, a set AttrSet of candi-
date attributes that can be used in shared trees, a dataset size threshold M inSize
for node-splitting termination, a weight vector w (wIG on information gain, wDS
on data distribution similarity, wAT D on aggregate tree diﬀerence), and an in-
teger k for the desired number of trees. PKSDT-Miner calls PKSDT-SplitNode
(Function 1) to split nodes for each tree.

Algorithm 1. PKSDT-Miner
Input: (D1 : D2): Two datasets

AttrSet: Set of candidate attributes that can be used
M inSize: Dataset size threshold for splitting termination
w = (wIG, wDS, wAT D): A weight vector on IG, DS, and AT D
k: Desired number of trees

Repeat

For p = 1 to k do

Output: A diversiﬁed shared decision tree set T S for (D1 : D2).
Method:
1. Create root node Vp for each tree Tp (1 ≤ p ≤ k);
2.
3.
4.
5.
6.
7. Output the diversiﬁed shared decision tree set T S.

Until there are no more trees with nodes that can be splitb;

let node V be the next nodea of tree Tp to split;
Call PKSDT-SplitNode(T, V, D1, D2, AttrSet, T S, M inSize, w);

a The next node of a tree to split is determined by a tree traversal method, which can

be depth ﬁrst, breadth ﬁrst, and so on. We use depth ﬁrst here.

b No more node to split means that all candidate split nodes satisfy the termination

conditions deﬁned in Function ShouldTerminate.

PKSDT-SplitNode splits the data of a node V of a tree T by picking the
split attribute and split value that optimize the IDT score. Let T be the tree
that we wish to split, and let T S be the other trees that we have built. Let
V be T ’s current node to split, and A and aV be resp. a candidate splitting
attribute/value. Let T (A, aV ) be the tree obtained by splitting V using A and
aV . Then the IDT scoring function is deﬁned by:

IDT (T (A, aV ), T S) = wIG ∗ IG(A, aV )+
wDSN ∗ DSN (A, aV ) + wAT D ∗ AT D(T (A, aV ), T S),

where IG(A, aV ) is the information gain for V when split by A and aV ,
DSN (A, aV ) is the average DSN value of the two children nodes of V .

Function ShouldTerminate determines if nodes splitting should terminate. (Our
algorithms aim to build simple trees and avoid “overﬁtting”.) It uses two
techniques. (1) When many attribtues are available, we restrict the candidate
attributes to those whose IG is ranked high in both datasets, so avoiding non-
discriminative attributes that are locally discriminative at a given node. (2) We
stop splitting for a given tree node when at least one dataset is small or pure.

542

G. Dong and Q. Han

(cid:2)
1, D

(cid:2)
2, AttrSet, T S, M inSize, w)

(cid:2)
2, M inSize, AttrSet) then assign

(cid:2)
1, D
(cid:2)
1 and D

(cid:2)
2 as class label of V and return;
2. Select the attribute B and value bV that maximize IDT , that is

Function 1. PKSDT-SplitNode(T, V, D
1. If ShouldTerminate(V, D
the majority class in D
IDT (T (B, bV ), T S)=max{IDT (T (A, aV ), T S) | A ∈ AttrSet, and aV
3. Create left child node Vl of V , with “B ≤ bV ” as the corresponding

is a common candidate split value for A at V };
il = {t ∈ D
i | t satisﬁes “B ≤ bV ”} for i = 1, 2;
(cid:2)
(cid:2)
4. Create right child node Vr of V , with “B > bV ” as the corresponding
i | t satisﬁes “B > bV ”} for i = 1, 2.
ir = {t ∈ D
(cid:2)
(cid:2)

edge’s label, and let D

edge’s label, and let D

4.5 Weight Vector Pools

Diﬀerent dataset pairs have diﬀerent characteristics concerning IG, DS and ATD.
To mine the best SDT set, we need to treat diﬀerent characteristics using appro-
priate focus/bias. (It is open if one can determine the characteristics of a dataset
pair without performing SDT set mining.) We solve the problem by using a pool
of weight vectors to help mine (near) optimal SDTs eﬃciently. Such a pool is a
small representative set of all possible weight vectors.

We consider two possible weight vector pools: W V P1 contains 36 weight
vectors, deﬁned by W V P1 = {x | x is a multiple of 0.1 and 0 < x < 1}.
W V P2={(0.1,0.1,0.8),(0.1,0.3,0.6),(0.1,0.5,0.4),(0.1,0.7,0.2),(0.3,0.1,0.6), (0.3,
0.4, 0.3),(0.3,0.5,0.2),(0.5,0.1,0.4),(0.5,0.3,0.2),(0.7,0.2,0.1)}. So W V P2 contains
10 representative vectors selected from W V P1. For each of the three factors,
each pool contains some vectors where the given factor plays the dominant role.

5 Experimental Evaluation

This section uses experiments to evaluate KSDT-Miner, using real-world and also
(pseudo) synthetic datasets. It reports that (1) PKSDT-Miner tends to build more
diversiﬁed high quality SDT sets on average, which conﬁrms the advantages
of PKSDT-Miner analyzed in Section 4, and (2) KSDT-Miner is scalable w.r.t.
number of tuples/attributes/trees. It discusses (3) how KSDT-Miner performs
concerning the use of weight vectors. It also examines (4) how KSDT-Miner
performs when it uses diﬀerent AUSμ and ATDμ,α measures. Finally, it reports
that KSDT-Miner outperforms SDT-Miner regarding mining one single SDT.
In the experiments, we set (cid:2) = 3, M inSize = 0.02 ∗ min(|D1|,|D2|) and
AttrSet = {A | rank1(A) + rank2(A) is among the smallest 20% of all shared
attributes, where ranki(A) is the position of A when Di’s attributes are listed in
decreasing IG order}. Experiments were conducted on a 2.20 GHz AMD Athlon
with 2 GB memory running Windows XP, with codes implemented in Matlab.
To save space, the tables may list results on subsets of the 15 dataset pairs on
the 6 microarray datasets, although the listed averages are for all 15 pairs.

Mining Diversiﬁed Shared Decision Tree Sets

543

5.1 Datasets and Their Preprocessing

Our experiments used six real-world
microarray gene expression datasets
for cancers.5 ArrayTrack [20] was used
to identify shared (equivalent) at-
tributes. Two genes are shared if they
represent the same gene in diﬀerent
gene name systems. Table 1 lists the
number of shared attributes for the 15
dataset pairs.

Table 1. Number of Shared Attributes
(NSA) between Dataset Pairs

Dataset Pair
NSA
(BC:CN), (BC:DH), (BC:LM) 5114
(CN:DH), (CN:LM), (DH:LM) 7129
(CN:LB), (DH:LB), (LB:LM) 5313
(CN:PC), (DH:PC), (LM:PC) 5317
8123
(BC:LB)
8124
(BC:PC)
(LB:PC)
9030

In some dataset pairs

the two
datasets have very diﬀerent class ra-
tios. The class ratio of a dataset is
likely an artifact of the data collection process and may not have practical im-
plications. However, class ratio diﬀerence can make it hard to compare quality
values for results mined from diﬀerent dataset pairs. To address this, we use
sampling with replacement method to replicate tuples so that class ratios for
the two datasets are nearly the same.

5.2 Example Diversiﬁed SDT Set Mined from (DH:LM)

We now give6 an example diversiﬁed set of two shared decision trees mined from
real (cancer) dataset pair (DH:LM) in Figures 3 and 4. For each tree, data in
two datasets have very similar distributions at tree nodes (the average DSN for
each tree is about 0.977) and the leaf nodes are very pure with average shared
classiﬁcation accuracy of leaf nodes being about 0.963. For the diversiﬁed tree

set {T1, T2}, tree diversity is 1 since these two trees don’t share any splitting
attributes, and the SDTSQ is about 0.944.

5.3 KSDT-Miners Mine Diversiﬁed High Quality SDT Sets

Experiments show that KSDT-Miners are able to mine diversiﬁed high quality
SDT sets. Table 2 lists the statistics of best SDT sets mined by either PKSDT-
Miner or SKSDT-Miner from each of the 15 dataset pairs in Table 1 (using all
weight vectors). For the 15 dataset pairs, PKSDT-Miner got the best SDT sets
in 9 pairs, and SKSDT-Miner got the best in 6 pairs. We include the result for
(BC: LM) to indicate that it is not possible to always have high quality SDTs
(as expected).

5 References for the datasets: BC (breast cancer) [21], CN (Central Nervous Sys-
tem) [17], DH (DLBCL-Harvard*) [18], LB (Lung Cancer-BAWH) [11], LM (Lung
Cancer-Michigan*) [1], PC (Prostate Cancer) [19].

6 We draw shared decision tree ﬁgures as follows: For each node V , we show CDV1(V )
s right, and show DSN (V ) below V .

s left, show CDV2(V ) for D2 at V

for D1 at V

(cid:2)

(cid:2)

544

G. Dong and Q. Han

Fig. 3. Shared Decision Tree T1

Fig. 4. Shared Decision Tree T2

Table 2. Stats of Best SDT Sets

Table 3. PKSDT(P) vs SKSDT(S)

Dataset Pair DS SA TD SDTSQ

(BC: CN) 0.98 0.98 0.99
(BC: DH) 0.98 0.97 1
(BC: LB) 0.97 0.97 1
(BC: LM) 0.94 0.74 1
(BC: PC) 0.97 0.97 1
(CN: PC) 0.98 0.98 0.99
Average*
0.96 0.95 0.99

0.96
0.95
0.95
0.67
0.95
0.96
0.92

Dataset TD TD SDTSQ SDTSQ

Pair

P

S

P

(BC: CN) 0.97 0.97
0.93
(BC: LB) 0.94 0.91 0.88*
0.61
(BC: LM) 0.92 0.92
(BC: PC) 0.95 0.94
0.89
(CN: LB) 0.98 0.97
0.93
(DH: LB) 0.96 0.95 0.91*
Average* 0.94 0.93
0.86

S

0.93
0.85
0.63*
0.88
0.92
0.88
0.85

5.4 Comparison between PKSDT-Miner and SKSDT-Miner
Experiments show that PKSDT-Miner is better than SKSDT-Miner. Indeed,
PKSDT-Miner gets SDT sets of higher quality values on average, albeit slightly,
and it never gets SDT sets of lower quality values (see Table 3, which gives the
average TD and SDTSQ values for best diversiﬁed tree sets mined by PKSDT-
Miner and SKSDT-Miner respectively when using all weight vectors). As noted for
Table 2, PKSDT-Miner got the best SDT sets in 9, whereas SKSDT-Miner got the
best in only 6, out of the 15 dataset pairs. Below we only consider PKSDT-Miner.

5.5 Comparison of AUS and ATD Variants
Experimental results demonstrate that (1) AUSLNC produces better results than
AUSLLC, and (2) ATDμ,avgmin(cid:2) outperforms ATDμ,min (reason: when it is used a
highly similar outlier may give too much inﬂuence) and ATDμ,avg (reason: when
it is used the highly dissimilar cases may give big inﬂuence). The details are
omitted to save space.

5.6 Weight Vector Issues

We examined the “best” and “worst” (wIG,wDS,wAT D) weight vectors, which
produce the SDT sets with the highest and lowest SDTSQ mined by PKSDT-
Miner(LNC,avgmin(cid:2)). (1) We observed that the average relative improvement of

Mining Diversiﬁed Shared Decision Tree Sets

545

the “best” over the “worst” is an impressive 4.8% and the largest is 20.5%. This
indicates that the choice of weight vector has signiﬁcant impact on the tree set
quality mined by KSDT-Miner. (2) We also saw that no single weight vector is
the best weight vector for all dataset pairs. This reﬂects the fact that diﬀerent
dataset pairs have diﬀerent characteristics regarding which of IG, DS and AT D
is most important.

Regarding which weight vectors may be better suited for which kinds of
dataset pairs, we observed that there are three cases. (A) For some dataset
pairs (e.g. (LM:PC)), weight vectors with high IG weight (and low DS weight,
low AT D weight) tend to yield SDT sets with high SDTSQ. (B) For some dataset
pairs (e.g. (BC:DH)), weight vectors with high DS weight tend to yield SDT sets
with high SDTSQ. (C) For some dataset pairs (e.g. (BC:CN)), weight vectors
with high AT D weight tend to yield SDT sets with high SDTSQ.

Experiment showed that using multiple weight vectors leads to much better
performance than using a single weight vector. Moreover, SDTSQ scores of best
SDT sets obtained using W V P2 are almost identical to those obtained using
W V P1. Since W V P2 is smaller (having 10 weight vectors) than W V P1 (having
36 weight vectors), W V P2 is preferred since it requires less computation time.

5.7 KSDT-Miner Outperforms SDT-Miner on SDTQ

Both KSDT-Miner and SDT-Miner can be used to mine a single high quality
SDT, by having KSDT-Miner return the best tree in the SDT set it constructs.
Experiments show that KSDT-Miner gives better performance than SDT-Miner.
Indeed, the average relative SDTQ improvement by KSDT-Miner over SDT-Miner
for all dataset pairs is 13.8%. For some dataset pairs, the relative improvement is
about 45.3%. Through more detailed comparison, the average relative improve-
ment on DS by KSDT-Miner over SDT-Miner for all dataset pairs is 3.2%, and
on SA is 5.4%. Clearly, better single SDT can be mined when tree set diversity
is considered.

5.8 Scalability of KSDT-Miner

We experimented to see how KSDT-Miner’s execution time changes when the
number of tuples/attributes/trees increases. Experiments show that execution
time increases roughly linearly. (The ﬁgure is omitted to save space.) The experi-
ments used synthetic datasets obtained by replicating tuples with added random
noises up to a bound given by P % of the maximum attribute value magnitude
(in order to get a desired number N of tuples), and by attribute elimination.

5.9 Using Fewer Attributes Leads to Poor SDT Sets

Incidently, we compared the SDTSQ of SDT sets mined from real dataset pairs
using all available attributes against those obtained from projected data using
fewer attributes (e.g. 100). On average, SDTSQ using all attributes is about
34.2% better than SDTSQ using only the ﬁrst 100 attributes.

546

G. Dong and Q. Han

6 Concluding Remarks and Future Directions

In this paper we motivated the diversiﬁed shared decision tree set mining prob-
lem, presented two algorithms of KSDT-Miner, and evaluated the algorithms us-
ing real microarray gene expression data for cancers and using synthetic datasets.
Experimental results show that KSDT-Miner can eﬃciently mine high quality
shared decision trees. Future research directions include mining other types of
shared knowledge structures (including those capturing alignable diﬀerences, de-
ﬁned as shared knowledge structures that capture cross-domain similarities and
cross-domain diﬀerences within the context of the similarities given elsewhere
in the shared knowledge structures) and utilizing such mined results to solve
various research and development problems in challenging domains.

References

1. Beer, D.G., et al.: Gene-expression proﬁles predict survival of patients with lung

adenocarcinoma. Nature Medicine 8, 816–824 (2002)

2. Breiman, L.: Bagging predictors. Machine Learning 24(2), 123–140 (1996)
3. Christie, S., Gentner, D.: Where hypotheses come from: Learning new relations by
structural alignment. Journal of Cognition and Development 11(3), 356–373 (2010)
4. Dong, G.: Cross domain similarity mining: Research issues and potential applica-
tions including supporting research by analogy. ACM SIGKDD Explorations (June
2012)

5. Dong, G., Han, Q.: Mining accurate shared decision trees from microarray gene ex-
pression data for diﬀerent cancers. In: International Conference on Bioinformatics
and Computational Biology, BIOCOMP 2013 (2013)

6. Fauconnier, G.: Mappings in Thought and Language. Cambridge University Press

(1997)

7. Freund, Y., Schapire, R.E.: Experiments with a new boosting algorithm. In: ICML,

pp. 148–156 (1996)

8. Gentner, D.: Structure mapping: A theoretical framework for analogy. Cognitive

Science 7, 155–170 (1983)

9. Gentner, D., Colhoun, J.: Analogical processes in human thinking and learning.
In: Glatzeder, B., Goel, V., von M¨uller, A. (eds.) Towards a Theory of Thinking.
On Thinking, vol. 2. Springer, Heidelberg (2010)

10. Gentner, D., Markman, A.B.: Structure mapping in analogy and similarity. Amer-

ican Psychologist 52(1), 45–56 (1997)

11. Gordon, G.J., et al.: Translation of microarray data into clinically relevant cancer
diagnostic tests using gene expression ratios in lung cancer and mesothelioma.
Cancer Research 62, 4963–4967 (2002)

12. Han, Q., Dong, G.: Using attribute behavior diversity to build accurate decision
tree committees for microarray data. J. Bioinformatics and Computational Biol-
ogy 10(4) (2012)

13. Ho, T.K.: The random subspace method for constructing decision forests. IEEE
Transactions on Pattern Analysis and Machine Intelligence 20(8), 832–844 (1998)
14. Kuncheva, L.I., Whitaker, C.J.: Measures of diversity in classiﬁer ensembles and
their relationship with the ensemble accuracy. Machine Learning 51(2), 181–207
(2003)

Mining Diversiﬁed Shared Decision Tree Sets

547

15. Li, J., Liu, H.: Ensembles of cascading trees. In: ICDM, pp. 585–588 (2003)
16. Pan, S.J., Yang, Q.: A survey on transfer learning. IEEE Transactions on Knowl-

edge and Data Engineering 22(10), 1345–1359 (2010)

17. Pomeroy, S.L., et al.: Prediction of central nervous system embryonal tumour out-

come based on gene expression. Nature 415, 436–442 (2002)

18. Shipp, M.A., et al.: Diﬀuse large b-cell lymphoma outcome prediction by gene-
expression proﬁling and supervised machine learning. Nature Medicine 8, 68–74
(2002)

19. Singh, D., et al.: Gene expression correlates of clinical prostate cancer behavior.

Cancer Cell 1(2), 203–209 (2002)

20. Tong, W., et al.: ArrayTrack-Supporting toxicogenomic research at the FDA’s Na-
tional Center for Toxicological Research (NCTR). EHP Toxicogenomics 111(15),
1819–1826 (2003)

21. Van’t Veer, L.J., et al.: Gene expression proﬁling predicts clinical outcome of breast

cancer. Nature 415, 530–536 (2002)


