Balancing the Analysis of Frequent Patterns

Arnaud Giacometti, Dominique H. Li, and Arnaud Soulet

Universit´e Fran¸cois Rabelais Tours, LI EA 6300

3 Place Jean Jaur`es, F-41029 Blois, France

firstname.lastname@univ-tours.fr

Abstract. A main challenge in pattern mining is to focus the discovery
on high-quality patterns. One popular solution is to compute a numerical
score on how well each discovered pattern describes the data. The best
rating patterns are then the most analyzed by the data expert. In this
paper, we evaluate the quality of discovered patterns by anticipating
of how user analyzes them. We show that the examination of frequent
patterns with the notion of support led to an unbalanced analysis of the
dataset. Certain transactions are indeed completely ignored. Hence, we
propose the notion of balanced support that weights the transactions
to let each of them receive user speciﬁed attention. We also develop an
algorithm Absolute for calculating these weights leading to evaluate
the quality of patterns. Our experiments on frequent itemsets validate
its eﬀectiveness and show the relevance of the balanced support.

Keywords: Pattern mining, stochastic model, interestingness measure.

1

Introduction

For twenty years, the pattern mining algorithms have gained performance and
now arrive to quickly extract patterns from large amounts of data. However,
evaluate and ensure the quality of extracted patterns remains a very open issue.
In general, a pattern is considered to be relevant if it deviates from what was
expected from a knowledge model. In the literature, there are two broad cate-
gories of knowledge models [1,2,3]: user-driven and data-driven ones. User-driven
approaches discover interesting patterns with subjective models based on user
oriented information, such as domain knowledge, beliefs or preferences. Data-
driven approaches discover interesting patterns with objective models based on
the statistical properties applied to data, such as frequency of patterns. Most
often these methods neglect how the user will analyze the collection of patterns.
In this paper, we present a novel approach, named analysis-driven, to evaluate
discovered patterns by foreseeing how the collection will be analyzed.
Before presenting in depth our motivations, we ﬁrst recall the context of fre-
quent itemset mining [4]. Let I be a set of distinct literals called items, an item-
set corresponds to a non-null subset of I. A transactional dataset is a multi-set
of itemsets, named transactions. Table 1 (a) presents such a dataset D where
4 transactions t1, . . . , t4 are described by 4 items A, . . . , D. The support of an

V.S. Tseng et al. (Eds.): PAKDD 2014, Part I, LNAI 8443, pp. 53–64, 2014.
c(cid:2) Springer International Publishing Switzerland 2014

54

A. Giacometti, D.H. Li, and A. Soulet

itemset X in a dataset D is the fraction of transactions containing X. An itemset
is frequent if it appears in a database with support no less than a user speciﬁed
threshold. For instance, Table 1 (b) shows the frequent itemsets of D (with 0.25
as minimal support) that are ranked according to the support.

Table 1. An unbalanced analysis of a dataset by frequent patterns

(a) Dataset D (b) The mined pattern set P (c) Analysis resulting from the scoring

TID Items
t1
t2
t3
t4 D

AB
AC
BC

PID Pattern Support
p1 A
p2 B
p3
C
p4 D
p5 AB
p6 AC
p7 BC

0.50
0.50
0.50
0.25
0.25
0.25
0.25

Reality

Expected

TID Length Proportion Π Preference ρ
t1
t2
t3
t4

> 0.25
> 0.25
> 0.25
< 0.25

1.25
1.25
1.25
0.25

0.3125
0.3125
0.3125
0.0625

Sum:

4

1

1

In a post-analysis process based on the scoring of patterns, we assume that
an analyst examines each pattern with a diligence proportional to an interest-
ingness measure (here, the support) for analyzing the dataset (we will justify
our proposal of analysis model in Section 3). Hence, it is not diﬃcult to see that
the attention paid to A is twice that for AB since their support is respectively
0.5 and 0.25. We also assume that the analysis proportion of each transaction
is proportional to the sum of the time spent on each pattern covering it. The
transaction t1 covered by A, B and AB is then analyzed for a length of 1.25
(= 0.5 + 0.5 + 0.25). Therefore, according to this observation, the individual
analysis of t1, t2, or t3 in the analysis is 5 times greater than that of t4 as shown
by the third column of Table 1 (c). Assuming that the user considers all trans-
actions equally interesting (see the preference vector ρ in the fourth colmun),
we say that such an analysis is unbalanced. It means that some transactions are
understudied while others are overstudied! For us, a good way to balance the
analysis is to increase the score (here the support) of the pattern D. Its score
increase will also increase the analysis proportion of t4 and decrease that of the
other three transactions. More interestingly, we think that D is peculiar and
deserves a higher valuation because it describes a singular transaction that is
described by no other pattern.

(cid:2)

In this work, we seek to balance the analysis of the dataset by proposing a new
interestingness measure to rate the patterns. More precisely, a preference vector
ρ : D → (0, 1] indicates the intensity of his/her interest for each transaction such
t∈D ρ(t) = 1. Indeed, the user does not always devote the same interest in
that
all transactions as in our example above. Sometimes he/she prefers to focus on
rare data as it is often the case in fraud detection or with biomedical data. An
analysis is balanced when each transaction t is studied with an acuity Π(t,M)
corresponding to that speciﬁed by a user-preference vector ρ(t): Π(t,M) =
ρ(t) where Π(t,M) is the analysis proportion of the transaction t given an
analysis model M that simulates analysis sessions conducted to understand the
data. To the best of our knowledge, we propose the ﬁrst model, called scoring

Balancing the Analysis of Frequent Patterns

55

analysis model, to simulate the sessions of analyzing pattern sets according to an
interestingness measure. Its strength is to rely on a stochastic model successfully
used in Information Retrieval [5,6] while integrating the preferences given by the
user. As main contribution, we then introduce the balanced support that induces
balanced analysis under our model. This measure removes the equalized axiom
of support saying that every transaction has the same weight in its calculation.
It gives a higher weight to the most singular transactions in the calculation
of the balanced support. For instance, in Table 1, the transaction t4 will be
weighted 5 times more than others so that it receives the same attention as
other transactions. We also develop an algorithm, Absolute, to compute the
balanced support. Our experiments show its eﬀectiveness for balancing frequent
itemsets and compare the balanced support with the traditional support.

The rest of this paper is organized as follows. Section 2 introduces the basic
notations. In Section 3, we introduce the scoring analysis model allowing us to
simulate the behavior of an analyst. Under this model, we propose in Section 4
the balanced support and the algorithm Absolute to compute it. Section 5
presents experiments demonstrating its eﬃciency and the interest of the balanced
support. Section 6 reviews some related work. We conclude in Section 7.

2 Preliminaries

(cid:4)l2 (resp. l1 (cid:4)l(cid:4)

2) for any pattern l(cid:4)

1

1

, and l(cid:4)

For the sake of clarity, we illustrate our deﬁnitions with the notion of itemsets
but, our problem is not limited to a particular type of pattern. We consider a
language L and a dataset D that is a multiset of L (or another language). A
specialization relation (cid:3) is a partial order relation on L [7]. Given a specialization
relation (cid:3) on L, l (cid:3) l(cid:4)
means that l is more general than l(cid:4)
is more speciﬁc
than l. For instance, A is more general than AB w.r.t ⊆.
Given two posets (L1,(cid:3)1) and (L2,(cid:3)2), a binary relation (cid:4) ⊆ L1 × L2 is a
(cid:3)1 l1
cover relation iﬀ for any l1 (cid:4)l2, we have l(cid:4)
(resp. l2 (cid:3)2 l(cid:4)
2). The relation l1 (cid:4) l2 means that l1 covers l2, and l2 is covered
by l1. The cover relation is useful to relate diﬀerent languages together (e.g.,
for linking patterns to data). Note that a specialization relation on L is also a
cover relation on L×L. For instance, the set inclusion is used for determinating
which patterns of P cover a transaction of D. Given two pattern sets L ⊆ L,
L(cid:4) ⊆ L(cid:4)
and a cover relation (cid:4) ⊆ L × L(cid:4)
by l ∈ L is
(cid:4)l = {l(cid:4) ∈ L(cid:4)|l (cid:4) l(cid:4)}. Dually,
the set of patterns of L(cid:4)
the covering patterns of L for l(cid:4) ∈ L(cid:4)
is the set of patterns of L covering the
: L(cid:5)l(cid:2) = {l ∈ L|l (cid:4) l(cid:4)}. With Table 1, we obtain that D⊇A = {t1, t2} and
pattern l(cid:4)
P⊆t1 = {A, B, AB}.
Pattern discovery takes advantage of interestingness measures to evaluate the
relevancy of a pattern. The support of a pattern ϕ in the dataset D can be
considered as the proportion of transactions covered by ϕ [4]: Supp(ϕ,D) =
|D(cid:4)ϕ|/|D|. A pattern is said to be frequent when its support exceeds a user-
speciﬁed minimal threshold. For instance, with a minimal threshold 0.25, the
pattern A is frequent because Supp(A,D) = |{t1, t2}|/4 (≥ 0.25). Thereafter, any

covered by the pattern l: L(cid:4)

, the covered patterns of L(cid:4)

56

A. Giacometti, D.H. Li, and A. Soulet

function f : L → R is extended to any pattern set P ⊆ L by considering (cid:3)f (P ) =
(cid:2)
ϕ∈P Supp(ϕ,D) = 2.5
with P = {A, B, C, D, AB, AC, BC} in Table 1.

ϕ∈P f (ϕ). For instance, (cid:2)Supp(P,D) corresponds to

(cid:2)

3 Simulating Analysis Using a Scoring of Patterns

3.1 Scoring Analysis Model

In this section, we propose the scoring analysis model to simulate an analyst
faced with a set of scored patterns. This model generates sessions by randomly
picking patterns taking into account the scoring of patterns. More precisely, the
“simulated analyst” randomly draws a pattern by favoring those with the highest
measure, and then studies each transaction covered by this pattern during a
constant period weighted by its preference vector. Indeed, it is important to
beneﬁt from these preferences for better approximating the user behavior. After
each pattern analysis, the session can be interrupted (if the analyst is satisﬁed,
no longer has time to pursue, etc.) or continued (if the analyst is dissatisﬁed,
wants more information, etc). This interruption of the session of analysis can be
modeled by a halting probability. We now formalize this model:
Deﬁnition 1 (Scoring analysis model). Let D be a dataset, P ⊆ L a pattern
set, m : L → [0, 1] an interestingness measure and ρ a preference vector.
The scoring analysis model with a halting probability α ∈ (0, 1) and a unit
length δ > 0, denoted by Sρ,α,δ, generates sessions with the following process:
1. Pick (with replacement) a pattern ϕ of P with probability distribution p(γ) =
2. Study each transaction t ∈ D covered by ϕ during a length δ × ρ(t).
3. Stop the session with probability α or then, continue at Step 1.

m(γ)/ (cid:3)m(P ) (where γ ∈ P ).

Basically, Step 1 favors the analysis of patterns having the highest measure
(with replacement because the end-user can re-analyze a pattern in the light
of another). Step 2 takes into account the user-preferences for the analysis of
transactions. Simulating a data expert by randomly picking patterns may seem
strange and unrealistic at ﬁrst. However, this mechanism has been successfully
used in other high-level tasks such as web browsing [5] and text analysis [6].
We think that the strength of our stochastic model is to describe the average
behavior of users. By analogy with the random surfer model, each pattern would
be a web page. The web pages would then be completely interconnected where
each link is weighted by the support of the destination page. In this context, the
probability α would correspond to the probability of interrupting navigation.

3.2 Analysis Proportion of a Transaction under Sρ,α,δ

Starting from the scoring analysis model, we desire to derive the analysis pro-
portion of each transaction.

Balancing the Analysis of Frequent Patterns

57

Theorem 1 (Analysis proportion). The analysis proportion Π(t,Sρ,α,δ) of
the transaction t is:

Π(t,Sρ,α,δ) =

(cid:2)

(cid:3)m(P(cid:5)t) × ρ(t)
t(cid:2)∈D (cid:3)m(P(cid:5)t(cid:2) ) × ρ(t(cid:4))

(cid:2)

Theorem 1 (proofs are omitted due to lack of space) means that the analysis
proportion of a pattern is independent of the parameters α and δ. Therefore,
in the following, Π(t,Sρ,α,δ) is simply denoted Π(t,Sρ). Let us consider the
analysis proportion of each transaction of Table 1 in light of Theorem 1 us-
ing a uniform preference vector. As the itemsets A, B and AB cover t1, we
obtain that (cid:2)Supp(P(cid:5)t1 ,D) = 0.5 + 0.5 + 0.25 = 1.25. The same result is ob-
tained for t2 and t3 and similarly, (cid:2)Supp(P(cid:5)t4 ,D) = 0.25. Finally, Π(t1,Sρ) =
(cid:2)Supp(P(cid:5)t1 ,D)/
t(cid:2)∈D (cid:2)Supp(P(cid:5)t(cid:2) ,D) = 1.25/(3 × 1.25 + 0.25) = 5/16 = 0.3125
and Π(t4,Sρ) = 0.25/4 = 1/16 = 0.0625. It means that under the scoring anal-
ysis model, the transaction t4 will be less analyzed than the transactions t1, t2
or t3 as indicated in Table 1 (c).
3.3 Balanced Analysis under Sρ,α,δ
We now deduce what a balanced analysis with respect to ρ is under the scoring
analysis model:
Property 1 (Balanced analysis). The analysis of D by the pattern set P with
m is balanced with respect to ρ under the scoring analysis model iﬀ for any
transaction t ∈ D, the following relations holds:
1|D| × (cid:4)

(cid:3)m(P(cid:5)t) =

(cid:3)m(P(cid:5)t(cid:2) )

t(cid:2)∈D

The crucial observation highlighted by Property 1 is that the balance of an
analysis is independent of the preference vector speciﬁed by the user, under the
scoring analysis model. Indeed, the preference vector ρ involved in the right side
of the equation Π(t,M) = ρ(t) is canceled by the one appearing in the analysis
proportion (see Theorem 1). Consequently, if the analysis of a dataset D by a
pattern set P with a measure m is balanced with respect to a given preference
vector ρ, then it is also balanced with respect to any other preference vector.
However, note that the analysis length of a transaction will take into account
the considered preference vector.
Let us compute whether the analysis of the dataset D by the pattern set
P with the measure Supp (see Table 1) is balanced under the model Sρ using
Property 1. First, the transaction t1 is too much studied because (cid:2)Supp(P(cid:5)t1 ,D) =
(cid:2)Supp({A, B, AB},D) = 1.25 and 1/|D| × (cid:2)
t(cid:2)∈D (cid:2)Supp(P(cid:5)t(cid:2) ,D) = 1/4 × (3 ×
1.25 + 0.25) = 1. Conversely, as (cid:2)Supp(P(cid:5)t4 ,D) = (cid:2)Supp({D},D) = 0.25 (< 1), the
transaction t4 is not studied enough. In Section 5, we observe that the use of
frequent patterns with the support for the analysis of datasets coming from the
UCI repository always leads to an unbalanced analysis.

58

A. Giacometti, D.H. Li, and A. Soulet

4 Balancing the Analysis of Patterns

4.1 Axiomatization of Support

Under the scoring analysis model, we aim at balancing the analysis of the dataset
by proposing a new interestingness measure that satisﬁes the equation of Prop-
erty 1. At this stage, the right question is “what characteristics should satisfy this
measure?” Unfortunately we found that the support does not lead to balanced
analysis. However, this extremely popular measure is both intuitive for experts
and useful in many applications. Moreover, it is an essential atomic element
to build many other interestingness measures. For all these reasons, we desire
a measure that leads to balanced analysis while maintaining the fundamental
properties of the support. To achieve this goal, we ﬁrst dissect the support by
means of its axiomatization (we only focus on the support measure and not on
the frequent itemset mining as proposed in [8]).

Property 2 (Support axioms). The support is the only interestingness measure
m that simultaneously satisﬁes the three below axioms for any dataset D:
1. Normalized: If a pattern ϕ covers no transaction (resp. all transactions),

then its value m(ϕ) is equal to 0 (resp. 1).
2. Cumulative: If patterns ϕ1 and ϕ2 cover respectively the set of transactions
T1 and T2 such that T1∩ T2 = ∅, then the value m(ϕ) of a pattern ϕ covering
exactly T1 ∪ T2 is m(ϕ1) + m(ϕ2).

3. Equalized: If two patterns cover the same number of transactions, then

they have the same value for m.

Clearly the ﬁrst axiom does not constitute the keystone of support, since
similar normalizations are widely used by other measures (e.g., conﬁdence or J-
measure). Furthermore, it has no impact on the fact that an analysis is balanced
or not, since Step 1 of scoring analysis model performs another normalization.
Conversely, we believe that the other two axioms (not veriﬁed by other measures)
are the main characteristics of the support. If we do not ﬁnd reason to reconsider
the cumulative axiom, we think the third is not fair. Ideally, an interestingness
measure should favor the patterns covering the least covered transactions as
explained in the introduction. Thus, the value of a measure should not only
depend on the number of transactions covered but also on the singularity of
these transactions. To this end, we propose to retain the ﬁrst two axioms and
to substitute the equalized axiom by the axiom of balance: a measure of interest
must lead to the balanced analysis of the dataset by the pattern set.

4.2 Balanced Support

We ﬁrst introduce a relaxation of the support by removing the constraint due
to the equalized axiom:
Deﬁnition 2 (Weighted support). Given a function w : D → (cid:10)+, the
weighted support of a pattern ϕ in the dataset D is deﬁned as: Suppw(ϕ,D) =
(cid:2)

(cid:2)

t∈D(cid:2)ϕ

w(t)/

t∈D w(t).

Balancing the Analysis of Frequent Patterns

59

It is not diﬃcult to see that the weighted support satisﬁes the normalized
axiom and the cumulative axiom. Now it only remains to choose the right vec-
tor w to get a balanced analysis. A naive idea would be to use the preference
vector ρ to weight the support. That does not work in the general case: Suppρu
(where ρu : t (cid:11)→ 1/|D|) corresponds exactly to the support Supp which lead to
an unbalanced analysis as shown by our running example (see Section 3.3) or
observed in experimental study (see Section 5). In fact, to ﬁnd the right weights,
it is necessary to solve the equation of Property 1 by using the deﬁnition of the
weighted support. Then, the weighted support induced by these weights deﬁnes
the optimal balanced support :

Deﬁnition 3 (Optimal balanced support). If it exists, the optimal balanced
support of a pattern ϕ ∈ P in the dataset D with the pattern set P , denoted by
(ϕ,D, P ), is the weighted support where the weight w satisﬁes the following
BS∗
equation for all transactions t ∈ D:
(cid:3)Suppw(P(cid:5)t,D) =

(cid:3)Suppw(P(cid:5)t(cid:2) ,D)

1|D| × (cid:4)

t(cid:2)∈D

Interestingly, this deﬁnition underlines that the whole set of mined patterns
P is necessary to compute the optimal balanced support of any individual
pattern. Let us illustrate the above equation with the example given by Ta-
ble 1. With the weight wbal where t1, t2, t3 (cid:11)→ 1/8 et t4 (cid:11)→ 5/8, we obtain
Suppwbal(A,D) = Suppwbal(B,D) = 1/8 + 1/8 = 2/8 ; Suppwbal(AB,D) = 1/8
and Suppwbal(D,D) = 5/8. Then, we can check that the equation of Deﬁnition 3
is satisﬁed (cid:3)Suppwbal(P(cid:5)t1 ,D) = (cid:3)Suppwbal({A, B, AB},D) = 2/8+2/8+1/8 = 5/8
(similar for t2 and t3) and (cid:3)Suppwbal(P(cid:5)t4 ,D) = (cid:3)Suppwbal({D},D) = 5/8. In
other words, Suppwbal corresponds exactly to the optimal balanced support
BS∗

(ϕ,D, P ).

Theorem 2. The optimal balanced support (if it exists) is the single interest-
ingness measure that satisﬁes the normalized and cumulative axioms, and that
leads to a balanced analysis.

Theorem 2 achieves our main goal as stated in introduction. However, the
equation of Deﬁnition 3 can admit no solution and then the optimal balanced
support is not deﬁned. For instance, it is impossible to adjust the weighted
support for balancing the analysis of D = {A, B, AB} by P = {A, B, AB}.
Indeed, whatever the weighted support, the transaction AB is still more analyzed
than the other two since it is covered by all patterns. So, the next section proposes
an algorithm to approximate the optimal balanced support by minimizing the
deviation between (cid:3)Suppw(P(cid:5)t,D) and

t(cid:2)∈D (cid:3)Suppw(P(cid:5)t(cid:2) ,D)/|D|.

(cid:2)

4.3 Approximating the Balanced Support
Absolute (for an anagram of balanced support) returns the weights w such
that the analysis Sρ,α,δ(D, P, Suppw) is balanced as better as possible. Its in-
put parameters consist in a pattern set P , a dataset D and a threshold . The

60

A. Giacometti, D.H. Li, and A. Soulet

latter is the maximal diﬀerence expected between two weight vectors stemming
from consecutive iterations before terminating the algorithm. The weights out-
putted by Absolute enable us to deﬁne the (approximated) balanced support
BS(ϕ,D, P ).

Algorithm 1. Absolute
Input: a dataset D, a set of patterns P , a diﬀerence threshold 
Output: a weight vector that balances the support
1: for all t ∈ D do
w0[t] ← 1/|D|
2:
3: end for
4: i ← 0
5: repeat
6: W ← 0
7:
8:

(cid:4)t(cid:2) ,D)

t(cid:2)∈D (cid:2)Suppw(P
(P(cid:4)t ,D)
(cid:2)Suppwi

// Correct the weight of t

for all t ∈ D do
wi+1[t] ← wi[t] × 1|D| ×(cid:2)
W ← W + wi+1[t]
end for
diff ← 0
for all t ∈ D do

9:
10:
11:
12:
13:
14:
15:
16:
17: until diff /|D| < 
18: return wi

end for
i ← i + 1

wi+1[t] ← wi+1[t]/W // Normalize the weight of t
diff ← diff + |wi+1[t] − wi[t]|

// Update diff

Note that in Algorithm 1, wi are symbol tables where the keys are transac-
tions. Lines 1-3 initialize all the weigths with 1/|D|. The main loop (Lines 5-17)
adjusts the weigths until the sum of diﬀerences between wi+1 and wi is less
than . More precisely, Lines 7-10 correct the weight of each transaction. Using
Deﬁnition 3, Line 8 computes the new weight wi+1[t] by multiplying the pre-
vious weight wi[t] by the ratio between the average coverage (i.e., a constant
1/|D| × (cid:2)
t(cid:2)∈D (cid:3)Suppwi(P(cid:5)t(cid:2) ,D) shared by all transactions) and the coverage of t
(i.e., (cid:3)Suppwi(P(cid:5)t,D)). For instance, if the coverage of t is below the average cov-
erage, the ratio is above 1 and the new weight is stronger. Thus, it increases the
support of all the patterns covering this transaction. This operation therefore
operates a local balance for each transaction. Nevertheless, there is also a global
modiﬁcation since a normalization is performed on these weights at Line 13
(where W is computed Line 9). Line 14 updates diff (initialized Line 11) ac-
cumulating the diﬀerence between wi+1 and wi for all the transactions. Finally,
Line 18 returns the last weights that correspond to a balanced analysis.

5 Experimental Evaluation

This section evaluates the eﬀectiveness of the algorithm for balancing the analy-
sis and to compare the quality of the balanced support with respect to the usual
one. All experiments reported below were conducted with a diﬀerence thresh-
−5 on datasets coming from the UCI Machine Learning Repository
old  = 10

Balancing the Analysis of Frequent Patterns

61

(www.ics.uci.edu/~mlearn/MLRepository.html). Given a minimal support thresh-
old minsupp, we select all the frequent itemsets for P . Increasing the weight of
singular transactions does not cause the extraction of random noise patterns
because the ﬁnal patterns are selected from the collection of frequent patterns.
For simplicity, we use the uniform preference vector ρu : t (cid:11)→ 1/|D| for ρ.

Table 2. Absolute on UCI benchmarks for frequent itemsets (minsupp = 0.05)

|P| # of iter. DKL

ρu
Supp DKL

Dataset
2,527
abalone
25,766
anneal
austral 20,386
2,226
breast
11,661
cleve
2,789
cmc
34,619
crx
german 124,517
3,146
glass
16,859
heart
hepatic 511,071
17,084
horse
275,278
lymph
3,190
page
vehicle 187,449
12,656
wine
586,579
zoo

17
24
29
41
35
23
24
17
52
37
13
19
34
42
24
46
34

0.180
0.339
0.076
0.145
0.172
0.091
0.122
0.172
0.084
0.116
0.568
0.275
0.138
0.054
0.636
0.154
0.353

ρu
BS Gain
0.021
8.72
0.013 26.2
0.006 12.6
0.006 22.5
0.012 14.7
0.002 50.8
0.011 10.9
0.029
5.96
0.005 17.7
0.014
7.95
0.040 14.1
0.017 15.9
0.016
8.34
0.004 14.8
0.020 31.3
0.009 17.8
0.019 18.2

Eﬃciency of Absolute Table 2 (columns 2-3) presents the number of patterns
and the number of iterations required by Absolute for balancing all the fre-
quent patterns. Note that we do not provide running times because they are very
low. Indeed, the worst case is the balancing time for all the frequent patterns on
zoo, but it does not exceed 16 seconds performed on a 2.5 GHz Xeon processor
with the Linux operating system and 2 GB of RAM memory (Absolute is im-
plemented in C++). Table 2 shows that the number of iterations varies between
13 and 52. No simple relationship was found between the number of iterations
and the features of datasets.

(cid:2)

i P (i) × log

Table 2 also reports the Kullback-Leibler divergence for support and BS
(columns 4-6). Let us recall that Kullback-Leibler divergence deﬁned by
DKL(P||Q) =
P (i)
Q(i) measures the diﬀerence between two proba-
bility distributions P and Q [9]. For any transaction t, we ﬁx P (t) = ρu(t) as
reference and Q(t) = Π(t,Mρu ) as model. Table 2 shows that Absolute reaches
its goal since the Kullback-Leibler divergence is always signiﬁcantly reduced by
beneﬁting from the balanced support. This divergence is at least divided by 5
and it is even divided by more than 10 in 13 datasets. The average gain is 17.56
for frequent itemsets. Similar experiments conducted on collections of free and
closed itemsets [10] gave respectively an average gain of 12.36 and 11.94.
Eﬀectiveness of Balanced Support. We desire to quantify the number of
non-correlated patterns (i.e., the number of extracted patterns that are spurious)
with a usual/balanced support. Unfortunately, the pattern discovery process is
unsupervised and the (ir)relevant patterns are unknown. We tackle this issue

62

A. Giacometti, D.H. Li, and A. Soulet

Fig. 1. Estimating the number of non-correlated patterns for Supp and BS

by using an experimental protocol inspired by [11]. The idea is to make the
assumption that a pattern is non-correlated if this pattern is also extracted (by
having the same characteristics as D
the same method) in a random dataset D∗
(i.e., the same dimensions and the same support for each item).
Figure 1 depicts the ratio of non-correlated patterns (averaged from 10 ran-
dom datasets D∗
) for abalone and anneal for frequent itemsets with a minimal
usual/balanced support varying between 0 and 0.5. This ratio is the number of
non-correlated patterns divided by the total number of patterns. For the bal-
anced support, we use three collections of frequent patterns P obtained with
minsupp = 0.01/0.05/0.10 independently of the second threshold applied to
balanced support. Given a minimal threshold (see horizontal axis), the ratio of
non-correlated patterns for Supp is always higher than that of BS and most
of times, with a signiﬁcant diﬀerence. Interestingly, the change of minsupp for
the collection of patterns has a marginal impact on the ratio of non-correlated
patterns. Recall that balanced support only diﬀers from the traditional one by
replacing the equalized axiom by the axiom of balance (see Section 4.1). So it
is this axiom that enables our measure to keep out uncorrelated patterns. More
generally, this experience justiﬁes the interest of a balanced analysis and even
the usefulness of the scoring analysis model for simulating an analysis.

6 Related Work

As mentioned in the introduction, many interestingness measures have been
proposed for evaluating the pattern interest as alternative to the support [2,12,3].
They can be categorized into two sets [1]: user-driven measures and data-driven
ones. Among the data-driven approaches, the statistical models are often based
on the null hypothesis. A pattern is interesting if it covers more transactions
than what was expected. Some models simply require the frequency of items
forming the itemset [12], others rely on its subsets [13,14] or even, patterns
already extracted [15]. These methods consider that all transactions have the
same weight. However, in practice, the user tends to attach more importance

Balancing the Analysis of Frequent Patterns

63

to information that describes the least common facts. Thus, the most singular
transactions should have an important weight in the evaluation of patterns that
describe such transactions. In this sense, this paper proposes another alternative
resting on the integration of the analysis method into the metric. To the best
of our knowledge, this way has not yet been explored in the literature. A major
and original consequence of our approach lies in the fact that each transaction
contributes with a diﬀerent weight in the balanced support and this weight
depends on the entire extracted collection.

However, the problem of unbalance induced by a pattern set is indirectly ad-
dressed by several approaches removing the patterns that describe transactions
covered by other patterns. For instance, the condensed representations [10] which
remove redundant patterns, often decrease the unbalanced of the analysis. But,
empirical experiments have shown that the unbalance remains important (see
Section 5). In the same way, global models based on patterns [16,17,18] favor
balanced analyses of the dataset. Indeed, one goal of these approaches is to de-
scribe all the data by choosing the smallest set of patterns. The overlap between
the coverings of the diﬀerent patterns is very reduced (ideally each transaction
should be described by a unique pattern as it is the case with a decision tree). Un-
fortunately, relevant patterns may be removed from such models. Our approach
balances the analysis of the dataset by preserving the whole set of patterns to
avoid losing information.

Rather than modifying the collection of mined patterns, it would be possible
to modify the initial dataset in order to satisfy user preferences. Sampling meth-
ods [19,20] are widely used in machine learning and data mining in particular
to correct a problem of unbalance between classes. There is no reason that the
change of the dataset with a usual sampling method leads to a balanced analysis.
We think that our approach is complementary to those of sampling.

7 Conclusion

In this paper, we introduce the scoring analysis model for simulating analysis
sessions of a dataset by means of a pattern set. Under this model, we deﬁne the
balanced support that induces a balanced analysis of the dataset for any user-
speciﬁed preference vector. We propose the algorithm Absolute to iteratively
calculate transaction weights leading to the balanced support. This new inter-
estingness measure strongly balances the analysis and in parallel, it enables us
to ﬁlter-out non-correlated patterns. The originality of our work is to show that
the integration of the analysis method to drive the data mining is proﬁtable.

In future work, we are interested in examining our approach on real-world
data for better understanding the semantic of the balanced support: what are
the patterns which balanced support is much higher than traditional support?
What are domains and datasets where the balanced support is most appropri-
ate? Dually, we must also study the properties of the weights resulting from
Absolute that could be interesting to identify the outliers. Furthermore, the
prospects of using the scoring analysis model are manifold. For instance, this
model could be used to balance other measures of interest like the conﬁdence.

64

A. Giacometti, D.H. Li, and A. Soulet

References

1. Freitas, A.A.: Are we really discovering “interesting” knowledge from data. Expert

Update (the BCS-SGAI Magazine) 9, 41–47 (2006)

2. McGarry, K.: A survey of interestingness measures for knowledge discovery. Knowl-

edge Eng. Review 20(1), 39–61 (2005)

3. Geng, L., Hamilton, H.J.: Interestingness measures for data mining: A survey. ACM

Comput. Surv. 38(3) (2006)

4. Agrawal, R., Srikant, R.: Fast algorithms for mining association rules in large

databases. In: VLDB, pp. 487–499. Morgan Kaufmann (1994)

5. Brin, S., Page, L.: The anatomy of a large-scale hypertextual web search engine.

Computer Networks 30(1-7), 107–117 (1998)

6. Mihalcea, R., Tarau, P.: Textrank: Bringing order into text. In: EMNLP, pp. 404–

411. ACL (2004)

7. Mannila, H., Toivonen, H.: Levelwise search and borders of theories in knowledge

discovery. Data Min. Knowl. Discov. 1(3), 241–258 (1997)

8. Calders, T., Paredaens, J.: Axiomatization of frequent itemsets. Theor. Comput.

Sci. 290(1), 669–693 (2003)

9. Kullback, S., Leibler, R.A.: On information and suﬃciency. Annals of Mathematical

Statistics 22, 49–86 (1951)

10. Calders, T., Rigotti, C., Boulicaut, J.F.: A survey on condensed representations
for frequent sets. In: Boulicaut, J.-F., De Raedt, L., Mannila, H. (eds.) Constraint-
Based Mining. LNCS (LNAI), vol. 3848, pp. 64–80. Springer, Heidelberg (2006)

11. Gionis, A., Mannila, H., Mielik¨ainen, T., Tsaparas, P.: Assessing data mining re-

sults via swap randomization. TKDD 1(3) (2007)

12. Omiecinski, E.: Alternative interest measures for mining associations in databases.

IEEE Trans. Knowl. Data Eng. 15(1), 57–69 (2003)

13. Tatti, N.: Probably the best itemsets. In: Rao, B., Krishnapuram, B., Tomkins, A.,

Yang, Q. (eds.) KDD, pp. 293–302. ACM (2010)

14. Webb, G.I.: Self-suﬃcient itemsets: An approach to screening potentially interest-

ing associations between items. TKDD 4(1) (2010)

15. Mampaey, M., Vreeken, J., Tatti, N.: Summarizing data succinctly with the most

informative itemsets. TKDD 6(4), 16 (2012)

16. Bringmann, B., Zimmermann, A.: The chosen few: On identifying valuable pat-

terns. In: ICDM, pp. 63–72. IEEE Computer Society (2007)

17. F¨urnkranz, J., Knobbe, A.: Guest editorial: Global modeling using local patterns.

Data Min. Knowl. Discov. 21(1), 1–8 (2010)

18. Gamberger, D., Lavrac, N.: Expert-guided subgroup discovery: Methodology and

application. J. Artif. Intell. Res. (JAIR) 17, 501–527 (2002)

19. Chawla, N.V.: Data mining for imbalanced datasets: An overview. In: Data Mining

and Knowledge Discovery Handbook, pp. 875–886. Springer (2010)

20. Liu, H., Motoda, H.: On issues of instance selection. Data Min. Knowl. Discov. 6(2),

115–130 (2002)


