Extensions to Quantile Regression Forests

for Very High-Dimensional Data

Nguyen Thanh Tung1, Joshua Zhexue Huang2, Imran Khan1, Mark Junjie Li2,

and Graham Williams1

1 Shenzhen Key Laboratory of High Performance Data Mining. Shenzhen Institutes

of Advanced Technology, Chinese Academy of Sciences, Shenzhen 518055, China
2 College of Computer Science and Software Engineering, Shenzhen University
tungnt@wru.vn, {zx.huang,jj.li}@szu.edu.cn, imran.khan@siat.ac.cn,

Graham.Williams@togaware.com

Abstract. This paper describes new extensions to the state-of-the-art
regression random forests Quantile Regression Forests (QRF) for appli-
cations to high-dimensional data with thousands of features. We propose
a new subspace sampling method that randomly samples a subset of fea-
tures from two separate feature sets, one containing important features
and the other one containing less important features. The two feature
sets partition the input data based on the importance measures of fea-
tures. The partition is generated by using feature permutation to produce
raw importance feature scores ﬁrst and then applying p-value assessment
to separate important features from the less important ones. The new
subspace sampling method enables to generate trees from bagged sample
data with smaller regression errors. For point regression, we choose the
prediction value of Y from the range between two quantiles Q0.05 and
Q0.95 instead of the conditional mean used in regression random forests.
Our experiment results have shown that random forests with these ex-
tensions outperformed regression random forests and quantile regression
forests in reduction of root mean square residuals.

Keywords: Regression Random Forests, Quantile Regression Forests,
Data Mining, High-dimensional Data.

Introduction

1
Regression is a task of learning a function f (X) = E(Y |X) from a training data
L = {(X, Y ) = (X1, Y1), ..., (XN, YN )}, where N is the number of objects in L,
X ∈ RM are predictor variables or features and Y ∈ R1 is a response variable
or feature. The regression model has the form
Y = E(Y |X) + 

(1)

where error  ∼ N (0, σ2).

A parametric method assumes that a formula for conditional mean E(Y |X)
is known, for instance, linear equation Y = β0 + β1X1, . . . , βM XM . The linear

V.S. Tseng et al. (Eds.): PAKDD 2014, Part II, LNAI 8444, pp. 247–258, 2014.
c(cid:2) Springer International Publishing Switzerland 2014

248

N.T. Tung et al.

regression model is solved by estimating parameters β0, β1, . . . , βM from L with
least squares method to minimize the sum of square residuals. A nonparametric
method does not require that a model form be known. Instead, a model structure
is speciﬁed, such as a neural network and L is used to learn the model. Linear
regression models do not perform on nonlinear domains and suﬀer the problem
of curse of dimensionality. Neural networks are not scalable to big data.

Decision tree is a nonparametric regression model that works on nonlinear
situations. A decision tree model partitions the training data L into subsets of
leaf nodes and the prediction value in each leaf node is taken as the mean of Y
values of the objects in that leaf node. Decision tree model is unstable in high-
dimensional data because of the large prediction variance. This problem can be
remedied by using an ensemble of decision trees or random forests [3] built from
the bagged samples of L [2]. Regression random forests takes the average of
multiple decision tree predictions to reduce the prediction variance and increase
the accuracy of prediction.
Quantile regression forests (QRF) represents the state-of-the-art technique for
nonparametric regression [7]. Instead of modeling Y = E(Y |X), QRF models
F (y|X = x) = P (Y < y|X = x), i.e., the conditional distribution function.
Given a continuous distribution function and a probability α, the α-quantile
Qα(x) can be computed as

P (Y < Qα(x)|X = x) = α

(2)

where 0 < α < 1. Given two quantile probabilities αl and αh, QRF enables
to predict the range [Qαl(x), Qαh (x)] of Y with a given probability τ that
P (Qαl(x) < Y < Qαh (x)|X = x) = τ . Besides the range prediction, quantile re-

gression forests can perform well in situations where the conditional distribution
function is not in normal distribution.

Both regression random forests and quantile regression forests suﬀer perfor-
mance problems in high-dimensional data with thousands of features. The main
cause is that in the process of growing a tree from the bagged sample data,
the subspace of features randomly sampled from the thousands of features in L
to split a node of the tree is often dominated by less important features, and
the tree grown from such randomly sampled subspace features will have a low
accuracy in prediction which aﬀects the ﬁnal prediction of the random forests.
In this paper, we propose a new subspace feature sampling method to grow
trees for regression random forests. Given a training data set L, we ﬁrst use
feature permutation method to measure the importance of features and produce
raw feature importance scores. Then, we apply p-value assessment to separate
important features from the less important ones and partition the set of fea-
tures in L into two subsets, one containing important features and one con-
taining less important features. We independently sample features from the two
subsets and put them together as the subspace features for splitting the data
at a node. Since the subspace always contains important features which can
guarantee a better split at the node, this subspace feature sampling method en-
ables to generate trees from bagged sample data with smaller regression errors.

eQRF for Very High-Dimensional Data

249

For point regression, we choose the prediction value of Y from the range between
two quantiles Q0.05 and Q0.95 instead of the conditional mean used in regression
random forests. Our experiment results have shown that random forests with
these extensions outperformed regression random forests and quantile regression
forests in reduction of root mean square residuals (RMSR).

2 Random Forests for Regression

2.1 Regression Random Forests

Given a training data L = {(X1, Y1), ..., (XN, YN )}, where N is the number of
objects in L, a regression random forests model is built as follows.

– Step 1: Draw a bagged sample Lk from L.
– Step 2: Grow a regression tree Tk from Lk. At each node t, the split is deter-
xi∈t(Yi − ¯Yt)/N (t),
mined by the decrease in impurity that is deﬁned as
where N (t) is the number of objects and ¯Yt is the mean value of all Yi at
node t. At each leaf node, ¯Yt is assigned as the prediction value of the node.
– Step 3: Let ˆY k be the prediction of tree Tk given input X. The prediction

(cid:2)

of regression random forests with K trees is

ˆY =

1
K

K(cid:3)

k=1

ˆY k

Since each tree is grown from a bagged sample, it is grown with only two-
third of objects in L. About one-third of objects are left out and these objects
are called out-of-bag (OOB) samples which are used to estimate the prediction
errors.

2.2 Quantile Regression Forests

Quantile Regression Forests (QRF) uses the same method as described above to
grow trees [7]. However, at each leaf node, it retains all Y values instead of only
the mean of Y values. Therefore, QRF keeps the raw distribution of Y values at
leaf node.

To describe QRF with notation by Breiman [3], we compute a positive weight

wi(x, θk) by each tree for each case Xi ∈ L, where θk indicates the kth tree for
a new given x. Let l(x, θk) be a leaf node t. All Xi ∈ l(x, θk) are assigned to an
equal weight wi(x, θk) = 1/N (t) and Xi /∈ l(x, θk) are assigned to 0 otherwise,
where N (t) is the number of objects in l(x, θk). For single tree prediction, given
X = x, the prediction value is

ˆY k =

N(cid:3)

i=1

wi(x, θk)Yi =

(cid:3)

x,Xi∈l(x,θk)

wi(x, θk)Yi =

1

N (t)

(cid:3)

x,Xi∈l(x,θk)

Yi

(3)

250

N.T. Tung et al.

The weight wi(x) assigned by random forests is the average of weights by all

trees, that is

wi(x) =

1
K

K(cid:3)

k=1

wi(x, θk)

The prediction of regression random forests is

ˆY =

N(cid:3)

i=1

wi(x)Yi

(4)

(5)

We note that ˆY is the average of conditional mean values of all trees in the

regression random forests.

Given an input X, we can ﬁnd the leaf node lk(x, θk) from all trees and the
set of Yi in these leaf nodes. Given all Yi and the corresponding weights w(i),
we can estimate the conditional distribution function of Y given X as

ˆF (y|X = x) =

N(cid:3)

wi(x)I(Yi ≤ y)

(6)

where I(·) is the indicator function that is equal to 1 if Yi ≤ y and 0 if Yi > y.
Given a probability α, we can estimate the quantile Qα(X) as

i=1

ˆQα(X = x) = inf{y : ˆF (y|X = x) ≥ α}.

(7)

For range prediction, we have

[Qαl(X), Qαh(X)] = [inf{y : ˆF (y|X = x) ≥ αl}, inf{y : ˆF (y|X = x) ≥ αh}]
where αl < αh and (αh − αl) = τ . Here, τ is the probability that prediction Y
will fall in the range of [Qαl(X), Qαh(X)].

(8)

For point regression, the prediction can choose a value in a range such as
the mean or median of Yi values. The median surpasses the mean in robustness
towards extreme values/outliers. We use the median of Y values in the range of
two quantiles as the prediction of Y given input X = x.

3 Feature Weighting Subspace Selection

3.1 Importance Measure of Features by Permutation

Given a training data set L and a regression random forests model RF , Breiman
[3] described a permutation method to measure the importance of features in
the prediction. The procedure for computing the importance scores of features
consists of the following steps.

1. Let Loob

k be the out-of-bag samples of the kth tree. Given Xi ∈ Loob

k , use the

tree Tk to predict ˆY k

i , denoted as ˆf k

i (Xi).

eQRF for Very High-Dimensional Data

251

2. Choose a predictor feature j and randomly permute the value of feature j
k . Use tree Tk to obtain the new prediction on

in Xi with another case in Loob
the permuted Xi as ˆf k,p,j

i

(Xi). Repeat the permutation process P times.

3. For Mi trees grown without Xi, compute the out-of-bag prediction by RF

in the pth permutation of the jth predictor feature as

ˆf p,j
i

(Xi) =

1
Mi

(cid:3)

Xi∈Loob

k

ˆf k,p,j
i

(Xi)

.

4. Compute the two mean square residuals (MSR) with and without permuta-

tions of predictor feature j on Xi as M SRi = 1
Mi
(Xi) − Yi)2, respectively.
M SRj

p=1( ˆf p,j

i = 1
P

(cid:2)

P

i

(cid:2)

i (Xi) − Yi)2 and
− M SRi). The importance of feature j is
i . To normalize the importance measures, we have

k∈Mi ( ˆf k

i = max(0, M SRj
5. Let ΔM SRj
(cid:2)

i

IM Pj = 1
N
the raw importance score as

i∈L ΔM SRj

V Ij =

IM Pj(cid:2)
IM Pl

l

(9)

With the raw importance scores by (9) we can rank the features on the impor-
tance.

3.2 p-Value Feature Assessment

Permutation method only gives the importance ranking of features. We need to
identify important features from less important ones. To do so, we use Welch’s
two-sample t-test that compares the importance score of a feature with the max-
imum importance scores of generated noisy features called shadows. The shadow
features do not have prediction power to the response feature. Therefore, any
feature whose importance score is smaller than the maximum importance score
of noisy features, it is less important. Otherwise, it is considered as important.
This idea was introduced by Stoppiglia et al. [10], and were further developed
in [5], [11].

Table 1. The importance scores matrix of all real features and shadows with R repli-
cates

Iteration V I X1
V I x1,1
1
V I x2,1
2
...
R

...

V I xR,1 V I xR,2

V I X2
V I x1,2
V I x2,2

V I XM

V I AM +1
. . .
. . . V I x1,M V I a1,(M +1)
. . . V I x2,M V I a2,(M +1)

V I AM +2
V I a1,(M +2)
V I a2,(M +2)

V I A2M
. . .
. . . V I a1,2M
. . . V I a2,2M

. . . V I xR,M V I aR,(M +1)

V I aR,(M +2)

. . . V I aR,2M

...

252

N.T. Tung et al.

We build a random forests model RF from this extended data set. Following
the importance measure by permutation procedure, we use RF to compute 2M
importance scores for 2M features. We repeat the same process R times to
compute R replicates. Table 1 shows the importance measure of M features in
input data and M shadow features generated by permutating the values of the
corresponding feature in data.
each row and put it into the comparison sample V ∗
M + 1, ..2M ). For each data feature Xi, we compute t-statistic as:

From the replicates of shadow features, we extract the maximum value from

= max{Ari}, (r = 1, ..R; i =

∗

X i − V
(s2
1 + s2

ti =

(cid:4)

2)/R

(10)

1 and s2

where s2
2 are the unbiased estimators of the variances of the two samples.
For signiﬁcance test, the distribution of ti in (10) is approximated as an ordinary
Student’s distribution with the degrees of freedom df calculated as

df =

(s2
1

(s2
1

/n1 + s2
/n1)2/(n1 − 1) + (s2
2

/n2)2
2/n2)2/(n2 − 1)

(11)

where n1 = n2 = R.

Having computed the t statistic and df , we can compute the p-value for the
feature and perform hypothesis test on X i > V
. Given a statistical signiﬁcance
level, we can identify important features. This test conﬁrms that if a feature is
important, it consistently scores higher than the shadow over multiple permuta-
tions.

∗

3.3 Feature Partition and Subspace Selection

The p-value of a feature indicates the importance of the feature in prediction.
The smaller the p-value of a feature, the more correlated the predictor feature
to the response feature, and the more powerful the feature in prediction.

Given all p values for all features, we set a signiﬁcance level as the threshold
λ for instance λ = 0.05. Any feature whose p-value is smaller than λ is added to
the important feature subset Xhigh, and it is added to the less important feature
subset Xlow otherwise. The two subsets partitions the set of features in data.
Given Xhigh and Xlow, at each node, we randomly select some features from
Xhigh and some from Xlow to form the feature subspace for splitting the node.
Given a subspace size, we can form the subspace with 80% of features sampled
from Xhigh and 20% sampled from Xlow.

4 A New Quantile Regression Forests Algorithm

Now we can extend the quantile regression forests with the new feature subspace
sampling method to generate splits at the nodes of decision trees and select
prediction value of Y from the range of low and high quantiles with a high

eQRF for Very High-Dimensional Data

253

probability. The new quantile regression forests algorithm eQRF is summarized
as follows.

1. Given L, generate the extended data set Le in 2M dimensions by permutat-

ing the corresponding predictor feature values for shadow features.

2. Build a regression random forests model RF e from Le and compute R repli-
cates of raw importance scores of all predictor features and shadows with
RF e. Extract the maximum importance score of each replicate to form the
comparison sample V ∗

of R elements.

3. For each predictor feature, take R importance scores and compute t statistic

as (10).

4. Compute the degree of freedom df as (11).
5. Given t statistic and df , compute all p-values for all predictor features.
6. Given a signiﬁcance level threshold λ, separate important features from less

important features in two feature subsets Xlow and Xhigh.

7. Sample the training set L with replacement to generate bagged samples

L1, L2, .., LK.

8. For each sample Lk, grow a regression tree Tk as follows:

(a) At each node, select a subspace of m = (cid:6)√

M(cid:8)(m > 1) features randomly
and separately from Xlow and Xhigh and use the subspace features as
candidates for splitting the node.

(b) Each tree is grown nondeterministically, without pruning until the min-
imum node size nmin is reached. At each leaf node, all Y values of the
objects in the leaf node are kept.

out-of-bag samples.

(c) Compute the weights of each Xi by individual trees and the forests with
9. Given a probability τ , αl and αh for αh− αl = τ , compute the corresponding
quantile Qαl and Qαh with (8) (We set default values [αl = 0.05, αh = 0.95]
and τ = 0.9).

10. Given a X, estimate the prediction value from a value in the quantile range

of Qαl and Qαh such as mean or median.

5 Simulation Analysis

5.1 Simulation Data

We used three models as listed in Table 2 to generate synthetic data for sim-
ulation analysis. Each model has 5 predictor variables or features. With each
model, we ﬁrst created 200 objects in 5 dimensions plus a response feature. Af-
ter this, we expanded the data set with diﬀerent numbers of noisy features and
obtained 5 data sets named as {LM5, LM50, LM500, LM2000, LM5000} where
the number in the data name indicates dimensions of the data set. Similarly, we
generated extra 5 data sets with 1000 objects from each model as test data sets
named {HM5, HM50, HM500, HM2000, HM5000}.

254

N.T. Tung et al.

Table 2. Three simulation models for synthetic data generation. Each model uses 5 iid
predictor features from U (0, 1) and  from Exp(1) (exponential mean 1) distribution.

Model Error Distribution Simulation models

1
2
3

Exp(1)
Exp(1)
Exp(1)

Y = 10(X1 + X2 + X3 + X4 + X5 − 2.5)2 + 
Y = 10sin(πX1X2) + 20(X3 − 0.5)2 + 10X4 + 5X5 + 
Y = 0.1e4X1 + 4/[1 + e−20(X2−0.5)] + 3X3 + 2X4 + X5 + 

5.2 Evaluation Measure

The performance of a model was evaluated on test data with the root mean of
square residuals (RM SR) computed as

RM SR =

(cid:5)

1(cid:9)H(cid:9)

[ ˆfH(Xi) − Yi]2.

(cid:3)

Xi∈H

(12)

where ˆfH(Xi) is the prediction given Xi, H is a test data set and (cid:9)H(cid:9) is the
number of objects in test data H.

(a) Model 1

(b) Model 2

(c) Model 3

Fig. 1. Comparisons of three regression forests algorithms on 5 test data sets generated
with the simulation models in Table 2

5.3 Evaluation Results

We used regression random forests RF, quantile regression forests QRF and our
algorithm eQRF to build regression models from the training data sets and used
evaluation measure (12) to evaluate the models with the test data sets. We used
the latest RF and QRF packages randomForest, quantregForest in R in these
experiments [6], [8]. For each training data set, we built 100 regression models,
each with 500 trees and tested the 100 models with the corresponding test data.
Then, the result was evaluated with (12) and the average of 100 models was
computed.

eQRF for Very High-Dimensional Data

255

Figure 1 shows the evaluation results of three random forests regression meth-
ods in RMSR measures. Each random forests method produced 5 test results on
5 simulation data sets from the left to right as {HM5, HM50, HM500, HM2000,
HM5000}. We can see that the more noisy features in the data, the lower accu-
racy in the prediction model. Clearly, in all simulated data generated by the three
models in Table 2, eQRF performed the best and its RMSR was signiﬁcantly
lower than those of QRF and RF.

6 Experiments on Real Datasets

6.1 Real-World Data

Five real-world data sets were used to evaluate the performance of our new
regression random forests algorithm. The general characteristics of these data
sets are presented in Table 3.

The computed tomography (CT) data was taken from the UCI1 which was
used to build a regression model to calculate the relative locations of CT slices
on the axial axis. The data set was generated from 53,500 images taken from
74 patients (43 males and 31 females). Each CT slice was described by two
histograms in a polar space. The ﬁrst histogram describes the location of bone
structures in the image and the second represents the location of air inclusions
inside of the body. Both histograms are concatenated to form the feature vector.
TFIDF-2006 2 is a text data set containing ﬁnancial reports. Each document
is associated with an empirical measure of ﬁnancial risk. These measures are log
transformed volatilities of stock returns.

The Microarray data ”Diﬀuse Large B-cell Lymphoma” (DLBCL) was col-
lected from Rosenwald et al. [9]. The DLBCL data consisted of measurements of
7399 genes from 240 patients with diﬀuse large B-cell lymphoma. The outcome
was survival time, which was either observed or censored. We used observed
survival time as the response feature because censored data only indicates two
states, dead or alive. A detailed description can be found in [9].

”Leukemia” and ”Lung cancer” are two gene data sets taken from NCBI 3.
Each of those data sets contains two classes. We changed one class label to 1
and another label to 0. We treat 0 and 1 as continuous values and consider this
problem as a regression problem. We built a regression random forests model to
estimate the outcome and used a deﬁned threshold to divide the outcomes into
two classes.

6.2 Experiments and Results

For each real-world data set, we used two-third of data for training and one-third
for testing. We generated 10 models from each training data and each model

1 The data are available at http://archive.ics.uci.edu/
2

http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets
http://www.ncbi.nlm.nih.gov

3

256

N.T. Tung et al.

Table 3. Description of the real data sets sorted by the number of features and RMSR
performance of three regression algorithms

Dataset Name
1
2
3
4
5

CT Data
Leukemia
DLBCL Data
Lung cancer
TFIDF-2006

#training #testing #features
385
7,129
7,399
54,675
150,361

35,700
48
160
114
16,087

17,800
24
80
58
3,308

eQRF RF QRF
0.29 1.33 2.09
0.17 0.22 0.24
3.77 4.28 4.55
0.21 0.32 0.36
0.41 0.68 0.69

contained 200 trees. We computed the average of RMSRs of the 10 models with
(12). The average RMSRs of three regression random forests models on ﬁve real-
world data sets are shown in Table 3 on the right. We can see that eQRF had
the lowest average RMSR. RF performed better than QRF.

(a) RF

(b) eQRF

Fig. 2. Plots of predicted response values against the true values of CT test data. (a)
Result of RF. (b) Result of eQRF.

Figure 2 plots the predicted values by RF and eQRF against the true values
of the response feature in CT test data. We can see from Figure 2 (a) that there
are some regions that RF predicted higher than the true value, for instance [25
cm, 35 cm], and some regions that RF predicted lower than the true value, for
instance > 70 cm. These are the prediction error regions including shoulder [20-
30cm] and abdomen [60-75cm]. On the contrast, the predicted values of eQRF
were more close to the true values as shown in Figure 2 (b) and the prediction
results are consistent and more stable in all regions of human body.

Figure 3 shows the average RMSR box plots of three regression models from
the real-world data sets. Figure 3 (a) is the result of CT data and Figure 3 (b)
is the result of DLBCL data. We can see that eQRF produced less RMSR than
QRF and RF and the variance is also small.

eQRF for Very High-Dimensional Data

257

(a) CT Data

(b) DLBCL Data

Fig. 3. Boxplots of RMSR of three models RF, QRF and eQRF. (a) Result of CT test
data. (b) Result of DLBCL test data.

(a) CT Data

(b) DLBCL Data

Fig. 4. .Plots of computational time of three algorithms against the number of objects
in data. The experiments were conducted on a computer with 2.13 Ghz Intel Core 2
Quad processor and 24GB RAM. (a) Result of CT data. (b) Result of DLBCL data.

Figure 4 shows the computational time of three regression models on the two
data sets. We can see that the computational time of the three models linearly
increases as the number of objects increases if the size is small, such as DLBCL
data. However, for data set with a large number of objects as CT data, the
computational times of RF and QRF increase exponentially as shown in Figure
4 (a) but eQRF still maintains a linear increase as shown in Figure 4 (b).

258

N.T. Tung et al.

7 Conclusions

We have presented a new regression random forests algorithm for high-
dimensional data with thousands of features. In this algorithm, we have made
two extensions to the quantile regression forests. One is the subspace sampling
method to select the set of features for splitting a node in growing trees. The
other is to use the median of Y values in the range of two quantile as the predic-
tion of Y given an input X. The ﬁrst extension increases the prediction accuracy
of decision trees. The second extension reduces the eﬀect of outliers and reduces
the variance of random forests regression. Experiment results have demonstrated
the improvement in reduction of RMSR in comparison with regression random
forests and quantile regression forests.

Acknowledgment. This research is supported in part by NSFC under Grant
No.61203294, Shenzhen New Industry Development Fund under Grant No.JC201
005270342A, No.JCYJ20120617120716224, the National High-tech Research and
Development Program(No. 2012AA040912), and Guangdong-CAS project(No.
2011B090300025).

References

1. Breiman, L., Friedman, J.H., Olshen, R.A., Stone, C.: Classiﬁcation and Regression

Trees. Wadsworth International, Belmont (1984)

2. Breiman, L.: Bagging Predictors. Machine Learning 24(2), 123–140 (1996)
3. Breiman, L.: Random Forests. Machine Learning 45(1), 5–32 (2001)
4. Ho, T.: The random subspace method for constructing decision forests. IEEE
Transactions on Pattern Analysis and Machine Intelligence 20(8), 832–844 (1998)
5. Kursa, M.B., Rudnicki, W.R.: Feature Selection with the Boruta Package. Journal

of Statistical Software 36(11) (2010)

6. Liaw, A., Wiener, M.: randomForest 4.6-7. R package (2012),

http://cran.r-project.org

7. Meinshausen, N.: Quantile Random Forests. Journal Machine Learning Research,

983–999 (2006)

8. Meinshausen, N.: quantregForest 0.2-3. R package (2012),

http://cran.r-project.org

9. Rosenwald, A., et al.: The use of molecular proﬁling to predict survival after
chemotherapy for diﬀuse large-b-cell lymphoma. N. Engl. J. Med. 346, 1937–1947
(2002)

10. Stoppiglia, H., Dreyfus, G.: Ranking a random feature for variable and feature

selection. The Journal of Machine Learning Research 3, 1399–1414 (2003)

11. Tuv, E., Borisov, A., Runger, G., Torkkola, K.: Feature selection with ensembles,
artiﬁcial variables, and redundancy elimination. The Journal of Machine Learning
Research 10, 1341–1366 (2009)


