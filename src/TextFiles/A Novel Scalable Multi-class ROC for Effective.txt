A Novel Scalable Multi-class ROC for Eﬀective

Visualization and Computation

Md. Raﬁul Hassan1, Kotagiri Ramamohanarao1, Chandan Karmakar2,

M. Maruf Hossain1, and James Bailey1

1 Department of Computer Science and Software Engineering,

The University of Melbourne, VIC 3010, Australia

2 Department of Electrical Engineering,

The University of Melbourne, VIC 3010, Australia
{mrhassan,jbailey,rao}@csse.unimelb.edu.au

Abstract. This paper introduces a new cost function for evaluating the
multi-class classiﬁer. The new cost function facilitates both a way to vi-
sualize the performance (expected cost) of the multi-class classiﬁer and a
summary of the misclassiﬁcation costs. This function overcomes the lim-
itations of ROC in not being able to represent the classiﬁer performance
graphically when there are more than two classes. Here we present a
new scalable method for producing a scalar measurement that is used to
compare the performance of the multi-class classiﬁer. We mathematically
demonstrate that our technique can capture small variations in classiﬁer
performance.

Keywords: Multi-class, Receiver operating characteristics, classiﬁer
evaluation, cost-function.

1 Introduction

Receiver Operating Characteristic (ROC) analysis [1] [2] [3] is a widely used
technique for evaluating classiﬁers. The technique considers the confusion ma-
trix in order to generate a plot and compare the performance of the classiﬁers.
While evaluating the classiﬁer, the ROC plot considers all possible operating
(decision) points in the classiﬁer’s prediction as a means of identifying the oper-
ating point at which the best performance is achieved. A confusion matrix forms
at the operating point that consists of ”True positive”, ”True Negative”, ”False
positive” and ”False negative” values. The ROC plot is obtained for a classiﬁer
by plotting the true-positive rate (TPR) over the y-axis and the false-positive
rate (FPR) over the x-axis. The points obtained on the plot are connected to the
points where the values of both TPR and FPR are extreme: i.e. the points will
be (0,0) and (1,1). This is done to form a complete curve. The area under the
curve (AUC) is a scalar measurement used to evaluate the classiﬁer, while the
plot provides the visual representation that is used to compare the classiﬁers’
performances.
Challenges: The ROC plot and AUC have been a popular technique for eval-
uating classiﬁers, however their application is limited to binary class problems.

M.J. Zaki et al. (Eds.): PAKDD 2010, Part I, LNAI 6118, pp. 107–120, 2010.
© Springer-Verlag Berlin Heidelberg 2010

108

Md.R. Hassan et al.

Although the ROC and AUC have been developed in theory to deal with multi-
dimensional problems, their inherent computational complexity and representa-
tional comprehensibility hinders their use in practice. For instance, a confusion
matrix obtained from a problem of ’M’ classes is an M × M matrix, and d =
(M.(M − 1)) dimensions: i.e. to evaluate a classiﬁer, all the possible misclassiﬁca-
tion combinations are needed [4]. The ROC plot for a binary classiﬁer facilitates
a visual comparison of binary classiﬁers, however, the ROC method is a tedious
task if a plot is to be drawn for a multi-class classiﬁer, as the dimensionality of
the plot increases dramatically with just a small increase in the number of classes.
For example, if we were to generate a plot for a three-class classiﬁer the number
of dimensions we have to deal is six while that of binary class classiﬁer is two. The
problem is not only limited to the representation or visualization, but also to the
computational complexity. This cost is O(nd) for a convex hull of n points for d
dimensions.
Contributions: Our main contributions in this paper are as follows:

- Developing a new method for visualizing the performance of multi-class clas-

siﬁers on a single plot;

- Developing a new method for representing the performance of multi-class
classiﬁers using a scalar measurement with reduced computational cost; and
- Experimental investigation of the developed methodologies that demonstrates

that our methods can compare classiﬁers both visually and numerically.

Organization: The remainder of the paper is organized as follows. Section 2
lists the studies that developed cost function for multi-class problem using AUC.
In Section 3 we brieﬂy describe the receiver operating characteristics curves. Sec-
tion 4 describes the proposed methodology in details. Section 5 details about the
experimental setup and generation of dataset. In Section 6 we analyze the results
obtained using our method and compare with one of the existing techniques. Fi-
nally, in Section 7 we conclude the paper.

2 Related Work

In recent years, a few approaches have been developed that have extended the
AUC measure for multi-class problems. Srinivasan [5] developed a 6-dimensional
ROC surface for a three-class problem. These six dimensions represent six mis-
classiﬁcation cells. However, the challenge was to compute the volume of the
convex hull in multi-dimensional space. There are computational approaches for
calculating the volume of the convex hull however, there is the possibility of
missing some points in the construction of the convex hull due to the high di-
mensional surfaces [6] [2]. Amongst the existing approximations for the AUC
measure for more than two classes are: Hand and Till [2], Mossman [7] and
Ferri et al. [4]. Hand and Till [2] extended the AUC for two classes to multiple
classes by averaging the AUCs of all the possible combinations of pairs of classes.
Mossman [7] prioritized one class over the others and demonstrated his approx-
imation for only three classes. Ferri et al. [4] used the Monte Carlo method for

A Novel Scalable Multi-class ROC

109

their approximation for the AUC. They also introduced the method called the
Hyperpolyhedron Search Algorithm (HSA). These existing approximations have
the nature of intuitive extensions however, they lack a solid theoretical justiﬁca-
tion. Other limitations of these approaches include the theoretical limitation of
computing the maximum volumes in the high-dimensional problem, and the fact
that at least d(d − 1) dimensional variable for d classes is needed for obtaining
their volumes.

3 Receiver Operating Characteristics Curves

The fundamental issue for rating a classiﬁer’s performance is the confusion ma-
trix where the numbers represent the total number of actual classes and predicted
classes. However, the usual applications of the confusion matrix are limited to
two-class classiﬁers because the classiﬁer’s performance can be easily represented
using a two-dimensional matrix. In this matrix the numbers along the rows are
the actual class and the numbers along the columns are the predicted class. The
classes are predicted considering an operating point that divides the data into
the respective classes. Usually, the numbers in the confusion matrix are known
as the ”true positive (TP)”, ”false positive (FP)”, ”true negative (TN)” and
”false negative (FN)”. The deﬁnitions of these terms are as follows:

– True Positive (TP)= the number of classiﬁed positive data that are positive

– True Negative (TN)= the number of classiﬁed negative data that are negative

– False Positive (FP)= the number of classiﬁed positive data that are negative

in the actual data set.

in the actual data set.

in the actual data set.

in the actual data set.

– False Negative (FN)= the number of classiﬁed negative data that are positive

The Receiver Operating Characteristics (ROC) plot is obtained for a binary
class classiﬁer by plotting the true-positive rate (Sensitivity) over the y-axis and
false-positive rate (1- Speciﬁcity) over the x-axis. The true-positive and false
positive rates are calculated using the following equations:

True positive rate (Sensitivity) = T P/(T P + F N)
False-positive rate(1 − Speciﬁcity) = F P/(F P + T N)

Figure 1 shows one such ROC plot where they sweep through all the possi-
ble operating points, and plot the corresponding false-positive and true-positive
rates. There is also a simple three-point ROC curve (for discrete classiﬁers),
where there are three points considered in order to obtain the plot. These three
points are: (0,1), (FP, TP) and (1,0).The point (0,1) refers to a classiﬁer where
every instance has been classiﬁed as correct: i.e. a perfect classiﬁer is represented
by the point (0,1). The point (1,0) refers to a classiﬁer where every instance is
classiﬁed as wrong: i.e. this point refers to the worst classiﬁer. The point (FP,

110

Md.R. Hassan et al.

Fig. 1. A typical ROC plot

TP) represents the classiﬁer where the values of FP and TP are obtained from
the confusion matrix. The Area Under the Curve (AUC) is a scalar value that
can be used to order which ROC curves are better than others.

4 Our Method

To extend the existing ROC plot and AUC for multi-class classiﬁers, we use the
properties of the ROC plot for binary classiﬁers. In this extension we calculate
all the possible pairwise (pair of classes) AUCs of the multiple classes.
Let us consider three classes - 1, 2 and 3 - and assume that for an operating
point P1 (i.e. if the predicted output is Y (cid:2) P1 classify the instance as class 1,
and if the predicted output is Y > P1 classify the instance as class 2), between
the two classes 1 and 2, the values of TP and FP are 0.9 and 0.2 respectively.
For these TP and FP we obtain the AUC of 0.85. Similarly, we obtain the values
of TP and FP for an operating point P2, while classifying classes 2 and 3, as 0.8
and 0.2. The AUC value for classes 2 and 3 is 0.8. In a similar way we obtain
the AUC value for classes 3 and 1 as 0.75 (for the operating point considered as
P3). For this three-class problem we obtain
= 3 AUC values for the three
pair-wise classes.

(cid:2)

To visualize this performance using the usual ROC plot we should consider
a surface/curve plotted on a six-dimensional surface, as there are six features
(i.e. three pairs of true positive and false positive rates) that need to be consid-
ered to plot the curve. Even though there are a few studies that have provided
methodologies for generating the surface for multi-class problems, these plots are
not helpful in comparing the classiﬁers’ performances through visualization. An
alternative solution is to represent the pairwise AUCs onto a multidimensional
surface. However, the problem still remains as to how to analyze the surface
for multiple classes. In our method we consider representing the AUCs using
polar co-ordinates. This representation is somewhat similar to the cobweb plot,
although there is some notable dissimilarity.

(cid:3)

3
2

A Novel Scalable Multi-class ROC

111

Initially, we consider that the range of angle is 0 → 2π. Because there are
three AUC values, we divide the maximum angle (2π) by three. For a three-class
3 π (i.e., 120◦). There will be three
problem after the division the angle becomes 2
lines that form a 120◦ angle with each other. We consider that the maximum
length of each of the lines is 1 because the maximum achievable value of an
AUC for a binary class problem is 1. In our method we consider the binary AUC
values for each of the pair of classes and draw the plot using these AUC values
on an equiangular line drawn on a polar co-ordinate. The equiangular lines for
a three-class problem are shown in Fig. 2. In this ﬁgure the length of each line
is 1, assuming that the AUC values for each of the pair of classes are maximum.
Each radius (r) on the plot represents the AUC value for a diﬀerent pair
AUC values for q
of classes. If there are m classes, there will be q =
number of pairs of classes; hence, to visualize the performance of a classiﬁer
for an m-class problem we must have q number of equiangular lines. Let us
assume that these AUC values are: r1, r2, r3,··· , rq. The corresponding angles
are: θ1, θ2, θ3,··· , θq, where, θ1 = θ2 = θ3 = ··· = θq = 2π/q. If we consider all
the AUC values are equal, i.e., ∀iri = r where 1 (cid:2) i (cid:2) q, the area covered by
the AUC values is:

m
2

(cid:2)

(cid:2)

(cid:3)

(cid:3)

Area = qr2

2 sin

2π
q

The range of minimum-maximum area for multiple classes: The maxi-
mum area for a given number of classes is obtained, if we consider all the AUC
for each pairwise classes are 1 (because the maximum AUC for a binary class
problem is 1). Thus, for an m-class problem and q =
AUC values, the
(cid:3)
maximum area is:

m
2

(cid:2)

(cid:3)

(cid:2)

areamax = q

2 r2sin

2π
q

If the values of pairwise AUCs are not equal to each other as shown in Fig. 3
(i.e., r1 (cid:4)= r2 (cid:4)= r3 (cid:4)= ··· (cid:4)= rq), the area covered by the AUC values is:

(cid:6)

(cid:6)

area =

1
2 sinθ

ri × ri+1

+ rq × r1

(cid:4)(cid:4)

q−1(cid:5)

i=1

In Eq. 2, AUC values r1, r2,··· , rq are plotted such that r1 is neighbor to r2, r2
is neighbor to r3, ··· , rq is neighbor to r1.

Before using the area formed through connecting the neighboring points on
the plot as a classiﬁer performance measurement, we must address the following
issues:

1. What is the range of the minimum-maximum area obtained for a diﬀerent

2. How the calculated area is aﬀected by the ordering of the AUC values plotted

number of classes; and

on the graph.

(1)

(2)

(3)

112

Md.R. Hassan et al.

Fig. 2. Visual representation for a three-
class classiﬁer using pairwise(class) AUC
values where each of the AUC values is 1

Fig. 3. The area covered by the
three points for a three-class classi-
ﬁer where the values of pairwise AUC
are: AU C(1,2) = 0.76, AU C(2,3) =
0.84, AU C(3,1) = 0.65

Similarly, the minimum area is obtained, if we consider all the AUC for each
pairwise classes are 0.5 (because the minimum AUC for a binary class problem
is 0.5). We obtain the minimum area for an m-class problem from the following
equation:

(cid:2)

(cid:7)

(cid:8)2

(4)

areamin = q
2

r
2

sin

(cid:3)

2π
q

Table 1 lists the minimum and maximum values of areas for a diﬀerent number
of classes.

The ordering of the AUCs: The maximum and minimum area is calculated
considering the maximum (r = 1) AUC of each pair of classes and minimum (r
= 0.5) AUC of each pair of classes. As a result the radius value remains the same
and the value of total area is not aﬀected by the ordering of the AUC values
when plotting them onto the equiangular graph. However, in the real world we
obtain varying AUC values for diﬀerent pairs of classes.
Let us consider the q AUC values are: r1, r2, r3,··· , ri−1, ri, rj, rj+1,··· , rq,
where r1 (cid:3) r2 (cid:3) r3 (cid:3) ··· (cid:3) rq and j = i + 1. The area γ for these values is:
γ ∝ r1 × r2 + r2 × r3 + · ·· + ri−1 × ri + ri × rj + rj × rj+1 + ··· rq−1 × rq + rq × r1
Let us interchange the positions of AUC values ri with rj in the above ordering.
Now, the area γ
γ ∝ r1 × r2 + r2 × r3 + · ·· + ri−1 × rj + rj × ri + ri × rj+1 + ··· rq−1 × rq + rq × r1

(cid:3) is:

A Novel Scalable Multi-class ROC

113

Fig. 4. Arranging AUC values such that the maximum area is found

By subtracting γ

(cid:3) from γ we ﬁnd that γ (cid:4)= γ

(cid:3).

This example shows that the ordering of the AUC values aﬀects the calculated
area that we use to compare the performances of the classiﬁers. We propose to use
the maximum area, achieved through re-ordering the AUC values, to compare
the performances of the classiﬁers. The maximum area is found if we arrange
the AUC values following lemma.

Lemma 1. The maximum area is found if the AUC values are ordered on the
plot in a way such that the neighboring value of a given point (or AUC value) is
the next available large value. That is, while calculating the area, the large values
will be multiplied to each other.
Proof. Let us consider the AUC values are: r1, r2, r3,··· , ri−1, ri, rj , rj+1,··· , rm.
Here, r1 (cid:3) r2 (cid:3) r3 ··· ri (cid:3) rj (cid:3) rj+1 (cid:3) ··· rm−1 (cid:3) rm and j > i. According to
the lemma, a maximum area will be found if the values are arranged as in Fig. 4.
In this ﬁgure, the neighboring points of r1 are the next available large values r2
and r3, while the neighboring point of r2 (the largest of r2 and r3) is r4, which is
the next available large value to r2. As there is no other neighboring space of r2
on which to put any other values we move forward to r3 and put the next large
value r5 as its neighboring point. The process is continued until all AUC values
are allocated.

If the AUC values are arranged as described according to this lemma, the area

covered by the AUC values is obtained using the following equation:

area = (r1 × r2) +

Here, k = q
2

− 1.

i<k(cid:5)

i<k(cid:5)

(r2i−1 × r2i+1) +

(r2i × r2i+2) + (rq−1 × rq)

(5)

i=1

i=1

114

Md.R. Hassan et al.

There are the following two scenarios that can aﬀect the area:

– Replace any pair of neighboring points with each other in the above list
– Replace any pair of points with each other, where the points are not neighbor

to each other in the above list.

Let us replace any pair of neighboring points ri with rj in the above list (where,
j > i). The area α is calculated using Eq. 2 for this list. The area diﬀerence
between α and γ is:

case i = 1 and j=2

γ − α = ri × ri+2 − rj × ri+2 + rj × rj+2 − ri × rj+2

= (ri − rj)(ri+2 − rj+2) ≥ 0

case i = 1 and j= 3

γ − α = ri × ri+1 − rj × ri+1 + rj × rj+2 − ri × rj+2

= (ri − rj)(ri+1 − rj+2) ≥ 0

case i = 2 and j=4

γ − α = ri × ri−1 − rj × ri−1 + rj × rj−2 − ri × rj−2

= (ri − rj)(ri−1 − rj−2) ≥ 0

case i = q-1 and j=q

γ − α = ri × ri−2 − rj × ri−2 + rj × rj−2 − ri × rj−2

= (ri − rj)(ri−2 − rj−2) ≥ 0

case i = q-3 and j=q-1

γ − α = ri × ri−2 − rj × ri−2 + rj × rj+1 − ri × rj+1

= (ri − rj)(ri−2 − rj+1) ≥ 0

case i = q-2 and j=q

γ − α = ri × ri−2 − rj × ri−2 + rj × rj−2 − ri × rj−2

= (ri − rj)(ri−2 − rj−2) ≥ 0

All other cases

γ − α = ri × ri−2 − rj × ri−2 + rj × rj+2 − ri × rj+2

= (ri − rj)(ri−2 − rj+2) ≥ 0

Let us replace any pair of points (other than the neighboring points) ri with rj
in the above list (where, j > i). The area α is calculated using Eq. 2 for this list.
The area diﬀerence between α and γ is:

A Novel Scalable Multi-class ROC

115

case i = 1 and j = q

γ − α = (ri − rj)(ri+1 − rj−2 + ri+2 − rj−1) ≥ 0

case i = 1 and j = q-1

γ − α = (ri − rj)(ri+1 − rj−2 + ri+2 − rj+1) ≥ 0

case i = 1 and j = any other values

γ − α = (ri − rj)(ri+1 − rj−2 + ri+2 − rj−2) ≥ 0

case i = 2 and j = q

γ − α = (ri − rj)(ri−1 − rj−2 + ri+2 − rj−1) ≥ 0

case i = 2 and j = q-1

γ − α = (ri − rj)(ri−1 − rj−2 + ri+2 − rj+1) ≥ 0

case i = 2 and j = any other values

γ − α = (ri − rj)(ri−1 − rj−2 + ri+2 − rj+2) ≥ 0

case i = any other values and j = q

γ − α = (ri − rj)(ri−2 − rj−2 + ri+2 − rj−1) ≥ 0

case i = any other values and j = q-1

γ − α = (ri − rj)(ri−2 − rj−2 + ri+2 − rj+1) ≥ 0

All other cases

γ − α = (ri − rj)(ri−2 − rj−2 + ri+2 − rj+2) ≥ 0

Therefore, it is proved that the maximum area is found if the AUC values are
ordered on the plot in a way such that the neighboring value of a given point
(or AUC value) is the next available large value.

It should be noted that the AUC values needed to be arranged while computing
the area to represent the performance using a scalar value. To compare the
performances of diﬀerent classiﬁers visually, there is no need to rearrange the
AUC values.

116

Md.R. Hassan et al.

Advantages of Our Method Compared with the Generalized AUC

In generalized AUC, the AUC that signiﬁes a classiﬁer’s performance is obtained
through a linear combination of individual AUC values for each pair of classes.
Our method, on the other hand, considers a nonlinear combination among the
individual AUC values. As a result, our method can ﬁnd the diﬀerences in the
performances of two classiﬁers if, the change in one pairwise AUC value is equal
to the change of another pairwise AUC value for these two classiﬁers.

(cid:2)

(cid:3)

Let us assume we have n (n =

for m-class problem) AUC values of
r1, r2, ..., rn (here the values are ordered randomly). The area γ obtained using
our method is: γ ∝ r1 × r2 + r2 × r3 + ··· + rn−1 × rn + rn × r1. The area
AU Cgeneralized is obtained as follows: AU Cgeneralized ∝ r1 + r2 + ... + rn.

Let us take a small change δ in r1 that is compensated by r2 The area

m
2

AU C

(cid:3)
generalized is obtained as follows:

AU C

(cid:3)
generalized

∝ (r1 − δ) + (r2 + δ) + ... + rn
∝ r1 + r2 + ... + rn ∝ AU Cgeneralized

(6)

The area γ

(cid:3) in this case is

γ

(cid:3) ∝ (r1 − δ) × (r2 + δ) + (r2 + δ) × r3 + ··· + rn−1 × rn + rn × (r1 − δ) (7)
∝ r1 × r2 + δ(r1 − r2) − δ2 + r3 × δ + r2 × r3 + ··· + rn−1 × rn
+ rn × r1 − δ × rn

We see that the generalized AUC has not been aﬀected through the changes in
AUC values as mentioned above. However, in our method these changes have
(cid:3)) which is −{δ × (r1 − r2) + δ2 +
the eﬀect of changing the area (since, γ (cid:4)= γ
δ × r3 − δrn}. This quality does not have to be zero and does allow us to ﬁnd
diﬀerences in the performance of two classiﬁers unlike the generalized AUC[3].

Complexity of the Proposed Method

The overall complexity of computing the scalar measurement for a multi-class
ROC consists of the complexity of computing AUC for all pairs of classes. For
a dataset D of n examples (points) the algorithm would need an O(nlogn) com-
putation to compute AUC for a pair of classes. For M classes there will be a
total M C2 pairs of classes. Therefore, M C2 pairwise AUC values will require
O(M C2log(M C2)) computation to arrange the AUCs for the calculation of a
maximum area. Thus the total complexity is O(M C2nlogn +M C2log(M C2)).

5 Experimental Design and Data Sets

Initially we have generated a synthetic dataset by randomly generating numbers.
These numbers are generated following normal distribution where the mean and
standard deviations were varied to generate a three class dataset. This dataset

A Novel Scalable Multi-class ROC

117

consists of three classes where each of the classes has 100 data instances. We
classiﬁed the dataset using four classiﬁers: Random Forest, Support Vector Ma-
chine (SVM), Random Tree and Decision Tree (C4.5). We have also classiﬁed
the Segment dataset [8] using above classiﬁers. There are 2310 data instances
and 7 classes in this dataset.

6 Results and Discussion

Results: Figure 5 shows the area covered by the maximum AUC values obtained
for each of the pairs of classes. For this dataset, we obtained the performance
using a generalized AUC as 0.92833, while our method produces a scalar value
of 1.1188. Table 3 lists the pairwise AUC values for Segment dataset for each
of the classiﬁers and provides the generalized AUC and AUC calculated using
our method. Fig. 7 compares the performances of four classiﬁers (SVM, Random
Forest, Random Tree and C4.5) for the segment dataset.

Fig. 5. The total area that signiﬁes the performance of the classiﬁer

Fig. 6. Comparison of classiﬁers’ performances using visual representation for ran-
domly generated 3-class dataset

118

Md.R. Hassan et al.

Fig. 7. Comparison of classiﬁers’ performances using visual representation for Segment
dataset

Discussion: The experimental results show that our method can measure the per-
formance of a classiﬁer while classifying more than two classes. As shown in Tab. 2,
for the synthetically generated dataset, the performance measurement using a gen-
eralized AUC cannot diﬀerentiate the performances between SVM and Random
Forest. It is interesting that our method can rank these classiﬁers according to
their performances. Fig. 6 also represents the performance variation visually.

Table. 3 presents the pair-wise AUC values for the four classiﬁers: SVM, Ran-
dom Forest, Random Tree and C4.5 when classifying the segment dataset. The
overall performance using our method for these classiﬁers is: SVM: 3.00, C4.5:
3.04, Random Tree: 2.95 and Random Forest: 3.06. The performances using gen-
eralized AUC are: SVM: 0.98, C4.5: 0.99, Random Tree: 0.98 and Random Forest:
0.99. We see that the generalized AUC [2] cannot diﬀerentiate the performances
of SVM and Random Tree, while our method can distinguish between the per-
formances of these two classiﬁers. As indicated by the area, C4.5 performs better
than does SVM for the dataset. Similarly, we ﬁnd that amongst the four clas-
siﬁers, Random Forest performs the best. According to the area obtained using
our method the rank of the classiﬁers is (from the best to the worst): Random
Forest, C4.5, SVM and Random Tree.

Fig. 7 represents the visual comparison of the performances of four classiﬁers
(SVM, Random Forest, Random Tree and C4.5) for the segment dataset that has
seven classes. The plot shows that the SVM performs worse than other classiﬁers
when classifying between class 3 and class 4. The classiﬁcation performances for
both Random Tree and SVM are the same for most of the pairs of classes except
at a few points (e.g. for the pair class 1 versus class 7, where Random Tree is
worse than SVM; and for the pair class 3 versus class 4, where Random Tree
performs better than SVM). This plot reveals that the performance of Random
Forest is the best among the all classiﬁers.

A Novel Scalable Multi-class ROC

119

Table 1. The minimum and maximum
values of areas for a diﬀerent number of
classes

Table 3. The
comparison of per-
formances measurement between our
method and Generalized AUC [2] using
segment dataset [8]

# of Minimum Maximum

Class X vs. SVM C4.5 Rand Tree RF

Class Y

Classes Area
0.3248
0.6495
0.7347
0.7626
0.7737
0.7788
0.7814
0.7828
0.7837
0.7842
0.7845
0.7848
0.7849
0.7850

3
4
5
6
7
8
9
10
11
12
13
14
15
16

Area
1.2990
2.5981
2.9389
3.0505
3.0949
3.1153
3.1257
3.1314
3.1348
3.1368
3.1382
3.1391
3.1397
3.1402

Table 2. The comparison of perfor-
mances measurement between our method
and Generalized AUC [2] using synthetic
dataset

Class X Vs. SVM C4.5 Rand Tree RF

Class Y

X=1, Y=2 0.82 0.70
0.84 0.90
X=1,Y=3
X=2,Y=3
0.85 0.90

0.60
0.7
0.90

0.78
0.86
0.87

Generalized 0.837 0.833

0.733

0.837

AUC [2]

Our method 0.908 0.896

0.689

0.909

7 Conclusion

1

1

1

1

0.998

0.998

1
1

1
1
1
1
1
1
1
1

0.999
0.998

0.998
0.998

X=1, Y=2
X=1, Y=3
X=1,Y= 4 0.998
X=1, Y= 5
X=1, Y= 6
X=1, Y= 7
X=2 , Y= 3
X=2, Y= 4
X=2 , Y= 5
X=2,Y= 6
X=2, Y=7
0.995
X=3, Y= 4 0.837 0.941
X=3, Y= 5
0.995
X=3, Y= 6 0.983 0.998
X=3, Y=7 0.967 0.984
X=4, Y=5
0.998
X=4, Y= 6 0.988 0.96
X=4 ,Y= 7 0.92 0.977
X=5, Y=6
0.998
X=5, Y= 7
0.998
X=6, Y=7 0.995 0.99

1

1

1
1

0.994
0.993
0.981
0.993
0.995
0.942
0.985
0.981

1

0.985
0.984
0.881
0.991
0.978
0.965
0.998
0.971
0.914

1

0.993
0.974

0.98

1
1
1
1
1
1
1
1
1
1
1

0.97

1
1

0.95

1

0.990
0.994

1
1

0.990

0.99

Generalized 0.98 0.99

AUC [2]

Our method 3.00 3.04

2.95

3.06

In this paper, we have proposed a new method to represent performance of a
classiﬁer both visually and numerically when there are more than two classes
in dataset. Using the proposed method, classiﬁers can be compared and ranked
according to their cost performance. The method considers optimal conﬁguration
of pairwise AUC values of a classiﬁer providing more accurate performance of the
classiﬁer. Furthermore, the computational cost of the method is O(M C2nlogn+M
C2log(M C2)) that is far less than O(nd); d = M.(M − 1) (if we consider d-
dimensions to plot and compute the volume under ROC surface) for a dataset of
M classes and n datapoints. This method is a milestone to ﬁll up the limitation
of the existing AUC and ROC plot for binary class problem.

120

Md.R. Hassan et al.

References

1. Flach, P., Blockeel, H., Ferri, C., Hern´andez-Orallo, J.S.: Decision support for data
mining; introduction to roc analysis and its applications. Data Mining and Decision
Support: Integration and Collaboration, 81–90 (2003)

2. Hand, D.J., Till, R.J.: A simple generalisation of the area under the roc curve for

multiple class classiﬁcation problems. Machine Learning 45, 171–186 (2001)

3. Swets, J., Dawes, R., Monahan, J.: Better decisions through science. Scientiﬁc Amer-

ican, 82–87 (2000)

4. Ferri, C., Hern´andez-Orallo, J., Salido, M.: Volume under the roc surface for multi-

class problems. In: Proceedings of ECML (2003)

5. Srinivasan, A.: Note on the location of optimal classiﬁers in roc space. Technical

report, Oxford University Technical Report PRG-TR-2-99 (1999)

6. Li, Y.: A generalization of AUC to an ordered multi-class diagnosis and applica-
tion to longitudinal data analysis on intellectual outcome in pediatric brain-tumor
patient. PhD thesis, Georgia University (2009)

7. Mossman, D.: Three-way roc’s. Medical Decision Making 19, 78–89 (1999)
8. Asuncion, A., Newman, D.: UCI machine learning repository (2007)


