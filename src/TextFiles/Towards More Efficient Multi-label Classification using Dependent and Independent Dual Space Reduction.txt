Towards More Eﬃcient Multi-label

Classiﬁcation Using Dependent and Independent

Dual Space Reduction

Eakasit Pacharawongsakda and Thanaruk Theeramunkong

School of Information, Computer, and Communication Technology

Sirindhorn International Institute of Technology

Thammasat University, Thailand
{eakasit,thanaruk}@siit.tu.ac.th

Abstract. While multi-label classiﬁcation can be widely applied for
problems where multiple classes can be assigned to an object, its eﬀec-
tiveness may be sacriﬁced due to curse of dimensionality in the feature
space and sparseness of dimensionality in the label space. Moreover, it
suﬀers with high computational cost when there exist a high number of
dimensions, as well as with lower accuracy when there are a number of
noisy examples. As a solution, this paper presents two alternative meth-
ods, namely Dependent Dual Space Reduction and Independent Dual
Space Reduction, to reduce dimensions in the dual spaces, i.e., the fea-
ture and label spaces, using Singular Value Decomposition (SVD). The
ﬁrst approach constructs the covariance matrix to represent dependency
between the features and labels, project both of them into a single re-
duced space, and then perform prediction on the reduced space. On the
other hand, the second approach handles the feature space and the label
space separately by constructing a covariance matrix for each space to
represent feature dependency and label dependency before performing
SVD on dependency proﬁle of each space to reduce dimension and for
noise elimination and then predicting using their reduced dimensions. A
number of experiments evidence that prediction on the reduced spaces for
both dependent and independent reduction approaches can obtain better
classiﬁcation performance as well as faster computation, compared to the
prediction using the original spaces. The dependent approach helps sav-
ing computational time while the independent approach tends to obtain
better classiﬁcation performance.

Keywords: multi-label classiﬁcation, Singular Value Decomposition,
SVD, dimensionality reduction, Problem Transformation

1

Introduction

In the past, most traditional classiﬁcation techniques usually assumed a sin-
gle category for each object to be classiﬁed by means of minimum distance.
However, in some tasks it is natural to assign more than one categories to an
object. For examples, some news articles can be categorized into both politic

P.-N. Tan et al. (Eds.): PAKDD 2012, Part II, LNAI 7302, pp. 383–394, 2012.
c(cid:2) Springer-Verlag Berlin Heidelberg 2012

384

E. Pacharawongsakda and T. Theeramunkong

and crime, or some movies can be labeled as action and comedy, simultane-
ously. As a special type of task, multi-label classiﬁcation was initially studied by
Schapire and Singer (2000) [10] in text categorization. Later many techniques in
multi-label classiﬁcation have been proposed for various applications such as se-
mantic scene classiﬁcation [2], music emotion categorization [12] and automated
tag recommendation [8]. However, these methods can be grouped into two main
approaches: Algorithm Adaptation (AA) and Problem Transformation (PT) as
suggested in [13]. The former approach modiﬁes existing classiﬁcation methods
to handle multi-label data [4,10]. On the other hand, the latter approach trans-
forms a multi-label classiﬁcation task into several single classiﬁcation tasks and
then applies traditional classiﬁcation method on each task [2,9,14].

Residing in these two main approaches, one major issue is curse of dimen-
sionality, which causes a well-known overﬁtting problem. To solve this issue,
many techniques have been proposed, e.g., sparse regularization [6], feature se-
lection [17] and dimensionality reduction [15,16,18]. Among these methods, the
dimensionality reduction which transforms data in a high-dimensional space to
those in the lower-dimensional space, has been focused for multi-label classiﬁ-
cation problem. The dimensionality reduction in multi-label data was formerly
studied by Yu et al. [16]. In their work, Multi-label Latent Semantic Index-
ing (MLSI) was proposed to project the original feature space into a reduced
feature space. Motivated by MLSI, Multi-label Dimensionality Reduction via
Dependence Maximization (MDDM) was introduced by Zhang and Zhou in [18].
In MDDM, Hilbert-Schmidt Independence Criterion (HSIC) was applied rather
than LSI and its aim was to identify reduced feature space that maximizes de-
pendency between the original feature space and the label space. Recently, Wang
et al. [15] has proposed a method to extend Linear Discriminant Analysis (LDA),
a well-known dimensionality reduction method, to handle multi-label data. Such
methods mainly focused on how to project the original feature space into a
smaller one, but still suﬀered with high dimensionality in the label space. By
this reason, these methods usually have high time complexity in the classiﬁcation
task.

On the other hand, for the label space reduction, to improve the eﬃciency of
multi-label classiﬁcation, Hsu et al. [7] posed a sparseness problem that mostly
occurred in the label space and then applied Compressive Sensing (CS) tech-
nique, widely used in the image processing ﬁeld, to encode and decode the label
space. While the encoding step of this CS method seems eﬃcient but the de-
coding step does not. Toward this issue, Tai and Lin [11] proposed Principle
Label Space Transformation (PLST) to transform the label space into a smaller
linear label space using Singular Value Decomposition (SVD) [5] with a simple
threshold setting (i.e., 0.5). More recently, Bi and Kwok (2011) [1] extended
the PLST to handle the labels which are organized in the form of a tree or
directed acyclic graph (DAG). Although the PLST-based methods are eﬀective
by reducing dimensions in the label space, it seems not handle neither the curse
of dimensionality in the feature space nor the correlation (dependency) among
labels in the label space.

Towards More Eﬃcient Multi-label Classiﬁcation

385

Toward these issues, this paper presents an approach that considers both the
curse of dimensionality problem in the feature space and the sparseness problem
in the label space. Moreover, the dependency proﬁle among features and labels,
the dependency proﬁle among features and the dependency proﬁle among labels
are also taken into account. Two alternative methods, namely Dependent Dual
Space Reduction (DDSR) and Independent Dual Space Reduction (IDSR), are
proposed to reduce dimensions in the dual spaces to eliminate redundancy as
well as noise using Singular Value Decomposition (SVD) for better prediction
and lower computational cost.

In the rest of this paper, Section 2 gives a formal description of the multi-
label classiﬁcation task and literature review to the SVD method. Section 3
presents two dual dimensionality reduction approaches, Dependent Dual Space
Reduction (DDSR) and Independent Dual Space Reduction (IDSR). The multi-
label benchmark datasets and experimental settings are described in Section 4.
In Section 5, the experimental results using seven datasets are given and ﬁnally
Section 6 provides conclusion of this work.

2 Preliminaries

2.1 Deﬁnition of Multi-label Classiﬁcation Task
M and Y = {0, 1}L be an M -dimensional feature space and L-
Let X = R
dimensional binary label space, where M is the number of features and L is a
number of possible labels, i.e. classes. Let D = {(cid:2)x1, y1(cid:3),(cid:2)x2, y2(cid:3), ...,(cid:2)xN , yN(cid:3)}
is a set of N objects (e.g., documents, images, etc.) in a training dataset, where
xi ∈ X is a feature vector that represents an i-th object and yi ∈ Y is a label
vector with the length of L, [yi1, yi2, ..., yiL]. Here, yij indicates whether the i-th
object belongs (1) or not (0) to the j-th class (the j-th label or not).

In general, two main phases are exploited in a multi-label classiﬁcation prob-
lem: (1) model training phase and (2) classiﬁcation phase. The goal of the model
training phase is to build a classiﬁcation model that can predict the label vector
yt for a new object with the feature vector xt. This classiﬁcation model is a
M → {0, 1}L can predict a target value closest to its
mapping function H : R
actual value in total. The classiﬁcation phase uses this classiﬁcation model to
assign labels. For convenience, XN×M = [x1, ..., xN ]T denotes the feature matrix
with N rows and M columns and YN×L = [y1, ..., yN ]T represents the label
matrix with N rows and L columns , where [·]T denotes matrix transpose.

2.2 Singular Value Decomposition (SVD)

This subsection gives a brief introduction to SVD, which was developed as a
method for dimensionality reduction using a least-squared technique [5]. The
(cid:3)
SVD transforms a feature matrix X to a lower-dimensional matrix X
such that
the distance between the original matrix and a matrix in a lower-dimensional
space (i.e., the 2-norm (cid:6) X − X

(cid:3) (cid:6)2) are minimum.

386

E. Pacharawongsakda and T. Theeramunkong

Generally, a feature matrix X can be decomposed into the product of three

matrices as shown in (1).

XN×M = UN×M × ΣM×M × VT

M×M ,

(1)

where N is a number of objects, M is a number of features and M < N . The
matrices U and V are two orthogonal matrices, where UT ×U = I and VT ×V =
I. The columns in the matrix U are called the left singular vectors while as the
columns in matrix V are called the right singular vectors. The matrix Σ is a
diagonal matrix, where Σi,j = 0 for i (cid:7)= j, and the diagonal elements of Σ are
the singular values of matrix X. The singular values in the matrix Σ are sorted
by descending order such that Σ1,1 ≥ Σ2,2 ≥ ... ≥ ΣM,M . To discard noise, it
is possible to ignore singular values less than ΣK,K, where K (cid:9) M . By this
ignorance the three matrices are reduced to (2).

(cid:3)
(cid:3)
N×M = U
X

(cid:3)T
M×K,
N×M is expected to be close XN×M , i.e. (cid:6) X − X
(cid:3)
where X
reduced matrix of UN×M , Σ
K dimensions and V

(cid:3)
M×K is a reduced matrix of VM×M .

(cid:3) (cid:6)2< δ, U
(cid:3)
N×K is a
K×K is the reduced version of ΣM×M from M to

(cid:3)

N×K × Σ

(cid:3)

K×K × V

(2)

In the next section, we show our two approaches that deploy the SVD tech-
nique to construct lower-dimensional space for both features and labels for multi-
label classiﬁcation.

3 Two Proposed Approaches

As mentioned earlier, most of previous approaches were presented to handle ei-
ther the problem of a curse of dimensionality in the feature space or the sparse-
ness problem in the label space.

This work presents two alternative approaches to deal with these aforemen-
tioned problems. In both approaches, the Singular Value Decomposition (SVD)
is used to project both feature and label spaces into reduced spaces then a clas-
siﬁcation method can be applied. In the classiﬁcation phase, on the other hand,
SVD is used to reconstruct the original higher-dimensional label space from the
prediction result in the constructed lower-dimensional label space.

In this work, we propose two alternative methods, called Dependent Dual
Space Reduction (DDSR) and Independent Dual Space Reduction (IDSR). To
promote the dependency among features and labels, DDSR computes depen-
dency matrix between features and labels then applies SVD to eliminate the less
correlated data. These lower-dimensional matrices computed from SVD are used
to project both feature and label spaces into a common lower-dimensional space.
While the DDSR approach retains only data with high correlated between fea-
tures and labels, it neither considers dependency among features nor dependency
among labels. As our second proposed method, the Independent Dual Space Re-
duction (IDSR) uses the feature dependency matrix built from the feature space
and label dependency matrix computed from the label space as two projection

Towards More Eﬃcient Multi-label Classiﬁcation

387

matrices. After that two independent SVDs are applied to these matrices and
project feature space and label space to lower-dimensional spaces. The predic-
tion can be done on lower-dimensional spaces before transforming back to the
original space. The next subsections describe DDSR and IDSR in order.

3.1 Dependent Dual Space Reduction (DDSR)

As previously mentioned, it is possible to utilize the characteristic that the fea-
ture space and the label space may have some dependency with each other.
While it is possible to characterize a dependency among feature and label spaces,
for example, cosine similarity, entropy and symmetric uncertainty, with limited
space, we considered only covariance matrix. In this work, both spaces can be
simultaneously compressed by performing SVD on the feature-label covariance,
viewed as a dependency proﬁle between features and labels. Equation (3) shows
construction of a covariance matrix SM×L to represent a dependency between
feature and label spaces.

(cid:3)

(cid:3)

(cid:3)

SM×L = E[(XN×M − E[XN×M ])T (YN×L − E[YN×L])],

(3)
where X is the feature matrix, Y is the label matrix and E[·] is an expected
value of the matrix.

Applying SVD, the covariance matrix SM×L is later decomposed to matrices
U, Σ and V. To retain only signiﬁcant dimensions and reduce noise, the ﬁrst
K (≤ min(M, L)) dimensions from matrices U and V are selected as U
(cid:3)
M×K
and V
, can be created as
(cid:3)
M×K . In the same way, a lower-dimensional label matrix
X
(cid:3)
L×K. These tasks constitute the
Y
pre-processing phase.

(cid:3)
M×K . Here, a lower-dimensional feature matrix X

N×K = XN×M × U
(cid:3)
can be computed as Y

N×K = YN×L × V

(cid:3)
In the next step, these two lower-dimensional matrices, X

, are used for
building a classiﬁcation model. Among existing methods on multi-label classiﬁ-
cation, Binary Relevance (BR) is a simple approach and widely used. BR simply
reduces the multi-label classiﬁcation task to a set of binary classiﬁcations and
then builds a classiﬁcation model for each class. However, in this approach, the
projected label matrix Y
contains numeric values rather than discrete classes.
By this situation, a regression method can be applied to estimate these numeric
has K dimensions, it is possible to
values. While the projected label matrix Y
construct a regression model for each dimension. That is, K regression models
are constructed for K lower-dimensions. Moreover, each model, later denoted by
(cid:3)
[k] using the
rk(X
(cid:3)
matrix X
. While the regression model returns continuous values, we propose a
method to ﬁnd the optimal threshold for mapping a continuous value to binary
decision (0 or 1) as described in Section 3.3.
In the classiﬁcation phase, ﬁrstly a test feature vector ˆX is transformed to the
lower-dimensional feature vector ˆX(cid:3) using ˆX(cid:3)
M×K. Then this
vector is fed to a series of regression models r( ˆX(cid:3)) to estimate the numeric value
in each dimension of the predicted lower-dimensional label vector ˆY(cid:3)
1×K[k].

) is a regression model built for predicting each column Y

1×K = ˆX1×M × U
(cid:3)

(cid:3)

and Y

(cid:3)

(cid:3)

(cid:3)

388

E. Pacharawongsakda and T. Theeramunkong

(cid:3)T

(cid:3)

, an orthogonal matrix of the matrix V

After that, a matrix V
, is multiplied
to reconstruct the lower-dimensional label vector ˆY1×K back to the higher-
dimensional label vector ˆY1×L. Next the predicted values in the label vector
ˆY1×L are rounded to the value in {0,1} by the predeﬁned threshold. The set of

predicted multiple labels is the union of the dimensions which have the value of 1.

3.2 Independent Dual Space Reduction (IDSR)

As opposed to the former approach, the Independent Dual Space Reduction
(IDSR) approach presents how to use two independent SVDs for transforming
the feature space and label space into the two lower-dimensional spaces similar
to DDSR even there are several possibilities of dependency calculation. In this
work, to consider the dependency in the feature space, the covariance matrix
SM×M is computed from the feature matrix XN×M . On the other hand, the
dependency among labels in the label space can be derived by calculating the
covariance matrix RL×L.

(cid:3)

(cid:3)

(cid:3)

(cid:3)

can be formulated by Y

(cid:3)
can be constructed by X

In the pre-processing phase of this approach, a feature dependency matrix
SM×M is built from a feature matrix XN×M and then it is decomposed to three
matrices Ux, Σx and Vx and select the top D dimensions from the matrix Ux.
(cid:3)
N×D =
Then the lower-dimensional feature matrix X
XN×M × U
(cid:3)
xM×D. The label dependency matrix RL×L is constructed from the
label matrix Y. Likewise, this matrix is decomposed to three matrices Uy, Σy
and Vy and the top K dimensions are selected from the matrix Uy. The lower-
N×K = YN×L × U
(cid:3)
dimensional label matrix Y
yL×K.
While the original label matrix YN×L contains either 0 or 1 as its members, its
N×K may include non-binary numeric values.
lower-dimensional label matrix Y
Moreover, it is not necessary that the dimension of the lower-dimensional feature
space D and that of the lower-dimensional label space K are identical. Note that
this condition is not the same with the DDSR approach, where D always equals
to K. After that, as the model training phase, we can construct K regression
N×D. Note that each regression model is for
models to predict Y
each of K dimensions of Y
. To transform a numeric value to a binary value a
threshold is established. Section 3.3 describes our proposed method for searching
the best threshold for each label. This step is done in the model training phase.
In the classiﬁcation phase, the feature vector ˆX of an unseen object will be
1×D = ˆX1×M ×
reduced to the lower-dimensional feature vector ˆX(cid:3) using ˆX(cid:3)
(cid:3)
xM×D. Then the regression models estimate the numeric value in the lower-
U
dimensional label vector ˆY(cid:3) based on the feature vector ˆX(cid:3). Next the matrix
(cid:3)
(cid:3)
, an orthogonal matrix of the matrix U
U
y, is used to reconstruct the orig-
y
inal higher-dimensional label vector ˆY from the prediction result in the lower-
dimensional label vector ˆY(cid:3) i.e., ˆYN×L = ˆYN×K × U
(cid:3)
T
L×K. To assign labels to
y
an unseen object, the prediction values in the label vector ˆY need to be rounded
to {0,1}. At this point, the threshold found in the model training phase can be
applied. Finally, the assigned label set is the union set of the dimensions that
have the value of 1.

T

(cid:3)

(cid:3)
N×K from X

Towards More Eﬃcient Multi-label Classiﬁcation

389

3.3 Threshold Selection

As stated above, by the orthogonal property of SVD, it can be used to reconstruct
an original label space from a lower-dimensional label space. As the result, the
reconstructed label vector may include non-binary values. To interpret the values
as binary decision, a threshold need to be set to map these values to either 0
or 1 for representing whether the object belongs to the class or not. As a naive
approach, the ﬁxed value of 0.5 is used to assign 0 if the value is less than
0.5, otherwise 1 [11]. As a more eﬃcient method, it is possible to apply an
adaptive threshold. In this work, we propose a method to determine an optimal
threshold by selecting the value that maximizes classiﬁcation accuracy in the
training dataset that is similar to the mechanism in Han et al [6]. In other
words, the threshold selection is done by ﬁrst sorting prediction values in each
label dimension in a descending order and examine performance (e.g., macro
F-measure) for each rank position from the top to the bottom to ﬁnd the point
that maximize the performance. Then, the threshold for binary decision is set
based on that point.

4 Datasets and Experimental Settings

To evaluate the performance of our two proposed approaches, the benchmark
multi-label datasets are downloaded from MULAN1. Table 1 shows the char-
acteristics of seven multi-label datasets. For each dataset, N , M and L denote
the total number of objects, the number of features and the number of labels,
respectively. LC represents the label cardinality, the average number of labels per
example and LD stands for label density, the normalized value of label cardinality
as introduced by Read et al [9].

Table 1. Characteristics of the datasets used in our experiments

Dataset

Domain

N

bibtex
corel5k
enron
medical
emotions
scene
yeast

text
images
text
text
music
image
biology

7,395
5,000
1,702
978
593
2,407
2,417

M

Nominal

Numeric

1,836
499
1,001
1,449
-
-
-

-
-
-
-
72
294
103

L

159
374
53
45
6
6
14

LC

LD

2.402
3.522
3.378
1.245
1.869
1.074
4.237

0.015
0.009
0.064
0.028
0.311
0.179
0.303

Since each object in the dataset can be associated with multiple labels simul-
taneously, the traditional evaluation metric of single-label classiﬁcation could
not be applied. The well-known multi-label evaluation metrics are of two types
[9]. As the ﬁrst type, a label-based metric evaluates each label separately such

1

http://mulan.sourceforge.net/datasets.html

390

E. Pacharawongsakda and T. Theeramunkong

as hamming loss and macro F-measure. As the second type, a label set-based
metric considers a set of labels simultaneously, i.e., accuracy and 0/1 loss. In
this work, hamming loss, macro F-measure, accuracy and 0/1 loss are used to
assess the eﬀectiveness of the multi-label classiﬁcation methods. Their detailed
descriptions can be found in several literatures such as those in Read et al [9].

Table 2. Performance comparison (mean) between BR, BR+, CC, PLST, DDSR,
and IDSR in terms of hamming loss (HL), macro F-measure (F1), accuracy, 0/1
loss on the seven datasets. (↓ indicates the smaller the better; ↑ indicates the larger
denotes the numeric-feature
the better;
dataset, the superscript (x,y) shows the percentage of dimensions reduced from the
original ones for the feature space and that for the label space.)

denotes the nominal-feature dataset and

†

‡

Dataset

†

bibtex

†

corel5k

†

enron

†
medical

‡
emotions

‡
scene

‡

yeast

Metrics
HL ↓
F1 ↑
Accuracy ↑
0/1 Loss ↓
HL ↓
F1 ↑
Accuracy ↑
0/1 Loss ↓
HL ↓
F1 ↑
Accuracy ↑
0/1 Loss ↓
HL ↓
F1 ↑
Accuracy ↑
0/1 Loss ↓
HL ↓
F1 ↑
Accuracy ↑
0/1 Loss ↓
HL ↓
F1 ↑
Accuracy ↑
0/1 Loss ↓
HL ↓
F1 ↑
Accuracy ↑
0/1 Loss ↓

BR

BR+

CC

0.0172
0.0047
0.0000
1.0000
0.0094
0.0284
0.0528
0.9948
0.1019
0.1957
0.3328
0.9089
0.0996
0.3383
0.4017
0.7167
0.2068
0.6317
0.5091
0.7467
0.1105
0.6480
0.5302
0.5186
0.2008

0.4455
0.5019
0.8510

0.0163
0.0022
0.0001
1.0000
0.0195
0.0710
0.1432
0.9938
0.0997
0.1882
0.3274
0.9083
0.0943
0.3370
0.3997
0.7085
0.2264
0.6673
0.5537
0.7267

0.2583
0.5351
0.5744
0.5148
0.2229
0.4053
0.4838
0.8564

0.0166
0.0038
0.0001
1.0000
0.0094
0.0354
0.0584
0.9918

0.1014
0.1920
0.3317
0.9066
0.0974
0.3351
0.4060
0.7095
0.2211
0.6243
0.5176
0.7451
0.1162
0.6903
0.6579
0.3831

0.2165
0.4404
0.4796
0.8105

PLST
0.0155[20]
0.0075[100]
0.0012[100]
0.9999[40]
0.0094[20]
0.0286[40]
0.0533[40]
0.9944[20]
0.0721[20]
0.2170[40]
0.3475[20]
0.9077[60]
0.0556[20]
0.3411[60]
0.4074[40]
0.7167[100]
0.2037[60]
0.6339[60]
0.5091[100]
0.7300[60]
0.1105[100]
0.6480[100]
0.5302[100]
0.5186[100]
0.2491[20]
0.2688[20]
0.3425[20]
0.9785[40]

DDSR

0.0137(9,100)
0.3949(9,100)
0.1543(3,40)
0.8270(9,100)
0.0145(30,40)
0.1215(15,20)
0.1479(60,80)
0.9944(45,60)
0.0529(3,60)
0.3118(5,100)
0.4723(5,100)
0.8713(5,100)
0.0114(3,100)
0.6485(3,100)
0.7288(3,100)
0.3731(3,100)
0.2728(8,100)
0.6455(8,100)
0.5090(8,100)
0.8293(8,100)
0.1190(2,80)
0.7046(2,100)
0.6109(2,80)
0.5106(2,80)
0.2807(14,100)
0.4926(3,20)
0.4884(8,60)
0.9259(14,100)

IDSR
0.0140(40,100)
0.4025(80,100)
0.3699(80,100)
0.8301(60,100)
0.0150(60,100)
0.1143(60,100)
0.1502(60,100)
0.9948(60,100)
0.0532(20,100)
0.2926(20,100)
0.4617(20,100)
0.8766(60,100)
0.0107(20,100)
0.6815(40,100)
0.7603(20,100)
0.3507(20,100)
0.2152(100,60)
0.6804(100,20)
0.5534(60,40)
0.7417(60,60)
0.1113(60,80)
0.7201(20,80)
0.6451(20,80)
0.4778(40,80)
0.2626(80,100)
0.4927(40,100)
0.5021(60,60)
0.9086(80,100)

In this work, DDSR and IDSR are compared with four multi-label classiﬁca-
tion techniques; BR, BR+ [3], CC [9] and PLST [11]. BR+ (Binary Relevance
with label dependency consideration) and CC (Classiﬁer Chains) are two well-
known methods, which incorporate label dependency in multi-label classiﬁcation.
PLST (Principle Label Space Transformation) is an eﬃcient algorithm that uses
the reduction of label space dimension. Using ten-fold cross validation method,
the results of the four evaluation metrics and the execution time are recorded
and shown in Table 2 and 3, respectively. All multi-label classiﬁcation methods
used in this work is implemented in R environment version 2.11.12 and linear

2

http://www.R-project.org/

Towards More Eﬃcient Multi-label Classiﬁcation

391

regression is used for the regression model in the model training phase. For PLST
and IDSR method, we experiments with K ranging from 20% to 100% of the
dimension of the original label matrix, with 20% as interval. Likewise, the pa-
rameter D is also varied from 20% to 100% of the dimension of the original
feature matrix, with 20% as interval. Though, the K parameter in DDSR ap-
proach is calculated from the minimum value between a number of features and
labels, this parameter is also used the same criteria as PLST and IDSR method.
To compare the computational time, all methods were performed on the AMD
Opteron Quad Core 8356 1.1 GHz Processor with 512 KB of cache, 64GB RAM.

5 Experimental Results

To evaluate our proposed approaches, seven datasets are used to compare per-
formance of BR, BR+, CC, PLST, DDSR and IDSR. Table 2 reports the best
value for each evaluation metric computed from all datasets. The numbers in
the superscript (x,y) represents the percentages of dimensions reduced in the
feature space and that in the label space, respectively. In the DDSR method,
the maximum number of reduced dimensions K cannot excess the minimum be-
tween the number of features (M ) and the number of labels (L) since the reduced
dimension has the same size of both feature and label space. The superscript [y]
in the PLST approach means the percentage of reduce labels, compared to the
original. Note that PLST does not reduce the feature space. In the Table 2, the
best value for each row is emphasized by bold font.

From the table, we can make some observations as follows. First, we observe
that both DDSR and IDSR give comparable performance in terms of hamming
loss, compared to BR, BR+, CC and PLST. On the other hand, DDSR and IDSR
approaches gain an average gap of 16% macro F-measure increment. Moreover,
the DDSR approach shows an average gap of 11% accuracy improvement while
the IDSR approach improves with an average gap of 15%. Likewise, the DDSR
approach can reduce the 0/1 loss with decrement of 5% while IDSR can reduce
with 8% gap. Note that the medical dataset whose number of features are greater
than the number of objects, gives the maximum improvement when both spaces
are reduced.

As shown in Table 3, the execution time of the DDSR approach was reduced
with a factor of 10, compared to PLST and approximately 100 times, com-
pared to the traditional BR approach. Likewise, the IDSR approach with lower-
dimensional features used less time than the PLST and the BR method. We can
conclude that our two proposed methods, DDSR and IDSR, could transform the
feature and label spaces into the reduced spaces with less computational time
than the traditional BR, BR+, CC and PLST. As an additional observation,
IDSR is better than DDSR in several datasets while DDSR can be executed
faster than IDSR. The smaller K and D are, the faster we can compute.

In more details, Table 4 presents the complexity of learning process. However,
the time used for the transformation process and covariance calculation is trivial.
The N , M and L denote the number of objects, features and labels, respectively.

392

E. Pacharawongsakda and T. Theeramunkong

Table 3. Average execution time (in seconds) on the seven datasets.
denotes the
denotes the numeric-feature dataset. Here, D is the
nominal-feature dataset and
reduced number of feature dimensions. K is the reduced number of label dimensions.

‡

†

Dataset

†

bibtex

†

corel5k

†

enron

†
medical

‡
emotions

‡
scene

‡

yeast

Method

BR
BR+
CC
PLST
DDSR
IDSR (D=20%×M )
IDSR (D=60%×M )
IDSR (D=100%×M )
BR
BR+
CC
PLST
DDSR
IDSR (D=20%×M )
IDSR (D=60%×M )
IDSR (D=100%×M )
BR
BR+
CC
PLST
DDSR
IDSR (D=20%×M )
IDSR (D=60%×M )
IDSR (D=100%×M )
BR
BR+
CC
PLST
DDSR
IDSR (D=20%×M )
IDSR (D=60%×M )
IDSR (D=100%×M )
BR
BR+
CC
PLST
DDSR
IDSR (D=20%×M )
IDSR (D=60%×M )
IDSR (D=100%×M )
BR
BR+
CC
PLST
DDSR
IDSR (D=20%×M )
IDSR (D=60%×M )
IDSR (D=100%×M )
BR
BR+
CC
PLST
DDSR
IDSR (D=20%×M )
IDSR (D=60%×M )
IDSR (D=100%×M )

20%×L
-
-
-
7101.01
69.85
1338.02
4656.35
14800.45
-
-
-
1172.75
69.73
364.17
817.59
1905.29
-
-
-
197.43
4.62
29.85
60.48
99.27
-
-
-
192.49
1.35
238.56
170.31
449.11
-
-
-
0.19
0.41
0.48
0.51
0.57
-
-
-
2.31
2.70
3.52
4.80
6.54
-
-
-
3.93
10.52
11.68
11.82
12.33

K

60%×L
-
-
-
21961.42
126.56
2360.49
14636.93
27392.90
-
-
-
3174.11
366.07
560.04
2667.54
5273.13
-
-
-
537.04
6.57
41.03
120.46
233.00
-
-
-
509.45
2.57
256.15
324.86
735.24
-
-
-
0.47
0.47
0.56
0.68
1.00
-
-
-
12.08
2.79
4.09
6.84
10.72
-
-
-
9.23
10.82
11.99
12.85
14.15

100%×L
40442.05
33537.76
19700.05
29815.07
283.36
3855.54
18787.84
46642.77
5721.81
11971.31
7007.41
5386.21
1265.54
818.53
4572.47
6397.23
656.94
1904.89
799.58
824.20
11.56
50.12
175.46
355.83
1353.93
2896.23
1414.56
1240.37
4.76
234.38
383.25
982.16
1.10
4.09
0.87
0.79
0.53
0.59
0.85
1.27
9.13
57.33
22.26
10.80
2.87
4.47
8.13
13.24
16.23
37.98
15.62
10.15
11.37
12.04
14.26
16.20

For our two approaches, D and K are the reduced number of dimensions. The
f (X, Y ) is the complexity of the model that depends on the number of objects
(X) and the number of features (Y ). When linear regression is applied for the
model training phase, it requires O(4XY 2 + X 3 + 2XY ) and O(Y ) for the clas-
siﬁcation phase. We can observe that the BR+ method is recognized as the
slowest algorithm since it appends labels to the feature space for incorporating
label dependency and it requires two learning process, initial prediction step and
ﬁnal prediction step, to complete the classiﬁcation process. On the other hand,

Towards More Eﬃcient Multi-label Classiﬁcation

393

Table 4. The learning complexity of BR, BR+, CC, PLST, DDSR and IDSR.
Note that the complexity is in the function f (X, Y ), where X is the number of objects
and Y is the number of features.

Methods Complexity (O)
O(L × f (N, M ))
O((L × f (N, M )) + (L × f (N, (M + L − 1))))
O(L × f (N, (M + L/2)))

BR
BR+
CC
PLST O(K × f (N, M ))
DDSR O(K × f (N, K))
O(K × f (N, D))
IDSR

our DDSR approach is the fastest method because the feature and label spaces
are transformed to the lower-dimensional space before classiﬁcation technique is
applied.

6 Conclusion

This paper presents two alternative approaches to handle the curse of dimen-
sionality problem in the feature space as well as the sparseness problem in the
label space. The Dependent Dual Dimensionality Reduction (DDSR) considers
the dependency between feature and label spaces before transforming the feature
and label spaces into a single reduced space. On the other hand, the Independent
Dual Space Reduction (IDSR) approach transforms the feature space and label
space into the two lower-dimensionality spaces. Experiments with a broad range
of multi-label datasets show that our two proposed approaches achieve a better
performance, compared to PLST and BR, as well as other recent methods such
as Classiﬁer Chains (CC) and BRplus (BR+). In addition, the DDSR approach
helps saving computational time while the IDSR approach tends to obtain better
classiﬁcation performance As our future work, we will analyze three dependen-
cies, feature-label, feature-feature, and label-label, in detail. The ensemble of
these dependencies may help in improving the performance.

Acknowledgement. This work has been supported by the TRF Royal Golden
Jubilee Ph.D. Program [PHD/0304/2551] and partially supported by the Na-
tional Research University Project of Thailand Oﬃce of Higher Education Com-
mission as well as the National Electronics and Computer Technology Center
(NECTEC) under Project Number NT-B-22-KE-38-54-01.

References

1. Bi, W., Kwok, J.: Multi-label classiﬁcation on tree- and dag-structured hierarchies.
In: Getoor, L., Scheﬀer, T. (eds.) Proceedings of the 28th International Conference
on Machine Learning (ICML 2011), pp. 17–24. ACM, New York (2011)

2. Boutell, M., Luo, J., Shen, X., Brown, C.: Learning multi-label scene classiﬁcation.

Pattern Recognition 37(9), 1757–1771 (2004)

394

E. Pacharawongsakda and T. Theeramunkong

3. Cherman, E.A., Metz, J., Monard, M.C.: Incorporating label dependency into the
binary relevance framework for multi-label classiﬁcation. Expert Systems with Ap-
plications (2011)

4. Elisseeﬀ, A., Weston, J.: A kernel method for multi-labelled classiﬁcation. In: Pro-
ceedings of the Advances in Neural Information Processing Systems, vol. 14, pp.
681–687. MIT Press (2001)

5. Golub, G., Reinsch, C.: Singular value decomposition and least squares solutions.

Numerische Mathematik 14, 403–420 (1970)

6. Han, Y., Wu, F., Jia, J., Zhuang, Y., Yu, B.: Multi-task sparse discriminant analysis
(mtsda) with overlapping categories. In: Proceedings of the Twenty-Fourth AAAI
Conference on Artiﬁcial Intelligence, pp. 469–474 (2010)

7. Hsu, D., Kakade, S., Langford, J., Zhang, T.: Multi-label prediction via compressed
sensing. In: Proceedings of the Advances in Neural Information Processing Systems,
vol. 22, pp. 772–780 (2009)

8. Katakis, I., Tsoumakas, G., Vlahavas, I.: Multilabel text classiﬁcation for auto-
mated tag suggestion. In: Proceedings of the the ECML/PKDD 2008 Discovery
Challenge (2008)

9. Read, J., Pfahringer, B., Holmes, G., Frank, E.: Classiﬁer chains for multi-label

classiﬁcation. Machine Learning, 1–27 (2011)

10. Schapire, R., Singer, Y.: Boostexter: A boosting-based system for text categoriza-

tion. Machine Learning 39(2/3), 135–168 (2000)

11. Tai, F., Lin, H.T.: Multi-label classiﬁcation with principle label space transforma-
tion. In: Proceedings of the 2nd International Workshop on Learning from Multi-
Label Data (MLD 2010), pp. 45–52 (2010)

12. Trohidis, K., Tsoumakas, G., Kalliris, G., Vlahavas, I.: Multi-label classiﬁcation of
music into emotions. In: Proceedings of the International Symposium/Conference
on Music Information Retrieval, pp. 325–330 (2008)

13. Tsoumakas, G., Katakis, I., Vlahavas, I.: Mining Multi-label Data. In: Data Mining

and Knowledge Discovery Handbook, 2nd edn. Springer (2010)

14. Tsoumakas, G., Katakis, I., Vlahavas, I.: Random k-labelsets for multilabel clas-
siﬁcation. IEEE Transactions on Knowledge and Data Engineering 23, 1079–1089
(2011)

15. Wang, H., Ding, C., Huang, H.: Multi-label Linear Discriminant Analysis. In: Dani-
ilidis, K., Maragos, P., Paragios, N. (eds.) ECCV 2010, Part VI. LNCS, vol. 6316,
pp. 126–139. Springer, Heidelberg (2010)

16. Yu, K., Yu, S., Tresp, V.: Multi-label informed latent semantic indexing. In: Pro-
ceedings of the 28th Annual International ACM SIGIR Conference on Research
and Development in Information Retrieval, pp. 258–265 (2005)

17. Zhang, M.L., Pea, J.M., Robles, V.: Feature selection for multi-label naive bayes

classiﬁcation. Information Science, 3218–3229 (2009)

18. Zhang, Y., Zhou, Z.H.: Multilabel dimensionality reduction via dependence max-
imization. ACM Transactions on Knowledge Discovery from Data (TKDD) 4(3),
1–21 (2010)


