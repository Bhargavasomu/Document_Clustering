Exploring Groups from Heterogeneous Data

via Sparse Learning

Huawen Liu1,3, Jiuyong Li2, Lin Liu2, Jixue Liu2,

Ivan Lee2, and Jianmin Zhao1

1 Zhejiang Normal University, Jinhua 321004, China

2 University of South Australia, Adelaide, SA5095, Australia

3 Academy of Mathematics and Systems Science, CAS, Beijing 100190, China

Abstract. Complexity networks, such as social networks, biological net-
works and co-citation networks, are ubiquitous in reality. Identifying
groups from data is critical for network analysis, for it can oﬀer deep
insights in understanding the structural properties and functions of com-
plex networks. Over the past decades, many endeavors from interdisci-
plinary ﬁelds have been attempted to identify groups from data. However,
little attention has been paid on exploring groups and their relationships
from diﬀerent views. In this work, we address this issue by using canon-
ical correlation analysis (CCA) to analyze groups and their interplays
in the networks. To further improve the interpretability of results, we
solve the optimization problem with sparse learning, and then propose a
generalized framework of group discovery from heterogeneous data. This
framework enables us to ﬁnd groups and explicitly model their relation-
ships from diverse views simultaneously. Extensive experimental studies
conducted on both synthetic and DBLP datasets demonstrate the eﬀec-
tiveness of the proposed method.

Keywords: Group discovery, complexity network, canonical correlation
analysis, LASSO, Sparse learning.

1

Introduction

It is well known that everything in the universe is relevant to others and nothing
independently exists. According to diﬀerent functions and properties, entities
intertwined with each other form groups, resulting in complex networks [14].
Typical examples include functional regulation models of proteins in biology
[10], trophic pathways of species in ecology [3,8], communities of people in soci-
ology [16], interlinks of web pages in World Wide Web [5], collaboration relation-
ships of authors in bibliography [11], and many others. Since complex networks
are ubiquitous in reality, network learning now gains increasing attentions from
a variety of disciplines including computer science, physics, economics, business
marketing, biology, engineering, epidemiology, social and behavioral science [4].
Exploring the structures, functions, as well as the interactions of networks is
very important, because it may provide us an insightful understanding how the

J. Pei et al. (Eds.): PAKDD 2013, Part I, LNAI 7818, pp. 556–567, 2013.
c(cid:2) Springer-Verlag Berlin Heidelberg 2013

Exploring Groups from Heterogeneous Data via Sparse Learning

557

networks work [16,4,20]. For example, identifying collaboration relationships of
scientists provides us an indication to which topics are popular and what the
research trends will be potentially studied in future [11]; uncovering functional
modules of proteins helps us understand which causes lead to certain diseases
and how the proteins are co-regularized [10]; revealing social communities of
people aids us in ﬁnding out what the interests and opinions of people are [23].
An essential yet challenging task of network learning is to discover functional
units in complex networks [4]. In literature, the functional unit is also known
as community, module, clique, cluster or coalition, depending on the speciﬁc
contexts or applications at hand [16]. These terms cover the entities only. Here we
call the functional unit as group, for the interplays between the entities have also
been taken into account. Since group discovery is helpful to analyze the network
structures and further capture knowledge about the functions and properties of
the network systems, it is not surprised that it has attracted many attentions
from diﬀerent domains [20,17,19].

Over the last decades, group discovery has been extensively investigated and
dozens of discovery methods, including graph partition models, clique based
models, clustering models, modularity maximization models, and so on, have
been developed [4]. However, most of them place emphases merely on identify-
ing function units. To the best of our knowledge, the interrelations of groups,
which are very common in nature, have not been fully exploited. In some cases,
identifying the interrelations is more crucial to understand the structural and
functional properties, for it may oﬀer us an insightful perspective to the net-
works.

Another issue is that the existing methods only take the entities with the
same type into consideration and ignore other information with diﬀerent types.
Empirical studies show that the network structures in real world are complex
and often involve the entities derived from heterogeneous data sources. The
information with diﬀerent types may bring beneﬁts to explore and analyze the
complex networks if it had been taken into account in network learning.

In this paper, we present a generalized framework to explicitly address the
problems mentioned above. It adopts canonical correlation analysis (CCA) to
analyze groups and their interrelations simultaneously. It not only allows to
handle the groups with diﬀerent types, but also provides an eﬀective solution
to scale their interrelations in a quantitative manner. Furthermore, we turn the
objective function of CCA into a LASSO penalized least square problem (i.e.,
(cid:2)1-norm penalty) by using complex linear algebra equivalent transformations.
Consequently, the proposed method can obtain an optimal sparse solution for
large-scale complex networks. Speciﬁcally, the contributions of this work are
twofold: 1) Our method can handle the entities from the networks with diﬀerent
types, and is also extensible to the multi-view or multi-slice situations after some
revisions have been made. 2) Since our method adopts LASSO ((cid:2)1-norm penalty)
to uncover groups and their interrelations, it has sparse property and the ﬁnal
results can be interpreted easily.

558

H. Liu et al.

The rest of this paper is organized as follows. Section 2 brieﬂy recalls previous
related work. We presents the basic concept of CCA in Section 3. Section 4 pro-
poses a new group discovery method by using sparse learning. The experimental
results and discussions on artiﬁcial and real world networks have been provided
in Section 5, followed by the conclusions in Section 6.

2 Related Work

Group discovery is a hot topic in network learning. Over the past years, a consid-
erable number of discovery methods have been witnessed. Here only the latest
methods will be discussed brieﬂy. Interested readers can refer to good survey
literature (see e.g., [4]) and references therein to get more information.

Since complex networks are often represented as graphs, graph analysis, which
has solid mathematical and theoretical fundamentals, has been extensively inves-
tigated in network learning. This kind of discovery methods apply graph theory
to explore groups, which tightly connected with each other by edges [4]. Typical
examples of such kind include clique-based, graph partition-based, ratio cut-
based, normalized cut-based and max-ﬂow-min-cut detection approaches. Note
that graph partitioning is a NP-hard problem. Moreover, it is also need the
number of groups and even their sizes, which are usually unknown in advance.
Clustering is another solution for group discovery. The clustering techniques,
such as hierarchical clustering, partitional clustering and spectral clustering,
have been taken to identify groups from data. For instance, Chi et al. [2] dis-
closed communities and evaluated their evolution process by using the spectral
clustering, which usually partitions nodes in a graph into clusters in terms of
the eigenvectors of its matrix representation. Like the graph-based methods, the
limitation of clustering is its relatively high computational cost, and the number
of clusters should also be pre-speciﬁed in some situations.

Modularity is widely used as a stopping measure for clustering, resulting in the
prevalence of the modularity maximization-based community discovery meth-
ods [13]. The great success of this framework relies on the modularity assump-
tion, that is, the higher a modularity is, the better its corresponding partition
is. This implies that the partition with maximum modularity on a given graph
is the best group. As a typical example, Jiang and McQuay [7] exploited modu-
larity Laplacian to discover communities by optimizing the modularity functions
with additional nonnegative constraint. However, the modularity optimization
is also NP-complete. Additionally, in real world the assumption of modularity
maximum is not always true [4].

Statistical inference is a powerful tool to deduce properties of data in ma-
chine learning, and has also been used to model and analyze graph topological
structures. The discovery methods based on block modeling, Bayesian inference,
latent Dirichlet allocation and model selection belong to this kind of representa-
tive cases. For example, Yang et al. [22] estimated parameters with a Bayesian

Exploring Groups from Heterogeneous Data via Sparse Learning

559

treatment in modeling networks and then developed a dynamic stochastic block
model to ﬁnd communities and their evolution in dynamic situations.

More recent studies of group discovery focus on exploring the evolution or
behaviors of groups from the multi-slice or multi-dimension prospective. The
representative examples of such framework have been illustrated in [14,22,12,18].
Pons and Latapy [15] integrated the communities discovered by diﬀerent meth-
ods in a post-processing way. Tang et al. [20] exploited four integration strategies,
i.e., network interactions, utility functions, structural features and community
partitions, to fuse the communities derived from multi-dimension sources. Lan-
cichinetti et al. [9] took edge directions, edge weights, overlapping communities,
hierarchies and community dynamics into account to identify the signiﬁcant
communities.

It is worthy to notice that most of discovery algorithms mentioned above can
not handle data from heterogeneous sources and the interrelations of groups. The
networks in reality, however, often encounter groups with diﬀerent types. Thus
identifying groups with diﬀerent types and their interrelations is important, be-
cause it may bring more information and provide us a deep insight to understand
the working mechanisms of the networks. Recently, author-topic model (ATM)
has gained much attraction in information retrieval [1]. It mainly adopts graph-
based (e.g., LDA and pLDA), semantic-based (e.g., PCA, LSI and pLSI) or their
extensions with other techniques (e.g., Gibbs sampling and HMM) to reveal the
groups of authors and topics [1]. However, ATM only qualitatively describes the
relationships of groups. As far as we are aware, little attention has been put on
measuring the interrelations of groups in a quantitative way.

3 Canonical Correlation Analysis

Canonical correlation analysis (for short, CCA) proposed by Hotelling is a well-

known multivariate technique [6]. Let X = {x1, ..., xp} and Y = {y1, ..., yq} be
i=1 xi = 0 and
two sets of variables, and both of them are centralized, i.e.,
(cid:2)q
i=1 yi = 0. CCA aims at obtaining two weighted linear combinations ωX and

(cid:2)p

ωY of X and Y , respectively, such that their correlation is maximal, i.e.,

ρ(ωX , ωY ) = max

< ωX, ωY >
(cid:2)ωX(cid:2)(cid:2)ωY (cid:2) ,

(1)

(cid:3)

= (v1, ..., vq).

where ωX = Xu and ωY = Y v are canonical variates with the weight vectors
(cid:3)
u

= (u1, ..., up) and v
The intuitive meanings of CCA is that it projects two sets of variables into a
lower-dimensional space in which they are maximally correlated. Since solving
the maximal value of ρ(ωX , ωY ) is invariant to the scaling of u and v either
together or independently, Eq. 1 can be rewritten as follows:

arg maxu,v u
s.t. u

(cid:3)
(cid:3)

(cid:3)
(cid:3)

X
X

Y v
Xu = 1, v

(cid:3)

Y

(cid:3)

Y v = 1.

(2)

For the optimization problem in Eq. 2, one of frequently used solutions is to for-
mulate it as a Lagrangian optimization form. Due to the limitation of space, here

560

H. Liu et al.

we will not provide the details of inferences step by step. Eventually, the CCA
formulation seeks for solving the eigenvectors and eigenvalues of the following
generalized eigenvalue problem:

(cid:3)
(cid:3)

X
Y

Y (Y
X(X

(cid:3)
Y )
(cid:3)
X)

(cid:3)
−1Y
−1X

Xu = λX
(cid:3)
Y v = λY

(cid:3)
Xu
(cid:3)
Y v

(3)

where λ is the eigenvalue, and u and v are its corresponding eigenvectors with
X is invertible (i.e., non-singular), the
respect to X and Y , respectively. If X
ﬁrst formula of Eq. 3 can be further formulated as a standard eigenvalue problem
Xu = λu. Otherwise, other strategies, such as gen-
of (X
eralized inverses and regularization, should be considered to obtain the inverse
of X

−1X
(cid:3)

X or Y

−1Y

Y (Y

Y )

Y .

(cid:3)

X)

(cid:3)

(cid:3)

(cid:3)

(cid:3)

(cid:3)

4 Group Relationship Discovery

4.1 Problem Statement

Assume X = {x1, x2, ..., xp} ∈ (cid:4)n×p and Y = {y1, y2, ..., yq} ∈ (cid:4)n×q are two sets
of variables (or features) representing n instances (or samples), where xi∩yj = ∅
for i = 1, ..., p and j = 1, ..., q. They can be treated as the n instances observed
from two diﬀerent perspectives.
In this paper, the purpose of group discovery is twofold. The ﬁrst one is, for
each set of variables, e.g., X, to identify a set of groups GX = {GXi|GXi ⊆
X, i = 1, ..., k}, such that the elements are highly relative to each other in the
same group GXi, while irrelative to those in other groups GXj (i (cid:8)= j). Similar
operations can be performed on Y to obtain GY . The second task of this paper
is to scale the relationships between groups derived from diﬀerent types, e.g.,
GXi ∈ GX and GY j ∈ GY , in a quantitative manner.

Since the group interrelations are often hidden and implicitly observed through
a large number of variables, it is not appropriate to evaluate correlations be-
tween pairs of variables individually, or simply calculate the accumulative total
of dependencies between variables from groups with diﬀerent views. A desirable
solution is to take the groups as a whole, rather than their individual variables,
into account in extracting the group interrelations.

4.2 Obtaining the First Group

Given two sets of variables X and Y , CCA can eﬀectively obtain their canon-
ical variates such that the correlations between them are maximal. However,
one of the CCA problems is that the computational cost of matrix decomposi-
tion is relatively high, especially when the quantity of variables exceeds tens of
thousands. The non-singular property of matrix is the second issue that should
also be taken into consideration. Here we go further and eﬃciently solve it via
(cid:2)1-norm regularization.

For the Eq. 2, we have the following property:

Exploring Groups from Heterogeneous Data via Sparse Learning

561

Property 1. The optimization problem of CCA (i.e., Eq. 2) is equivalent to a
distance minimization problem between two matrices, i.e.,

arg minu,v f (u, v) = (cid:2)Xu − Y v(cid:2)2

(cid:3)

(cid:3)

(cid:3)

(cid:3)

s.t. u

X

Xu = 1, v

Y

Y v = 1.

(4)

(5)

One may observe that this formulation is a least square if either Xu or Y v is
ﬁxed, i.e.,

f (u) = (cid:2)Xu − α(cid:2)2
f (v) = (cid:2)β − Y v(cid:2)2

To solve this optimization problem above, Partial Least Squares (PLS) seems
to be an eﬀective technique. The oﬀ-the-shelf method of PLS performs a re-
gression operation on each formulation within Eq. 5 alternately, and ultimately
obtains u and v. However, a possible drawback of PLS is that interpreting the
derived results becomes impossible, since u and v are weighted combinations of
all available variables in X and Y , respectively.

The interpretability of the obtained results is very important for the practi-
cal applications. It directly aﬀects users understanding data, such as providing
evidences for decision-makers, boosting product promotion for businessmen, un-
covering pathogenies of diseases for doctors. An eﬀective strategy is to make the
results sparse via variable selection.

We resort to a least absolute shrinkage and selection operator (for short,
LASSO) penalized model [21] to fulﬁll the purpose of variable selection. The un-
derlying is that LASSO enables us estimating the objective function and achiev-
ing variable selection simultaneously in one stage, where variables will be selected
by assigning zeros to the weights of variables with very small coeﬃcients. Specif-
ically, the ﬁrst objective function f (u) in Eq. 5 has been transformed to the
following form after a (cid:2)1-norm constraint has been performed on u.

f (u) = (cid:2)Xu − α(cid:2)2 + λu

(cid:3)|ui|

(6)
where λu (λu ≥ 0) is a tuning constant for u. Under this constraint, the weights
of some variables become zero if λu is enough small. Speciﬁcally, each weight
coeﬃcient ui decreases after comparing with a threshold. If ui is lower than the
threshold it will be set to zero, otherwise it will be modiﬁed or preserved. Thus
the purpose of sparse solution can be achieved. Given λu, ui is determined via
the soft-thresholding strategy, i.e.,

(cid:3)
ui = sgn((α

X)i)(|(α
(cid:3)

X)i| − λu)+

(7)

where sgn(z) is the sign function of z, and (z)+ is deﬁned to z if z > 0 and
0 if z ≤ 0. Similarly, f (v) can also be handled by performing another (cid:2)1-norm

penalty regularization λv.

After u and v have been obtained, the new coordinate systems of X and Y are
formed and represented as Xu and Y v, respectively. Since u and v are sparse,
Xu and Y v have good interpretability and represent the ﬁrst group derived
from X and Y , respectively. More importantly, this pair of groups has maximal
correlation, which can be measured in a quantitative way, i.e., ρ =< Xu, Y v >,
after normalized.

562

H. Liu et al.

4.3 Obtaining the Rest Groups

Assume the same instances are observed from two diﬀerent views. Obtaining only
one group from data is not enough, since it is unlikely to describe all relationships
of variables, especially in high-dimensional settings. Therefore, additional groups
should also be uncovered from data. To untie this knot, an alternative solution
is to minimize the criterion of discovering the ﬁrst group (i.e., Eq. 4) repeatedly,
each time on the residual variables obtained by wiping oﬀ the information of the
groups found previously.

To achieve the goal, a trick is available, where the original data will be updated
according to the information of the obtained groups. The central idea is similar
to the whitening step in image processing, which has often been used to lessen
dependencies or correlations of images. Speciﬁcally, after the ﬁrst group Xu has
ρ ], where ρ is
been disclosed, the original data X is extended as X
the canonical correlation. This extension aims to lessen the eﬀect of Xu, so as
not to frustrate the process of discovering other groups in succession. Y can also
be handled in a similar way. Indeed, this extension process is lossless, because
the original and extension matrices have the same canonical variates. Under the
context of Xn (or Yn), we can obtain its ﬁrst canonical variate Xnu2 (or Ynv2),
which actually is the second group of X (or Y ). Akin to u2, other groups can
be identiﬁed by updating X continuously, until the extension matrix contain no
more useful information.

(cid:3)
n = [X

(cid:3)

u√

4.4 Group Discovery Algorithm

Based on the analysis above, we present a new group discovery algorithm shown
as Alg. 1. It consists of two main loops, one nested within the other. The inner
iteration aims to identify one group, whereas the outer loop obtains all groups
hidden behind data.

The algorithm starts at initializing relative parameters, such as normalizing
X and Y . In the outer loop, the initial weights of variables vk (or uk) is set,
guaranteeing the inner iteration convergence. For simplicity, here we take the
ﬁrst right singular vector of Yk as the initial value of vk. Once the vector has been
assign an initial value, the inner iteration can solve the optimization problem
of CCA by using (cid:2)1-norm penalty. The process is repeated until the residual
matrices have no more useful information, or k is larger than a pre-speciﬁed
threshold K.

5 Simulation Experiments

5.1 Experiments on Synthetic Datasets

The synthetic data consists of two datasets. Each one represents the same 100
samples observed from diﬀerent views. The ﬁrst dataset X has 60 variables

Exploring Groups from Heterogeneous Data via Sparse Learning

563

Algorithm 1. Identifying groups and their relationships
1). Initialize parameters, e.g., X1=X and Y1=Y .
2). For k = 1, ..., K:

2.1) Initialize v and normalize it.
2.2) Repeat until convergence

2.2.1) Obtain u by virtue of Eq. 7.
2.2.2) Normalize u as u = u(cid:2)u(cid:2) .
2.2.3) Obtain v by virtue of Eq. 7.
2.2.4) Normalize v as v = v(cid:2)v(cid:2) .

2.3) Estimate the correlation ρ(Xu, Y v).
2.4) Update the residual matrices Xk and Yk.

in each group. The second dataset Y represents the same samples from another

{x1, ..., x60} that belongs to six groups GX = {xg1, ..., xg6} with 10 variables
view. It contains 150 variables {y1, ..., y150} also organized into six groups GY =
{yg1, ..., yg6} with the mean number of variables. Links between GX and GY are

generated as follows. For each sample in each dataset, it is randomly assigned to
one of groups and the probabilities of variables corresponding to the group are
larger than 0.2. For other groups, the values are less than 0.1.
The experimental results on the artiﬁcial datasets are presented in Table 1,
where |xg| denotes the number of variables contained within the i-th group xgi.
From this table, we know that the proposed method has excellent performance.
It not only identiﬁed all groups from both datasets, but also correctly measured
the interrelations between them. Additionally, the correlations of the groups
were also simultaneously calculated in a quantitative way after the groups were
obtained. For example, the group xg5 (the 6th line) obtained from X was ex-
actly matched with the group yg5 from Y , and their correlation coeﬃcient was
0.832. From the perspective of individual groups, the performance of the pro-

Table 1. Experimental results on the synthetic data

No. Group pair |yg| |xg| Correlation coeﬃcient
1
2
3
4
5
6

(xg1, yg1) 23
9
(xg2, yg2) 23
9
(xg6, yg6) 25 10
(xg4, yg4) 24 10
(xg3, yg3) 24 10
(xg5, yg5) 25 10

0.950
0.879
0.877
0.870
0.841
0.832

posed method is also quite well (see the |xg| and |yg| columns in Table 1). It
successfully identiﬁed all groups from the artiﬁcial data. More importantly, the
disclosed groups contain the right members, and are subsets of the corresponding
assumption ones. For instance, the xg2 group is a subset of the assumption one
in X, for it contains all members, i.e., x21, ..., x30, except x22. Besides, the size
of each disclosed group also approaches that of the original one.

564

H. Liu et al.

To further demonstrate the eﬀectiveness, we calculated the similarity of the
disclosed results to the assumption ones. Since the distributions of groups are
known in advance, we took normalized mutual information (NMI) and Jaccard
coeﬃcient as our measurements in the simulation experiments. In our exper-

iments, the values of NMI of the discovered groups XG = {xg1, .., xg6} and
Y G = {yg1, ..., yg6} to the assumption ones GX and GY are 0.992 and 0.991,

respectively. Correspondingly, the Jaccard coeﬃcients are 0.967 and 0.96, respec-
tively. These factors indicate that the proposed method is good at identifying
groups from data and the groups identiﬁed by our method are quite similar the
assumption ones.

5.2 Experiments on DBLP Datasets

A typical application of group discovery is science bibliography, where the groups
of keywords and authors are quite ubiquitous. Here we also carried out ex-
periments on the DBLP database [19]. For the convenience of discussion, we
downloaded the DBLP database1 and extracted 1071 papers published in ﬁve
international conferences from 2000 to 2004. In our experiment, the papers were
organized into two datasets, where the ﬁrst one A involved 2022 authors, while
the second one T contained 1065 terms extracted from paper titles.

After performing the proposed method on A and T , we obtained 143 groups
of authors GA and terms GT , where the authors in the ith group GAi are as-
sociated with the terms within the ith group GT i. Table 2 lists 10 over 143
disclosed groups, where CC denotes the correlation coeﬃcient of GAi to GT i,
while #P, #A and #T indicate how many papers, authors and terms were in-
volved in each group. As an example, the 8th group of authors GA8 (GT 8) covers
two papers involving 7 authors (7 terms). The topic of this pair mainly concerns
semantic information by parsing structures of texts.

Table 2. Ten groups discovered from the DBLP corpus

CC #P #A #T Authors

No
Gp1 0.999 3
Gp2 0.998 3
Gp4 0.994 3
Gp5 0.993 4
Gp6 0.993 2
Gp8 0.991 2
Gp11 0.990 2
Gp25 0.986 2
Gp38 0.979 3
Gp107 0.946 3

5
5
6
6
4
7
3
3
3
9

11 S Donoho, S Zhu, ...
9 D Cau, S Acid, ...
6 C Borgelt, K Lou, ...
5 ZW Ras, R Wong, ...
6 S Zhang, Y Zhu, ...
7 S Pradhan, L Lloyd, ...
4 M Sauban, BF White,...
5 J Adibi, E Oja,...
6 PW Eklund, G Stumme,... discovery, ontologies, ...
6 P Kim, J Ye, ...

Terms
option, markets, ...
eﬀects, causality, ...
expectation, fuzzy,...
proﬁt, rules, ...
elastic, burst, ...
structure, parsing, ...
proﬁling, document, ...
hidden, markov, ...

media, stream, ...

1

http://www.public.asu.edu/~{}ltang9/data/dblp.tar.gz

Exploring Groups from Heterogeneous Data via Sparse Learning

565

An interesting discovery made by the proposed method is that co-authors were
not always be included into the same author group. For instance, the three papers
in the 107th author group had 10 authors totally, while only 9 was included
within this author group. The missing co-author is M. Vazirgiannis. In fact,
M. Vazirgiannis published more papers about clustering than semantics. This
means that the results identiﬁed by our method rely on keywords in each group
on the whole, rather simply collect co-occurrence information.

The interrelations between GAi and GT i were also estimated by the proposed
method and illustrated as a dot graph (see Fig. 1), where the x-axis denotes
the group pairs Gpi = (GAi, GT i). The large values of correlation coeﬃcients
elucidate the eﬀectiveness of our method, for in each group pair the disclosed
groups with diﬀerent types were tightly related with each other. Among the 143
coeﬃcients, only six of them were lower than 0.90, whereas the minimal one was
still larger than 0.81. This, however, is reasonable for the last six group pairs
were identiﬁed upon the residual information the previously discovered groups
left and covered more authors and terms.

Fig. 1. The correlation coeﬃcients of group pairs

Identifying hot topics is one of major tasks extensively studied in the ﬁeld of
author-topic models. This goal can also be achieved by our method. We obtained
the hot topics in terms of the quantities of papers covered by the group pairs
for the sake of simpliﬁcation. Fig. 2 provides us an intuitive observation about
the popularity of topics during the ﬁve years. From this graph, one may easily
observe what topics are hot during the past years. For instance, Gp97 followed
by Gp140 and Gp135 was the most popular topic comprising 15 research papers.
It concerned the study of category, mining and relationship, and included six
authors, e.g., P Yu, R Hilderman, H Hamilton, etc. Another interesting fact is
that identifying the groups and their relationships can also oﬀer some insights
into the research trends and the collaboration relationships between diﬀerent
research teams if the latest information is available.

566

H. Liu et al.

Fig. 2. The cloud graph of groups

6 Conclusions

Generally, additional information from heterogeneous sources is helpful to ana-
lyze and understand network structures and functions. In this paper, we propose
a statistical learning framework to discover groups and capture their interrela-
tions from diﬀerent data sources. The central idea of our method is to formulate
group discovery as an optimization problem of CCA, and then extend it to a
LASSO problem to achieve the sparse purpose. Within this framework, group
discovery and their relationship measurement are turned out to be easily fulﬁlled
in one stage. Simulation experiments were conducted on both carefully designed
synthetic datasets and the DBLP corpus. The experimental results show that
the proposed method tends to identify accurate group information and reveal
useful insights in a given network from two diﬀerent views.

Acknowledgments. This work is partially supported by the Australian Re-
search Council(ARC) (DP130104090), the National NSF of China (61100119,
61272130, 61272468), and the Open Project Program of the National Labora-
tory of Pattern Recognition (NLPR).

References

1. Blei, D.: Introduction to probabilistic topic models. Comm. of the ACM 55(4),

77–84 (2012)

2. Chi, Y., Song, X., Zhou, D., Hino, K., Tseng, B.: Evolutionary spectral clustering
by incorporating temporal smoothness. In: Proc. of the 13th ACM SIGKDD Int’l
Conf. on Know. Disc. and Data Mining, pp. 153–162. ACM (2007)

3. Chiua, G., Westveld, A.: A unifying approach for food webs, phylogeny, social
networks, and statistics. Proc. Natl. Acad. Sci. USA 108(38), 15881–15886 (2011)
4. Fortunato, S.: Community detection in graphs. Phy. Rept. 486(3), 75–174 (2010)
5. Getoor, L., Diehl, C.: Link mining: a survey. SIGKDD Explor. Newsl. 7, 3–12

(2005)

Exploring Groups from Heterogeneous Data via Sparse Learning

567

6. Hotelling, H.: Relations between two sets of variables. Biometrika 28(3/4), 312–377

(1936)

7. Jiang, J.Q., McQuay, L.J.: Modularity functions maximization with nonnegative
relaxation facilitates community detection in networks. Phys. A 391(2), 854–865
(2012)

8. Krause, A., Frank, K., Mason, D., Ulanowicz, R., Taylor, W.: Compartments re-

vealed in food-web structure. Nature 426(6964), 282–285 (2003)

9. Lancichinetti, A., Radicchi, F., Ramasco, J., Fortunato, S.: Finding statistically

signiﬁcant communities in networks. PLoS ONE 6(4), e18961 (2011)

10. Liu, B., Liu, L., Tsykin, A., Goodall, G., Green, J., Zhu, M., Kim, C., Li, J.:
Identifying functional mirna-mrna regulatory modules with correspondence latent
dirichlet allocation. Bioinformatics 26(24), 3105–3111 (2010)

11. Michal, R.Z., Chaitanya, C., Thomas, G., Padhraic, S., Mark, S.: Learning author-

topic models from text corpora. ACM Trans. Inf. Syst. 28(1), Article 4 (2010)

12. Mucha, P., Richardson, T., Macon, K., Porter, M., Onnela, J.P.: Community struc-
ture in time-dependent, multiscale, and multiplex networks. Science 328, 876–878
(2010)

13. Newman, M., Girvan, M.: Finding and evaluating community structure in net-

works. Phys. Rev. E 69(2), 026113 (2004)

14. Palla, G., Barab´asi, A.L., Vicsek, T.: Quantifying social group evolution.

Nature 446(7136), 664–667 (2007)

15. Pons, P., Latapy, M.: Post-processing hierarchical community structures: Quality

improvements and multi-scale view. Theo. Comp. Sci. 412(8), 892–900 (2011)

16. Scott, J.: Social Network Analysis: A Handbook. SAGE Publications, London

(2000)

17. Serrour, B., Arenas, A., G´omez, S.: Detecting communities of triangles in complex

networks using spectral optimization. Comp. Comm. 34(5), 629–634 (2011)

18. Shen, H.W., Cheng, X.Q., Fang, B.X.: Covariance, correlation matrix, and the

multiscale community structure of networks. Phy. Rev. E 82(1), 016114 (2010)

19. Tang, L., Liu, H., Zhang, J., Nazeri, Z.: Community evolution in dynamic multi-
mode networks. In: Proc. of the 14th ACM SIGKDD Intl’ Conf. on Knowl. Disc.
and Data Mining, pp. 677–685. ACM (2008)

20. Tang, L., Wang, X., Liu, H.: Community detection via heterogeneous interaction

analysis. Know. Dis. Dat. Min. 25(1), 1–33 (2012)

21. Tibshirani, R.: Regression shrinkage and selection via the lasso: a retrospective. J.

R. Statist. Soc. B 73(3), 273–282 (2011)

22. Yang, T., Chi, Y., Zhu, S., Gong, Y., Jin, R.: Detecting communities and their
evolutions in dynamic social networks—a bayesian approach. Mach. Learn. 82(2),
157–189 (2011)

23. Yang, Z., Tang, J., Li, J.: Social community analysis via factor graph model. IEEE

Intelligent Sys. 26(3), 58–65 (2011)


