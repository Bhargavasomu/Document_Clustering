Feature Enriched Nonparametric Bayesian

Co-clustering

Pu Wang, Carlotta Domeniconi, Huzefa Rangwala, and Kathryn B. Laskey

George Mason University

4400 University Ave., Fairfax, VA 22030 USA
{pwang7,carlotta,rangwala}@cs.gmu.edu

klaskey@gmu.edu

Abstract. Co-clustering has emerged as an important technique for mining
relational data, especially when data are sparse and high-dimensional. Co-
clustering simultaneously groups the different kinds of objects involved in
a relation. Most co-clustering techniques typically only leverage the entries
of the given contingency matrix to perform the two-way clustering. As a
consequence, they cannot predict the interaction values for new objects. In
many applications, though, additional features associated to the objects of
interest are available. The Inﬁnite Hidden Relational Model (IHRM) has been
proposed to make use of these features. As such, IHRM has the capability
to forecast relationships among previously unseen data. The work on
IHRM lacks an evaluation of the improvement that can be achieved when
leveraging features to make predictions for unseen objects. In this work,
we ﬁll this gap and re-interpret IHRM from a co-clustering point of view.
We focus on the empirical evaluation of forecasting relationships between
previously unseen objects by leveraging object features. The empirical
evaluation demonstrates the effectiveness of the feature-enriched approach
and identiﬁes the conditions under which the use of features is most useful,
i.e., with sparse data.
Keywords: Bayesian Nonparametrics; Dirichlet Processes; Co-clustering;
Protein-molecule interaction data

Introduction

1
Co-clustering [11] has emerged as an important approach for mining relational
data. Often, data can be organized in a matrix, where rows and columns present
a symmetrical relation. Co-clustering simultaneously groups the different kinds
of objects involved in a relation; for example, proteins and molecules indexing a
contingency matrix that holds information about their interaction. Molecules
are grouped based on their binding patterns to proteins; similarly, proteins are
clustered based on the molecules they interact with. The two clustering processes
are inter-dependent. Understanding these interactions provides insight into the
underlying biological processes and is useful for designing therapeutic drugs.
Existing co-clustering techniques typically only leverage the entries of the
given contingency matrix to perform the two-way clustering. As a consequence,
they cannot predict the interaction values for new objects. This greatly limits the
applicability of current co-clustering approaches.

In many applications additional features associated to the objects of inter-
est are available, e.g., sequence information for proteins. Such features can be

2

Feature Enriched Nonparametric Bayesian Co-clustering

leveraged to perform predictions on new data. The Inﬁnite Hidden Relational
Model (IHRM) [36] has been proposed to leverage features associated to the rows
and columns of the contingency matrix to forecast relationships among previ-
ously unseen data. Although IHRM was originally introduced from a relational
learning point of view, it is essentially a co-clustering model that overcomes the
aforementioned limitations of existing co-clustering techniques. In particular,
IHRM is a nonparametric Bayesian model, which learns the number of row and
column clusters from the given samples. This is achieved by assuming Dirichlet
Process priors to the rows and columns of the contingency matrix. As such,
IHRM does not require the a priori speciﬁcation of the numbers of row and
column clusters in the data.

Existing Bayesian co-clustering models [30, 35, 19] are related to IHRM, but
none makes use of features associated to the rows and columns of the contin-
gency matrix. As a consequence, these methods can handle missing entries only
for already observed rows and columns (e.g., for a protein and a molecule used
during training, although not necessarily in combination). In particular, IHRM
can be viewed as an extension to the nonparametric Bayesian co-clustering
(NBCC) model [19]. IHRM adds to NBCC the ability to exploit features associ-
ated to rows and columns, thus enabling IHRM to predict entries for unseen
rows and/or columns. The authors in [36] have applied IHRM to collaborative
ﬁltering [27]. Co-clustering techniques have also been applied to collaborative
ﬁltering [33, 15, 10], but again none of these involve features associated to rows
or columns of the data matrix.

The work on IHRM [36] lacks an evaluation of the improvement that can be
achieved when leveraging features to make predictions for unseen objects. In this
work, we ﬁll this gap and re-interpret IHRM from a co-clustering point of view.
We call the resulting method Feature Enriched Dirichlet Process Co-clustering
(FE-DPCC). We focus on the empirical evaluation of forecasting relationships
between previously unseen objects by leveraging object features.
2 Related Work
Researchers have proposed several discriminative and generative co-clustering
models, e.g. [7, 29]. Bayesian Co-clustering (BCC) [30] maintains separate Dirich-
let priors for row- and column-cluster probabilities. To generate an entry in
the data matrix, the model ﬁrst generates the row and column clusters for the
entry from their respective Dirichlet-multinomial distributions. The entry is then
generated from a distribution speciﬁc to the row- and column-cluster. Like the
original Latent Dirichlet Allocation (LDA) [5] model, BCC assumes symmetric
Dirichlet priors for the data distributions given the row- and column-clusters.
Shan and Banerjee [30] proposed a variational Bayesian algorithm to perform
inference with the BCC model. In [35], the authors proposed a variation of BCC,
and developed a collapsed Gibbs sampling and a collapsed variational algorithm
to perform inference. All aforementioned co-clustering models are parametric,
i.e., they need to have speciﬁed the number of row- and column-clusters.

A nonparametric Bayesian co-clustering (NBCC) approach has been pro-
posed in [19]. NBCC assumes two independent Bayesian priors on rows and

Feature Enriched Nonparametric Bayesian Co-clustering

3

columns. As such, NBCC does not require a priori the number of row- and
column-clusters. NBCC assumes a Pitman-Yor Process [24] prior, which general-
izes the Dirichlet Process. The feature-enriched method we introduce here is an
extension of NBCC, where features associated to rows and columns are used.
Such features enable our technique to predict entries for unseen rows/columns.
A related work is Bayesian matrix factorization. In [17], the authors alleviated
overﬁtting in singular value decomposition (SVD) by specifying a prior distribu-
tion over parameters, and performing variational inference. In [26], the authors
proposed a Bayesian probabilistic matrix factorization method, that assigns a
prior distribution to the Gaussian parameters involved in the model. These
Bayesian approaches to matrix factorization are parametric. Nonparametric
Bayesian matrix factorization models include [8, 32, 25].

Our work is also related to collaborative ﬁltering (CF) [27]. CF learns the
relationships between users and items using only user preferences to items, and
then recommends items to users based on the learned relationships. Various
approaches have been proposed to discover underlying patterns in user con-
sumption behaviors [6, 16, 1, 18, 17, 26, 31, 12, 14]. Co-clustering techniques have
already been applied to CF [33, 15, 10]. None of these techniques involve features
associated to rows or columns of the data matrix. On the contrary, content-based
(CB) recommendation systems [3] predict user preferences to items using user
and item features. In practice, CB methods are usually combined with CF. The
approach we introduce in this paper is a Bayesian combination of CF and CB.
3 Background: Dirichlet Process
The Dirichlet process (DP) [9] is an inﬁnite-dimensional generalization of the
Dirichlet distribution. Formally, let S be a set, G0 a measure on S, and α0 a posi-
tive real number. The random probability distribution G on S is distributed as a
DP with concentration parameter α0 (also called the pseudo-count) and base mea-
sure G0 if, for any ﬁnite partition {Bk}1≤k≤K of S: (G(B1), G(B2),· · · , G(BK)) ∼
Dir(α0G0(B1), α0G0(B2),· · · , α0G0(BK)).
Let G be a sample drawn from a DP. Then with probability 1, G is a discrete
distribution [9]. Further, if the ﬁrst N − 1 draws from G yield K distinct values
θ∗1:K with multiplicities n1:K, then the probability of the Nth draw conditioned
on the previous N − 1 draws is given by the P´olya urn scheme [4]:

(cid:40)

θN =

θ∗k ,
with prob
θ∗K+1 ∼ G0, with prob

nk

N−1+α0
N−1+α0

α0

, k ∈ {1,· · · , K}

The DP is often used as a nonparametric prior in Bayesian mixture models [2].
Assume the data are generated from the following generative procedure: G ∼
Dir(α0, G0); θ1:N ∼ G; x1:N ∼ ∏N
n=1 F(·|θn), where the F(·|θn) are probability
distributions known as mixture components. Typically, there are duplicates
among the θ1:N; thus, multiple data points are generated from the same mixture
component. It is natural to deﬁne a cluster as those observations generated from
a given mixture component. This model is known as the Dirichlet process mixture
(DPM) model. Although any ﬁnite sample contains only ﬁnitely many clusters,

4

Feature Enriched Nonparametric Bayesian Co-clustering

there is no bound on the number of clusters and any new data point has non-zero
probability of being drawn from a new cluster [20]. Therefore, DPM is known as
an “inﬁnite” mixture model.

The DP can be generated via the stick-breaking construction [28]. Stick-
breaking draws two inﬁnite sequences of independent random variables, vk ∼
Beta(1, α0) and θ∗k ∼ G0 for k = {1, 2,· · · }. Let G be deﬁned as:

πk = vk

k−1∏

j=1

(1 − vj)

G =

∞
∑
k=1

πkδ(θ∗k )

(1)

where π = (cid:104)πk|k = 1, 2,· · · (cid:105) are mixing proportions and δ(θ) is the distribution
that samples the value θ with probability 1. Then G ∼ Dir(α0, G0). It is helpful
to use an indicator variable zn to denote which mixture component is associated
with xn. The generative process for the DPM model using stick-breaking is as
follows (additional details on the DPM model can be found in [20, 23]):
1. Draw vk ∼ Beta(1, α0), k = {1, 2,· · · } and calculate π as in Eq (1).
2. Draw θ∗k ∼ G0, k = {1, 2,· · · }
3. For each data point n = {1, 2,· · · , N}:
4 Feature Enriched Dirichlet Process Co-clustering
The observed data X of FE-DPCC are composed of three parts: the observed
row features X R, the observed column features XC, and the observed relational
features XE between rows and columns. If there are R rows and C columns,
then X R = (cid:104)xR
rc|r =
{1,· · · , R}, c = {1,· · · , C}(cid:105). XE may have missing data, i.e., some entries may
not be observed.

– Draw zn ∼ Discrete(π); Draw xn ∼ F(·|θ∗zn )

c |c = {1,· · · , C}(cid:105), and XE = (cid:104)xE

r |r = {1,· · · , R}(cid:105), XC = (cid:104)xC

c = l.

0 , GR

FE-DPCC is a generative model that assumes
two independent DPM priors on rows and columns.
We follow a stick-breaking representation to de-
scribe the FE-DPCC model. Speciﬁcally, assum-
ing row and column DP priors Dir(αR
0 ) and
Dir(αC
0 , GC
0 ), FE-DPCC draws row-cluster param-
0 , for k = {1,· · · , ∞}, column-
eters θ∗R
from GR
k
0 , for l = {1,· · · , ∞},
cluster parameters θ∗C
from GC
l
and co-cluster parameters θ∗E
kl from GE
0 , for each combination of k and l1; then
draws row mixture proportion πR and column mixture proportion πC as de-
ﬁned in Eq. 1. For each row r and each column c, FE-DPCC draws the row-cluster
indicator zR
c according to πR and πC, respec-
tively. Further, FE-DPCC assumes the observed features of each row r and each
column c are drawn from two parametric distributions F(·|θ∗R
k ) and F(·|θ∗C
l ),
respectively, and each entry, xE
rc, of the relational feature matrix is drawn from a
parametric distribution F(·|θ∗E
kl ), where zR
1 Every co-cluster is indexed by a row-cluster ID and a column-cluster ID. Thus, we
denote a co-cluster deﬁned by the kth row-cluster and the lth column-cluster as (k, l).

r and column-cluster indicator zC

Fig. 1: FE-DPCC model

r = k and zC

∞!πRzRrR!θ∗RkGR0∞GC0!θ∗Cl!πCzCcGE0CαR0αC0!θ∗Ekl!xRr!xCcxErcFeature Enriched Nonparametric Bayesian Co-clustering

5

The generative process for FE-DPCC is as follows and the FE-DPCC model

0 ), for k = {1,· · · , ∞} and calculate πR as in Eq (1)
0 ), for l = {1,· · · , ∞} and calculate πC as in Eq (1)

is illustrated in Figure 1.
k ∼ Beta(1, αR
1. Draw vR
0 , for k = {1,· · · , ∞}
2. Draw θ∗R
k ∼ GR
l ∼ Beta(1, αC
3. Draw vC
0 , for l = {1,· · · , ∞}
l ∼ GC
4. Draw θ∗C
0 , for k = {1,· · · , ∞} and l = {1,· · · , ∞}
5. Draw θ∗E
kl ∼ GE
6. For each row r = {1,· · · , R}, draw zR
7. For each column c = {1,· · · , C}, draw zC
rc ∼ F(·|θ∗E
8. For each entry xE
zR
r zC
c

r ∼ Discrete(πR), and draw xR

c ∼ Discrete(πC), and draw xC

rc, draw xE

)

r ∼ F(·|θ∗R

zR
r

)

c ∼ F(·|θ∗C

zC
c

)

4.1

Inference

f (xC

c |θ∗C

zC
c

)g(θ∗C
zC

c |ζC)dθ∗C

zC
c

c=1

r=1

c=1

zC
c

zR
r

))

))(

))(

r=1

r=1

c=1

c=1

R∏

C∏

R∏

f (xE

f (xC

The likelihood of the observed data is given by:
C∏

c |θ∗C

rc|θ∗E
zR
r zC
c

l ) and F(·|θ∗E

p(X|ZR, ZC, θ∗R, θ∗C, θ∗E) =(

where f (·|θ∗R
k ), f (·|θ∗C
functions of F(·|θ∗R
{1,· · · , R}(cid:105); ZC = (cid:104)zC
(cid:104)θ∗C
θ∗R, θ∗C, and θ∗E is:
p(X|ZR, ZC, GR

f (xR
l ) and f (·|θ∗E
k ), F(·|θ∗C
c |c = {1,· · · , C}(cid:105); θ∗R = (cid:104)θ∗R

r |θ∗R
kl ) denote the probability density (or mass)
r |r =
k |k = {1,· · · , ∞}(cid:105); θ∗C =
|l = {1,· · · , ∞}(cid:105); and θ∗E = (cid:104)θ∗E
The marginal likelihood obtained by integrating out the model parameters
(cid:90)
(cid:33)(cid:32) R∏

kl ), respectively; ZR = (cid:104)zR
kl |k = {1,· · · , ∞}, l = {1,· · · , ∞}(cid:105).

r |θ∗R
C∏

(cid:32) C∏

(cid:32) R∏

r |ζR)dθ∗R

)g(θ∗R
zR

0 , GC

0 , GE

(cid:33)

(cid:33)

0 ) =

f (xR

(cid:90)

(cid:90)

(2)

r=1

zR
r

zR
r

l

f (xE

rc|θ∗E
zR
r zC
c

)g(θ∗E
c |ζE)dθ∗E
r zC
zR
zR
r zC
c

0 and GE
kl ) and GE

where g(·|ζR), g(·|ζC) and g(·|ζE) denote the probability density functions of
0 , respectively. We assume F(·|θ∗R
GR
0 , GC
0 , and
F(·|θ∗E
0 are all pairwise conjugate. Thus, there is a closed form expres-
sion for the marginal likelihood (2). The conditional distribution for sampling the
row-cluster indicator variable zR
r is as follows. For populated
row-clusters k ∈ {ZR
p(zR

r(cid:48)}r(cid:48)={1,··· ,r−1,r+1,··· ,R},

r for the rth row xR

0 , F(·|θ∗C

l ) and GC

k ) and GR

(3)

r = k|xR
N ¬r

k

(cid:18)(cid:90)
rc}c∈{1,··· ,C}, X R¬r, XE¬r, ZR¬r) ∝
r ,{xE
f (xR

(cid:19) C∏

k

0

kzC
c

f (xE

r |θ∗R

)g(θ∗E
kzC

rc|θ∗E

|ζ∗R¬r

k )g(θ∗R
k

c |ζ∗E¬r

)dθ∗R
k
R − 1 + αR
c=1
where ¬r means excluding the rth row, N ¬r
is the number of rows assigned to the
k
kth row-cluster excluding the rth row, ζ∗R¬r
is the hyperparameter of the posterior
distribution of the kth row-cluster parameter θ∗R
k given all rows assigned to the
kth row-cluster excluding the rth row, and ζ∗E¬r
is the hyperparameter of the
kzC
c
posterior distribution of the co-cluster (k, zC
c ) given all entries assigned to it
excluding the entries in the rth row. When k /∈ {zR
r(cid:48)}r(cid:48)={1,··· ,r−1,r+1,··· ,R}, i.e., zR
r is
being set to its own singleton row-cluster, the conditional distribution becomes:

)dθ∗E
kzC
c

kzC
c

k

(cid:18)(cid:90)

(cid:19)

6

Feature Enriched Nonparametric Bayesian Co-clustering

p(zR

r = k|xR

αR
0

(cid:18)(cid:90)
(cid:18)(cid:90)
rc}c∈{1,··· ,C}, X R¬r, XE¬r, ZR¬r) ∝
r ,{xE
f (xR

(cid:19) C∏

k )g(θ∗R
k

|ζR)dθ∗R

k

r |θ∗R

(4)

(cid:19)

f (xE

rc|θ∗E

kzC
c

)g(θ∗E
kzC

c |ζ∗E¬r

kzC
c

)dθ∗E
kzC
c

l

l

0

0

c=1

(5)

f (xE

(cid:19)

)dθ∗C
l

c |θ∗C

|ζ∗C¬c

l )g(θ∗C
l

C − 1 + αC

(cid:19) R∏

c = l|xC
N ¬c

R − 1 + αR
The conditional distribution for sampling the column-cluster indicator vari-
c is obtained analogously. For populated column-

c for the cth column xC

able zC
clusters l ∈ {ZC
c(cid:48)}c(cid:48)={1,··· ,c−1,c+1,··· ,C},
(cid:18)(cid:90)
rc}r∈{1,··· ,R}, XC¬c, XE¬c, ZC¬c) ∝
p(zC
c ,{xE
f (xC

(cid:18)(cid:90)
rc|θ∗E
where ¬c means excluding the cth column, N ¬c
is the number of columns
assigned to the lth column-cluster excluding the cth column, ζ∗C¬c
is the hyper-
parameter of the posterior distribution of the lth column-cluster parameter θ∗C
l
given all columns assigned to the lth column-cluster excluding the cth column,
and ζ∗E¬c
is the hyperparameter of the posterior distribution of the co-cluster
zR
r l
r , l) given all entries assigned to it excluding the entries in the cth column.
(zR
c /∈ {zC
If zC
c is being assigned to its own singleton
column-cluster, the conditional distribution becomes:
p(zC
c = l|xC

c(cid:48)}c(cid:48)={1,··· ,c−1,c+1,··· ,C}, i.e., zC
(cid:18)(cid:90)
(cid:18)(cid:90)
rc}r∈{1,··· ,R}, XC¬c, XE¬c, ZC¬c) ∝
c ,{xE
f (xC

(cid:19) R∏

r l)g(θ∗E
zR
zR

r l|ζ∗E¬c
zR
r l

)dθ∗E
zR
r l

(cid:19)

αC
0

(6)

r=1

l

l

C − 1 + αC

0

c |θ∗C

l )g(θ∗C
l

|ζC)dθ∗C

l

r=1

f (xE

rc|θ∗E

r l)g(θ∗E
zR
zR

r l|ζ∗E¬c
zR
r l

)dθ∗E
zR
r l

5 Experimental Evaluation
We conducted experiments on two rating datasets and two protein-molecule
interaction datasets. MovieLens2 is a movie recommendation dataset containing
100,000 ratings in a sparse data matrix for 1682 movies rated by 943 users. Jester3
is a joke rating dataset. The original dataset contains 4.1 million continuous
ratings of 140 jokes from 73,421 users. We chose a subset containing 100,000
ratings. Following [30], we uniformly discretized the ratings into 10 bins.

We also used two protein-molecule interaction datasets. The ﬁrst dataset
(MP14) consists of G-protein coupled receptor (GPCR) proteins and their in-
teraction with small molecules [13]. These interactions are the product of an
experiment that determines whether a particular protein target is modulated by
a molecule. MP1 had 4051 interactions between 166 proteins and 2687 molecules.
The second dataset (MP25) [21] differs from MP1 in that the protein targets
belong to a more general class and are not restricted to GPCRs. The use of targets
restricted to a speciﬁc group of proteins (GPCRs) is similar to a chemogenomics

2 http://www.grouplens.org/node/73
3 http://goldberg.berkeley.edu/jester-data/
4 http://pharminfo.pharm.kyoto-u.ac.jp/services/glida/
5 http://pubchem.ncbi.nlm.nih.gov/

Feature Enriched Nonparametric Bayesian Co-clustering

7

Table 2: Average Test Perplexity

DPCC

FE-DPCC

MovieLens

Jester

MP1

MP2

Row and Column Observed 3.327 (0.020) 17.111 (0.031) 1.430 (0.011) 1.484 (0.013)
Row or Column Unseen
4.427 (0.047) 19.322 (0.025) 8.845 (0.011) 7.987 (0.011)
Overall Perplexity
4.424 (0.087) 18.116 (0.035) 8.843 (0.013) 7.980 (0.021)
Row and Column Observed 3.344 (0.021) 17.125 (0.040) 1.435 (0.024) 1.489 (0.023)
3.892 (0.026) 17.836 (0.053) 1.453 (0.026) 1.509 (0.024)
Row or Column Unseen
Overall Perplexity
3.889 (0.031) 17.836 (0.062) 1.450 (0.046) 1.501 (0.045)

approach where the assumption is that proteins in the same family have a similar
activity/interaction proﬁle. MP2 had 154 proteins, 2876 molecules and a total of
7146 positive interactions. Table 1 summarizes the dataset characteristics.

5.1 Experimental Methodology and Feature Information

Train

Table 1: Training and Test Data

We ﬁrst compared FE-DPCC with a
variant of NBCC, called Dirichlet Process
Co-clustering (DPCC). DPCC restricts
the Pitman-Yor priors of NBCC to the
special case of independent Dirichlet
Process priors on rows and columns,
so as to compare with FE-DPCC fairly.
So, the difference between FE-DPCC
and DPCC is that FE-DPCC augments
DPCC to exploit row and column features. We ran 1000 iterations of Gibbs
sampling for both FE-DPCC and DPCC. We used perplexity as an evaluation
metric to compare FE-DPCC with DPCC on all the test data. The perplexity of
a dataset D is deﬁned as perplexity(D) = exp (−L(D)/N), where L(D) is the
log-likelihood of D, and N is the number of data points in D. The higher the
log-likelihood, the lower the perplexity, and the better a model ﬁts the data.

MovieLens Jester MP1 MP2
2674
943
149
1650
80000
5000
5.142% 1.708% 2.508% 1.255%
1647
927
145
1407
2146
20000
1.533% 0.991% 1.806% 0.899%

# Rows
# Columns
# Entries
Density
# Rows
# Columns
# Entries
Density

33459
140
80000

14523
139
20000

1961
61
3000

856
68
1051

Test

kl ), and g(θ∗E

The relational features in our data are discrete. We assume f (·|θ∗E
kl ) is a
kl |ζE) is a Dirichlet distribution,
categorical distribution, Cat(·|θ∗E
kl |ϕ), with ζE = ϕ. Because of conjugacy, we can marginalize out θ∗E
Dir(θ∗E
kl .
Without loss of generality, we assume that f (·|θ∗E
kl ) is a D-dimensional categor-
ical distribution with support {1,· · · , D}, and we denote the Dirichlet hyper-
parameter as ζE = ϕ = (cid:104)ϕd|d = {1,· · · , D}(cid:105). The predictive distribution of the
co-cluster (k, l) to observe a new entry xE

(cid:90)

p(xE

r(cid:48)c(cid:48) = d|ζ∗E
Cat(xE

kl , zR
r(cid:48)c(cid:48) = d|θ∗E

r(cid:48) = k, zC
kl )Dir(θ∗E

c(cid:48) = l) =
kl |ϕ∗kl)dθ∗E

kl

(cid:90)

r(cid:48)c(cid:48) = d, d ∈ {1,· · · , D}, is:
r(cid:48)c(cid:48) = d|θ∗E
f (xE
kl )dθ∗E
∝ N d
(k,l) + ϕd

kl )g(θ∗E

kl |ζ∗E

kl =

where ϕ∗kl is the posterior hyperparameter of the Dirichlet distribution of the
co-cluster (k, l), and N d
(k,l) is the number of entries assigned to the co-cluster
(k, l) and is equal to d.

In MovieLens, users (rows) are represented with age, gender, and occu-
pation, whereas the movies (columns) are associated with a 19-dimensional
genre-representing binary vector. We assumed independence among the row
features and the column features conditional on row- and column-clusters. We
modeled age as drawn from a Poisson distribution, Poi(·|λ), with a conjugate

8

Feature Enriched Nonparametric Bayesian Co-clustering

Gamma prior, Gamma(λ|, ς). We modeled gender as drawn from a Bernoulli
distribution, Ber(·|ϑ), with a conjugate Beta prior Beta(ϑ|κ, ). The occupation
feature is categorical, modeled as Cat(·|φ), with Dirichlet prior, Dir(φ|ϕ). Thus,
the row feature parameter is given by θ∗R
k = (cid:104)λ∗k, ϑ∗k , φ∗k(cid:105), and the row feature
prior hyperparameter is ζR = (cid:104), ς, ϑ, ϕ(cid:105). We denote the feature vector of a
new user as xR
r(cid:48) = (cid:104)ar(cid:48), gr(cid:48), or(cid:48)(cid:105), where ar(cid:48), gr(cid:48), and or(cid:48) represent the age, gender
and occupation, respectively. The predictive distribution of the kth row-cluster
observing a new user, xR

p(xR

r(cid:48), is:
(cid:18)(cid:90)
r(cid:48)|∗k , ς∗k, κ∗k , ∗k , ϕ∗k , zR
Ber(gr(cid:48)|ϑ∗k )Beta(ϑ∗k|κ∗k , ∗k )dϑ∗k

r(cid:48) = k) =

(cid:18)(cid:90)
(cid:19)
(cid:19)(cid:18)(cid:90)
Poi(ar(cid:48)|λ∗k )Gamma(λ∗k|∗k , ς∗k)dλ∗k
Cat(or(cid:48)|φ∗k )Dir(φ∗k|ϕ∗k )dφ∗k

(cid:19)

(7)

c(cid:48)|ψ∗l )Dir(ψ∗l |ϕ∗l )dψ∗l , where ζ∗C

c(cid:48) = l) =(cid:82) Mul(xC

c(cid:48) = l) =(cid:82) Mul(xC

where ∗k, ς∗k, κ∗k , ∗k , and ϕ∗k are the posterior hyperparameters (k indexes the
k = (cid:104)∗k, ς∗k, κ∗k , ∗k , ϕ∗k(cid:105). We assume that features asso-
row-clusters). Denote ζ∗R
ciated with movies are generated from a Multinomial distribution, Mul(·|ψ),
l = ψ∗l , and ζC = ϕ. The pre-
with Dirichlet prior, Dir(ψ|ϕ). Accordingly, θ∗C
dictive distribution of the lth column-cluster observing a new movie, xC
c(cid:48), is:
c(cid:48)|ϕ∗l , zC
p(xC
l = ϕ∗l is the poste-
rior hyperparameter of the Dirichlet distribution (l indexes the column-clusters).
In Jester, there are no features associated with the users (rows), thus row-
clusters cannot predict an unseen user. We used a bag-of-word representa-
tion for joke features, and assumed each joke feature vector is generated from
a Multinomial distribution, Mul(·|ψ), with a Dirichlet prior, Dir(ψ|ϕ). The
predictive distribution of the lth column-cluster observing a new joke, xC
c(cid:48), is:
p(xC

c(cid:48)|ϕ∗l , zC
For MP1 and MP2, rows represent molecules and columns represent proteins.
We extracted k-mer features from protein sequences. For MP1, we also used
hierarchical features for proteins obtained from annotation databases. We used
a graph-fragment-based feature representation that computes the frequency
of different length cycles and paths for each molecule. These graph-fragment-
based features were derived using AFGEN [34] (default parameters were used),
known to capture structural aspects of molecules effectively. We assumed each
protein was generated from a Multinomial distribution, Mul(·|ψp), with a Dirich-
let prior, Dir(ψp|ϕp). We also assumed each molecule was generated from a
Multinomial distribution, Mul(·|ψm), with a Dirichlet prior, Dir(ψm|ϕm). The
predictive distribution of the kth row-cluster observing a new molecule, xR
r(cid:48), is:
p(xR

r(cid:48) = k) =(cid:82) Mul(xR

c(cid:48)|ψ∗l )Dir(ψ∗l |ϕ∗l )dψ∗l .

r(cid:48)|ψ∗m

, zR
c(cid:48)|ϕ∗p

c(cid:48) = l) =(cid:82) Mul(xC

r(cid:48)|ϕ∗m
The predictive distribution of the lth column-cluster observing a new protein,
xC
c(cid:48), is: p(xC
5.2 Results
We performed a series of experiments to evaluate the performance of FE-DPCC
across the four datasets. All experiments were repeated ﬁve times, and we

k )dψ∗m
|ϕ∗m
k
|ϕ∗p
l )dψ∗p
l )Dir(ψ∗p
l
l

k )Dir(ψ∗m
k
c(cid:48)|ψ∗p

, zC

k

.

.

l

Feature Enriched Nonparametric Bayesian Co-clustering

9

Table 3: Evaluation of featu-
re enrichment on MP1.

Perplexity
Shufﬂe P
3.034 (0.083)
Exchange M 2.945 (0.083)
Exchange P
2.932 (0.071)
Exchange M&P 2.991 (0.095)
Use Only M 7.235 (0.043)
Use Only P
7.789 (0.045)
1.450 (0.046)
Use M and P

Table 4: Evaluation of
protein features on MP1.

Perplexity

Table 5: RMSE on Test Data.

FE-DPCC

Slope One

Movie 0.838 (0.031) 0.924 (0.035)
Jester 0.896 (0.062) 0.961 (0.065)

2-mer 1.471 (0.057)
3-mer 1.437 (0.044)
4-mer 1.441 (0.049)
5-mer 1.450 (0.046)
HF
1.413 (0.010)

report the average (and standard deviation) perplexity across the ﬁve runs.
The experiments were performed on an Intel four core, Linux server with 4GB
memory. The average running time for FE-DPCC was 1, 3, 3.5 and 2.5 hours on
the MovieLens, Jester, MP1 and MP2 datasets, respectively.
Feature Enrichment Evaluation Table 2 shows the average perplexity (and
standard deviations) across ﬁve runs on the test data. To analyze the effect of new
rows and columns on the prediction capabilities of the algorithms, we split each
test set into subsets based on whether the subset contains new rows or columns
w.r.t. the corresponding training data. Table 2 shows that the overall perplexity
of FE-DPCC is lower than that of DPCC on all data, with an improvement of
12%, 1.5%, 84% and 81% for MovieLens, Jester, MP1 and MP2, respectively.

FE-DPCC is signiﬁcantly better than DPCC on the portion of the test data that
contains unseen rows and/or columns. These test sets consist of entries for rows
and columns that are not included in the training set. The DPCC algorithm does
not use features; as such it can predict entries for the new rows and columns
using prior probabilities only. In contrast, the FE-DPCC algorithm leverages
features along with prior probabilities; this enables our approach to predict
values for the independent test entries more accurately. This ability is a major
strength of our FE-DPCC algorithm. For the portion of the test data whose
rows and columns are observed in the training as well, the perplexity values of
FE-DPCC and DPCC are comparable. The standard deviations indicate that the
algorithms are stable, yielding consistent results across different runs.

To accurately assess the performance of FE-DPCC, we performed a set of
experiments that involved a perturbation of the protein and molecule features
on MP1. Results are in Table 3. For these experiments, we used k-mer sequence
features. First, we took the protein sequences (i.e., columns) and shufﬂed the
ordering of the amino acids. This alters the ordering of the protein sequence
but maintains the same composition (i.e., the shufﬂed sequences have the same
number of characters or amino acids). We refer to this scheme as “Shufﬂe”. It
achieves an average perplexity of 3.034, versus the average perplexity of 1.450
achieved by FE-DPCC (with no shufﬂing of features). We also devised a scheme
in which the row and/or column features are exchanged, e.g., the features of a
particular molecule are exchanged with the features of another molecule. Such
an exchange causes the inclusion of incorrect information within the FE-DPCC

10

Feature Enriched Nonparametric Bayesian Co-clustering

(a) MovieLens

(b) Jester

(a) MovieLens

(b) Jester

Fig. 2: Co-clusters Learned by FE-DPCC

Fig. 3: Test Perplexity with Different Densities

algorithm. Our aim was to assess the strength of FE-DPCC when enriched with
meaningful and correct features. We refer to this scheme as “Exchange.” Table 3
shows the results of exchanging molecule features only (Exchange M), protein
features only (Exchange P), and both (Exchange M and P). We noticed an average
perplexity of 2.9 in each case. We also evaluated the FE-DPCC algorithm when
only molecule or only protein features are used (“Use Only M” and “Use only
P” in Table 3). The use of only one set of features prevents the co-clustering
algorithm from making inferences on the unseen rows or columns in the test set.
For MP1 we performed additional experiments to evaluate the sequence
features. The features are overlapping subsequences of a ﬁxed length extracted
from the protein sequences. We used k-mer lengths of 2, 3, 4 and 5, and observed
that the average perplexity (Table 4) remained similar. As such, we used 5-mer
features in all the experiments. We also compared the sequence features for the
proteins to an alternate feature derived from a hierarchical biological annotation
of the proteins. For MP1 the hierarchical features were extracted as done in the
previous study [13, 22]. From Table 4 we observe that the hierarchical features
(HF) achieved a slightly lower perplexity as compared to the 5-mer features. This
is encouraging, as it suggests that sequence features perform similarly to manual
annotation (hierarchy), that may not be easily available for all the proteins.

Comparative Performance We compared FE-DPCC with a well known collabo-
rative ﬁltering model, Slope One [16]. We used a Slope One implementation from
the Apache Mahout machine learning library6. We used the root mean square
error (RMSE) [6] to compare FE-DPCC and Slope One on MovieLens and Jester.
Table 5 shows the RMSE values (and standard deviations) of FE-DPCC and Slope
One across ﬁve runs on the test sets7. These results show that incorporating row
and column features is beneﬁcial for the prediction of relationships.

Visualization of Co-clusters In Figure 2 we illustrate the co-cluster structures
learned by FE-DPCC on MovieLens and Jester. We calculate the mean entry
value for each co-cluster, and plot the resulting mean values.

Data Density We varied the density of MovieLens and Jester to see how it
affects the perplexity of FE-DPCC and DPCC. We varied the matrix density by
randomly sampling 25%, 50% and 75% of the entries in the training data. The
sampled matrices were then given as input to DPCC and FE-DPCC to train

6 http://mahout.apache.org/
7 No new rows or columns in the test sets w.r.t. the training sets.

UsersMovies  1234567123456781.522.533.54JokesUsers  123123456789−4−202460.250.50.7512468101214Matrix Density PercentagePerplexity  FE−DPCCDPCC0.250.50.75116182022242628Matrix Density PercentagePerplexity  FE−DPCCDPCCFeature Enriched Nonparametric Bayesian Co-clustering

11

a model and infer unknown entries on the test data. Figure 3 illustrates the
results averaged across ﬁve iterations. As the sparsity of the relational matrix
increases the test perplexity increases for both FE-DPCC and DPCC. But DPCC
has far higher perplexity for a sparser matrix. As the matrix sparsity increases,
the information within the relational matrix is lost and the FE-DPCC algorithm
relies on the row and column features. Thus, for sparser matrices FE-DPCC
shows far better results than DPCC. These experiments suggest the reason why
we see a more dramatic difference between the two algorithms for MP1 and
MP2, which are very sparse (see Table 1).
6 Conclusion
In this work, we focus on the empirical evaluation of FE-DPCC to predict relation-
ships between previously unseen objects by using object features. We conducted
experiments on a variety of relational data, including protein-molecule interac-
tion data. The evaluation demonstrates the effectiveness of the feature-enriched
approach and demonstrates that features are most useful when data are sparse.

Acknowledgements

This work was in part supported by NSF III-0905117.
References

1. D. Agarwal and B.-C. Chen. Regression-based latent factor models. In Proceedings
of the ACM International Conference on Knowledge Discovery and Data Mining, pages
19–28, 2009.

2. C. E. Antoniak. Mixtures of Dirichlet Processes with Applications to Bayesian Non-

parametric Problems. The Annals of Statistics, 2(6):1152–1174, 1974.

3. M. Balabanovic and Y. Shoham. Fab: content-based, collaborative recommendation.

4. D. Blackwell and J. B. Macqueen. Ferguson distributions via P´olya urn schemes. The

Commun. ACM, 40(3):66–72, 1997.

Annals of Statistics, 1:353–355, 1973.

5. D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet Allocation. Journal of Machine

Learning Research, 3(4-5):993–1022, 2003.

6. Y.-H. Chen and E. I. George. A bayesian model for collaborative ﬁltering. In 7th

International Workshop on Artiﬁcial Intelligence and Statistics, 1999.

7. I. S. Dhillon, S. Mallela, and D. S. Modha. Information-theoretic co-clustering. In
Proceedings of the ACM International Conference on Knowledge Discovery and Data Mining,
pages 89–98, 2003.

8. D. B. Dunson, Y. Xue, and L. Carin. The matrix stick-breaking process: Flexible Bayes
meta-analysis. Journal of the American Statistical Association, 103(481):317–327, 2008.
9. T. S. Ferguson. A Bayesian analysis of some nonparametric problems. The Annals of

Statistics, 1(2):209–230, 1973.

10. T. George and S. Merugu. A scalable collaborative ﬁltering framework based on
co-clustering. In Proceedings of the IEEE International Conference on Data Mining, pages
625–628, 2005.

11. J. A. Hartigan. Direct clustering of a data matrix. Journal of the American Statistical

12. T. Hofmann. Latent semantic models for collaborative ﬁltering. ACM Trans. Inf. Syst.,

Association, 67(337):123–129, 1972.

22:89–115, January 2004.

12

Feature Enriched Nonparametric Bayesian Co-clustering

13. L. Jacob, B. Hoffmann, V. Stoven, and J.-P. Vert. Virtual screening of GPCRs: an in

silico chemogenomics approach. BMC Bioinformatics, 9(1):363, 2008.

14. R. Jin, L. Si, and C. Zhai. A study of mixture models for collaborative ﬁltering. Journal

of Information Retrieval, 9:357–382, 2006.

15. M. Khoshneshin and W. N. Street. Incremental collaborative ﬁltering via evolutionary
co-clustering. In Proceedings of the fourth ACM conference on Recommender systems,
RecSys ’10, pages 325–328, New York, NY, USA, 2010. ACM.

16. D. Lemire and A. Maclachlan. Slope one predictors for online rating-based collabora-

tive ﬁltering. In Proceedings of the SIAM Data Mining (SDM), 2005.

17. Y. J. Lim and Y. W. Teh. Variational Bayesian Approach to Movie Rating Prediction.

In Proceedings of KDD Cup and Workshop, 2007.

18. B. Marlin. Modeling user rating proﬁles for collaborative ﬁltering. In Advances in

Neural Information Processing Systems (NIPS), volume 17, 2003.

19. E. Meeds and S. Roweis. Nonparametric Bayesian Biclustering. Technical Report
UTML TR 2007-001, Department of Computer Science, University of Toronto, 2007.
20. R. M. Neal. Markov Chain Sampling Methods for Dirichlet Process Mixture Models.

Journal of Computational and Graphical Statistics, 9(2):249–265, 2000.

21. X. Ning, H. Rangwala, and G. Karypis. Multi-assay-based structure activity rela-
tionship models: Improving structure activity relationship models by incorporating
activity information from related targets. Journal of Chemical Information and Modeling,
49(11):2444–2456, 2009. PMID: 19842624.

22. Y. Okuno, J. Yang, K. Taneishi, H. Yabuuchi, and G. Tsujimoto. GLIDA: GPCR-
ligand database for chemical genomic drug discovery. Nucleic Acids Research,
34(suppl1):D673–D677, 2006.

23. O. Papaspiliopoulos and G. O. Roberts. Retrospective Markov chain Monte Carlo
methods for Dirichlet process hierarchical models. Biometrika, 95(1):169–186, March
2008.

24. J. Pitman and M. Yor. The two-parameter Poisson-Dirichlet distribution derived from

a stable subordinator. Annals of Probability, 25(2):855–900, 1997.

25. I. Porteous, A. Asuncion, and M. Welling. Bayesian matrix factorization with side

information and dirichlet process mixtures. In AAAI, 2010.

26. R. Salakhyuditnov and A. Mnih. Bayesian Probabilistic Matrix Factorization using

Markov Chain Monte Carlo. In International Conference on Machine Learning, 2008.

27. J. B. Schafer, J. Konstan, and J. Riedi. Recommender systems in e-commerce. In

Proceedings of the ACM cConference on Electronic Commerce, pages 158–166, 1999.

28. J. Sethuraman. A constructive deﬁnition of Dirichlet priors. Statistica Sinica, 4:639–650,

29. M. Shaﬁei and E. Milios. Latent Dirichlet co-clustering. In IEEE International Conference

on Data Mining, pages 542–551, 2006.

30. H. Shan and A. Banerjee. Bayesian co-clustering. In IEEE International Conference on

1994.

Data Mining, 2008.

31. H. Shan and A. Banerjee. Generalized probabilistic matrix factorizations for collab-
orative ﬁltering. In Proceedings of the IEEE International Conference on Data Mining,
pages 1025–1030, 2010.

32. I. Sutskever, R. Salakhutdinov, and J. Tenenbaum. Modelling relational data using
Bayesian clustered tensor factorization. In Advances in Neural Information Processing
Systems 22, pages 1821–1828, 2009.

33. P. Symeonidis, A. Nanopoulos, A. Papadopoulos, and Y. Manolopoulos. Nearest-

34. N. Wale and G. Karypis. AFGEN. Technical report, Department of Computer Science

Biclusters Collaborative Filtering. In WEBKDD-06, 2006.
& Enigneering, University of Minnesota, 2007. www.cs.umn.edu/∼karypis.
Proceedings of the European Conference on Machine Learning, pages 522–537, 2009.

35. P. Wang, C. Domeniconi, and K. Laskey. Latent Dirichlet Bayesian co-clustering. In

36. Z. Xu, V. Tresp, K. Yu, and H. Kriegel. Inﬁnite hidden relational models. In Proceedings

of the International Conference on Uncertainity in Artiﬁcial Intelligence, 2006.


