Subclass-Oriented Dimension Reduction with

Constraint Transformation and Manifold

Regularization

Bin Tong and Einoshin Suzuki

Graduate School of Systems Life Sciences, Kyushu University, Japan

{bintong,suzuki}@i.kyushu-u.ac.jp

Abstract. We propose a new method, called Subclass-oriented Dimension Re-
duction with Pairwise Constraints (SODRPaC), for dimension reduction on high
dimensional data. Current linear semi-supervised dimension reduction methods
using pairwise constraints, e.g., must-link constraints and cannot-link constraints,
can not handle appropriately the data of multiple subclasses where the points of a
class are separately distributed in different groups. To illustrate this problem, we
particularly classify the must-link constraint into two categories, which are the
inter-subclass must-link constraint and the intra-subclass must-link constraint,
respectively. We argue that handling the inter-subclass must-link constraint is
challenging for current discriminant criteria. Inspired by the above observation
and the cluster assumption that nearby points are possible in the same class, we
carefully transform must-link constraints into cannot-link constraints, and then
propose a new discriminant criterion by employing the cannot-link constraints
and the compactness of shared nearest neighbors. For the reason that the local
data structure is one of the most signiﬁcant features for the data of multiple sub-
classes, manifold regularization is also incorporated in our dimension reduction
framework. Extensive experiments on both synthetic and practical data sets illus-
trate the effectiveness of our method.

1 Introduction

In various applications, such as gene expression, image retrieval, etc., one is often con-
fronted with high dimensional data [1]. Dimension reduction, which maps data points
in a high-dimensional space into those in a low-dimensional space, is thus viewed as
one of the most crucial preprocessing steps of data analysis. Dimension reduction meth-
ods can be divided into three categories, which are supervised ones [2], unsupervised
ones[3], and semi-supervised ones[4]. The input data in these three categories are la-
beled data, unlabeled data, and both of them, respectively. In a typical real-world ap-
plication, only a small number of labeled data points are available due to the high cost
to obtain them [4]. Hence the semi-supervised dimension reduction may be considered
to ﬁt into the practical setting. Instead of labeled data points, some semi-supervised
methods assume pairwise constraints, for it is easier for experts to specify them than to
assign the class labels of data points. More speciﬁcally speaking, pairwise constraints
consist of must-link constraints and cannot-link constraints. The pair of data points in

M.J. Zaki et al. (Eds.): PAKDD 2010, Part II, LNAI 6119, pp. 1–13, 2010.
c(cid:2) Springer-Verlag Berlin Heidelberg 2010

2

B. Tong and E. Suzuki

a must-link constraint shares the same class label, while the pair of data points in a
cannot-link constraint is given different class labels.

From another viewpoint, dimension reduction methods can be divided into nonlinear
and linear ones. The former allows a nonlinear transformation in the mapping while
the latter restricts itself to linear transformation. We consider a complex distribution
of points that are distributed in multiple subclasses. In other words, the data points of
one class form several separated clusters. A nonlinear method has a higher degree of
freedom and hence can handle data with complex distribution effectively while a linear
method tends to be incompetent in such a case.

In this paper, we restrict our attention to the linear semi-supervised dimension re-
duction for the data of multiple subclasses with pairwise constraints. Previously rele-
vant methods [5] [6] [7] [8] implicitly assume that a class consists of a single cluster.
If the points are of multiple subclasses, handling the pairwise constraints to project
the points into multiple subclasses in the transformed space is challenging for linear di-
mension reduction. For a deep analysis, we particularly classify the must-link constraint
into two categories. If two points in a must-link constraint reside in a single subclass,
we deﬁne such a must-link constraint as an intra-subclass must-link constraint. On the
contrary, if two points in a must-link constraint come from different subclasses, we
deﬁne such kind of must-link constraint as an inter-subclass must-link constraint. We
attribute the reason of the improper behavior of current linear methods to the fact that
the inter-subclass must-link constraint most probably confuses the discriminant criteria
of existing methods. The problem resulted from the inter-subclass must-link constraint
is also encountered by the constraint transformation. For instance, a method in [9] trans-
forms multiple must-link constraints, which are connected via points in two different
classes, into a cannot-link constraint between the centroids of the points of two classes.
This method fails to give a comprehensible meaning if the points belong to different
subclasses because the centroids may fall into the region of another class.

To overcome above problems, we propose SODRPaC, which consists of two steps. In
the ﬁrst step, must-link constraints which satisfy several conditions are transformed into
cannot-link constraints and the remaining must-link constraints are deleted. The idea
behind this step is to reduce the harmfulness of the inter-subclass must-link constraints
while exploiting the must-link constraint information as much as possible by respecting
the cluster assumption [10]: nearby points on the same manifold structure in the original
space are likely to belong to the same class. In the second step, we obtain a projection
mapping by inventing a new discriminant criterion for dimension reduction, which is
suitable for the data of multiple subclasses, and employing the manifold regularization
[11], which is helpful for discovering the local structure of data.

2 Problem Setting and Motivation
The problem setting is deﬁned as follows. We are given a set of N points X = {x1, x2,
. . . , xN}, where xi represents a point in a d-dimensional space, a set of must-link
constraints M = {m1, m2, . . . , mNML}, and a set of cannot-link constraints C =
{c1, c2, . . . , cNCL}. Here mi consists of a pair of points belonging to the same class
while ci consists of a pair of points belonging to different classes. The output is a

Subclass-Oriented Dimension Reduction

3
d× l transformation matrix W which consists of l projective vectors {w1, w2, . . . , wl}
(l (cid:2) d). W maps x1,x2,...,xN to a set of lower dimensional points Y = {y1, ..., yN}.
Hence yi = W T xi where yi represents a point in a l-dimensional space. After mak-
ing data projection, we only consider the classiﬁcation task in the transformed space.
For avoiding the bias caused by the choice of the classiﬁcation method, the accuracy of
nearest neighborhood (1-NN) classiﬁer is considered as a measurement for the good-
ness of dimension reduction with the 20×5-fold cross validation.

10

5

0

−5

−10

 

SODRPaC
NPSSDR
SSDR
CLPP
CMM
Discriminant Criterion

−2

−1

0

1

2

(a)

 

 

15

10

5

0

−5

−10

 

−2

0

2

(b)

Fig. 1. Motivating examples. The data points are of Gaussian distribution. In (a), the blue and
red points are distributed in different clusters. In (b), the red points reside in different subclasses.
Must-link constraints and cannot-link constraints are denoted by black solid and dashed lines,
respectively.

Fig. 1 presents the motivating examples, where d = 2 and l = 1. The task for di-
mension reduction here is thus to project the two dimensional data onto a line, where
the points from different classes can be differentiated. A horizontal line is supposed to
be the best projection while a vertical one is the worst projection. To better illustrate
the motivation of our method, previously relevant methods are ﬁrstly retrospected. In
the aspect of pairwise constraints, SSDR [5] and CMM [6] are to maximize the aver-
age distance between the points in cannot-link constraints, and to minimize the average
distance between the points in must-link constraints simultaneously. We can see that
minimizing the average distance between the points in must-link constraints is reason-
able in the case shown in Fig. 1a, where all the must-link constraints are intra-subclass
must-link constraints. However, it disturbs to maximize the average distance between
the points in cannot-link constraints in the case shown in Fig. 1b, where all the must-
link constraints are inter-subclass must-link constraints. CLPP [7] builds an afﬁnity
matrix, each entry of which indicates the similarity between two points. To utilize the
constraint information, the afﬁnity matrix is revised by setting the similarity degree be-
tween non-neighboring points involved in pairwise constraints. For example, given a
must-link constraint, the similarity degree between two points is revised to be 1, indi-
cating two points are close (similar) to each other, no matter the two points are distant
(dissimilar) or not. Suppose that the must-link constraint is inter-subclass must-link
constraint, it implies that the two points are not geometrically nearby each other. This

4

B. Tong and E. Suzuki

arbitrary updating may damage the geometrical structure of data points. This problem
is also confronted by NPSSDR [8]. The above analysis explains the reason why CMM,
SSDR, CLPP and NPSSDR are capable of obtaining excellent performance as shown
in Fig. 1a, while they fail to reach the same ﬁne performance in the multiple subclass
case shown in Fig. 1b.

In the light of observations, we argue that the inter-subclass must-link constraint is
probably harmful for the discriminant criteria of existing methods. For this reason, we
attempt to design a new discriminant criterion that is able to behave appropriately in the
case of multiple subclasses. The new discrimination criterion marked as ‘Discriminant
Criterion’ is able to obtain almost the same performance as others, as shown in Fig.
1a, and can even outperform previous methods, as shown in Fig. 1b. Moreover, the
manifold regularization is helpful for discovering the local structure of data which is
considered as one of the most principle characteristics of the data of multiple subclasses
[12]. We therefore consider to make the new discriminant criterion and the manifold
regularization work together in a collaborative way. Fig. 1b also demonstrates that our
method SODRPaC, which is the combination of the new discrimination criterion and
the manifold regularization, can obtain the best performance.

3 Subclass-Oriented Dimension Reduction with Pairwise

Constraints

The overview of our SODRPaC involves two steps described as follows:

(1) Transformation. This step transforms must-link constraints into cannot-link con-

straints under the cluster assumption.

(2) Dimension reduction. This step includes two components. The ﬁrst component
is the new discriminant criterion suitable for the case of multiple subclasses. The
other one is the manifold regularization, which helps discovering the local structure
of data.

3.1 Transformation from Must-Link Constraints

Although a method that transforms must-link constraints into cannot-link constraints
is provided in [9], we would point out that its purpose that the plentiful amount of
constraints are reduced is substantially different from ours. Moreover, it becomes in-
effective due to the inter-subclass must-link constraint. In a high dimensional space,
the boundaries of subclasses and the number of subclasses within one class can not be
explicitly detected by using the unlabeled data and link constraints. Thus, it is difﬁcult
to identify whether a must-link constraint is of inter-subclass must-link constraint or
not. To reduce the harmfulness of inter-subclass must-link constraints, removing all the
must-link constraints is, therefore, the most straightforward way. However, it can be
regarded as a waste of must-link constraint information. Preserving the useful must-
link constraints as much as possible in the form of cannot-link constraints is then the
fundamental idea behind the transformation.

In our method, the transformation from must-link constraints into cannot-link con-
straints basically occurs when a must-link constraint and a cannot-link constraint are

Subclass-Oriented Dimension Reduction

5

S , Nx2

S , . . . , NxN

S } where Nxi

connected. Under the cluster assumption, it is natural to consider two nearby points as
another form of must-link constraint, so that we have more opportunities to transform
must-link constraints into cannot-link constraints. In this paper, we employ shared near-
est neighbor (SNN) [13] to formulate the sense of ‘nearby’ points. A set of shared near-
est neighbors is denoted by NS = {Nx1
S ={{xi, xj}|xi ∈
N(xj), xj ∈ N(xi)}. N(xi) denotes the k nearest neighbors set of xi. Let |NS| be the
number of the pairs of shared nearest neighbors, where |·| denotes the cardinality of a
set. The value of SNN between xi and xj is deﬁned as the number of points shared by
their neighbors SN N(i, j) = |N(xi) ∩ N(xj)|. The larger the value of SNN between
two points is, the closer the two points are. It should be noted that we design a N × N
matrix L to specify a kind of reliability for cannot-link constraints, which could be also
deemed as the trustiness to them. Suppose that all the previously speciﬁed constraints
are correct, for the previously given cannot-link constraints and the generated cannot-
link constraints by using must-link constraints, their reliabilities are set to be 1. For the
generated cannot-link constraints by using shared nearest neighbors, their reliabilities
are equal to the similarities between the shared nearest neighbors. It is because that
transformation by employing shared nearest neighbors are considered to be less trustful
than that by using must-link constraints. We believe it is natural to take the similarity
between the shared nearest neighbors as a measurement for the trustiness. For exam-
ple, given a pair of shared nearest neighbors {xi, xj}, we represent the reliability of
a generated cannot-link constraint by using it as a Gaussian kernel, which is a simple
kernel and has been widely applied in research ﬁelds. The reliability is formulated as
θ(xi, xj) = exp(− (cid:5)xi − xj(cid:5)2 /γ), where (cid:5)·(cid:5) denotes the Euclidian norm and γ is the
kernel parameter. Note that, for the convenient access to the matrix L, given a cannot-
link constraint c = {xi, xj}, we use L(c) to denote the entries of Lij and Lji, thus L
is a symmetric matrix.

a

b

c

e

a

c

e

a

c

e

a

c

d

(a)

f

b

d

(b)

f

b

d

(c)

f

b

d

(d)

e

f

Fig. 2. Four simple cases for the transformation. The previously speciﬁed must-link constraints
and cannot-link constraints are denoted by the black solid and dashed lines, respectively. The
shared nearest neighbor is presented as the blue dotted line. The red dash-dotted line speciﬁes the
new cannot-link constraint.

Fig. 2 shows four fundamental scenarios of the transformation. The set {a, b, e, f},
and {c, d} represent different classes of data points. We explain these four scenarios in a
metaphorical way where the must-link constraint is taken as a friend relationship while
the cannot-link constraint is considered as an enemy one. Standing from the viewpoint
of point ‘a’, it is given a friend relationship, say {a, e}, as shown in Fig. 2a, which is

6

B. Tong and E. Suzuki

called as a basic form. If ‘d’ is my enemy, instead of keeping my friend ‘e’, consider
that ‘e’ is the enemy of my enemy ‘d’. Fig. 2b shows an extension of the basic form
with an enemy’s friend rule. If my enemy ‘d’ has a friend ‘c’, ‘c’ is the enemy of my
friend ‘e’ and me. In these two cases, the reliabilities for the new enemy relationships
are set to be 1. Fig. 2c presents an extension of the basic form, which is called as a
proximity form. If I have no enemy but my neighbor ‘b’ has an enemy ‘d’, ‘d’ is the
enemy of my friend ‘e’ and me. Fig. 2d shows an extension of the proximity form with
the enemy’s friend rule. Note that, in the latter two cases, the reliabilities for the new
enemy relationships are set to be the similarity between my neighbor ‘b’ and me. The
pseudo code for the summary of these four cases is illustrated in Algorithm 1.

L(c) = 1.

Algorithm 1. Transformation from Must-link Constraints into Cannot-link Constraints
Input: M, C, k, γ.
Output: C, L.
1: create a N × N zero matrix L.
2: for each c ∈ C do
3:
4: end for
5: if ∃ c ∈ C, m ∈ M s.t. m ∩ c (cid:5)= ∅ then
6:
7:
8:
9:
10:

deﬁne a ∈ m ∩ c, e ∈ m − m ∩ c, d ∈ c − m ∩ c.
create a new cannot-link constraint c(cid:2) = {d, e}; if c(cid:2) /∈ C then C ← C∪{c(cid:2)}, L(c(cid:2)) = 1.
if ∃ m(cid:2) ∈ M s.t. d ∈ m(cid:2)
, m(cid:2) (cid:5)= {d, e} then
deﬁne c ∈ m(cid:2) − m(cid:2) ∩ c.
create two new cannot-link constraints c(cid:2)
if c(cid:2)
end if

i /∈ C, then C ← C ∪ {c(cid:2)

2 = {e, c}; for each c(cid:2)

1 = {a, c}, c(cid:2)

i}, L(c(cid:2)

i, i = 1, 2,

i) = 1.

S, c ∩ na

S (cid:5)= ∅, a /∈ c ∩ na

S then

1 = {a, d}, c(cid:2)
i}, L(c(cid:2)

2 = {e, d} and r = θ(a, b); for
i) = r.

S s.t. c /∈ Na

11:
12: end if
13: if ∃ m ∈ M, c ∈ C, ∀a ∈ m, ∀na
14:
15:

S ∈ Na
S, e ∈ m − m ∩ c.
deﬁne d ∈ c − c ∩ na
create two new cannot-link constraints c(cid:2)
i /∈ C, then C ← C ∪ {c(cid:2)
each c(cid:2)
i, i = 1, 2, if c(cid:2)
if ∃ m(cid:2) ∈ M s.t. d ∈ m(cid:2)
and m(cid:2) (cid:5)= {d, e} then
deﬁne c ∈ m(cid:2) − m(cid:2) ∪ c.
create two new cannot-link constraints c(cid:2)
each c(cid:2)

i /∈ C, then C ← C ∪ {c(cid:2)

i, i = 1, 2, if c(cid:2)

16:
17:
18:

1 = {a, c}, c(cid:2)
i}, L(c(cid:2)

2 = {e, c} and r = θ(a, b); for
i) = r.

end if

19:
20: end if

3.2 Dimension Reduction

In this section, we explain the dimension reduction which is based on a novel discrim-
inant criterion and the manifold regularization. As mentioned in section 2, minimizing
the average distance between the points in must-link constraints is inappropriate when
the must-link constraints are inter-subclass must-link constraints. Under the cluster as-
sumption, the shared nearest neighbors could be naturally deemed as another kind of
intra-subclass must-link constraints. Thus, minimizing the average distance between
the points in intra-subclass must-link constraints could be relaxed as making the shared

Subclass-Oriented Dimension Reduction

7

nearest neighbors closer in the transformed space. Furthermore, the pair of points in
the shared nearest neighbors probably resides in the same subclass, such that this re-
laxation would not suffer from the harmfulness of inter-subclass must-link constraints.
Therefore, the discriminant criterion, which maximizes the average distance between
the points in cannot-link constraints and minimizes the average distance between the
shared nearest neighbors, is expected to be suitable for the data of multiple subclasses.
j along the direction wk,

Suppose that xi and xj are projected to the image yk

i and yk

the new discriminant criterion is deﬁned as follows:

∂(wk) =

(cid:2)

i,j:{xi,xj}∈C

Lij

(cid:3)(cid:3)2

(cid:3)(cid:3)yk
i − yk
2|C|

j

− (cid:2)
i,j:{xi,xj}∈NS

Hij

where the elements of H are given below:

(cid:4)

Hij =

SN N(i, j),

0,

{xi, xj} ∈ Nxi
otherwise

S

(cid:3)(cid:3)2

(cid:3)(cid:3)yk
i − yk
2|NS|

j

(1)

(2)

Inspired by the local scatter [14], the intuition behind the latter part of the right side
of Eq. 1 could be regarded as the compactness of shared nearest neighbors, since two
points are more likely to be close if the value of SNN between them is large. The differ-
ence from the local scatter lies in the fact that a weighted matrix H which handles the
similarity degree between shared nearest neighbors is employed. Since SNN provides
a robust property that the side effect caused by the noisy points could be reduced to
some degree, the compactness of shared nearest neighbors is more reliable than that of
local scatter. The compactness of shared nearest neighbors could be also re-written as
follows:
(cid:2)

(cid:2)

(cid:2)

(cid:3)(cid:3)2

(cid:3)(cid:3)yk
i − yk
2|NS| =

j

Hij

i,j:{xi,xj}∈NS

1

i

2|NS|
⎡
⎣ 1
2|NS|

k

= wT

Hij(wT

k xi − wT

k xj)2

j

(cid:2)

(cid:2)

i

j

Hij(xi − xj)(xi − xj)T

⎤
⎦ wk

where S1 =

1

2|NS|

(cid:9)

(cid:9)

i

j

= wT

k S1wk

(3)
Hij(xi − xj)(xi − xj)T . S1 then could be computed as

follows:

S1 =

=

=

⎛
⎝

1

2|NS|
⎛
⎝

1
|NS|
1
|NS|

(cid:2)

(cid:2)

HijxixT

i +

(cid:2)

(cid:2)

HijxjxT

j − 2

i

j

i

j

⎞
⎠

(cid:2)

j

HijxixT
j

(cid:2)

DiixixT

i − (cid:2)
(cid:15)
XDX T − XHX T

i

i

(cid:14)

⎞
⎠

(cid:2)

(cid:2)

i

j

HijxixT
j

(4)

8

B. Tong and E. Suzuki

where D is a diagonal matrix whose entries are column sums of H, Dii =

(cid:9)

j

Hij.

Similarly, the ﬁrst part of right hand of Eq. 1 could be reformulated as:
k xi − wT

Lij(wT

(cid:2)

(cid:2)

(cid:2)

(cid:3)(cid:3)2

Lij

(cid:3)(cid:3)yk
i − yk
2|C|

j

i,j:{xi,xj}∈C

i

=

1
2|C|
= wT
k S2wk
(5)
where G is a diagonal matrix whose entries are

k xj)2

j

(cid:14)
XGX T − XLX T

(cid:15)

where S2 = 1|C|
column sums of L, Gii =

(cid:9)

j

Lij. Then, ∂(wk) can be brieﬂy written as:

∂(wk) = wT

k X(P − Q)X T wk

where P = D − H, and Q = G − L. For all the wk, k = 1, ..., l, we can arrive at

(cid:16)
W T X(P − Q)X T W

(cid:17)

∂ = tr

(7)
where tr denotes the trace operator. As illustrated in Fig. 1b, the manifold regularization
[11] is helpful for enhancing the performance obtained by the new discriminant crite-
rion. We therefore incorporate it into our dimension reduction framework. The manifold
regularization is deﬁned as:

ξ = tr

(8)
where M = I− K−1/2UK−1/2 is deﬁned as a normalized graph Laplacian. I is a unit
matrix, and K is a diagonal matrix whose entries are column sums of U, Kii =
Uij,
where U is deﬁned as follows:
(cid:4)

(cid:9)

j

(cid:16)
W T XMX T W

(cid:17)

Uij =

exp((cid:5)xi − xj(cid:5)2 /γ),

0,

xi ∈ N(xj)
otherwise.

(6)

(9)

ξ is expected to be minimized in order to preserve the sub-manifold of data. At last,
the ﬁnal objective function that combines Eq. 7 and Eq. 8 together is expected to be
maximized, and is derived as
arg max
W∈R
s.t.W T W =I

W T X(P − Q − λM)X T W

(10)

d×l

tr

(cid:16)

(cid:17)

where λ is a parameter to control the impact of manifold regularization. By introducing
the Lagrangian, the objective function is given by the maximum eigenvalue solution to
the following generalized eigenvector problem:

X(P − Q − λM)X T w = φw

(11)
where φ is the eigenvalue of P − Q − λM , and w is the corresponding eigenvector.
One may argue that, when the graph of SNN is equal to the k-NN graph of the manifold
regularization, Q is almost equivalent to M on preserving the local structure. As shown
in [13], this situation would rarely happen since the two types of graph are dramatically
different from each other in the general case. Moreover, to minimize the average dis-
tance between the shared nearest neighbors, which are considered as another form of
must-link constraints, is conceptually distinct from preserving the local structure.

Subclass-Oriented Dimension Reduction

9

4 Evaluation by Experiments

4.1 Experiments Setup

We use public data sets to evaluate the performance of SODRPaC. Table 1 summarizes
the characteristics of the data sets. All the data come from the UCI repository [15]
except for GCM [16] that is of very high dimensionality. For the ‘monks-1’, ‘monks-2’,
and ‘monks-3’ data, we combined the train and test sets into a whole one. For the ‘letter’
data, we chose ‘A’, ‘B’, ‘C’, and ‘D’ letters from the train and test sets respectively by
randomly picking up 100 samples for each letter, and then assembled them into a whole
set.

Table 1. Summary of the benchmark data sets

Data set Dimension Instance Class
monks-1
monks-3

556
554
270

2
2
4

6
6
13

heart

Data set Dimension Instance Class
monks-2

6
16

letter(ABCD)

GCM

16063

601
800
198

2
4
14

8 , δ2

4 , δ2

16 , δ2

As shown in Eq. 10, λ is the parameter that controls the balance between P − Q
and M. In this experiments setting, the parameter λ is searched from 2α, where α ∈
{α| − 5 ≤ α ≤ 10, α ∈ Z}. A weighted 5-nearest-neighbor graph is employed to
construct the manifold regularizer. In addition, the kernel parameter γ follows the sug-
2 ,δ2, 2δ2,4δ2,8δ2,16δ2},
gestion in [17] that it is searched from the grid { δ2
where δ is the mean norm of data. The parameter λ and the manifold regularizer are
then optimized by means of the 5-fold cross-validation. As to the parameter settings
of other competitive methods, we follow the parameters recommended by them, which
are considered to be optimal. Without speciﬁc explanation, the number of must-link
constraints is always set to be equal to that of cannot-link constraints, as the equal
equilibrium between must-link constraints and cannot-link constraints is favorable for
the existing methods. In addition, the value of k for searching shared nearest neigh-
bors is set to be 3. The reason of this setting is to guarantee that the pairs of points
in shared nearest neighbors reside in the same subclass, and to make the constraint
transformation have more opportunities to be performed. In our experiments, must-link
constraints and cannot-link constraints are selected according to the ground-truth of
data labels.

4.2 Analysis of Experiments

First, the effectiveness of SODRPaC is exhibited by changing the number of constraints.
Apart from the semi-supervised dimension reduction methods, we also take PCA as the
baseline. As illustrated in Fig. 3, SODRPaC always keeps the best performance when
the number of available constraints increases from 10 to 150. As seen in Fig. 3a, Fig. 3b,
Fig. 3d, and Fig.3f, CLPP is inferior to PCA even if the number of constraints is small.
The side effect of inter-subclass must-link constraints, in this case, can be neglected.

10

B. Tong and E. Suzuki

SODRPaC
NPSSDR
SSDR
CLPP
CMM
PCA
120

140

20

40

60

80

100

Number of Constraints
(a) monks-1 (d=2)

0.7

0.65

y
c
a
r
u
c
c
A

0.6

0.55

0.5

 

0.79

0.78

0.77

y
c
a
r
u
c
c
A

0.76

0.75

0.74

0.73

0.72

 

20

40

60

80

100

120

140

Number of Constraints
(d) heart (d=10)

 

0.88

y
c
a
r
u
c
c
A

0.86

0.84

0.82

0.8

0.78

0.76

0.74

 

 

0.995

y
c
a
r
u
c
c
A

0.99

0.985

0.98

0.975

0.97

0.965

0.96

0.955

 

20

40

60

80

100

120

Number of Constraints
(b) monks-2 (d=4)

140

 

0.9

y
c
a
r
u
c
c
A

0.88

0.86

0.84

0.82

0.8

0.78

 

 

0.6

y
c
a
r
u
c
c
A

0.5

0.4

0.3

0.2

20

40

60

80

100

120

Number of Constraints
(c) monks-3 (d=5)

 

 

140

140

20

40

60

80

100

120

Number of Constraints

140

0.1

 

20

(e) letter(abcd) (d=10)

40

60

80

100

120

Number of Constraints
(f) GCM (d=150)

Fig. 3. The performance with different numbers of constraints (d: reduced dimensionality)

The reason is probably that the feature of discovering the local structure of data points
could not help CLPP to outperform PCA. However, our SODRPaC, which also utilizes
the manifold regularization due to its property of discovering the local structure, ob-
tains the best performance. We can judge that the new discriminant criterion boosts the
performance. It is also presented in Fig. 3d that the performance of SSDR decreases
to some extent with the increase of the number of constraints. The possible reason is
that increasing the number of available constraints makes the opportunity higher that
inter-subclass must-link constraints exist, which deteriorates the optimization on the
ﬁne dimension reduction. It should be also pointed out that SODRPaC does not sig-
niﬁcantly outperform other methods. A possible reason is that the Euclidean distance,
which is employed to formulate the similarity between points in the original space, is
likely to be meaningless in the high dimensional space.

We then examine the relative impact between must-link constraints and cannot-link
constraints on the performance of SODRPaC. In this experiment, given 150 available
constraints, the ratio of must-link constraints to cannot-link constraints is varied from 0
to 1. Fig. 4 presents that SODRPaC has a much smoother behavior than others with the
change of ratio. It indicates that SODRPaC is more robust than other semi-supervised
methods in terms of the imbalance between must-link constraints and cannot-link con-
straints. As shown in Fig. 4b and Fig. 4f, SODRPaC presents an obvious degradation of

SODRPaC
NPSSDR
SSDR
CLPP
CMM
PCA

0.2

0.4

0.6

0.8

Rate of Must−link Set
(a) monks-1 (d=2)

Subclass-Oriented Dimension Reduction

11

 

1

 

y
c
a
r
u
c
c
A

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55

 

0.5
0

1

0.95

y
c
a
r
u
c
c
A

0.9

0.85

0.2

0.4

0.6

Rate of Must−link Set
(b) monks-2 (d=4)

0.8

 

1

 

0.2

0.4

0.6

0.8

Rate of Must−link Set
(c) monks-3 (d=5)

 

1

1

 

y
c
a
r
u
c
c
A

0.95

0.9

0.85

0.8

 

0.75
0

0.8

0.7

0.6

0.5

0.4

0.3

0.2

y
c
a
r
u
c
c
A

0.2

0.4

0.6

Rate of Must−link Set
(d) heart (d=10)

0.8

1

 

0.8
0

0.2

0.4

0.6

Rate of Must−link Set

0.8

1

 

0.1
0

(e) letter (abcd) (d=10)

0.6

0.8

1

0.2

0.4

Rate of Must−link Set
(f) GCM (d=150)

y
c
a
r
u
c
c
A

0.7

0.68

0.66

0.64

0.62

0.6

0.58

0.56

 

0.54
0

y
c
a
r
u
c
c
A

0.79

0.78

0.77

0.76

0.75

0.74

0.73

0.72

 

0.71
0

Fig. 4. The performance with the change of rate for must-link set (d: reduced dimensionality)

performance when all constraints are must-link ones. The most probable reason would
be that the transformation from must-link constraints into cannot-link constraints can
not be performed when the necessary cannot-link constraints lack. This behavior is
consistent with the conclusion demonstrated in [9] that cannot-link constraints are more
important than must-link constraints in guiding the dimension reduction.
As implicated in the previous sections, the parameter λ that controls the balance
between P − Q and M, and the factor γ that is related to computing the similarity
between two points would inﬂuence the performance of SODRPaC. An analysis on
the two parameters is necessary to provide the guideline about how to choose their
values. PCA is employed as the baseline because existing methods can not hold such
two parameters simultaneously. Because of the different scale between λ and γ, λ-axis
and γ-axis are thus plotted as λ/(1 + λ) and γ/(1 + γ), respectively. The axis values
are then in the interval (0, 1). We empirically uncover two interesting patterns for most
of data sets and reduced dimensions as well. There are two regions where SODRPaC
are more likely to obtain its best performance. The ﬁrst region is where λ/(1 + λ) is
small, as shown Fig. 5a, Fig. 5b, Fig. 5c, Fig. 5d, Fig. 5e and Fig. 5g. In this situation,
the variation of γ/(1 + γ) would not cause the dramatic change for the performance
of SODRPaC. The second region is where both λ/(1 + λ) and γ/(1 + γ) are large, as
shown in Fig. 5b, Fig. 5e, Fig. 5f, and Fig. 5h.

12

B. Tong and E. Suzuki

SODRPaC
PCA

 

 

0.85

0.8

0.75

0.7

0.65

y
c
a
r
u
c
c
A

0.9

0.8

0.7

y
c
a
r
u
c
c
A

0.6

0.8

0.7

0.6

y
c
a
r
u
c
c
A

0.5

 

0.5
γ/(1+γ)

1 0.2 0.4 0.6 0.8

λ/(1+λ)
(a) monks-1 (d=3)

0.6

0.5

 

1

0.2

γ/(1+γ)

0.4
λ/(1+λ)
(b) monks-1 (d=4)

0.8

0.5

γ/(1+γ)

1

0.8

0.2

0.6

0.4
λ/(1+λ)

(c) monks-2 (d=3)

0.9

0.85

0.8

0.75

0.7

y
c
a
r
u
c
c
A

0.5

0.2 0.4 0.6 0.8

1

γ/(1+γ)

λ/(1+λ)
(d) monks-2 (d=4)

0.8

0.75

0.7

0.65

0.6

y
c
a
r
u
c
c
A

0.9

0.85

0.8

0.75

0.7

y
c
a
r
u
c
c
A

0.8

0.7

0.6

y
c
a
r
u
c
c
A

0.99

y
c
a
r
u
c
c
A

0.98

0.97

0.5

1
γ/(1+γ)

0.8

0.2

0.6

0.4
λ/(1+λ)

0.5

γ/(1+γ)

1

0.8

0.2

0.6

0.4
λ/(1+λ)

0.5

γ/(1+γ)

1

0.2 0.4 0.6 0.8
λ/(1+λ)

0.5

1

γ/(1+γ)

0.2 0.4 0.6 0.8
λ/(1+λ)

(e) monks-3 (d=3)

(f) monks-3 (d=4)

(g) letter(abcd) (d=3)

(h) letter(abcd) (d=15)

Fig. 5. The analysis for λ and γ (d: reduced dimensionality)

5 Conclusions and Future Works

In this paper, we have proposed a new linear dimension reduction method with must-
link constraints and cannot-link constraints, called SODRPaC, that can deal with the
multiple subclasses data. Inspired by the observation that handling the inter-subclass
must-link constraint is challenging for the existing methods, a new discriminant crite-
rion is invented by primarily transforming must-link constraints into cannot-link con-
straints. We also combine the manifold regularization into our dimension reduction
framework. The results of extensive experiments show the effectiveness of our method.
There are some other aspects of this work that merit further research. Although the
empirical choice of λ and γ is suggested, we do not as yet have a good understanding
of how to choose these two parameters which are also correlated with choice of the
number of the reduced dimensionality. Therefore, we are interested in automatically
identifying these three parameters and uncovering relationships among them. Another
possible would be to integrate the semi-supervised dimension reduction and clustering
in a joint framework with automatic subspace selection.

Acknowledgments. A part of this research is supported by the Strategic International
Cooperative Program funded by Japan Science and Technology Agency (JST) and by
the grant-in-aid for scientiﬁc research on fundamental research (B) 21300053 from the
Japanese Ministry of Education, Culture, Sports, Science and Technology. Bin Tong is
sponsored by the China Scholarship Council (CSC).

Subclass-Oriented Dimension Reduction

13

References

1. Parsons, L., Haque, E., Liu, H.: Subspace Clustering for High Dimensional Data: A Review.

In: SIGKDD Explorations, pp. 90–105 (2004)

2. Fukunaga, K.: Introduction to Statistical Pattern Recognition. Academic Press, San Diego

(1990)

3. Joliffe, I.: Principal Component Analysis. Springer, New York (1986)
4. Zhu, X.: Semi-supervised Learning Literature Survey. Technical Report Computer Sciences

1530, University of Wisconsin-Madison (2007)

5. Zhang, D., Zhou, Z.H., Chen, S.: Semi-supervised Dimensionality Reduction. In: Proceed-

ings of the 7th SIAM International Conference on Data Mining (2007)

6. Wang, F., Chen, S., Li, T., Zhang, C.: Semi-supervised Metric Learning by
Maximizing Constraint Margin. In: ACM 17th Conference on Information and Knowledge
Management, pp. 1457–1458 (2008)

7. Cevikalp, H., Verbeek, J., Jurie, F., Klaser, A.: Semi-supervised Dimensionality Reduction
Using Pairwise Equivalence Constraints. In: International Conference on Computer Vision
Theory and Applications (VISAPP), pp. 489–496 (2008)

8. Wei, J., Peng, H.: Neighborhood Preserving Based Semi-supervised Dimensionality Reduc-

tion. Electronics Letters 44, 1190–1191 (2008)

9. Tang, W., Xiong, H., Zhong, S., Wu, J.: Enhancing Semi-supervised Clustering: A Feature
Projection Perspective. In: Proc. of the 13th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, pp. 707–716 (2007)

10. Zhou, D., Bousquet, O., Lal, T.N., Weston, J., Sch¨olkopf, B.: Learning with Local and Global
Consistency. In: Proc. Advances in Neural Information Processing Systems, pp. 321–328
(2004)

11. Belkin, M., Niyogi, P.: Manifold Regularization: A Geometric Framework for Learning From
Labeled and Unlabeled Examples. Journal of Machine Learning Research 7, 2399–2434
(2006)

12. Sugiyama, M.: Dimensionality Reduction of Multimodal Labeled Data by Local Fisher Dis-

criminant Analysis. Journal of Machine Learning Research 8, 1027–1061 (2007)

13. Ert¨oz, L., Steinbach, M., Kumar, V.: A New Shared Nearest Neighbor Clustering Algorithm
and its Applications. In: Proc. of the Workshop on Clustering High Dimensional Data and its
Applications, Second SIAM International Conference on Data Mining (2002)

14. Yang, J., Zhang, D., Yang, J.Y., Niu, B.: Globally Maximizing, Locally Minimizing: Un-
supervised Discriminant Projection with Applications to Face and Palm Biometrics. IEEE
Trans. Pattern Analysis and Machine Intelligence 29(4), 650–664 (2007)

15. Blake, C., Keogh, E., Merz, C.J.: UCI Repository of Machine Learning Databases,

http://www.ics.uci.edu/˜mlearn/MLRRepository.html

16. Ramaswamy, S., Tamayo, P., Rifkin, R., et al.: Multiclass Cancer Diagnosis Using Tu-
mor Gene Expression Signatures. Proceedings of the National Academy of Sciences,
15149–15154 (1998)

17. He, X., Yan, S., Hu, Y., Niyogi, P.: Face Recognition Using Laplacianfaces. IEEE Transaction

on Pattern Analysis and Machine Intelligence 27, 316–327 (2005)


