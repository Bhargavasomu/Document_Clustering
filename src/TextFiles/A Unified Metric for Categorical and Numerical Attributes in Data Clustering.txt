A Uniﬁed Metric for Categorical and Numerical

Attributes in Data Clustering

Yiu-ming Cheung1,2 and Hong Jia1

1 Department of Computer Science and Institute of Computational and

Theoretical Studies, Hong Kong Baptist University, Hong Kong SAR, China

Beijing Normal University - Hong Kong Baptist University, Zhuhai, China

2 United International College,
{ymc,hjia}@comp.hkbu.edu.hk

Abstract. Most of the existing clustering approaches are applicable to
purely numerical or categorical data only, but not both. In general, it is
a nontrivial task to perform clustering on mixed data composed of nu-
merical and categorical attributes because there exists an awkward gap
between the similarity metrics for categorical and numerical data. This
paper therefore presents a general clustering framework based on the
concept of object-cluster similarity and gives a uniﬁed similarity met-
ric which can be simply applied to the data with categorical, numeri-
cal, or mixed attributes. Accordingly, an iterative clustering algorithm
is developed, whose eﬃcacy is experimentally demonstrated on diﬀerent
benchmark data sets.

1

Introduction

To discover the natural group structure of objects represented in numerical or
categorical attributes [1], clustering analysis has been widely applied to a va-
riety of scientiﬁc areas. Traditionally, clustering analysis mostly concentrates
on purely numerical data only. The typical clustering algorithms include the
k-means [2] and EM algorithm [3]. Since the objective functions of these two
algorithms are both numerically deﬁned, they are not essentially applicable to
the data sets with categorical attributes. Under the circumstances, a straight-
forward way to overcome this problem is to transform the categorical values
into numerical ones, e.g. the binary strings, and then apply the aforementioned
numerical-value based clustering methods. Nevertheless, such a method has ig-
nored the similarity information embedded in the categorical values and cannot
faithfully reveal the similarity structure of the data sets [4]. Hence, it is desirable
to solve this problem by ﬁnding a uniﬁed similarity metric for categorical and
numerical attributes such that the metric gap between numerical and categori-
cal data can be eliminated. Subsequently, a general clustering algorithm which
is applicable to various data types can be presented based on this uniﬁed metric.
In this paper, we will propose a uniﬁed clustering approach for both categorical
and numeric data sets. Firstly, we present a general clustering framework based

J. Pei et al. (Eds.): PAKDD 2013, Part II, LNAI 7819, pp. 135–146, 2013.
c(cid:2) Springer-Verlag Berlin Heidelberg 2013

136

Y.-m. Cheung and H. Jia

on the concept of object-cluster similarity. Then, a new metric for both of numer-
ical and categorical attributes is proposed. Under this metric, the object-cluster
similarity for either categorical or numerical attributes has a uniform criterion.
Hence, transformation and parameter adjustment between categorical and nu-
merical values in data clustering are circumvented. Subsequently, analogous to
the framework of k-means, an iterative algorithm is introduced to implement the
data clustering. This algorithm conducts an eﬃcient clustering analysis without
manually adjusting parameters and is applicable to the three types of data: nu-
merical, categorical, or mixed data, i.e. the data with both of numerical and
categorical attributes. Empirical studies have shown the promising results.

2 Related Works

Roughly, the existing clustering approaches dealing with data sets which con-
tain categorical attributes can be summarized into the four categories [5]. The
ﬁrst category of the methods is based on the perspective of similarity. For ex-
ample, based on Goodall similarity metric [6] that assigns a greater weight to
uncommon feature value matching in similarity computations without assuming
the underlying distributions of the feature values, paper [7] presents the Sim-
ilarity Based Agglomerative Clustering (SBAC) algorithm. This method has a
good capability of dealing with the mixed numeric and categorical attributes,
but its computation is quite laborious. Beside the similarity concepts, the sec-
ond category is based on graph partitioning. A typical example is the CLICKS
algorithm [8], which mines subspace clusters for categorical data sets. This novel
method encodes a data set into a weighted graph structure, where each weighted
vertex stands for an attribute value and two nodes are connected if there is a
sample in which the corresponding attribute values co-occur. It is experimen-
tally demonstrated that CLICKS outperforms ROCK algorithm [9] and scales
better for high-dimensional data sets. However, this algorithm is not applicable
to data mixed with categorical and numerical attributes and its performance
also depends upon a set of parameters whose tuning is quite diﬃcult from the
practical viewpoint. The third category is entropy-based methods. For example,
the COOLCAT algorithm [10] utilizes the information entropy to measure the
closeness between objects and presents a scheme to ﬁnd a clustering structure
via minimizing the expected entropy of clusters. The performance of this al-
gorithm is stable for diﬀerent data sizes and parameter settings. Nevertheless,
this method can only be applied to purely categorical data and cannot handle
numerical attributes. The last category of approaches attempts to give a dis-
tance metric between categorical values so that the distance-based clustering
algorithms (e.g. the k-means) can be directly adopted. Along this line, the most
cost-eﬀective one may be the k-prototype algorithm proposed by Huang [11]. In
this method, the distance between two categorical values is deﬁned as 0 if they
are the same, and 1 otherwise while the distance between numerical values is
quantiﬁed with Euclidean distance. Subsequently, the k-means paradigm is uti-
lized for clustering. However, since diﬀerent metrics are adopted for numerical

Uniﬁed Metric for Categorical and Numerical Attributes in Data Clustering

137

and categorical attributes, a user-deﬁned parameter is utilized to control the
proportions of numerical distance and categorical distance. Nevertheless, vari-
ous settings of this parameter will lead to a totally diﬀerent clustering result.
A simpliﬁed version of k-prototype algorithm namely k-modes [12, 13], which is
applicable for purely categorical data clustering, has also been widely utilized
due to its satisfactory eﬃciency, and diﬀerent improvement strategies have been
explored on this method [14–16].

3 Object-Cluster Similarity Metric

The general task of clustering is to classify the given objects into several clusters
such that the similarities between objects in the same group are high while the
similarities between objects in diﬀerent groups are low [17]. Therefore, clustering

a set of N objects, {x1, x2, . . . , xN}, into k diﬀerent clusters, denoted as C1, C2,

∗

via the following objective

. . ., Ck, can be formulated to ﬁnd the optimal Q
function:
k(cid:2)

N(cid:2)

∗

Q

= arg max

Q

F (Q) = arg max
[

Q

j=1

i=1

qijs(xi, Cj )]

(1)

where s(xi, Cj) is the similarity between object xi and Cluster Cj, and Q = (qij)
is an N × k partition matrix satisfying

k(cid:2)

j=1

N(cid:2)

qij = 1, and 0 <

qij < N,

i=1

with

qij ∈ [0, 1], i = 1, 2, . . . , N, j = 1, 2, . . . , k.

(2)

(3)

Evidently, the desired clusters can be obtained by (1) as long as the metric of
object-cluster similarity is determined. In the following sub-sections, we shall
therefore study the similarity metric.

3.1 Similarity Metric for Mixed Data

This sub-section will study the object-cluster similarity metric for mixed data.
Suppose the mixed data xi with d diﬀerent attributes consists of dc categorical at-
tributes and du numerical attributes, i.e. dc + du = d. xi can be therefore denoted
as [xc
idu )T .
i
Then, we have xu
ir (r = 1, 2, . . . , dc)
belonging to dom(Ar), where {A1, A2, . . . , Adc} are the dc categorical attributes

ir (r = 1, 2, . . . , du) belonging to R and xc

idc)T and xu

i1, xu

i2, . . . , xu

i1, xc

i2, . . . , xc

i = (xu

T , xu
i

T ]T with xc

i = (xc

and dom(Ar) contains all possible values that can be chosen by attribute Ar. For
categorical attributes, the value domains are ﬁnite and unordered, dom(Ar) with

mr elements can be therefore represented with dom(Ar) = {ar1, ar2, . . . , armr}.

Firstly, we focus on the diﬀerence between categorical attributes and numeri-
cal attributes. For categorical attributes, each attribute can usually represent an

138

Y.-m. Cheung and H. Jia

important feature of the given object. Therefore, when we conduct classiﬁcation
or clustering analysis, we often investigate the categorical attributes one by one
such as Decision Tree method. By contrast, the numerical attributes are often
treated as a vector and handled together in clustering analysis. Based on these
observations, for the mixed data xi, the du numerical attributes can be treated
as a whole but the dc categorical attributes should be investigated individually.
Let the object-cluster similarity between xi and cluster Cj , denoted as s(xi, Cj),
be the average of the similarity calculated based on each attribute, we will have

s(xi, Cj ) =

=

1
d

1
d

s(xc
dc(cid:2)

r=1

i1, Cj) +

1
d

s(xc

i2, Cj) + ... +

1
d

s(xc

idc , Cj ) +

du
d

s(xu

i , Cj)

s(xc

ir, Cj) +

du
d

s(xu

i , Cj).

(4)

That is, the similarity between each numerical attribute and the cluster Cj is
replaced with the similarity between the cluster and the whole numerical vector
xu
i . Moreover, if we denote the similarity between xc
i , Cj), we can
get

i and Cj as s(xc

dc(cid:2)

dc(cid:2)

s(xc

i , Cj ) =

1
dc

s(xc

ir, Cj) =

r=1

r=1

1
dc

s(xc

ir, Cj).

(5)

Then, (4) can be further rewritten as

s(xi, Cj) =

dc(cid:2)

r=1

1
dc

dc
d

s(xc

ir, Cj) +

du
d

s(xu

i , Cj) =

dc
d

s(xc

i , Cj) +

du
d

s(xu

i , Cj), (6)

i , Cj) is actually the similarity on categorical attributes and s(xu

where s(xc
i , Cj)
is the similarity on numerical attributes. Subsequently, the object-cluster simi-
larity metric can be obtained based on the deﬁnitions of s(xc
i , Cj).

i , Cj) and s(xu

Similarity Metric for Categorical Attributes. In (5), we have assumed
that each categorical attribute has the same contribution to the calculation of
similarity on categorical part. However, from the practical viewpoint, due to the
diﬀerent distributions of attribute values, categorical attributes each often have
unequal importance for clustering analysis. In light of this characteristic, (5)
should be further modiﬁed with

dc(cid:2)

s(xc

wrs(xc

ir, Cj),

r=1

i , Cj) =

(7)
where wr is the weight of categorical attribute Ar satisfying 0 ≤ wr ≤ 1 and
dc(cid:3)
wr = 1. That is, the object-cluster similarity for categorical part is the

r=1
weighted summation of the similarity between the cluster and each attribute
value. Weight factor wr describes the importance of each categorical attribute
and is utilized to control the contribution of attribute-cluster similarity to object-
cluster similarity.

Uniﬁed Metric for Categorical and Numerical Attributes in Data Clustering

139

Deﬁnition 1. The similarity between a categorical attribute value xc
ir and clus-
ter Cj, i ∈ {1, 2, . . . , N}, r ∈ {1, 2, . . . , dc}, j ∈ {1, 2, . . . , k}, is deﬁned as:

s(xc

ir, Cj) =

,

σAr =xc
ir (Cj )
σAr(cid:4)=N ULL(Cj )
ir (Cj ) counts the number of objects (also
ir for attribute Ar in cluster

(8)

where NULL refers to empty, and σAr =xc
called instances hereinafter) that have the value xc
Cj.

From Deﬁnition 1, we can ﬁnd that this metric of attribute-cluster similarity
has the following properties:
(1) 0 ≤ s(xc
(2) s(xc

ir, Cj ) = 1 only if all the instances belonging to cluster Cj have the value
ir, Cj) = 0 only if no instance belonging to

ir, Cj) ≤ 1;

xc
ir for attribute Ar, and s(xc
cluster Cj has the value xc

ir for attribute Ar.

According to (7) and (8), the object-cluster similarity for categorical part can
be therefore calculated by

s(xc

i , Cj ) =

dc(cid:2)

r=1

wrs(xc

ir, Cj ) =

dc(cid:2)

r=1

wr

σAr=xc
ir (Cj)
σAr(cid:4)=N ULL(Cj)

,

(9)

where i ∈ {1, 2, . . . , N}, and j ∈ {1, 2, . . . , k}.
dc(cid:3)
Remark 1. Since 0 ≤ s(xc

ir, Cj) ≤ 1 and

wr = 1, we have:

r=1

s(xc

i , Cj) =

dc(cid:2)

r=1

ir, Cj ) ≥ dc(cid:2)

wrs(xc

(wr · 0) = 0,

r=1

and

s(xc

i , Cj) =

dc(cid:2)

ir, Cj) ≤ dc(cid:2)

wrs(xc

(wr · 1) =

dc(cid:2)

wr = 1.

That is, for any i ∈ {1, 2, . . . , N} and j ∈ {1, 2, . . . , k}, the value of s(xc
will fall into the interval [0, 1].

i , Cj)

r=1

r=1

r=1

Next, we discuss how to estimate the importance of each categorical attribute.
From the view point of information theory, the signiﬁcance of an attribute can
be regarded as the inhomogeneity degree of the data set with respect to this
attribute. Furthermore, it is described in [18] that if the information content of an
attribute is high, the inhomogeneity of the data set is also high for this attribute.
Hence, the importance of any categorical attribute Ar (r ∈ {1, 2, . . . , dc}) can
be calculated by

HAr = − mr(cid:2)

t=1

p(art) log p(art)

(10)

140

Y.-m. Cheung and H. Jia

with

σAr =art (X)
σAr(cid:4)=N ULL(X)

,

p(art) =

(11)
where art ∈ dom(Ar), p(art) is the probability of attribute value art, mr is
the total number of values that can be chosen by Ar and X is the whole data
set. Furthermore, according to (10), the more diﬀerent values an attribute has,
the higher its signiﬁcance is. However, in practice, an attribute with too many
diﬀerent values may have little contribution to clustering. For example, the ID
number of instances is unique for each instance, but this information is useless
for clustering analysis. Hence, (10) can be further modiﬁed with

HAr = − 1
mr

mr(cid:2)

t=1

p(art) log p(art).

(12)

That is, the importance of an attribute is quantiﬁed by its average entropy over
each attribute value. The weight of each attribute is then computed as

wr =

HAr
dc(cid:3)

HAt

t=1

, r = 1, 2, . . . , dc.

(13)

Subsequently, the object-cluster similarity on categorical part can be given by

⎛

⎞

s(xc

i , Cj) =

⎜⎜⎜⎝

dc(cid:2)

r=1

HAr
dc(cid:3)

HAt

t=1

· σAr=xc
ir (Cj)
σAr(cid:4)=N ULL(Cj )

⎟⎟⎟⎠.

(14)

In practice, for an attribute Ar, if all the instances to be classiﬁed have the
same value a, it can be obtained from (12) and (11) that the importance of this
attribute will be 0 as p(a) = 1 and log(1) = 0. Then, the corresponding attribute
weight will also be zero and this attribute will have no contribution to the whole
clustering learning.

Similarity Metric for Numerical Attributes. Since the distance between
each vector xu
i can be numerically calculated, the similarity metric for numerical
attributes can be deﬁned based on the measure of distance. According to [19]
and [20], it is a universal law that the distance and perceived similarity between
numerical vectors are related via an exponential function as follows:

s(xA, xB) = exp(−Dis(xA, xB)),

(15)

where Dis stands for a distance measure. Moreover, it can be observed that the
magnitudes of distances between instances from variant data sets may have a
signiﬁcant diﬀerence in practice. To avoid the potential inﬂuence of this scenario,
we can further use proportional distance instead of absolute distance to estimate
the similarity between numerical vectors.

Uniﬁed Metric for Categorical and Numerical Attributes in Data Clustering

141

Deﬁnition 2. The object-cluster similarity between numerical vector xu
cluster Cj , i ∈ {1, 2, . . . , N}, j ∈ {1, 2, . . . , k}, is given by
⎞
⎟⎟⎠ ,

⎛
⎜⎜⎝− Dis(xu

i , Cj ) = exp

s(xu

i , cj)

i and

(16)

k(cid:3)

t=1

Dis(xu

i , ct)

where cj is the center of all numerical vectors in cluster Cj .

It can be seen from Deﬁnition 2 that the values of this similarity metric also
fall into the interval [0, 1]. In practice, diﬀerent distance metrics can be utilized
to calculate Dis(xu
i , cj). For example, if the Minkowski distance is adopted, we
shall have:

(cid:10)

du(cid:2)

(cid:11)1/p

Dis(xu

i , cj) =

|xu
ir − cjr|p

,

(17)

where p > 0 is a constant which characterizes the distance function. A typically
special case of (17) is the Euclidean distance with p = 2.

Finally, according to (6), (14), and (16), the object-cluster similarity metric

r=1

for mixed data is deﬁned as

⎛

s(xi, Cj) =

⎜⎜⎜⎝

dc(cid:2)

r=1

dc
d

HAr
dc(cid:3)

HAt

t=1

⎞

⎟⎟⎟⎠ +

du
d

exp

· σAr =xc
ir (Cj )
σAr(cid:4)=N ULL(Cj )

⎛
⎜⎜⎝− Dis(xu

k(cid:3)

i , cj)

Dis(xu

i , ct)

t=1

⎞
⎟⎟⎠ ,

(18)
where i = 1, 2, . . . , N , j = 1, 2, . . . , k. It can be seen that the deﬁned similarities
for categorical and numerical attributes in (18) are in the same scale. Hence,
unlike k-prototype method, there is no need any more to manually adjust the
parameter to control the proportions of numerical and categorical distances for
diﬀerent data sets.

Iterative Clustering Algorithm

4
This paper concentrates on hard partition only, i.e., qij ∈ {0, 1}, although it can
be easily extended to the soft partition in terms of posterior probability. Under
ij} in (1) can
∗
∗
the circumstances, given a set of N objects, the optimal Q
be given by
1, if s(xi, Cj) ≥ s(xi, Cr), 1 ≤ r ≤ k,
0, otherwise.

∗
q
ij =

= {q

(19)

(cid:12)

Therefore, similar to the learning procedure of k-means, an iterative algorithm,
denoted as OCIL, can be conducted to implement the clustering analysis as
shown in Algorithm 1.

The ﬁrst step in OCIL algorithm, i.e. Step 1, is a procedure for the calcu-
lation of object-cluster similarity. Thus, we can ﬁnd that the iterative steps of

142

Y.-m. Cheung and H. Jia

if y

(new)
i

(cid:2)= y

(old)
i

then

7:
8:
9:

Algorithm 1 Iterative clustering learning based on object-cluster similarity
metric (OCIL)
Require: data set X = {x1, x2, . . . , xN}, number of clusters k
Ensure: cluster label Y = {y1, y2, . . . , yN}
1: Calculate the importance of each categorical attribute according to (12), if appli-
2: Set Y = {0, 0, . . . , 0} and randomly select k initial objects, one for each cluster
3: repeat
4:
5:
6:

Initialize noChange = true
for i = 1 to N do

= arg max

cable

(new)
i

y

j∈{1,...,k}[s(xi, Cj )]

noChange = f alse
Update the information of clusters Cy(new)
quency of each categorical value and the centroid of numerical vectors

and Cy(old)

, including the fre-

i

i

end if
end for

10:
11:
12: until noChange is true
13: return Y

OCIL algorithm is the same as the k-means algorithm and the only diﬀerence is
the measurement of similarity between object and clusters. Therefore, the eﬀec-
tiveness of the proposed similarity metric can be easily evaluated by comparing
OCIL with other similar algorithms, such as k-means and k-prototype. Next,
we further give the time complexity analysis of OCIL algorithm. It can be ob-
served that the computation cost of Step 1 is O(mN dc), where m is the average
number of diﬀerent values that can be chosen by each categorical attribute. For
each iteration, the cost of the “for” statement is O(mN kdc + N kdu). Hence,
the total time cost of this algorithm is O(t(mN kdc + N kdu)), where t stands
for the number of iterations. From the practical viewpoint, k, m and t can be
regarded as a constant in most cases. Therefore, the time complexity of this
algorithm approaches to O(dN ). Hence, the proposed algorithm is eﬃcient for
data clustering, particularly for a large data set.

5 Experiments

This section is to investigate the eﬀectiveness of the proposed approach to data
clustering. We applied it to various categorical and mixed data sets obtained from
UCI Machine Learning Data Repository1 and compared its performance with the
existing counterparts. Since the proposed method on numerical data degenerates
to the k-means algorithm, the eﬀectiveness of OCIL algorithm on numerical
data set is transparent. Hence, there is no need to investigate it any more. Each
algorithm was coded with MATLAB and all experiments were implemented by

1 See http://archive.ics.uci.edu/ml/

Uniﬁed Metric for Categorical and Numerical Attributes in Data Clustering

143

a desktop PC computer with Intel(R) Core(TM)2 Quad CPU, 2.40 GHz main
frequency, and 4GB DDR2 667 RAM.

Moreover, in our experiments, the clustering accuracy [21] for measuring the

clustering performance was estimated by

(cid:3)N

i=1 δ(ci, map(ri))

,

ACC =

N

where N is the number of instances in the data set, ci stands for the provided
label, map(ri) is a mapping function which maps the obtained cluster label ri to
the equivalent label from the data corpus by using the Kuhn-Munkres algorithm,
and the delta function δ(ci, map(ri)) = 1 only if ci = map(ri), otherwise 0.
Correspondingly, the clustering error rate is computed as e = 1 − ACC.

5.1 Performance on Mixed Data Sets

In the following experiments, we will investigate the performance of the proposed
algorithm on real data sets in comparison with the existing counterparts. Firstly,
experiments were conducted on mixed data and the information of selected data
sets is shown in Table 1. The performance of the proposed method has been
compared with the k-prototype algorithm [11] and k-means algorithm, whose
time complexity are also O(N d). In k-prototype method, the distance regulation
parameter γ was set at 0.5σ [11], where σ is the average standard deviation
of numerical attributes. When utilizing k-means, the categorical values were
transformed into integers in our experiments. Moreover, the Euclidean distance
has been adopted as the distance metric of numerical vectors for consistency.
Each algorithm has been run 100 times on each data set and the clustering
results are summarized in Table 2.

Table 1. Statistics of mixed data sets

Data set
Statlog Heart
Heart Disease
Credit Approval
German Credit
Dermatology
Adult

Instance Attribute (dc + du) Class

270
303
653
1000
366

30162

7 + 6
7 + 6
9 + 6
13 + 7
33 + 1
8 + 6

2
2
2
2
6
2

It can be seen that, both with random initializations, the proposed algorithm
OCIL has an obvious superiority in terms of clustering accuracy over the k-
prototype and k-means methods. This result shows that, in comparison with
numerically representing the distance between categorical values, the proposed
similarity metric in this paper is a more reasonable measurement for clustering

144

Y.-m. Cheung and H. Jia

Table 2. Clustering errors of OCIL on mixed data sets in comparison with k-prototype
and k-means

K-means

K-prototype

Data set
0.4047±0.0071 0.2306±0.0821 0.1716±0.0065
Statlog
0.4224±0.0131 0.2280±0.0903 0.1644±0.0030
Heart
0.4487±0.0016 0.2619±0.0976 0.2519±0.0966
Credit
0.3290±0.0014 0.3289±0.0006 0.3057±0.0007
German
Dermatology 0.7006±0.0216 0.6903±0.0255 0.3051±0.0896
0.3869±0.0067 0.3855±0.0143 0.3079±0.0305
Adult

OCIL

Table 3. Comparison of average convergent time and iterations between k-prototype
and OCIL

Data set

Statlog
Heart
Credit
German
Dermatol
Adult

Time

Iterations

K-prototype OCIL K-prototype OCIL
3.07
3.02
4.26
3.15
4.32
6.78

0.0516s
0.0576s
0.1625s
0.2023s
0.1888s
9.6774s

0.0519s
0.0639s
0.1323s
0.2999s
0.3674s
15.2795s

3.09
3.54
3.18
5.29
7.27
10.93

Table 4. Statistics of categorical data sets

Data set Instance Attribute Class
Soybean
Breast
Vote
Zoo

47
699
435
101

35
9
16
16

4
2
2
7

Table 5. Comparison of clustering errors on categorical data sets

Data set H’s k-modes N’s k-modes
Soybean 0.1691±0.1521 0.0964±0.1404 0.1017±0.1380
0.1655±0.1528 0.1356±0.0016 0.0934±0.0009
Breast
0.1387±0.0066 0.1345±0.0031 0.1213±0.0010
Vote
0.2873±0.1083 0.2730±0.0818 0.2681±0.0906
Zoo

OCIL

analysis on mixed data. Moreover, comparing the average running time of OCIL
and k-prototype algorithms listed in Table 3, we can ﬁnd that, although OCIL
needs additional time to calculate the weight of each categorical attribute, its
total running time is no more than k-prototype’s one. A plausible reason can

Uniﬁed Metric for Categorical and Numerical Attributes in Data Clustering

145

be found from Table 3 is that the convergence speed of OCIL is usually faster
than k-prototype in most cases we have tried so far. Therefore, the proposed
similarity metric is eﬃcient for mixed data clustering.

5.2 Performance on Categorical Data Sets

We further investigated the performance of the proposed algorithm on purely
categorical data. The information of four diﬀerent benchmark data sets we uti-
lized has been summarized in Table 4. To conduct comparative studies, we have
also implemented the other two existing categorical data clustering algorithms:
original k-modes (H’s k-modes) [12] and k-modes with Ng’s dissimilarity met-
ric (N’s k-modes) [16]. In this experiment, each algorithm was conducted with
random initializations. Table 5 lists the average value and standard deviation in
error obtained by OCIL and the other two algorithms, respectively. It can be
seen that the proposed clustering method has competitive advantage in terms of
clustering accuracy and robustness compared with the other two methods.

6 Conclusion

In this paper, we have proposed a general clustering framework based on object-
cluster similarity, through which a uniﬁed similarity metric for both categorical
and numerical attributes has been presented. Under this new metric, the object-
cluster similarity for categorical and numerical attributes are with the same scale,
which is beneﬁcial to clustering analysis on various data types. Subsequently,
analogous to k-means method, an iterative algorithm has been introduced to
implement the data clustering. The advantages of the proposed method have
been experimentally demonstrated in comparison with the existing counterparts.

Acknowledgment. The work described in this paper was supported by the
Faculty Research Grant of Hong Kong Baptist University with the Project Code:
FRG2/11-12/067, and the NSFC under grant 61272366.

References

1. Michalski, R.S., Bratko, I., Kubat, M.: Machine learning and data mining: methods

and applications. Wiley, New York (1998)

2. MacQueen, J.B.: Some methods for classiﬁcation and analysis of multivariate ob-
servations. In: Proceedings of the Fifth Berkeley Symposium on Mathematical
Statistics and Probability, pp. 281–297 (1967)

3. Dempster, A.P., Laird, N.M., Rubin, D.B.: Maximum likelihood from incomplete
data via the em algorithm. Journal of the Royal Statistical Society, Series B
(Methodological) 39(1), 1–38 (1977)

4. Hsu, C.C.: Generalizing self-organizing map for categorical data. IEEE Transac-

tions on Neural Networks 17(2), 294–304 (2006)

146

Y.-m. Cheung and H. Jia

5. Cesario, E., Manco, G., Ortale, R.: Top-down parameter-free clustering of high-
dimensional categorical data. IEEE Transactions on Knowledge and Data Engi-
neering 19(12), 1607–1624 (2007)

6. Goodall, D.W.: A new similarity index based on probability. Biometric 22(4), 882–

907 (1966)

7. Li, C., Biswas, G.: Unsupervised learning with mixed numeric and nominal data.

IEEE Transactions on Knowledge and Data Engineering 14(4), 673–690 (2002)

8. Zaki, M.J., Peters, M.: Click: Mining subspace clusters in categorical data via k-
partite maximal cliques. In: Proceedings of the 21st International Conference on
Data Engineering, pp. 355–356 (2005)

9. Guha, S., Rastogi, R., Shim, K.: Rock: A robust clustering algorithm for categorical

attributes. Information Systems 25(5), 345–366 (2001)

10. Barbara, D., Couto, J., Li, Y.: Coolcat: An entropy-based algorithm for categor-
ical clustering. In: Proceedings of the 11th ACM Conference on Information and
Knowledge Management, pp. 582–589 (2002)

11. Huang, Z.: Clustering large data sets with mixed numeric and categorical values.
In: Proceedings of the First Paciﬁc-Asia Conference on Knowledge Discovery and
Data Mining, pp. 21–24 (1997)

12. Huang, Z.: A fast clustering algorithm to cluster very large categorical data sets
in data mining. In: Proceedings of the SIGMOD Workshop on Research Issues on
Data Mining and Knowledge Discovery, pp. 1–8 (1997)

13. Huang, Z., Ng, M.K.: A note on k-modes clustering. Journal of Classiﬁcation 20(2),

257–261 (2003)

14. Khan, S.S., Kant, S.: Computation of initial modes for k-modes clustering algo-
rithm using evidence accumulation. In: Proceedings of the 20th International Joint
Conference on Artiﬁcial Intelligence (IJCAI 2007), pp. 2784–2789 (2007)

15. He, Z., Deng, S., Xu, X.: Improving k-modes algorithm considering frequencies of
attribute values in mode. In: Hao, Y., Liu, J., Wang, Y.-P., Cheung, Y.-m., Yin,
H., Jiao, L., Ma, J., Jiao, Y.-C. (eds.) CIS 2005. LNCS (LNAI), vol. 3801, pp.
157–162. Springer, Heidelberg (2005)

16. Ng, M.K., Li, M.J., Huang, J.Z., He, Z.: On the impact of dissimilarity measure in
k-modes clustering algorithm. IEEE Transactions on Pattern Analysis and Machine
Intelligence 29(3), 503–507 (2007)

17. Jain, A.K.: Data clustering: 50 years beyound k-means. Pattern Recognition Let-

ters 31(8), 651–666 (2010)

18. Basak, J., Krishnapuram, R.: Interpretable hierarchical clustering by constructing
an unsupervised decision tree. IEEE Transactions on Knowledge and Data Engi-
neering 17(1), 121–132 (2005)

19. Shepard, R.N.: Toward a universal law of generalization for physical science. Sci-

ence 237, 1317–1323 (1987)

20. Santini, S., Jain, R.: Similarity measures. IEEE Transactions on Pattern Analysis

and Machine Intelligence 21(9), 871–883 (1999)

21. He, X., Cai, D., Niyogi, P.: Laplacian score for feature selection. In: Advances in

Neural Information Processing Systems (2005)


