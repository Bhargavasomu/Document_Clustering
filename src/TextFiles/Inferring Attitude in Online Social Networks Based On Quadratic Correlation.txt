Inferring Attitude in Online Social Networks

Based on Quadratic Correlation

Cong Wang(cid:2) and Andrei A. Bulatov

Simon Fraser University Burnaby, B.C. Canada

{cwa9,abulatov}@sfu.ca

Abstract. The structure of an online social network in most cases can-
not be described just by links between its members. We study online
social networks, in which members may have certain attitude, positive
or negative, toward each other, and so the network consists of a mix-
ture of both positive and negative relationships. Our goal is to predict
the sign of a given relationship based on the evidences provided in the
current snapshot of the network. More precisely, using machine learning
techniques we develop a model that after being trained on a particular
network predicts the sign of an unknown or hidden link. The model uses
relationships and inﬂuences from peers as evidences for the guess, how-
ever, the set of peers used is not predeﬁned but rather learned during the
training process. We use quadratic correlation between peer members to
train the predictor. The model is tested on popular online datasets such
as Epinions, Slashdot, and Wikipedia. In many cases it shows almost
perfect prediction accuracy. Moreover, our model can also be eﬃciently
updated as the underlying social network evolves.

Keywords: Signed Networks, machine learning, quadratic optimization.

1

Introduction

Online social networks provide a convenient and ready to use model of rela-
tionships between individuals. Relationships representing a wide range of social
interactions in online communities are useful for understanding individual atti-
tude and behaviour as a part of a larger society.

While the bulk of research in the structure on social networks tries to ana-
lyze a network using the topology of links (relationships) in the network [21],
relationships between members of a network are much richer, and this addi-
tional information can be used in many areas of social networks analysis. In
this paper we consider signed social networks, which consist of a mixture of
both positive and negative relationships. This type of networks has attracted
attention of researchers in diﬀerent ﬁelds [6,10,17]. This framework is also quite
natural in recommender systems [3]where we can exploit similarities as well as
dissimilarities between users and products.

(cid:2) We’d like to thank Dr.Jian Pei for his valuable suggestions and feedbacks on our

work.

V.S. Tseng et al. (Eds.): PAKDD 2014, Part I, LNAI 8443, pp. 139–150, 2014.
c(cid:2) Springer International Publishing Switzerland 2014

140

C. Wang and A.A. Bulatov

Over the last several years there has been a substantial amount of work done
studying signed networks, see, e.g. [14,7,8,5,12,15,23,24]. Some of the studies
focused on a speciﬁc online network, such as Epinions [8,18], where users can
express trust or distrust to others, a technology news site Slashdot [12,13], whose
users can declare others ‘friends’ or ‘foes’, and voting results for adminship of
Wikipedia [5]. Others develop a general model that ﬁts several diﬀerent net-
works [7,14]. We build upon these works and attempt to combine the best in the
two approaches by designing a general model that nevertheless can be tuned up
for speciﬁc networks.

Edge Sign Prediction. Following Guha et al. [8] and Kleinberg et al. [14], [17]
we consider a signed network as a directed (or undirected) graph, every edge
of which has a sign, either positive to indicate friendship, support, approval, or
negative to indicate enmity, opposition, disagreement. In the edge sign prediction
problem, given a snapshot of the signed network, the goal is to predict the sign
of a given link using the information provided in the snapshot. Thus, the edge
sign problem is similar to the much studied link prediction problem [16,11], only
we need to predict the sign of a link rather than the link itself.

Several diﬀerent approaches have been taken to tackle this problem. Kunegis
et al. [4] studied the friends and foes on Slashdot using network characteris-
tics such as clustering coeﬃcient, centrality and PageRank; Guha et al. [8] used
propagation algorithms based on exponentiating the adjacency matrix to study
how trust and distrust propagate in Epinion. Later Kleinberg et al. [14] took a
machine learning approach to identify features, such as local relationship pat-
terns and degree of nodes, and their relative weight, to build a general model
predicting the sign of a given link. They train their predictor on some dataset,
to learn the weights of these features by logistic regression.

Our Contribution. In this paper we also take the machine learning approach, only
instead of focusing on a particular network or building a general model across
diﬀerent networks, we build a model that is unique to each individual network,
yet can be trained on diﬀerent networks. We suggest several new features into
both the modeling of signed networks and the method of processing the model.
The basic assumption of our model is that users’ attitude can be determined
by the opinions of their peers in the network (compare to the balance and status
theories [6] from social psychology, see [14]). Intuitively, peer opinions are guesses
from peers on the sign of the link from a source to target node. We assume that
peer opinions are only partially known, some of them are hidden. We introduce
two new components into the model: set of trusted peers and inﬂuence.

Not all peer opinions are equally reliable, and we therefore choose a set of
trusted peers whose opinions are important in determining the user’s action. The
set of trusted peers is one of the features our algorithm learns during the training
phase. The algorithm forms a set of trusted peers for each individual node. The
optimal composition of such a set is not quite trivial, because even trusted peers
may disagree, and sometimes it is beneﬁcial to have trusted peers who disagree.

Inferring Attitude in Online Social Networks Based on Quadratic Correlation

141

Thus, the set of trusted peers of a node has to form a wide knowledge base on
other nodes in the network.

While peer opinions provide important information, this knowledge is some-
times incomplete. Relying solely on peer opinions implies that the attitude of a
user would always agree with the attitude of a peer. However, it also matters is
how this opinion correlates with the opinion of the user we are evaluating. To
take this correlation into account we introduce another feature into the model,
inﬂuence. Suppose the goal is to learn the sign of the link between user A and
user B, and C is a peer of A. Then if A tends to disagree with C, then pos-
itive attitude of C towards B should be taken as indication that A’s attitude
towards B is less likely to be positive. The opinion of C is then considered to
be the product of his attitude towards B and his inﬂuence on A. Usually, inﬂu-
ence is not given in the snapshot of the network and has to be learned together
with other unknown parameters. We experiment with diﬀerent ways of deﬁning
peer opinion, and found that using relationships and inﬂuences together is more
eﬀective than using relationships alone.

To learn the weights of features providing the best accuracy we use the stan-
dard quadratic correlation technique from machine learning [9]. This method in-
volves ﬁnding an optimum of a quadratic polynomial, and while being relatively
computationally costly, tends to provide very good accuracy. To mitigate the
cost of computation we use two approaches. Firstly, we apply several techniques
to split the problem and avoid solving large quadratic problems. Secondly, we
attempt to make the main algorithm independent on a speciﬁc tool of quadratic
optimization so that this step consuming the bulk of processing time can be
easily improved as better solvers appear.

2 Approach

In our method, we start with the underlying model of a network, then proceed to
the machine learning formulation of the edge sign prediction problem, and ﬁnally
describe the method to solve the resulting quadratic optimization problem.

2.1 Underlying Model

We are given a snapshot of the current state of a network. Such a snapshot is
represented by a directed graph G = (V, E), where nodes represent the members
of the network and edges represent the links (relationships). Some of the links
are signed to indicate positive or negative relationships. Let sx,y denote the sign
of the relationship from x to y in the network. It may take two diﬀerent values,
{−1, 1}, indicating negative and positive relationships respectively.

To estimate the sign sx,y of a relationship from x to y, we collect peer opinions.
In diﬀerent versions of the model a peer can be any node of the network, or any
x,y ∈ {−1, 0, 1} denote the peer opinion of peer z on
node linked to x. Let pz
x,y = −1, it indicates that the z believes that
the sign sx,y. When pz
sx,y = 1 or sx,y = −1 respectively. When pz
x,y = 0 that means z does not have
enough knowledge to make a valid estimation.

x,y = 1 or pz

142

C. Wang and A.A. Bulatov

Another assumption made in our model is that not every peer can make a
reliable estimation. Therefore we divide all peers of a node into two categories,
and count the opinions only of the peers from the ﬁrst category, trusted peers.
The problem of how to select a set of trusted peers and use their opinions for
the estimation will be addressed later. Let Px denote the set of trusted peers
of a. We estimate the sign sx,y of a relationship from x to y by collecting the
opinions of peers z ∈ Px. If the sum of the opinions is nonnegative, then we say
sx,y should be 1, otherwise, it should be −1. This can be expressed as,

(cid:2)(cid:3)

(cid:4)

sx,y = sign

z∈Px

pz
x,y

.

2.2 Machine Learning Approach

Our approach to selecting an optimal set of trusted peers is to consider the
quadratic correlations between each pair of peers. The overall performance of
a set of peers is determined by the sum of the individual performances of each
of them together with the sum of their performance in pairs. The individual
performance measures the accuracy of individual estimations, while the pairwise
performance measures the degree of diﬀerence between the estimations of the
pair of peers. We want to maximize the accuracy of each individual and the
diversity of each pair at the same time.

Our goal is to use the information in G to build a predictor S(x, y) that
predicts the sign sx,y of an unknown relationship from x to y with high accuracy.
Function S(x, y) is deﬁned as the sign of the sum of peer opinions as follows. Let

pz
x,y

(1)

(cid:3)

Fx(y) =

z∈Px

(cid:5)

(cid:3)

Fx(y) =

z∈V

be the sum of individual peer opinions. Then set

S(x, y) =

1,
−1,

if Fx(y) ≥ 0 ,
if Fx(y) < 0.

Since Px is unknown, we introduce a new variable wz,x ∈ {0, 1} which indicates
if a node z ∈ V should be included into set Px. Hence, we rewrite (1) using the
characteristic function wz,x as,

wz,xpz

x,y.

(2)

Quadratic optimization problem. We are now ready to set the machine learning
problem. A training dataset (a subset of G) is given. Every entry of the training
dataset is a known edge along with its sign. Let a training dataset be D =
{(xi, yi, sxi,yi)|i = 1, ..., M}. The goal is to minimize the objective function,
ﬁnding the optimal weight vector w = {wx|x ∈ V }, where wx = {wz,x|z ∈ V }.

Inferring Attitude in Online Social Networks Based on Quadratic Correlation

143

We use machine learning methods [9] to train the predictor S(x, y) and learn an
optimal weight vector such that the objective function below is minimized.

wopt = argminw

(wz,xpz

x,y − sx,y)2 + λ|w|

⎛
⎝ (cid:3)

(x,y,sx,y)∈D

(cid:2)

(cid:3)

z∈V

1
N

(cid:4)⎞
⎠ .

Note that there will be more details on peer opinion terms pz

x,y.

2.3 Peer Opinion Variants

As mentioned earlier, we are going to test our model using diﬀerent peer opinion
(cid:4)
formulations. First, let s
x,y be extension of sx,y to edges with unknown sign and
also to pairs of nodes that are not edges deﬁned by

(cid:5)

(cid:4)
x,y =

s

sx,y, if sx,y exists,
0,

otherwise.

(cid:4)
z,y.

x,y = s

Simple-adjacent. The simplest option, later referred to as Simple-adjacent, is
based on the given information, to formulate peer opinions using existing rela-
tionships from peers to the target node, that is, pz
Standard-pq. In the Standard-pq option the inﬂuences rz,x ∈ {−1, 0, 1} associ-
ated with each pair of vertices z, x is an unknown parameter. A positive inﬂuence,
rz,x = 1, indicates that the attitude of z aﬀects x positively, while a negative
inﬂuence, rz,x = −1 indicates that the attitude of z aﬀects x negatively, and
(cid:4)
our expectation of pz
z,y has to be reversed, that is,
(cid:4)
pz
z,yrz,x. Also, in the Standard-pq mode we consider nonadjacent nodes
x,y = s
as potential peers to accommodate the problem of possible missing edges. De-
tails of the Standard-pq mode is explained in the experiment section. Since the
standard formulation gives us the best result in experiments, we use it through-
out our discussion. Using the standard formulation, we rewrite Equation (2) as

x,y based on z’s opinion s

wz,xs

(cid:4)
z,yrz,x.

(3)

(cid:3)

Fx(y) =

z∈V

(cid:10)

Standard-adjacent. Finally, in the Standard-adjacent option the peers of x are
(cid:4)
z,yrz,x. The rest is
restricted to the neighbours of x. Fx(y) =
deﬁned in the same way as for the Standard-pq option.

z∈N (x) wz,xs

2.4 Simplifying the Model

In our model, we are given a directed complete graph G = (V, E). In (3), both
wz,x and rz,x are unknown parameters. Since rz,x ∈ {−1, 0, 1}, we can reduce the
number of unknown parameters by considering all possible values of rz,x, and

144

z,xs

z,x, w

−
z,xs

z∈V w+

z,y − w
(cid:4)

C. Wang and A.A. Bulatov

(cid:10)
−
(cid:4)
z,y,where wz,x = w+
rewriting Fx(y) as Fx(y) =
z,x = 1, then z ∈ Px and rz,x = 1. Similarly, w
z,x ∈ {0, 1}. If w+
z,x
−
−
for w+
z,x = 1
indicates that z ∈ Px and rz,x = −1. When both w
−
z,x = 0 and w+
z,x = 0,
then z (cid:4)∈ Px. When w
z,x = 1, the two term cancel out each
other. Moreover, the regularization term ensures such case will not happen as
−
z,x = w+
w
z,x = 1. Although rz,x can
take three possible values, there are only two terms in Equation (3) since when
rz,x = 0, the term is also zero regardless of the value of s
weight vector w = {wx|x ∈ V } where wx = {w+

Now to minimize the objective function, we need to determine the optimal

z,x|z ∈ V } such that
−

z,x = 0 is always better than w

−
z,x = 1 and w+

−
z,x = w+

z,x + w

(cid:4)
z,y.

z,x, w

(cid:4)2

⎞
⎠ .
+ λ|w|

wopt = argminw

z,x − w

(w+

−
z,x)s

z,y − sx,y
(cid:4)

⎛
⎝ (cid:3)

(cid:2)

(x,y,sx,y)∈D

(cid:3)

z∈V

1
N

(4)
From the deﬁnition, we know that wx and wy are independent for diﬀerent
for each

nodes x and y. Instead of solving for wopt directly, we can solve wopt
x |x ∈ V }
x ∈ V separately, and then combine their values to get wopt = {wopt
⎞
(cid:4)2
⎠ .
+ λ|wx|

⎛
⎝ (cid:3)

z,y − sx,y
(cid:4)

x = argminwx

z,x − w

(cid:3)

−
z,x)s

wopt

(cid:2)

(w+

x

(x,y,sx,y)∈D

1
N

z∈V

(5)
Now, instead of solving a QUBO of size 2n2, we could solve n QUBOs of size 2n
separately which can be solved approximately by a heuristic solver. Another ap-
proach (similar to [20]) is to further simplify the problem, as it is still challenging
to solve each of these size 2n QUBOs exactly.

Breaking down the problem. In order to ﬁnd a good approximation of the optimal
solution to the QUBO deﬁned by (4), we could break it down to much smaller
z,x|z ∈ U}, and deﬁne a
QUBOs. Given a subset U ⊂ V , let wx,U = {w+
−
⎞
restricted optimization problem as follows
⎠ .
+ λ|wx,u|

⎛
⎝ (cid:3)

opt
x,U = argminwx,U

z,y − sx,y
(cid:4)

z,x − w

(cid:3)

(cid:4)2

−
z,x)s

z,x, w

(cid:2)

(w+

w

(x,y,sx,y)∈D

1
N

z∈U

(cid:11)m

The optimal solution wopt
i=1 Ui. Next, we describe a method to decompose V and to combine w

can be approximated by combining w

x

opt
x,Ui .

(6)
opt
x,Ui for V =

2.5 Method

The optimization problem deﬁned by (4) is a quadratic unconstrained binary
optimization problem which is NP-hard in general. To solve the problem, we use
two approaches. First, we solve the problem for each individual node separately,
as given by (5), using METSLib Tabu search heuristics. The clear setback of

Inferring Attitude in Online Social Networks Based on Quadratic Correlation

145

a,u = rv,as

this method is that for large problems the tabu search does not ﬁnd the best
solution. Second, we apply a similar method as described in [20] to reduce the size
of the problem dramatically. In order to do that the variables are ﬁrst ordered
according to their individual prediction error: For each data point (a, u, sa,u) in
a,u (cid:4)= sa,u when
the training dataset, we count ev−, the number of instances pv
a,u (cid:4)= sa,u when rv,a = 1 separately.
rv,a = −1, and ev+, the number of instances pv
(cid:4)
Note that since pv
v,u, we can compute this number; and that if u is
not a neighbour of v then it contributes to both ev− and ev+ . Then, we replace
v by v+ and v− with individual prediction error, ev+ and ev− respectively. The
subset U is iteratively selected by picking the ﬁrst d nodes in the list that are
not yet considered. The value of d is an important parameter of the algorithm
and is selected manually at the beginning of the algorithm. In the experiment
section, we show how the prediction accuracy changes as the size of d changes.
The sorting and selecting processes not only reduce the amount of computation,
but also allow us to consider the relevant nodes ﬁrst. The small subproblems
are now solved by the brute-force method or Cplex. Algorithm 1 describes the
method we use to solve each subproblem. Algorithm 2 uses Algorithm 1 as a
subroutine and explains how the problem is broken down into subproblems and
also how to combine the solutions of subproblems to obtain an approximate
solution.

Algorithm 1. Learn the parameters for a subset
Require: training dataset: TD, validation dataset: VD, a subset of nodes: U
Ensure: values of w and Z ⊂ U where Z is the set of trusted peers

Z = ∅, emin = |T D|
for λ = λmin to λmax do

Zcurrent = ∅
solve the optimization wopt = argminw(
λ|w|)
if wz == 1 then

Zcurrent = Zcurrent ∪ z

(cid:2)M

i=1( 1

N

(cid:2)

z∈U (w+

z − w

z )sz,y − sx,y)2 +
−

end if
Measure the validation error eval on V D using Zcurrent
if eval < emin then

Z =Zcurrent, emin = eval

end if
end for

2.6 Running Time

Although the QUBO problem is NP-hard in general, the proposed algorithm
can be very eﬃcient when using the right solver and right parameter d. Let T (d)
denote the time of solving a size d optimization problem deﬁned by (6), and kλ
be the number of λ values tested. The running time of Algorithm 1 is O(kλT (d)).
Let nv denote the number of neighbours of a node v. By our deﬁnition, nv is at
most (cid:6)V (cid:6). In Algorithm 2, Algorithm 1 is repeated at most nv
d times. Therefore,

146

C. Wang and A.A. Bulatov

Algorithm 2. Solve the optimization problem
Require: training dataset: TD, validation dataset: VD, The size of the subset: d
Ensure: values of wz for z ∈ V , and the set of trusted peers Z

eold = |T D|, enew =|T D|-1, Z, Zcurrent = ∅
sort nodes of V by their individual prediction errors in increasing order
U = the ﬁrst d nodes in V
while eold > enew do
Z = Zcurrent ∪ Z
wz, Zcurrent = Algorithm 1(T D, V D, U ) eold=enew
Measure the validation error enew on vd using Z
update U with the next d nodes in V

end while

the running time of Algorithm 2 is O( nv kλ
d T (d)). Traditionally, the best λ for
a model is determined before training stage through cross validations. In our
model, we are building a personalized predictor for each node. Hence, we need
to pick a λ for each node separately. During the training stage, we test kλ = 25
diﬀerent values in the range [λmin = .001, λmax = 0.25], and use the λ which
gives the lowest validation error. Since kλ is a constant, the running time crucially
depends on the eﬃciency of the QUBO solver. For example, in the experiments,
when d = 10 and T (d) is limited to 1 sec for METSLib-solver [2], the predictor
for a node can be determined instantly. Yet, when d = 10, using Cplex-solver [1]
would take a few seconds to minutes to determine the predictor for a node.

Since the main focus of our paper is on the prediction accuracy of the model,
we do not measure and compare the running times for diﬀerent solvers, and
we keep our experiments on a standard set of solvers rather than some exclu-
sive ones. Although the model is currently limited by the power of the software
solvers, it has shown a good potential. Its performance will improve as bet-
ter solutions are found by more eﬃcient solvers. One of such solvers could be
the quantum system which is rapidly developing at D-wave System. A recent
work [19] which compares the performances of diﬀerent software solvers with D-
wave hardware on diﬀerent combinatorial optimization problems shows promise.

3 Experiment

3.1 Datasets

We use three datasets borrowed from [14]. In order to make comparison possible
the datasets are unchanged rather than updated to their current status. The
dataset statistics is therefore also from [14] (see Table 1).

3.2 Parameters of Datasets

In our experiment, we split each dataset into two parts. We randomly pick one
tenth of the dataset for testing. The remaining dataset is used for training.

Inferring Attitude in Online Social Networks Based on Quadratic Correlation

147

Table 1. Basic
datasets

statistics on the

Table 2. Number of edges passing the
threshold

Dataset Epinions Slashdot Wikipedia
Nodes
Edges

82144
549202
+1 edges 85.0% 77.4%
-1 edges
15.0% 22.6%

7118
103747
78.7%
21.2%

119217
841200

Dataset
(p,q)=(15,20) 247725
205796
embeddedness
25 ([14])

Epinions Slashdot Wikipedia

25436
21780

51372
28287

The training dataset is split into two equal parts, half for training and half for
validating during the training process.

When the dataset is sparse, it is hard to build good classiﬁers due to the
lack of training and testing data. In order to get a better understanding of
the performance of the model, edge embeddedness of an edge uv is introduced
in [14,7] as the number of common neighbours (in the undirected sense) of u
and v. Instead of testing the model over the entire dataset, they only consider
the performance restricted to subsets of edges of diﬀerent levels of minimum
embeddedness. For example, Kleinberg et al. [14] restrict the analysis to edges
with minimum embeddedness 25.

Similarly, we also introduce two parameters that restrict the analysis of our
model. Instead of considering the edge embeddedness of a link, we consider
the node embeddedness of the source and target nodes. The ﬁrst parameter,
p, controls which nodes are considered peers of a given node v. A node u is
considered a peer of v if it has at least p common neighbours with v. When p = 0,
we consider every node in the network as a peer of v. The second parameter, q,
restricts the set of links whose sign we attempt to predict. We try to predict the
sign su,v, only if u is connected to at least q peers of v.

In Table 3, we show the dependence of the prediction accuracy of our model
on diﬀerent values of p and q. The data in Table 3 is obtained using option
Standard-pq with d = 10 solved by Cplex. As p and q grows, the performance of
the model clearly improves. However, increasing the values of p and q severely
restricts the set of nodes that can be processed. We choose somewhat optimal
values of these parameters, q = 20 and p = 15 and use them in the rest of our
experiments. It is also worth to notice that values q = 20 and p = 15 are less
restrictive than edge embeddedness 25. As shown in Table 2, more edges pass
the q = 20 and p = 15 threshold than the edge embeddedness 25 threshold.

3.3 Experimental Results

As explained before, our model depends on several parameters: internal parame-
ters, such as, the peer opinion variant and the method of solving the QUBO, and
external parameter, such as, balancing the dataset. We ﬁrst make the comparison
for diﬀerent internal parameters using the original unbalanced datasets.

In Table 6, we compare the performance of our model using diﬀerent peer opin-
ion formulations. As shown in the table, standard formulations (Standard-adjacent,

148

C. Wang and A.A. Bulatov

Table 3. Prediction Accuracy for Dif-
ferent Values for p, q

Table 4. Prediction Accuracy of Bal-
anced Approach

Epinions Slashdot Wiki
Dataset
91.7% 84.2% 85.0%
(p,q)=(10,0)
(p,q)=(10,10) 92.8% 91.6% 86.5%
(p,q)=(10,20) 93.7% 93.9% 86.6%
(p,q)=(10,30) 95.6% 95.1% 87.6%
(p,q)=(15,0)
93.7% 87.7% 85.2%
(p,q)=(15,10) 95.8% 96.1% 86.3%
(p,q)=(15,20) 96.2% 97.9% 86.9%
(p,q)=(15,30) 96.3% 96.0% 88.5%
(p,q)=(20,0)
93.5% 87.4% 85.0%
(p,q)=(20,10) 96.2% 98.1% 86.8%
(p,q)=(20,20) 96.3% 98.6% 86.9%
(p,q)=(20,30) 96.5% 99.2% 89.0%

Dataset
Epinions Slashdot Wiki
Standard-pd 85.14 % 82.82% 62.58%
(average)
false negative 0.84% 0.50% 2.10%
false positive 28.89% 33.8% 72.6%
Standard-pq
89.36% 85.37% 76.81%
(balanced)
false negative 6.20% 14.37% 22.5%
false positive 22.93% 15.35% 23.90%
93.42% 93.51% 80.21%
All123 ([14])
EIG ([8])
85.30% N/A

N/A

Table 5. Prediction Accuracy of Dif-
ferent Values of d

Table 6. Prediction Accuracy of Dif-
ferent Algorithms

Dataset
Epinions Slashdot Wiki
d=10 (exact) 96.36% 98.00% 87.69%
d=10 (cplex) 96.17% 97.31% 86.98%
d=25 (cplex) 96.20% 97.52% 85.60%
93.03% 96.79% 86.22%
METSLib

Dataset
Epinions Slashdot Wiki
Standard-adj 96.59% 97.93% 87.29%
Standard-pq 96.36% 98.00% 87.69%
Simple-adj
95.93% 97.68% 87.03%
HOC-5 ([7])
90.80% 84.69% 86.05%
All123 ([14]) 90-95% 90-95% N/A
EIG ([8])
N/A

93.60% N/A

Standard-pq) have better prediction accuracy then the simple formulation (Simple-
adjacent ), so it is useful to introduce inﬂuences. For Slashdot and Wikipedia, re-
stricting peers to neighbours (Standard-adjacent ) is not as eﬀective as using the set
of nodes with at least p = 15 common neighbours as peers (Standard-pq), although
the diﬀerence is neglegible. Surprisingly, for Epinions, it is slightly better to only
consider neighbours as peers. We compare the results with those of [7] (HOC-5)
and [14] (All123). Unfortunately, Kleinberg et al. [14] provide only a (somewhat
wide) range of the results their model produces on such datasets. Yet,even such
partial results allow us to conclude that collecting opinions from trusted peers is
an eﬀective method to infer people’s attitude.

In Table 5, we compare the performance of our model with diﬀerent values of d.
We expected the prediction accuracy to increase as the value of d increases. How-
ever, experimental result shows that it is not the case. Limited by the strength
of each solver, accuracy of the algorithm is very sensitive to the quality of ap-
proximation. But still, we think the prediction accuracy should increase if we
can ﬁnd a better solution for the problem for larger value of d.

In [8] and [14] the authors use certain techniques to test their approaches
on unbiased datasets. They use, however, diﬀerent ways to balance the dataset
and/or results. For instance, [8] does not change the dataset (Epinions), but,

Inferring Attitude in Online Social Networks Based on Quadratic Correlation

149

since the dataset is biased toward positive links, they ﬁnd the error ratio sep-
arately for positive and negative links, and then average the results. More pre-
cisely, they test the method on a set of randomly sampled edges that naturally
contains more positive edges. Then they record the error rate on all negative
edges, sample randomly the same number of positive edges (from the test set),
ﬁnd the error rate on them, and report the mean of the two numbers.

The approach of [14] is diﬀerent. Instead of balancing the results they balance
the dataset itself. In order to do that they keep all the negative edges in the
datasets, and then sample the same number of positive edges removing the rest
of them. All the training and testing is done on the modiﬁed datasets.

Although we have reservations about both approaches, we tested our model in
both settings. The results are shown in Table 4. Observe that since our approach
is to train the predictor for a particular dataset rather than ﬁnding and tuning
up general features as it is done in [8] and [14], and the test datasets are biased
toward positive edges, it is natural to expect that predictions are biased toward
positive edges as well. This is clearly seen from Table 4. We therefore think that
average error rate does not properly reﬂect the performance of our algorithm.

In the case of balanced datasets our predictor does not produce biased results,
again as expected. This, however, is the only case when its performance is worse
than some of the previous results. One way to explain this is to note that density
of the dataset is crucial for accurate predictions made by the quadratic correla-
tion approach. Therefore we had to lower the embeddedness threshold used in
this part of the experiment to p = 5, q = 5, while [14] still tests only edges of
embeddedness at least 25.

4 Conclusion

We have investigated the link sign prediction problem in online social networks
with a mixture of both positive and negative relationships. We have shown that
a better prediction accuracy can be achieved using personalized features such as
peer opinions. Although the improvement upon previous results is not signiﬁcant,
a nearly perfect prediction accuracy is hard to achieve. Another advantage of
the model is that it accommodates the dynamic nature of online social networks
by building a predictor for each individual nodes independently. This enables
fast updates as the underlying network evolves over time.

References

1. IBM ILOG CPLEX Optimizer (2010), http://goo.gl/uKIx6K
2. METSLib (2011), https://projects.coin-or.org/metslib
3. Avesani, P., Massa, P., Tiella, R.: A trust-enhanced recommender system applica-

tion: Moleskiing. In: SAC, pp. 1589–1593 (2005)

4. Brzozowski, M., Hogg, T., Szab´o, G.: Friends and foes: ideological social network-

ing. In: CHI, pp. 817–820 (2008)

5. Burke, M., Kraut, R.: Mopping up: modeling Wikipedia promotion decisions. In:

CSCW, pp. 27–36 (2008)

150

C. Wang and A.A. Bulatov

6. Cartwright, D., Harary, F.: Structure balance: A generalization of Heider’s theory.

Psychological Review 63(5), 277–293 (1956)

7. Chiang, K.-Y., Natarajan, N., Tewari, A., Dhillon, I.: Exploiting longer cycles for

link prediction in signed networks. In: CIKM, pp. 1157–1162 (2011)

8. Guha, R.V., Kumar, R., Raghavan, P., Tomkins, A.: Propagation of trust and

distrust. In: WWW, pp. 403–412 (2004)

9. Guyon, I., Gunn, S., Nikravesh, M., Zadeh, L.A.: Feature extraction: foundations

and applications. STUDFUZZ, vol. 207. Springer, Heidelberg (2006)

10. Harary, F.: On the notion of balance of a signed graph. Michigan Math. J. 2(2),

143–146 (1953)

11. Al Hasan, M., Chaoji, V., Salem, S., Zaki, M.: Link prediction using supervised
learning. In: SDM, 2006 Workshop on Link Analysis, Counterterrorism and Secu-
rity (2006)

12. Kunegis, J., Lommatzsch, A., Bauckhage, C.: The Slashdot Zoo: mining a social

network with negative edges. In: WWW 2009, pp. 741–750 (2009)

13. Lampe, C., Johnston, E., Resnick, P.: Follow the reader: ﬁltering comments on

Slashdot. In: CHI, pp. 1253–1262 (2007)

14. Leskovec, J., Huttenlocher, D.P., Kleinberg, J.M.: Predicting positive and negative

links in online social networks. In: WWW, pp. 641–650 (2010)

15. Li, Y., Chen, W., Wang, Y., Zhang, Z.-L.: Inﬂuence diﬀusion dynamics and inﬂu-
ence maximization in social networks with friend and foe relationships. In: WSDM
2013, pp. 657–666 (2013)

16. Liben-Nowell, D., Kleinberg, J.: The link prediction problem for social networks.

In: CIKM 2003, pp. 556–559 (2003)

17. Liben-Nowell, D., Kleinberg, J.: The link-prediction problem for social networks.

J. Am. Soc. Inf. Sci. Technol. 58(7), 1019–1031 (2007)

18. Massa, P., Avesani, P.: Controversial users demand local trust metrics: An exper-

imental study on epinions.com community. In: AAAI, pp. 121–126 (2005)

19. McGeoch, C., Wang, C.: Experimental evaluation of an adiabiatic quantum system

for combinatorial optimization. In: CF 2013, pp. 23:1–23:11 (2013)

20. Neven, H., Denchev, V., Rose, G., Macready, W.: Training a large scale classiﬁer

with the quantum adiabatic algorithm. CoRR, abs/0912.0779 (2009)

21. Newman, M.E.J.: The structure and function of complex networks. SIAM Re-

view 45(2), 167–256 (2003)

22. Sarwar, B., Karypis, G., Konstan, J., Riedl, J.: Analysis of recommendation al-
gorithms for e-commerce. In: ACM Conf. on Electronic Commerce, pp. 158–167
(2000)

23. Symeonidis, P., Tiakas, E., Manolopoulos, Y.: Transitive node similarity for link
prediction in networks with positive and negative links. In: RecSys 2010, pp. 183–
190 (2010)

24. Wang, C., Satuluri, V., Parthasarathy, S.: Local probabilistic models for link pre-

diction. In: ICDM 2007, pp. 322–331 (2007)


