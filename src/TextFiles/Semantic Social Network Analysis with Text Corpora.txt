Semantic Social Network Analysis with Text Corpora 

Dong-mei Yang1, Hui Zheng1, Ji-kun Yan2, and Ye Jin2 

1 Science and Technology on Blind Signal Processing Laboratory 

ydm.123@foxmail.com, Zheng5739@163.com 

2 Southwest Electronics and Telecommunication Technology Research Institute 

yanjk@126.com, djws_1982@sina.com 

Abstract.  We  present  the  Document-Entity-Topic  (DET)  model  for  semantic 
social network analysis which tries to find out the interested entities through the 
topics we aim at, detect groups according to the entities which concern the simi-
lar topics, and rank the plentiful entities in a document to figure out the most 
valuable ones. DET model learns the topic distributions by the literal descrip-
tions of entities. The model is similar to Author-Topic (AT) model, adding the 
key  attribute that the distribution of entities in a document is not uniform but 
Dirichlet allocation. We experiment on the “Libya Event” data set which is col-
lected from the Internet. DET model increases the precision on tasks of social 
network analysis and gives much lower perplexity than AT model. 

Keywords: Semantic Social Network Analysis, Topic Model, Entity Modeling. 

1 

Introduction 

As far as we know, topic modeling has become a most popular technology to model 
large collection of corpus[1-3], such as Latent Dirichlet Allocation[4]. The basic idea 
of topic modeling is that the latent topics can be used to describe the relationship be-
tween  words  and  documents.  In  this  paper  we  consider  the  problem  of  using  latent 
topics to connect the  words and entities in documents (such as person, location, or-
ganization). We focus on the news articles which contain lots of entities in order to 
convey the information about who, what, when and where. The purpose we want most 
is modeling the entities in terms of latent topics so that we can 1) find out the interest-
ed entities through the topics we aim at; 2)recognize groups with supposing that the 
entities  (especially  the  persons)  which  concern  the  similar  topics  can  be  seen  as  a 
group; 3) rank the plentiful entities in a document to figure out the valuable ones by 
assuming that the more an entity contributes to a document’s topic(s), the more valu-
able it is in the precise one. We call the three tasks Semantic Social Network Analysis 
for the interactions been found based on the topics of the corpus. 

There are several related researches to achieve the relationship between words and 
entities (authors)  with topic models. The Author-Topic (AT) model[5-6] learns    the 
topics  of  a  document  conditioned  on  the  mixture  of  interests  with  the  authors.  AT 
model assumes that the authors equally contribute  to the topics of a document. The 
SwitchLDA  and  GESwitchLDA[7-8]  extend  LDA  to  capture  dependencies  between 
entities and topics, referring to entities as additional classes of words.   

P.-N. Tan et al. (Eds.): PAKDD 2012, Part I, LNAI 7301, pp. 493–504, 2012. 
© Springer-Verlag Berlin Heidelberg 2012 

494 

D. Yang et al.   

This paper presents the Document-Entity-Topic (DET) model, a directed graphical 
model by assuming that  words  were generated by the entities of the document. The 
model is similar to the AT model. However, it is not limited to the topic finding of 
authors, but tries to modeling topics of all related entities in the documents. For this 
application,  we  confront  more  unwanted  entities  of  the  corpus.  In  our  experiments, 
there are more than five person entities in most documents, and some entities such as 
news reporters have little significance to the topic(s). If all entities in a document have 
been assumed to be equally contributed to the mixture of topics as AT model, it is not 
enough for us to rank the importance of entities and many noisy entities will disturb 
the topic modeling of corpus. So our DET model presumes that the entities have dif-
ferent topical contributions to their document. We use the Dirichlet allocation to de-
scribe  the  distribution  between  document  and  its  entities;  a  document  gives  higher 
probabilities  to  several  more  valuable  entities  (not  all  entities)  and  valuable  entities 
have more contributes to the topic modeling. 

The outline of the paper is as follows: Section 2 describes the Document-Entity-
Topic model, and section 3 outlines how to learn the parameters from the documents. 
Section 4 discusses the application of the model to the data set we collected from the 
internet. Section 5 contains a brief discussion and concluding comments. 

2 

Document-Entity-Topic Model 

In this section we introduce the document-Entity-topic (DET) model. The DET model 
belongs  to  the  family  of  generative  models,  in  which  each  word  w  in  a  document  is 
associated with two latent variables: an entity assignment x , and a topic assignment z . 

2.1  Dirichlet Priori on Document-Entity Distribution 

The  entities  in  news  may  have  different  weights  to  be  described  by  the  words,  for 
example,  “a  reporter covers  that,  Mr.  A  and  B  contact  at a  national  conference  and 
have an educational barging, trying to improve the intercommunion and friendship of 
both.” We find that the relationship between A and B is closer than that between re-
porter and the two people. In order to discover the different weights for different enti-
ties, we use Dirichlet allocation as the prior distribution to describe the importance of 
each entity in a document, which is similar to LDA model by using Dirichlet alloca-
tion to describe the relationship between topics and the document. 

The reason to choose the Dirichlet is that, firstly, it can reflect the characteristic of 
document-entity relation, a document has primary and minor entities, and the weights 
can be adjusted by the hyperparameter of Dirichlet. Secondly, the conjugate prior of 
multinomial distribution is Dirichlet allocation, so it can simplify the computation for 
the posterior distribution which has the same functional form as the prior.   

Thus, we propose the Document-Entity-Topic (DET) model for mining the seman-
tic description of entities and using the topical distribution to carry out the social net-
working tasks. The generative process of DET model for a document can be summa-
rized  as  follows:  firstly,  an  entity  is  chosen  randomly  from  the  distribution  over   
 

 

Semantic Social Network Analysis with Text Corpora 

495 

entity-document;  next,  a  topic  label  is  sampled  for  each  word  from  the  distribution 
over topics associated  with the entities of that  word; finally, the  words are sampled 
from 
topic.  The  plate 
representation[9] for all models are shown in figure 1. 

the  distribution  over  words  associated  with  each 

α

D

T

θ

φ

β

,d nz

,d nw
dN

D

da

x

,d nz

,d nw
dN

D

α

A

T

θ

φ

β

γ

α

X

T

θ

φ

β

 

dΨ

x

,d nz

,d nw
dN

D

Fig. 1. Two related models and the DET model. In all models, each word w is generated from a 
topic-specific multinomial word distribution; however topics are sampled differently in each of 
the models. In LDA, a topic is sampled from a document-specific topic distribution which is 
sampled from a Dirichlet  with hyperparameter. In the AT  model, a topic is sampled from an 
author-specific  multinomial  distribution,  and  authors  are  sampled  uniformly  from  the  docu-
ment’s author set. In DET, Dirichlet prior has been introduced to the document-entity distribu-
tion, a topic is sampled from an entity-specific multinomial distribution, and entity assignment 
is sampled from the Dirichlet allocation of that document. 

2.2  Generative Process of DET Model 

In DET model, the generative process of generating a word is according to the proba-
bility distributions of firstly picking an entity followed by picking a topic. 

ψ

,

D

a) For each document

For each entity
t
For each topic

d Dirichlet γ
( )
~
; 

= … choose
1,
d
x Dirichlet α
= … choose
θ
~
)
(
,
1,
X
x
β . 
φ
= … choose
~
)
t Dirichlet
,
1,
T
= …  
1,
D
b) For each document
              Given the vector of entities Xd, for each word wi, out of the Nd words 
 
 

x Dirichlet ψ ; 
Conditioned on xd choose a persona
d
z Dirichlet θ
~
(
; 
Conditioned on xi choose a topic
w Dirichlet φ . 
(
)
Conditioned on zi choose a word

~

i

~

i

d

,

(

; 

)

(
)

x
i

z

i

 

i

496 

D. Yang et al.   

Under  this  generative  process,  each  entity  is  drawn  independently  conditioned 
on Ψ ;  each  topic  is  drawn  independently  conditioned Θ ;  and  each  word  is  drawn 
independently conditioned on Φ and z. The probability of the corpus w, conditioned 
on Ψ ,  Θ and Φ is shown as equation (1): 

(

P

w Θ,Φ,Ψ

|

)

D
∏=
=
1

d

(

P

)
w Θ,Φ,Ψ  

|

d

(1)

Summing  over  the  latent  variables  x  and  z,  we  can  obtain  the  probability  of  the 

dw as equation (2): 

w Θ,Φ,Ψ

|

i

)

d

d

|

N

=
1

(

)

(

i
N

P

P

=

w Θ,Φ,Ψ

words in each document
∏
∑∑∏
∑∑∏
∏

=

=

=

i
N

i
N

=
1

=
1

=
1

=
1

=
1

=
1

X

X

T

T

x

t

t

d

d

d

d

x

(
,
P w z
i

i

=

,
t x
i

=

x

|

Θ,Φ,Ψ

)

(
P w z
i

|

i

=

t

,

Φ

)

(
P z
i

=

|
t x
i

=

,
x

Θ

)

(
P x
i

=

x

|

Ψ

(2)

 

)

d

=
1

i

∑

∈
x X

d

ψ θ φ

⋅

⋅

xt

w t
i

x
d

T

∑

=
1

t

Factorizing in the third line of equation (2) uses the conditional independence assump-
tions of the model. The last line in the equations expresses the probability of the words w 
(
)
x= Ψ is the entity multinomial 
in terms of the parameter matrices Ψ ,  Θ and Φ . 
iP x
(
= Θ is the multi-
=
distribution dψ in Ψ which corresponds to document d, 
,
|
x
P z
t x
i
i
nomial distribution xθ in Θ that corresponds to entity x, and (
t= Φ   is the mul-
P w z
i
tinomial distribution 

φ  in  Φ   corresponding to topic t. 

)
)

,

|

|

i

t

3 

Learning the DET Model from Data 

The DET model contains three continuous random variables Ψ , Θ and Φ . The infe-
rence scheme used in this paper is based upon a Markov chain Monte Carlo (MCMC) 
algorithm  or  more  specifically,  Gibbs  sampling.  We  estimate  the  posterior  distribu-
tion (
. The  inference  scheme  is  based  upon  the  observation 
P
that 

)
D γαβ

Ψ,Θ,Φ

train

,

,

,

|

(
P
= ∑
P

,
z x

Ψ,Θ,Φ
(

|
D
Ψ,Θ,Φ

|

train

,

,

)
γαβ
,
)
γαβ
,

train

,

,

,
z x D

,

(
P z x D

,

|

)
γαβ
,

,

,

 

train

(3)

Where z is the topic variable and x is the entity assignment. This inference process 
involves  two  steps.  Firstly,  we  use  Gibbs  sampling  to  obtain  an  empirical   

 

Semantic Social Network Analysis with Text Corpora 

497 

(
P z x D γαβ .  Second,  we  compute  each  specific 
sample-based  estimate  of
sample corresponding to particular x and z using the conjugation trait between Dirich-
let and multinomial distribution.   

train

)

,

,

,

,

|

3.1  Gibbs Sampling 

|

,

,

,

,

,

,

)

train

Gibbs  sampling  is  a  widely  applicable  Markov  chain  Monte  Carlo  algorithm  which 
can be viewed as a special case of Metropolis Hastings algorithm. It often yields rela-
tively simple algorithms for approximate inference in high-dimensional models such 
as topic models[9]. Here we wish to construct a Markov chain which converges to the 
trainD γαand β.  Using  Gibbs  sam-
posterior  distribution  over  x  and  z  in  terms  of 
(
P z x D γαβ by  firstly  sampling  an 
pling  we  can  generate  a  sample  from 
entity assignment xi and a topic assignment zi for an individual word wi conditioned 
on initialized assignments of entities and topics for all other words in the corpus. Se-
condly, repeating this process for each word. A single Gibbs sampling iteration con-
sists  of  sequentially  performing  sampling  of  entity  and  topic  assignments  for  each 
individual word in the corpus. 
)
D γαβ

can  also  be  the 
Gibbs sampler. In this paper we use the blocked sampler where we sample xi and zi 
jointly. It can improve the mixing time of the sampler and the method also has been 
used  similarly  by  Rosen-Zvi  et  al[5].  In  Appendix,  we  derive  the  Gibbs  sampler  of 
document d and entity
=

)
D γαβ

as equation (4) 

(
P x
i

(
P z

and

x
,

train

train

=

=

=

x

z

z

x

t

,

,

,

,

,

,

,

,

−

i

,

−

i

|

|

d

i

+

−

i

γ
+

X

 

γ

(4)

x X∈
=
β
+

,
x z
+

i

−

i

W

(
P x
i
WT
C
−
,
wt
i
WT
C
' ,
w t

∑

w

'

∝

i

−

i

x
|
,
t w w
TX
C
−
,
tx
i
TX
C
'
t x

⋅
β

∑

t

'

−

i

,

i

−

z w
,
,
−
+
α
+

⋅
α

T

)
γαβ
,

,

i

,
XD
C
−
,
xd
i
XD
C
'
x d

,

∑

x

'

Here

XDC represents  the  document-entity  count  matrix,  where

XD
iC − is  the  number 
,
xd
of  words  assigned  to  entity  x  for  document  d  excluding  word  wi. 
TXC is  the  topic-
iC − is the number of words assigned to topic t for entity x 
entity count matrix, where
WTC is the number of words from the wth 
excluding the topic assignment to word wi. 
entry in the vocabulary assigned to topic t excluding the topic assignment to word wi. 
Finally, z-i, x-i , w-i stand for the vector of topic assignments, vector of entity assign-
ments and vector of word observations in corpus except for the ith word, respectively.   

TX
,
tx

3.2  The Posterior on Ψ ,  Θ and Φ  

,

z x
,

trainD γα

and β,  we  can  compute  the  posterior  distributions  on Ψ , 
Given ,
Θ and Φ directly. Using the fact that the Dirichlet is conjugate to the multinomial, we 
have 

,

498 

D. Yang et al.   

φ

,
w t

=

∑

WT
C
−
,
wt
i
WT
C
' ,
w t

w

'

+

−

i

β
+

V

θ

,
t x

=

,
β

∑

TX
C
−
,
tx
i
TX
C
'
t x

,

t

'

+

−

i

α
+

T

ψ
,
α

,
x d

=

∑

XD
C
−
,
xd
i
XD
C
'
x d

,

x

'

+

−

i

γ
+

X

 

γ

(5)

These  posteriors  provide  point  estimates  for Ψ ,  Θ and Φ .  Ψ corresponds  to  the 
posterior predictive distribution for the documents and entities, it obeys the Dirichlet 
allocation other than uniform distribution, and can get the more valuable entities who 
effect  the  topics  of  the  document  more.  Θ corresponds  to  the  posterior  predictive 
distribution for the entities and topics, every entity has a vector of topics, it can tell us 
what topics the entity associates with and which entities are interested in the similar 
topics, so groups can be extracted from Θ .  Φ corresponds to the posterior predictive 
distribution for the topics and words, we can get the word description of topics.   

4 

Experiment Result 

We train our DET model on the “Libya Event” dataset which is collected from Internet 
unique  entities 
(http://www.ifeng.com).  It  contains
(most  are  person  names), 
unique 
words. We preprocess the document set with tryout edition of ICTCLAS whose rights 
reserved by ictclas.org. All documents are written in Chinese, and we translate the re-
sults in English. 

tokens,  and  a  vocabulary  of

D =
782043

documents, 

15812

N =

V =

3784

4165

P =

We run the Markov chain for a fixed number of 2000 iterations. Furthermore, we 
find that the sensitivity to hyperparameters is not very strong, so that we use the fixed 
symmetric Dirichlet distributions
in all our experiments. 
In the comparing experiment of AT model, the author set a are entities extracted from 
the documents. 

0.01β=

α=
=

, and

0.5,

0.1

γ

4.1 

Perplexity Comparison between AT and DET 

Models  for  natural  languages  are  often  evaluated  by  perplexity  as  a  measure  of  the 
goodness  fit  of  models.  The  lower  perplexity  a  model  has,  the  better  it  predicts  the 
unseen words. The perplexity of a previously unseen document d consisting of words 
wd can be defined as equation (6) when the entities xd are given: 

Perplexity

(

w

=

)

d

exp

(

p

log

⎛
−⎜
⎝

d

|

w x
(
N

d

)

)

d

⎞
⎟
⎠

 

in which 

(

p

w x

|

d

)

=

d

d

N

∏

=
1

i

⎛
⎜
⎜
⎝

T

∑ ∑

ψ
ˆ

⋅

x

∈
x
x

d

=
1

t

φ
ˆ
w t
i

⋅

ˆ
θ

tx

( )
i
n
d

 

⎞
⎟
⎟
⎠

(6)

(7)

(cid:71)
φ
iw

Where ( )i

dn is the number of times token i has been observed in document d. 

can 
xθ and dψ need  to  be  derived  by  querying  the 

be  determined  by  the  training  set,  but

 

Semantic Social Network Analysis with Text Corpora 

499 

model. Firstly, initializing the algorithm by randomly assigning topics and entities to 
words  of  the  test  documents,  and  then  performing  a  number  of  loops  through  the 
Gibbs sampling update: 
=
(cid:4)

) (
⋅
α

Μ ∝

,
x w w

(
(cid:4)
p z

)
γ

ϕ

,

x w
−

,

+

+

=

=

)

(

⋅

(8)

;

−

i

i

(cid:4)
,
t x
i

i

n

(cid:4)
( )
t
−
,
x

i

(cid:4)
wt

|

i

z

−

i

(cid:4)
( )
x
n
−
,
d

i

 

(cid:4)
( )
t
in −
,
x

Where 

(cid:4)
is  the  number  of  topic  t  been  assigned  to  persona  x,  and ( )
x
in −
,
d

is  the 
number of entity x been assigned to document d. Both of them exclude the topic and 
entity  assignment  of  word  wi.  We  report  the  perplexities  with  different  number  of 
topics on  “Libya Event” test  data set  with 109 documents, about 10% of the  whole 
data set. 

 

 

Fig. 2. Perplexity comparison of AT and DET on “Libya” data set. DET model has significant-
ly better predictive power as AT over our document set. We can also find that the lowest per-
plexity obtained by DET is not achievable by AT with any topic number. It proves that DET 
can better adapt to the task of Semantic Social Network Analysis (SSNA), which discovers the 
topic-based relationship and group information of entities in documents.   

4.2 

Semantic Social Network Analysis with DET 

Topics and Entities. We get the latent topics after applying the Gibbs sampling algo-
rithm to DET model. We use the topic significance ranking  method[10] to rank the 
topics and show two most important topics in table 1. In each topic we list the most 
likely words in the topic with their probability and below that the most likely entities 
and the topic names are named by authors. 

During the experiment process, we have found that many topics own lots of same 
words  with  high  probabilities,  the  reason  we  think  is  that  all  documents  in  “Libya 
Event” data set talk about one event (similar topics). We introduce in the idea of tf-idf 

500 

D. Yang et al.   

algorithm  to  decide  which  words  have  high  probabilities.  The  probability  of 
φ and the tf_idf 
word w belonging to topic k depends on both of the DET result, i.e.,
value,  which  ranges  from  0  to  1  with  standardization.  So  the  final  probability  of  a 
word belonging to a topic is

φ δ φ

< < . 
δ
1

(
+ −
1

= ⋅

)
δ

idf

,0

tf

,k w

_

⋅

The probability of entity x belonging to topic k is not only decided by ,x k

θ , but also 
decided  by  the  number  of  entity x appearing  in  documents.  If x appears  in  docu-
ment d , the number adds 1, and the appearing frequency is 
. So 
θ
the probability of entity x with topic k is '

xdf = | {

θ=

} | |

∈

df

D

. 

d

x

⋅

|

k w

,

k w

,

k w

,

'

,
x k

x

,
x k

Table  1.  Two  topics  with  highest  probabilities  from  a  100-topic  DET  running  with  “Libya 
Event” data. In each topic we list the most likely words in lowercase with their probabilities, 
and below that the most likely entities in uppercase with initial. 

transition  committee 

Topic89: Conflicts of government and oppo-
sition in Libya 
the opposition 
demos 
fremdness 
relation 
find out 
reason 
in the past 
hours 
with responsibility for 
encounter 
Qaddafi 
Bangh acirc 
Qatar 

0.751071 
0.098968 
0.018027 
0.015836 
0.013272 
0.011508 
0.008329 
0.007549 
0.006162 
0.005506 
0.060839 
0.043085 
0.030726 

Reuters 

Italy 
Russia 
Abdul-Jelil 
Egypt 
Associated Press  
Muhammad 

0.027212 

0.020556 
0.014663 
0.014444 
0.013855 
0.008018 
0.007668 

Topic31:  National 
comes into existence 
committee 
transition 
nation 
admit 
chairman 
come into existence 
spokesman 
intraday 
leaguer 
promise 
Abdul-Jelil 
Bangh acirc 
Italy 
National  Transition  Commit-
tee 
Qatar 
Bani Walid 
Abdul-Jelil 
Beijing 
Paris 
London 

0.313636 
0.278701 
0.213317 
0.046756 
0.033091 
0.024036 
0.020482 
0.013421 
0.013068 
0.006441 
0.041501 
0.026637 
0.025412 

0.025164 

0.023441 
0.019576 
0.016731 
0.016373 
0.015959 
0.015528 

Entity Significance Ranking. We suppose that if the topic distribution of an entity is 
much related to that of the document, the entity is significant to this document. Usual-
ly, KL divergence is used to measure the similarity between the entity and document. 
We show the KL divergences, probabilities and frequencies of all entities in two doc-
uments for particular information in table 2. 

 

Semantic Social Network Analysis with Text Corpora 

501 

Table  2.  KL  divergences,  probabilities  and  frequencies  of  all  entities  in  two  documents  for 
particular information 

The National transition committee encounters plaster in Surt 

entity 

KL divergence 

frequency 

Misracirctah 
Abdul-Jelil 

Qaddafi 

Saif-Nasser 
New York 

Niger 
Surt 

Bani Walid 

Tripoli 

0.205826 
0.266745 
0.485460 
0.498184 
0.435166 
0.411440 
0.598851 
0.494996 
0.635240 
0.624283 

Abdul-Jelil 
Wei Wei 

Libya 
UN 

New York 
Gu Zhengqiu 
XinHua Net 

UNSC 

Ban ki-moon 

0.018252 
0.591291 
0.642270 
0.662659 
0.671120 
0.669310 
0.689017 
0.652071 
0.666539 

Jerusalem 
UN’s high conference of Libya appeals to picking up the reconstruction 
entity 

KL divergence 

frequency 

probability 
0.181452 
0.181452 
0.149194 

0.125 

0.084677 
0.060484 
0.044355 
0.03629 
0.03629 
0.028226 

probability 
0.444015 
0.374517 
0.104247 
0.042471 
0.019305 
0.003861 
0.003861 
0.003861 
0.003861 

1 
1 
10 
1 
2 
1 
3 
1 
1 
1 

1 
1 
16 
6 
1 
1 
1 
1 
1 

In most instances, if an entity which has a lower KL divergence with the document, 
the probability it belongs to that document will be higher, and the frequency is not a 
key factor to influence the belonging probability. In order to compare the entity rank-
ing performances between  AT and DET model on the  whole data, we further adopt 
the weighted KL divergence which is defined as equations (9) and (10): 

wKL AT

_

=

1
D

a

d

(
θ η

||

,
x t

,
d t

)

 

  (9)

KL

D

1
⋅ ∑∑
a

=
1

=
1

d

a

d

502 

D. Yang et al.   

wKL DET

_

=

⋅

1
D

dXD

∑∑

=
1

d

=
1

x

(
ψ

,
d x

⋅

KL

(
θ η

||

,
x t

,
d t

)

)

 

(10)

The smaller the weighted KL value is, the more similarity entities and documents 

own. In figure 3, we have shown the values with different topic numbers. 

 

 
Fig. 3. The weighted KL divergences of AT and DET model with different topic numbers. The 
values of DPT model are lower than AT model. It means that the more important entities (with 
lower KL divergence to document which they appear in) have higher probabilities belong to the 
document and contributes more to the topic generativity. 

5 

Conclusions 

We have presented the Document-Entity-Topic model, a probabilistic model for ex-
ploring the interactions of words, topics and entities within documents. It applies the 
probabilistic model to the social network analysis based on latent topics. In order to 
avoid the side effects of noisy entities and find out the entities which mainly affect the 
topics, we have introduced in the Dirichlet allocation for document-entity distribution 
other  than  uniform  allocation.  The  model  can  be  applied  to  discovering  topics   
conditioned  on  entities,  clustering  to  find  semantic  social  groups,  and  ranking  the 
significance of entities in a document. 

However, while there is no entity in a document, the topics of that document can 
not  be  modeled.  When  such  lack-of-entity  documents  arrive  at  a  certain  amount,   
the  topic  modeling  of  the  corpus  will  be  affected.  Consequently,  we  try  to  improve   
the  model  for  the  application  when  there  are  many  documents  lacking  of   
entities. 

 

Semantic Social Network Analysis with Text Corpora 

503 

References 

1.  Blei, D.: Introduction to Probabilistic Topic Models. Communications of the ACM (2011) 
2.  Steyvers,  M.,  Griffiths,  T.:  Probabilistic  Topic  Models.  Handbook  of  Latent  Semantic 

Analysis 427 (2007) 

3.  Blei, D., Carin, L., Dunson, D.: Topic Models. IEEE Signal Processing Magazine 27(6), 

55–65 (2010) 

4.  Blei, D., Ng, A., Jordan, M.: Latent Dirichlet Allocation. The Journal of Machine Learning 

Research 3, 993–1022 (2003) 

5.  Rosen-Zvi,  M.,  Chemudugunta,  C.,  Griffiths,  T.,  Smyth,  P.,  Steyvers,  M.:  Learning  Au-
thor-Topic  Models  from  Text  Corpora.  ACM  Transactions  on  Information  Systems 
(TOIS) 28(1), 1–38 (2010) 

6.  Rosen-Zvi,  M.,  Griffiths,  T.,  Steyvers,  M.,  Smyth,  P.:  The  Author-Topic  Model  for  Au-

thors and Documents, pp. 478–494. AUAI Press (2004) 

7.  Shiozaki, H., Eguchi, K., Ohkawa, T.: Entity Network Prediction Using Multitype Topic 

Models. Springer (2008) 

8.  Newman, D., Chemudugunta, C., Smyth, P.: Statistical Entity-Topic Models, pp. 680–686 

(2006) 

9.  Bishop, C.: Pattern Recognition and Machine Learning. Springer, New York (2006) 
10.  AlSumait,  L.,  Barbará,  D.,  Gentle,  J.,  Domeniconi,  C.:  Topic  Significance  Ranking  of 
LDA Generative Models. In: Buntine, W., Grobelnik, M., Mladenić, D., Shawe-Taylor, J. 
(eds.) ECML PKDD 2009. LNCS, vol. 5781, pp. 67–82. Springer, Heidelberg (2009) 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

504 

D. Yang et al.   

Appendix 

(
P x
i

=

=

=

,
x z
i

|
,
t w w

, the conditional distri-
We need to derive
bution for word wi given all other words’ topic and entity assignments z-i and x-i    to 
give out the Gibbs sampling procedure for DET model. We begin with the joint prob-
ability of the whole documents corpora. Here we can make use of conjugate priors to 
simplify the integrals.   

z w
−

x

,

,

,

−

−

i

i

i

i

)
γαβ
,

,

)
X
∏
γ
)

X

=
1

x

θ
(

p

x

|

T

∏
α
)

=
1

t

p

)

)

X

∏

=
1

x

−
1

ψ

γ
d
dx

X

D

⎞
∏∏
⎟
⎟
⎠

=
1

=
1

d

x

(
p x
di

ψ

|

d

)

(
p z

di

θ

|

)

P w
di

(

|

dx

di

φ

z

di

N

∏
φ β
)
(

|

d

t

=
1

i

ψ

n
dx
dx

Ψ

d

Φ Θ Ψ
)
d d

d

  

φ

n
tv
tv

Φ
d

P

=

=

∝

∝

(

x

x

x

d

d

d

|

|

D

D

,

(

(

=
1

p

x,

X
=
1

γαβ
,
,

w
z,
∏
∫∫∫
ψ
=
1
d
γ
Γ
∑∏
⎛
D
⎜
=
∫ ∏
1
γ
Γ
⎜
(
⎝
⎛ Γ
∑
∏
(
⎜
∏
⎜
⎝
∏ ∏
X
∫
∏
(
Γ
∑

X
=
1
X
=
1

γ
(
γ
(

∏

T
=
1

γ
d
dx

ψ

Γ

∫

×

=
1

−
1

=
1

=
1

=
1

n
dx

D

D

+

X

d

d

x

x

x

x

x

x

t

t

v

β
)
β
)

v

V

∏

=
1

v

−
1

φ
β
v
tv

⎞
⎟
⎟
⎠

T

V

∏∏

=
1

t

=
1

v

t

α
T
)
=
1
α
Γ
(
)

t

T

∏

=
1

t

ψ
d

×

d

+
+

n
dx

)

n
dx

)

)

t

x

T

X

X

=
1

=
1

−
1

α
θ
t
xt

⎞
∏∏
⎟
⎟
⎠
∏ ∏
T
α
θ
t
xt
∏
(
Γ
∑

∫
∏

T
=
1
T
=
1

×

=
1

=
1

=
1

+

X

n

x

x

t

t

t

Γ

θ Θ

d

n
xt
xt

×

−
1

xt

θ

d

x

×

t

α
(
α
(

t

+
+

n

xt

n

xt

t

v

v

T

T

V

V

=
1

=
1

∏

V
=
1
Γ
(

⎛
⎜
⎜
⎝
∫
∏

Γ
∑
(
∏
∏ ∏
φ
β
v
tv
∏
(
Γ
∑

)

×

=
1

=
1

=
1

)

)

+

V

T

v

v

t

t

n
tv

−
1

φ

t

d

Γ

(

=
1
V

(

=
1

v

v

β
β

v

+
+

n
tv

)

n
tv

)

)

Where dxn is the number of tokens assigned to persona x and document d, 

xtn is the 
tvn is  the  number  of  tokens  of 
number  of  tokens  assigned  to  topic  t  and  persona  x, 
word w assigned to topic t. Using the chain rule, we can obtain the conditional proba-
bility conveniently. We define

diw− as all word tokens except the token diw : 

,

)

αβγ
X
,
αβγ
,
,
,
,
,
−
αβγ
X
,
)
,

di
,

(
P x
di
(
P x

−

,

di
|

di
,

x
,
|
z
di
,
,
z w
di
di
di
x
(
|
P w
−
di
di
x, z, w
|
(
P
z w
,
,
−
−
di
−
+
1
n
dx
di
+
−
γ
(
) 1

z w
,
,
,
−
x
z w
−
−
di
di
z w
,
,
,
−
−
di
di
αβγ
X
,
,
,
)
αβγ
X
,
,
)
,
+
α
n
+
α
(

di

di

|

T

di

z

n
dx

∑

=
1

z

x

z

P

x
(
−
γ
∑

x
di
X
=
1

x

X

)

 

−

1
−
) 1

x z
di di
n

x z
di

w
di

β
∑

V

v

=
1

(

z w
di
di

+
n
nβ
+

v

z v
di

−

1
−
) 1

=

∝

∝

 

 


