Diversity Analysis on Boosting Nominal

Concepts

Nida Meddouri1, H´ela Khouﬁ1, and Mondher Sadok Maddouri2

1 Research Unit on Programming, Algorithmics and Heuristics - URPAH,

Faculty of Science of Tunis - FST,

Tunis - El Manar University

{nida.meddouri,hela.khoufi}@gmail.com

2 College of Community, Hinakiyah

Taibah University - Medinah Monawara

Kingdom of Saoudi Arabia
maddourimondher@yahoo.fr

Abstract. In this paper, we investigate how the diversity of nominal
classiﬁer ensembles aﬀects the AdaBoost performance [13]. Using 5 real
data sets from the UCI Machine Learning Repository and 3 diﬀerent di-
versity measures, we show that Q Statistic measure is mostly correlated
with AdaBoost performance for 2-class problems. The experimental re-
sults suggest that the performance of AdaBoost depend on the nominal
classiﬁer diversity that can be used as a stopping criteria in ensemble
learning.

1

Introduction

Boosting is an adaptive approach, which makes it possible to correctly classify
an object that can be badly classiﬁed by an ordinary classiﬁer. The main idea
of Boosting is to build many classiﬁers who complement each other, in order
to build a more powerful classiﬁer. Adaboost (Adaptive Boosting) is the most
known method of Boosting for classiﬁers generation and combination.

AdaBoost algorithm is iterative. At ﬁrst, it selects a subset of instances from
the learning data set (diﬀerent subset from the training data set in each itera-
tion). Then, it builds a classiﬁer using the selected instances. Next, it evaluates
the classiﬁer on the learning data set, and it starts again T times.

It has been found that this ingenious manipulation of training data can fa-
vorise diversity especially for linear classiﬁers [11]. However, there is no study
concerning the role of diversity on Nominal Concepts classiﬁers [13]. In this
paper, we study how diversity changes according to the nominal classiﬁer num-
bers and we show when adding new classiﬁers to the team can’t provide further
improvements.

This paper is organized as follows: section 2 presents the principle of Classiﬁer
of Nominal Concepts (CNC ) used in Boosting [7,13]. In section 3, we discuss
the diversity of classiﬁers and the diﬀerent measures that can be exploited in
the classiﬁers ensembles generation. Section 4 presents the experimental results
that prove when the diversity can be useful in Boosting of Nominal Concepts.

P.-N. Tan et al. (Eds.): PAKDD 2012, Part I, LNAI 7301, pp. 306–317, 2012.
c(cid:2) Springer-Verlag Berlin Heidelberg 2012

Diversity Analysis on Boosting Nominal Concepts

307

2 Boosting of CNC

CNC is a classiﬁer based on the Formal Concept Analysis. It is distinguished
from the other Formal Concept Analysis methods by handling nominal data.
It generates Nominal Concept that is used as classiﬁcation rule. Comparative
studies and experimental results have proved the beneﬁts of CNC compared to
existing ones (GRAND, RULEARNER, CITREC, IPR) [13].

2.1 Nominal Concepts
A nominal classiﬁer can be build using the whole of training instances O =
{o1, ..., oN} described by L nominal attributes AN (which are not necessary

binary).

AN = {ANl|l = {1, .., L}}.

(1)

At ﬁrst, the pertinent nominal concept AN
is extracted from the training in-
stances by selecting the nominal attribute which minimises the measure of Infor-
mational Gain [13]. Then, the associated instances are selected with each value
v j (j = {1,..,J} and J the number of diﬀerent value of a nominal attribut) from
this attribute as δ(AN

∗

∗

= vj). The δ operator is deﬁned by:
(o) = vj}.

= vj) = {o ∈ O|AN
∗

∗

δ(AN

(2)

Then, the other attributes describing all the extracted instances are determined
(using the closure operator δ ◦ ϕ (AN

= vj)) as follows:

∗

ϕ(B) = {vj|∀o, o ∈ B and ∃ANl ∈ AN|ANl(o) = vj}.

(3)

In [13], a method called BNC (Boosting Nominal Concepts) has been proposed.
The advantage of BNC is to build a part of the lattice covering the best nominal
concepts (the pertinent ) which is used as classiﬁcation rules (the Classiﬁer Nom-
inal Concepts). The BNC has the particularity to decide the number of nominal
classiﬁers in order to control the time of application and to provide the best
decision.

2.2 Learning Concept Based Classiﬁers
For K -class problem, let Y = {1, .., K} the class labels, with yi∈Y is the class
label associated for each instance oi (i=1 to N ). To generate T classiﬁers in
AdaBoost, the distribution of the weight of oi is initially determined as :

D0(i) = (1/N ).

The weight of oi is:

i,y = D0(i)/(K − 1) f or each y ∈ Y − {yi}
w1

(4)

(5)

308

N. Meddouri, H. Khouﬁ, and M.S. Maddouri

On each iteration t from 1 to T, we deﬁne:

W t

i =

(cid:2)

y(cid:4)=yi

wt

i,y and we set qt(i, y) =

wt
i,y
W t
i

f or each y (cid:6)= yi

The distribution of weights is calculated by:

Dt(i) =

W t
i(cid:3)N
i=1 W t

i

(6)

(7)

of oi.

Each generated nominal classiﬁer ht provides an estimated probability pt(oi,y i)
to the class y i from the entry oi. Three cases are presented:
– If pt(oi,y i) = 1 and pt(oi,y) = 0, ∀y(cid:6)=y i, ht has correctly predicted the class
– If pt(oi,y i) = 0 and pt(oi,y) = 1, ∀y(cid:6)=y i, ht has an opposed prediction of
– If pt(oi,y i) = pt(oi,y), ∀y(cid:6)=y i, the class of oi is selected randomly (y or y i).
The error rate of ht is calculated on the weighted training set. If an instance oi is
correctly classiﬁed by ht, then the weight of this instance is reduced. Otherwise,
the weightis increased. The pseudo-loss of the classiﬁer ht is deﬁned as:

the class of oi.

t = 0.5 × N(cid:2)

i=1

Dt(i)(1 − pt(oi, yi) +

(cid:2)

y(cid:4)=yi

qt(i, y)pt(oi, y))

(8)

The weights are then updated according to βt:
βt = εt/(1 − εt)

(9)

The procedure is repeated T times and the ﬁnal result of BNC is determined
via the combination of the generated classiﬁer outputs:

hf in(oi) = arg max
y∈Y

T(cid:2)

t=1

log(1/βt) × pt(oi, yi).

(10)

The ﬁrst variant of the AdaBoost algorithm is called Adaboost.M1 [5,6] that uses
the previous process and stops it when the error rate of a classiﬁer becomes over
0.5. The second variant is called AdaBoost.M2 [6] which has the particularity of
handling multi-class data and operating whatever the error rate is. In this study,
we use AdaBoost.M2 since Adaboost.M1 has the limit to stop Boosting if the
learning error exceeds 0.5. In some experiments, Adaboost.M1 can be stopped
after the generation of ﬁrst classiﬁer thus we cannot calculate the diversity of
classiﬁer ensemble in this particular case.

Recent researches have proved the importance of classiﬁer diversity in improv-
ing the performance of AdaBoost [1,4,8]. We shall discuss about that in the next
section.

Diversity Analysis on Boosting Nominal Concepts

309

3 Classiﬁer Diversity

According to [4], linear classiﬁers should be diﬀerent from each other, otherwise
the decision of the ensemble will be of lower quality than the individual decision.
This diﬀerence, also called diversity, can lead to better or worse overall decision
[3].

In [14], the authors found a consistent pattern of diversity showing that at the
beginning, the generated linear classiﬁers are highly diverse but as the learning
progresses, the diversity gradually returns to its starting level. This suggests
that it could be beneﬁcial to stop AdaBoost before diversity drops. The authors
conﬁrm that there are a consistent patterns of diversity with many measures
using linear classiﬁers. However, they report that the pattern might change if
other classiﬁer models are used. In the paper, we will prove that this pattern is
the same with nominal classiﬁers.
Many measures can be used to determine the diversity between classiﬁers [11].
In this section, we present three(3) of them: Q Statistic, Correlation Coeﬃcient
(CC) and Pairwise Interrater Agreement (kp). These pairwise measures have
the same diversity value (0) when the classiﬁers are statistically independent.
They are called pairwise because they consider the output classiﬁers, two at a
time and then they average the calculated pairwise diversity. These measures
are computed based on the agreement and the disagreement between each 2
classiﬁers (see Table 1).

Table 1. The agreement and disagreement between two classiﬁers

hj correct (1)
hj incorrect (0)

hk correct (1)

N 11
N 01

hk incorrect (0)

N 10
N 00

N =N 00+N 01+N 10+N 11

Nvw(v,w=1,0) is the number of instances correctly or incorrectly classiﬁed by

the two classiﬁers: hj and hk (j,k = 1..T ).

The Q Statistic: Using table 1, this measure is calculated as follows:

Qj,k =

N 11N 00 − N 10N 01
N 11N 00 + N 10N 01

(11)
Q Statistic varies between -1 and 1. Classiﬁers that tend to recognize correctly
the same instances, have positive Q values, and classiﬁers that commit errors
on diﬀerent instances have negative Q values. In [11], the authors showed that
the negative dependency of linear classiﬁers can oﬀer a dramatic improvement
in Boosting.

310

N. Meddouri, H. Khouﬁ, and M.S. Maddouri

The Correlation Coeﬃcient: The correlation between 2 classiﬁers is given
by:

N 11N 00 − N 10N 01

ρj,k =

(cid:4)

(N 11 + N 10)(N 01 + N 00)(N 11 + N 01)(N 10 + N 00)

(12)

The Pairwise Interrater agreement: For this measure, the agreement be-
tween each pair of classiﬁers is calculated as:

N 11N 00 − N 10N 01

kpj,k = 2

(N 11 + N 10)(N 01 + N 00)(N 11 + N 01)(N 10 + N 00)

(13)
For any pair of classiﬁers, Q and ρ have the same sign. The maximum value of
ρ and kp is 1 but the minimum value depends on the individual performance of
the classiﬁers.
In [11], it is reported that there is not unique choice of diversity measure.
But, for linear classiﬁers, the authors recommended the use of Q Statistic for
it’s simplicity and it’s signiﬁcant results. Then, it’s interesting to compare the
previous measures in Boosting Nominal Concept.

4 Experimental Study

The goal of this section is to study the relationship between the nominal classi-
ﬁers diversity and AdaBoost performance for 2-class problems.

Table 2. Characteristics of used data sets

Data Sets
Credit German
Diabetes
Ionosphere
Tic Tac Toe
Transfusion

Instances Attributes Data diversity

1000
768
351
958
748

20
8
4
9
4

98.59%
22.83%
90.2%
100%
1.07%

The experiments are performed on 5 real data sets extracted from UCI Ma-
chine Learning Repository 1 [2] and the algorithms are implemented in WEKA2, a
widely used toolkit.

The characteristics of these data sets are reported in Table 2. For each data
set, we respectively give the number of instances and the number of attributes.
Also, we present the data diversity rate that indicates the samples which are
diﬀerent (including the class label) in the data [9].

1

2

http://archive.ics.uci.edu/ml/
http://www.cs.waikato.ac.nz/ml/weka/

Diversity Analysis on Boosting Nominal Concepts

311

Fig. 1. Diversity and error rates of BNC on Credit German

The performance of BNC is evaluated in terms of error rates. To calculate
this performance, we report the average of 10 experimentations. Each experiment
was performed using 10 cross-validations, that is the most used method in the
literature for validation [10].

312

N. Meddouri, H. Khouﬁ, and M.S. Maddouri

Fig. 2. Diversity and error rates of BNC on Diabetes

Diversity Analysis on Boosting Nominal Concepts

313

Fig. 3. Diversity and error rates of BNC on Ionosphere

314

N. Meddouri, H. Khouﬁ, and M.S. Maddouri

Fig. 4. Diversity and error rates of BNC on Tic Tac Toe

Diversity Analysis on Boosting Nominal Concepts

315

Fig. 5. Diversity and error rates of BNC on Transfusion

316

N. Meddouri, H. Khouﬁ, and M.S. Maddouri

It consists on dividing the data sample into 10 subsets. In turn, each subset
will be used for testing and the rest are assembled together for learning. Finally,
the average of these 10 runs is reported.

Figures 1, 2, 3, 4 and 5 present the performance of BNC and the values of
the 3 diversity measures on the Credit German, Diabetes, Ionosphere, Tic Tac
Toc and Transfusion data sets respectively.

In ﬁgure 1.1, we remark that the performance of BNC starts to stabilize when
using ensembles of 20 classiﬁers, for high diversity data (DD =98.59%). The
classiﬁers generated are negatively depend (Q ≤ -0.02). The minimum values
of Q Statistic are obtained with classiﬁer numbers varying between 10 and 20.
From ﬁgure 1.3 and ﬁgure 1.4, the values of the 2 measures CC and Kp are very
divers and the variation curves are ascending, while the curve of the values of Q
is upward then downward.
In ﬁgure 2.1, the best performance of BNC is obtained with divers classiﬁer
ensembles (with Q=-0.3 as minimum average values). In ﬁgure 2.2, the min-
imum values of Q Statistic are obtained with 20 classiﬁers. For Diabetes data
(DD =22.83%), there is a relation between Q Statistic and the BNC performance.
With high divers data (ﬁgure 3.2), the ﬁrst generated classiﬁers are indepen-
dent but the rest are negatively depend (Q ≤ -0.15). The minimum values of Q
Statistic are obtained with classiﬁer numbers varying between 15 and 30. With
less than 20 classiﬁers, the error rate decreases about 40% (ﬁgure 3.1).

From ﬁgure 4.1, the diﬀerence between the error rates of the ﬁrst classiﬁer
and the generated thereafter, is not important. This show that Boosting can
converge to the best performance with few classiﬁers. For this case, Q Statistic
is informative. In Figure 4.3 and 4.4, the values of Kp and CC vary an a very
arbitrary way.

For the Transfusion data set (DD =1.07%), the classiﬁer generation does not
help to increase BNC performance. We conclude that it is not recommanded to
use AdaBoost for this type of data.

Concerning diversity measures, we can note that for 2-class problems, the
values of ρ and kp are not correlated with AdaBoost performance using nominal
classiﬁers. The Q Statistic seems like a good measure of model diversity that
has a relationship with the performance of AdaBoost and then for can be used
to stop classiﬁer learning .

5 Conclusions

In this paper, we have study the diversity of nominal classiﬁers in AdaBoost.M2.
We have compared 3 diversity measures for 2-class problems. We have found
that the Q Statistic is signiﬁcantly correlated with the AdaBoost performance,
especially for very divers data sets. Then, it’s possible to use this measure as
a stopping criteria for ensemble learning . But for very correlated data sets, no
measure is useful. This results should be conﬁrmed with more correlated data.
The diversity of data sets should then be taken into account in AdaBoost learning
process. It’s also interesting to study Q Statistic diversity to see if it can be used
in AdaBoost for many class problems.

Diversity Analysis on Boosting Nominal Concepts

317

References

1. Aksela, M., Laaksonen, J.: Using diversity of errors for selecting members of a

committee classiﬁer. Pattern Recognition 39(4), 608–623 (2006)

2. Asuncion, A., Newman, D.: Machine Learning Repository (2007)
3. Gavin, B., Jeremy, W., Rachel, H., Xin, Y.: Diversity creation methods: A survey

and categorisation. Journal of Information Fusion 6, 5–20 (2005)

4. Brown, G., Kuncheva, L.I.: “Good” and “Bad” Diversity in Majority Vote Ensem-
bles. In: El Gayar, N., Kittler, J., Roli, F. (eds.) MCS 2010. LNCS, vol. 5997, pp.
124–133. Springer, Heidelberg (2010)

5. Freund, Y.: Boosting a weak learning algorithm by majority. Information and Com-

putation 121, 256–285 (1995)

6. Freund, Y., Schapire, R.E.: Experiments with a new boosting algorithm. In: 13th

International Conference on Machine Learning, Bari, Italy (1996)

7. Freund, Y., Schapire, R.E.: A decision-theoretic generalization of on-line learning
and an application to boosting. Journal of Computer and System Sciences 55(1),
119–139 (1997)

8. Gacquer, D., Delcroix, V., Delmotte, F., Piechowiak, S.: On the Eﬀectiveness of
Diversity When Training Multiple Classiﬁer Systems. In: Sossai, C., Chemello, G.
(eds.) ECSQARU 2009. LNCS, vol. 5590, pp. 493–504. Springer, Heidelberg (2009)
9. Ko, A.H.R., Sabourin, R., Soares de Oliveira, L.E., de Souza Britto, A.: The impli-
cation of data diversity for a classiﬁer-free ensemble selection in random subspaces.
In: International Conference on Pattern Recognition, pp. 1–5 (2008)

10. Kohavi, R.: A Study of Cross-Validation and Bootstrap for Accuracy Estimation
and Model Selection. In: Actes d’International Joint Conference on Artiﬁcial In-
telligence, pp. 1137–1143 (1995)

11. Kuncheva, L.I., Skurichina, M., Duin, R.P.W.: An experimental study on diversity
for bagging and boosting with linear classiﬁers. Information Fusion 3(4), 245–258
(2002)

12. Kuncheva, L.I., Rodriguez, J.J.: Classiﬁer Ensembles for fMRI Data Analysis: An

Experiment. Magnetic Resonance Imaging 28(4), 583–593 (2010)

13. Meddouri, N., Maddouri, M.: Adaptive Learning of Nominal Concepts for Super-
vised Classiﬁcation. In: Setchi, R., Jordanov, I., Howlett, R.J., Jain, L.C. (eds.)
KES 2010. LNCS, vol. 6276, pp. 121–130. Springer, Heidelberg (2010)

14. Shipp, C.A., Kuncheva, L.I.: An investigation into how adaboost aﬀects classiﬁer
diversity. In: Proc. of Information Processing and Management of Uncertainty in
Knowledge-Based Systems, pp. 203–208 (2002)


