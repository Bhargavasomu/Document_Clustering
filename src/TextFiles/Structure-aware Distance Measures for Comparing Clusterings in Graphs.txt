Structure-aware Distance Measures for

Comparing Clusterings in Graphs

Jeﬀrey Chan(cid:63), Nguyen Xuan Vinh, Wei Liu, James Bailey, Christopher Leckie,

Kotagiri Ramamohanarao, and Jian Pei

1 Department of Computing and Information Systems, University of Melbourne,

2 School of Computing Science, Simon Fraser University, BC Canada

Australia

Abstract. Clustering in graphs aims to group vertices with similar pat-
terns of connections. Applications include discovering communities and
latent structures in graphs. Many algorithms have been proposed to ﬁnd
graph clusterings, but an open problem is the need for suitable com-
parison measures to quantitatively validate these algorithms, performing
consensus clustering and to track evolving (graph) clusters across time.
To date, most comparison measures have focused on comparing the ver-
tex groupings, and completely ignore the diﬀerence in the structural ap-
proximations in the clusterings, which can lead to counter-intuitive com-
parisons. In this paper, we propose new measures that account for diﬀer-
ences in the approximations. We focus on comparison measures for two
important graph clustering approaches, community detection and block-
modelling, and propose comparison measures that work for weighted
(and unweighted) graphs.

Keywords: blockmodelling; community; clustering; comparison; struc-
tural; weighted graphs

1

Introduction

Many data are relational, including friendship graphs, communication networks
and protein-protein interaction networks. An important type of analysis that is
graph clustering, which involves grouping the vertices based on the similarity of
their connectivity. Graph clusterings are used to discover communities with sim-
ilar interests (for marketing) and discovering the inherent structure of graphs.
Despite the popularity of graph clustering, existing graph cluster comparison
measures have focused on using membership based measures. To the best of the
authors’ knowledge, there are no work that evaluates if membership based com-
parison measures are appropriate for comparing graph clusterings. This analysis
is important, as comparison measures are often used for comparing algorithms
[1][2], form the basis of consensus clustering [3] and tracking algorithms [4],
and knowing which properties measures possess allows us to evaluate if a mea-
sure is appropriate for a task, and if not, propose new ones that are. One of

(cid:63) Corresponding author: jeﬀrey.chan@unimelb.edu.au

2

Chan et al.

(a) Graph.

(b) Adjacency matrix.

(c) Image diagram.

Fig. 1: Example of an online Q&A forum. Each vertex corresponds to a forum
user, and each edge represents replying between users (Figure 1a). For the cor-
responding adjacency matrix (Figure 1b), the positions induce a division of the
adjacency matrix into blocks, delineated by the red dotted lines. Edge weights
in Figure 1c are the expected edge probabilities between each pair of positions.

the important properties a measure should possess is the ability to be “spa-
tially”/structurally aware. In traditional clustering of points, it was found that
clusterings could be similar in memberships, but their points are distributed very
diﬀerently. This could lead to counter-intuitive situations where such clusterings
were considered similar [5][6]. We show the same scenario occurs when compar-
ing graph clusterings. Therefore, in this paper, we address the open problems
of analysing and showing why membership comparison measures can be inade-
quate for comparing graph clusterings and then proposing new measures that are
aware of structural diﬀerences. We shortly illustrate why comparison measures
should be structure aware, but we ﬁrst introduce graph clustering.

Clustering in Graphs – Community Detection and Blockmodelling: Two popu-
lar approaches for graph clustering are community detection [7] and blockmod-
elling [8]. These approaches are used in many important clustering applications
[9][1][2][4], and hence we focus on the comparison of communities and blockmod-
els in this paper. Community detection decomposes a graph into a community
structure, where vertices from the same communities have many edges between
themselves and vertices of diﬀerent communities have few edges. Community
structure has been found in many graphs, but it is only one of many alternatives
for grouping vertices and possible graph structure (such as core-periphery). In
contrast, an alternative approach is blockmodelling [8]. A blockmodel3 partitions
the set of vertices into groups (called positions), where for any pair of positions,
there are either many edges, or few edges, between the positions.

As an example of the diﬀerence between the two approaches, consider a
reply graph between a group of experts and questioners in a question and an-
swer (Q&A) forum, illustrated in Figure 1. The vertices represent users, and
the directed edges represent one user replying to another. If we use a popular
community detection algorithm [7] to decompose the graph, all the vertices are

3 We use the popular structural equivalence to assess similarity [8].

Structure-aware Distance Measures for Comparing Clusterings in Graphs

3

incorrectly assigned into a single group, as its underlying structure is not of
a community type, and no structure is discerned. If we use a blockmodelling
decomposition, we obtain the correct decomposition into two positions, the ex-
perts C1 ({Jan, Ann, Bob}) and the questioners C2 ({May, Ed, Pat, Li}) (see
Figure 1b, which illustrates the adjacency matrix rearranged according to the
positions). The experts C1 reply to questions from the questioners (C1 to C2),
the questioners C2 have their questions answered by the experts (C1 to C2) and
both groups seldom converse among themselves. The overall structure can be
succinctly summarised by the image diagram in Figure 1c, which shows the po-
sitions as vertices, and the position-to-position aggregated interactions as edges.
As can be seen, this graph has a two position bipartite structure, which the
blockmodelling decomposition can ﬁnd but a community decomposition fails to
discover. The similarity deﬁnition of blockmodelling actually includes the com-
munity one, hence blockmodelling can be considered as a generalisation of com-
munity detection and comparison measures that work with blockmodels would
work with comparing communities as well. Therefore we focus on measures that
compare blockmodels, since they can also compare communities.

Comparing Communities and Blockmodels: As discussed earlier, almost all ex-
isting literature compares blockmodels (and communities) based on their posi-
tions and ignores any diﬀerence in the adjacency structure within positions and
between positions. This can lead to unintuitive and undesirable behaviour. A
blockmodel comparison measure should possess the three following properties,
which we will introduce and motivate using two examples.

The ﬁrst property is that the measure should be sensitive to adjacency dif-
ferences in the blocks. Figures 2a (BM 11), 2b (BM 12) and 2c (BM 13) illus-
trate example 1. The vertices in each blockmodel are ordered according to their
positions, and the boundary between the positions in the adjacency matrix is
illustrated with red dotted lines. This eﬀectively divides the adjacency matrix
into blocks, which represent the position to position interactions. The positional
diﬀerences from BM 12 and BM 13 to BM 11 are the same. However, the dis-
tribution of edges (frequency of edges and non-edges) are very diﬀerent between
BM 11 and BM 13 (there is one single dense block with all the edges in BM 11
(and BM 12), while the edges are distributed across the blocks more evenly in
BM 13). Hence a measure should indicate BM 11 is more similar to BM 12 than
to BM 13. Positional measures fail to achieve this.

The second property is that a measure should account for diﬀerences in the
edge distributions across all blocks. In example 1, BM 11 and BM 12 have similar
distributions across all blocks since their edges are concentrated in a block, while
BM 13 is diﬀerent because the edges are spread quite evenly across all blocks. We
show that two existing comparison measures for blockmodels fail this property.
The third property is that a measure should be sensitive to weighted edge
distributions. Many graphs have weights on the vertices and edges. It might be
possible to binarise the weights, but this throws away important comparison
information. For example, if the graph in the blockmodels of Figures 2d to 2f

4

Chan et al.

(a) BM 11.

(b) BM 12.

(c) BM 13.

(d) BM 21.

(e) BM 22.

(f) BM 23.

Fig. 2: Example to illustrate the strength and weaknesses of diﬀerent blockmodel
comparison measures. Each row corresponds to 3 blockmodels of the same graph.

(BM 21 to BM 23, dubbed example 2) were binarised, then BM 22 and 23 have
exactly the same edge distribution diﬀerences from BM 21. But when the actual
weights are taken into account (darker pixels in the ﬁgures represent larger valued
weights), BM 22 will be correctly considered as more similar to BM 21, as most
of the high-value edge weights in the top left block match, while in BM 23 these
edges are evenly distributed among the four blocks.

These properties have important applications. For example in algorithm eval-
uation, the blockmodel output of the algorithms are compared to a gold stan-
dard. Imagine that the gold standard is BM 11 and two algorithms produced
BM 12 and 13. Measures without these properties will incorrectly rank the two
algorithms to be equally accurate since they have the same position diﬀerences to
BM 11. In contrast, a measure that possess the ﬁrst two properties will correctly
rank the algorithm producing BM 12 as more accurate. Another application is
in consensus blockmodelling, which ﬁnds a blockmodel that is the average of
a set of blockmodels. If a measure does not possess the 3rd property and only
considered position similarity, then the consensus blockmodel might have very
diﬀerent weight distribution to the other blockmodels and hence should not be
considered as a consensus (e.g., BM 23 as a consensus of BM 21 and 22).

In summary, our contributions are: a) we propose three structural-based
properties that a blockmodel comparison measure should possess; b) we analyse
existing measures and show that they do not possess these properties; c) we
propose new measures that satisfy these properties; and d) perform experiments
on synthetic and real data to study the monotonicity of the new measures.

Structure-aware Distance Measures for Comparing Clusterings in Graphs

5

2 Related Work

In this section, we describe related work in three key areas: set comparison, spa-
tially aware clustering comparison and subspace clustering comparison.

Set Comparison: There has been extensive work in using set comparison for
cluster comparison. Hence we discuss a few selected measures, and refer inter-
ested readers to the excellent survey of [10]. The ﬁrst class of measures in this
area involves computing the agreement between two clusterings in terms of how
many pairs of vertices are in the same and diﬀerent clusters. Examples include
the Rand and Jaccard indices [10]. The second class of measures are based on
set matching and information theory, where the two clusterings are compared
as two sets of sets. Popular examples include Normalised and Adjusted Mutual
Information (NMI, AMI) [11]. As demonstrated in Section 1, these set-based
measures do not take into account any adjacency diﬀerences between the block-
models, resulting in some counter-intuitive comparison behaviour.
Spatially Aware Clustering Comparison: In [12], Zhou et al. proposed a
measure that compares clusters based on membership and the distances between
their centroids. In [5], Bae et al. computed the density of clusters using a grid,
and then used the cosine similarity measure to compare the cluster distributions.
Coen et al. [6] took a similar approach but used a transportation distance to com-
pute the distances between clusters and between clusterings. All these measures
depend on a notion of a distance between points (between clusters of the two
clusterings). In blockmodel and community comparison, there are positions of
vertices and the edges between them, but no notion of a distance between ver-
tices across two blockmodels and hence existing spatial-aware measures cannot
be applied for blockmodel comparison.
Subspace Clustering Comparison: In subspace clustering, the aim is to ﬁnd a
group of objects that are close in a subset of the feature space. In [13], Patrikainen
and Meila proposed the ﬁrst subspace validation measure that considered both
the similarities in the object clusters and in the subspaces that the clusters
occupied. They treated subspace clusters as sets of (object, feature) pairs, and
then used set-based validation measures for comparing subspace clusters. When
extended to compare the sets of (vertex,vertex) pairs between blockmodels, these
measures are equivalent to comparing the positions (proof omitted due to lack
of space), and hence have the same issues as the set comparison measures.

In summary, there has been much related work in cluster comparison, but
none that address the unique problem of comparing blockmodels and communi-
ties. What is needed is a measure that is sensitive to diﬀerences in the adjacency
structure as well as working in the relation spaces of graphs.

3 Blockmodelling (Graph Clustering) Background and

Properties

In this section, we summarise the key ideas of blockmodelling (see [14][8] for more
detail). A graph G(V, E) consists of a set of vertices V and a set of edges E,

6

Chan et al.

where E ∈ {0, 1}|V |×|V | for unweighted graphs and E ∈ N |V |×|V | for weighted
graphs4. The edge relation can be represented by an adjacency matrix A whose
rows and columns are indexed by the vertices of G.
We use the Q&A example of Figure 1 to illustrate the notation. A blockmodel
partitions a set of vertices into a set of positions C = {C1, C2, . . . , Ck}. We
denote the number of positions in C by k. C can be alternatively speciﬁed by
φ(.), a mapping function from vertices to the set of positions (i.e., φ : V → C).
A block Ar ,c is a submatrix of A, with the rows and columns drawn from Cr
and Cc respectively, Cr, Cc ∈ C, e.g., A1 ,1 deﬁnes the upper left submatrix
in Figure 1b. Let Ψr,c denote the random variable representing the probability
mass function of the edge weights in block Ar ,c, e.g., p(Ψ1,1 = 0) = 7
9 , p(Ψ1,1 =
1) = 2
9 . This representation allow us to model both weighted and unweighted
graphs. For simplicity, we introduce Υr,c = p(Ψr,c = 1) for unweighed graphs. We
deﬁne M as the blockmodel image matrix that models inter-position densities,
M : C × C → [0, 1], Mr,c = Υr,c. For the rest of the paper, we use a superscript
notation to distinguish two diﬀerent instances of a variable (e.g., G(1) and G(2)).
A blockmodel B(l)(C(l), M(l)) is deﬁned by its set of positions C(l) (the
matrix version is C(l)) and its image matrix M(l). The (unweighted) block-
modelling problem can be considered as ﬁnding a blockmodel approximation of
A(l) as C(l)M(l)(C(l))T that minimises a sum of squared errors5[15], ||A(l) −
C(l)M(l)(C(l))T||2. We denote C(l)M(l)(C(l))T as ˆA(l). Given two blockmodels ,
the distance between two blockmodels B(1) and B(2) is deﬁned as d(B(1), B(2)).

3.1 Desired Properties of Comparison Measures

In this section, we formalise the properties that a blockmodel comparison mea-
sure should possess (as discussed in Section 1). These properties allow us to
formally evaluate the measures in the next section.
The following properties assume that there are three blockmodels with ap-
proximations ˆA(1), ˆA(2) and ˆA(3) of the same graph, ˆA(1) (cid:54)= ˆA(2) (cid:54)= ˆA(3) and
they are not co-linear, i.e., | ˆA(1) − ˆA(3)| (cid:54)= | ˆA(1) − ˆA(2)| + | ˆA(2) − ˆA(3)|. They
can be considered as measuring the sensitivity to diﬀerences in the structure
(approximation) of the blockmodels.

P1: Approximation sensitivity: Given an unweighted graph and three block-
models, a measure is approximation sensitive if d( ˆA(1), ˆA(2)) (cid:54)= d( ˆA(2), ˆA(3)).

P2: Block edge distribution sensitivity: Given dmKL( ˆA(2), ˆA(1)) < dmKL( ˆA(3), ˆA(1)),
then a measure is block edge distribution sensitive if d( ˆA(2), ˆA(1)) < d( ˆA(3), ˆA(1)).
dmKL( ˆA(1), ˆA(2)) is the KL divergence [16] for matrices, and is deﬁned as

dmKL( ˆA(1), ˆA(2)) = (cid:80)|V 1|

i,j

(cid:19)

(cid:18) ˆA(1)

i,j
ˆA(2)
i,j

ˆA(1)

i,j log

− ˆA(1)

i,j + ˆA(2)

i,j . It measures the

diﬀerence in the edge (weight) distribution across all the blocks.

4 For simplicity, we assume a discrete set of weights. But this can easily be extended

to continuous weight values.

5 This is one of several popular blockmodelling objective formulations. See [1] for an

objective for weighted graph.

Structure-aware Distance Measures for Comparing Clusterings in Graphs

7

P3: Weight sensitivity: Given a weighted graph and three blockmodels6, a

measure is weight sensitive if d( ˆA(1), ˆA(2)) (cid:54)= d( ˆA(2), ˆA(3)).

In addition, we evaluate the important monotonicity property of the mea-
sures. Basically, we desire measures that increase in value as the compared block-
models become more diﬀerent. We measure “more diﬀerent” by the minimum
number of position changes to transform one blockmodel to another.

4 Blockmodel (Graph Clustering) Comparison

Approaches

In this section, we describe existing measures, propose new blockmodel com-
parison approaches, and analyse their properties. Existing work for comparing
blockmodels falls into two categories: positional and reconstruction measures.
Positional measures compare the sets of positions associated with each block-
model [10]. Reconstruction measures compare the blockmodel approximation
of the graphs and can be expressed as d(B(1), B(2)) = d( ˆA(1), ˆA(2)). We next
provide details on two existing reconstruction blockmodel measures.

4.1 Edge and Block Reconstruction Distances

The edge and block reconstruction distances were proposed in [4] and [8] re-
spectively. The edge reconstruction distance [8] measures the diﬀerence in the
expected edge probabilities across all edges (recall that V 1 = V 2):

dRE(B(1), B(2)) =

|Υ (1)
φ(i),φ(j) − Υ (2)

φ(i),φ(j)|

(1)

|V 1|(cid:88)

|V 2|(cid:88)

i

j

The block reconstruction distance [4] measures the diﬀerence in block densi-

ties over all pairs of blocks, weighted by the overlap of the positions:

dRB(B(1), B(2)) =

|C (1)

r1 ∩ C (2)
r2 |

|C (1)

c1 ∩ C (2)
c2 |

n

n

· |Υ (1)

r1,c1 − Υ (2)
r2,c2|

(2)

k1(cid:88)

k2(cid:88)

r1,c1

r2,c2

The two measures in fact diﬀer only by a factor of 1

n2 (we prove this new result
in Theorem 1). It is easier to understand and propose new measures based on
the edge than the block reconstruction distance. But in terms of computational
complexity, it takes O(k2) to compute the block reconstruction distance while
O(n2) for the edge distance, hence it is more eﬃcient to compute the block
distance. Therefore Theorem 1 permits us to use whichever measure that is
more convenient for the task at hand.

Theorem 1. dRB(B(1), B(2)) = 1

n2 dRE(B(1), B(2))

6 ˆA consists of n2 PMFs of a random variable with the event space of edge weights.

8

Chan et al.

Proof. The proof involves arithmetric manipulation. We detail the proof in a
supplementary7 paper due to space limitations.

Both these reconstruction distances possess the approximation sensitivity
property (P1) but fail the block edge distribution property (P2). Reconsider the
example of Figures 2a to 2c. dRB(BM 11, BM 12) = 0.21 and dRB(BM 11, BM 13)
= 0.18. This means that the reconstruction distances incorrectly ranks BM 13
to be closer to BM 11 than BM 12. To explain why, we ﬁrst state that the
Earth Movers Distance (EMD) can be considered as a generalisation of the re-
construction measures (see Section 4.3). The EMD ﬁnds the minimum amount
of mass to move from one PMF (of a block) to another. What this means is that
the reconstruction distance considers the cost to move a unit of mass the same,
whether the source PMF is a unimodal distribution or uniformly distributed one.
Hence the reconstruction distances only consider the number of total units of
mass moved and do not consider the diﬀerences in distribution of edge densities
across all the blocks, leading them to fail property P2.

4.2 KL Reconstruction Measure

We propose to use the KL divergence [17] to compare the block densities (PMFs).
Although it is similar in form to the P2 property deﬁnition, the proposed measure
is in the form of the reconstruction distance and the KL divergence is a natural
approach to measure distribution diﬀerences. It is deﬁned as:

Deﬁnition 1. KL Reconstruction:

dRKL(B(1), B(2)) =

|Cr1 ∩ Cr2|

|Cc1 ∩ Cc2|

n

n

· dKL(A(1 )

r1 ,c1 , A(2 )

r2 ,c2 ) (3)

C(1)(cid:88)
C(2)(cid:88)
r2,c2) =(cid:80)

r1,c1

r2,c2

where dKL(Ψ (1)

r1,c1, Ψ (2)

x p(Ψ (1)

r1,c1 = x) log(

p(Ψ (1)
p(Ψ (2)

r1,c1=x)
r2,c2=x)

.

The KL reconstruction measure can be considered as using the edge distribu-
tions (across the blocks) in B(2) to encode the edge distributions in B(1). This
means it is sensitive to diﬀerences in the distribution of the block densities, and
it gives the correct ranking for example 1 (see Section 5).

The KL divergence is asymmetric, hence dRKL(.) is also asymmetric, and
can handle weighted graphs. The Jeﬀrey and Jenson-Shannon divergences [17]
are symmetric versions of the KL divergence, and are included for comparison.
Unfortunately they fail the block edge distribution property (see Section 5).

4.3 Weighted Block and Edge Reconstruction

In this section, we show how the edge/block reconstruction measures can be
generalised to weighted graphs. We compare the blockmodels of weighted graphs

7 Available at http://people.eng.unimelb.edu.au/jeffreyc/

Structure-aware Distance Measures for Comparing Clusterings in Graphs

9

by comparing the PMFs of the blocks. We desire a measure that can compare
multi-valued PMFs and consider the diﬀerence in the actual weight values. One
such measure is the Earth Mover’s distance (EMD) [18], which also reduces to
the reconstruction measures for unweighted graphs (see Theorem 2).

Deﬁnition 2. EMD Reconstruction Measure:

C(1)(cid:88)

r1,c1

r2,c2

|Cc1 ∩ Cc2|

|Cr1 ∩ Cr2|

C(2)(cid:88)
(cid:80)L
(cid:80)L
x,y = u); c) (cid:80)
w=0 mu,w · d(u, w), subject to:

· dED(Ψ (1)

u mu,w = p(Ψ (2)

u=0

n

n

dREM D(B(1), B(2)) =

where dED(Ψ (1)

a) mu,e ≥ 0; b) (cid:80)

d(u, w) = |u − w|.

x,y, Ψ (2)

a,b ) = minM
w mu,w = p(Ψ (1)

r1,c1, Ψ (2)

r2,c2) (4)

a,b = w) and d)

We now show that the EMD reconstruction measure is in fact a generalisation

of the reconstruction measures.

Theorem 2. When we are comparing unweighted graphs,

dED(A(1 )

x ,y , A(2 )

a,b) = |Υ (1)

x,y − Υ (2)
a,b |

and dREM D(B(1), B(2)) = dRB(B(1), B(2)).

Proof. The proof involves arithmetric manipulation and reasoning on the con-
straints. Due to space limitations, please refer to supplementary for details.

Theorem 2 is a useful result, as it helps to explain why the block recon-
struction distance fail property P2. In addition, it means EMD reconstruction
distance can be used in place of block distance, since Theorem 2 tells us that
the EMD distance is a generalisation of block one for unweighted graphs. At the
same time, the EMD distance satisﬁes property P3 while the block one does not.

5 Evaluation of the Measures

In this section, we evaluate the measures using the proposed properties and
empirically demonstrate their monotonicity (since it is diﬃcult to prove this an-
alytically). We also show how each of the measures perform in the examples from
Section 1. In the experiments, we compare NMI, a popular and representative
example of the positional measures, against the block (RB), EMD (REMD), KL
(RKL), Jeﬀrey (RsKL) and JS (RJS) reconstruction distances.

5.1 Evaluation of the Examples

Table 1 shows the comparison results between the original blockmodel (BM *1)
against the two other blockmodels (BM *2 and *3) of the examples in Figure 2.
As can be seen, NMI cannot distinguish between the three blockmodels across all
the datasets. RB fails to correctly rank the blockmodels of example 1, and fails

10

Chan et al.

Example 2

Example 3

NMI
RB

BM 12 BM 13 BM 22 BM 23
0.1887 0.1887
0.3845 0.3845
0.2188 0.2188
0.2100 0.1800
7.2266 10.5469
REMD 0.2100 0.1800
RKL
0.2803 4.4432
7.2407 7.2407
9.0060 9.0060
RsKL 5.5650 4.6501
RJS
0.1676 0.1259
0.2633 0.2633

Table 1: Measure values when comparing the original blockmodel (BM*1) against
the other blockmodels in the Karate club and other examples from Section 1. In
each case, ideally d(BM *2, BM *1) should be less than d(BM *3, BM *1).

to distinguish the weighted blockmodels of example 2. As expected, REMD has
the same values as RB for example 1, but correctly classiﬁes the relative ordering
of example 2. RsKL and RJS fail example 1. RKL correctly distinguishes the
ordered example 1, but like the other distributional measures (RsKL and RJS)
cannot distinguish the blockmodels of example 2.

5.2 Monotonicity Analysis

To evaluate monotonicity, we vary the membership of the positions for several
real datasets. Each position change corresponds to a change in the membership
of a vertex, and we ensure a vertex can only change membership once in each
simulation. From a starting blockmodel, we generated 100 diﬀerent runs, where
up to 50% of the vertices change position. Table 2 shows the statistics of the
three real datasets8 on which we evaluate monotonicity.

Dataset Vert # Edge # Weighted? Pos. #
Karate
Football
C.Elegans

2
12
5
Table 2: Statistics of the real datasets.

N
N
Y

34
115
297

78
613
2359

Figure 3 shows the results for REMD, RKL, RsKL and NMI (we plotted 1-
NMI). As can be seen, all measures are monotonically increasing as the positions
change. However, in the rare case where the adjacency approximation remains
the same after a position splits into two, then the reconstruction distances can
fail to distinguish the pre-split and post-split blockmodels.

5.3 Properties of the Measures

Table 3 shows the measures and the properties they have. For each measure, we
prove (see supplementary) whether it possesses each of the properties.

8 Available at http://www.personal-umich.edu/~mejn/netdata.

Structure-aware Distance Measures for Comparing Clusterings in Graphs

11

(a) Karate club.

(b) Football.

(c) C.Elegans.

Fig. 3: Evaluation of the monotonicity of the distance measures as the diﬀerence
in the positions of the starting blockmodel and modiﬁed blockmodel increases.

Property

P1: Edge. Dist. Sensitivity
P2: Block Dist. Sensitivity

P3: Weight Sensitivity

Monotonicity

(cid:88)
-
-

NMI RB REMD RKL RsKL RJS
(cid:88)
(cid:88)
(cid:88)
-
-*
-*
(cid:88)# (cid:88)# (cid:88)#

(cid:88)
-
-
-
(cid:88)
-
(cid:88) (cid:88)# (cid:88)#

(cid:88)
-
-*

Table 3: List of blockmodel measures and their properties. *: The KL-based
measures can fail when the distributions have the same shape but have diﬀerent
weight values. #: not strictly monotonic (can fail the co-incidence axiom).

Table 3 conﬁrms the empirical evaluation of Section 5.1. The positional mea-
sures (e.g., NMI) do not consider blockmodel approximations and hence fail the
structural sensitivity properties. The existing RB and RE distances cannot be
applied to weighted graphs and are not block edge distribution sensitive. The
proposed REMD generalises the reconstruction distances to weighted graphs,
but still possesses the same assumptions as those distances, and hence fails the
block edge distribution sensitivity property. The proposed KL-based distances
are block edge distribution sensitive, but not weight sensitive as they measure
distribution diﬀerences but ignore diﬀerence in weight values.

From these analyses, we recommend to use the KL reconstruction distance
when comparing unweighted blockmodels, as it possess the ﬁrst two properties.
When comparing weighted graphs, the EMD distance might be preferred as it
satisﬁes the weight sensitivity property. A further option is to combine several
measures together via ideas from multi-objective optimisation, e.g., as a weighted
linear sum, which we leave to future work.

6 Conclusion

Blockmodel comparison measures are used for validating blockmodelling and
community detection algorithms, ﬁnding consensus blockmodels and other tasks
that require the comparison of blockmodels or communities. In this paper, we
have shown that popular positional measures cannot distinguish important dif-
ferences in blockmodel structure and approximations because they do not re-

12

Chan et al.

ﬂect a number of key structural properties. We have also proposed two new
measures, one based on the EMD and the other based on KL divergence. We
formally proved these new measures possess a number of the desired structural
properties and used empirical experiments to show they are monotonic.

Future work includes introducing new measures for evaluating mixed mem-
bership blockmodels [1] and evaluating multi-objective optimisation approaches
for combining measures.

References

1. Airoldi, E.M., Blei, D.M., Fienberg, S.E., Xing, E.P.: Mixed membership stochastic

blockmodels. J. of Machine Learning Research 9 (June 2008) 1981–2014

2. Pinkert, S., Schultz, J., Reichardt, J.: Protein interaction networks–more than

mere modules. PLoS computational biology 6(1) (2010) e1000659

3. Lancichinetti, A., Fortunato, S.: Consensus clustering in complex networks. Nature

2(336) (2012)

4. Chan, J., Liu, W., Leckie, C., Bailey, J., Kotagiri, R.: SeqiBloc: Mining Multi-time

Spanning Blockmodels in Dynamic Graphs. In: Proceedings of KDD. (2012)

5. Bae, E., Bailey, J., Dong, G.: A clustering comparison measure using density
proﬁles and its application to the discovery of alternate clusterings. Data Mining
and Knowledge Discovery 21(3) (2010) 427–471

6. Coen, M.H., Ansari, H.M., Filllmore, N.: Comparing Clusterings in Space.

In:

Proceedings of ICML. (2010) 231–238

7. Rosvall, M., Bergstrom, C.T.: Maps of random walks on complex networks reveal

community structure. Proceedings of PNAS 105 (2008) 1118–1123

8. Wasserman, S., Faust, K.: Social Network Analysis: Methods and Applications.

Cambridge Univ. Press (1994)

9. Chan, J., Lam, S., Hayes, C.: Increasing the Scalability of the Fitting of Generalised

Block Models for Social Networks. In: Proceedings of IJCAI. (2011)

10. Halkidi, M., Batistakis, Y., Vazirgiannis, M.: On Clustering Validation Techniques.

J. of Intelligent Information Systems 17(2/3) (2001) 107–145

11. Vinh, N., Epps, J., Bailey, J.: Information theoretic measures for clusterings com-
parison: Variants, properties, normalization and correction for chance. The J. of
Machine Learning Research 11 (2010) 2837–2854

12. Zhou, D., Li, J., Zha, H.: A new Mallows distance based metric for comparing

clusterings. In: Proceedings of the ICDM. (2005) 1028–1035

13. Patrikainen, A., Meila, M.: Comparing Subspace Clusterings.

IEEE Trans. on

Know. Eng. 18(7) (2006) 902–916

14. Doreian, P., Batagelj, V., Ferligoj, A.: Generalized blockmodeling. Cambridge

Univ. Press (2005)

15. Chan, J., Liu, W., Kan, A., Leckie, C., Bailey, J., Kotagiri, R.: Discovering latent
blockmodels in sparse and noisy graphs using non-negative matrix factorisation.
In: Proceedings of CIKM. (2013) To appear.

16. Lee, D., Seung, H.: Algorithms for non-negative matrix factorization. In: Proceed-

ings of NIPS. (2000) 556–562

17. Cover, T., Thomas, J.: Elements of Information Theory. Wiley-Interscience (2006)
18. Rubner, Y., Tomasi, C., Guibas, L.: The earth mover’s distance as a metric for

image retrieval. International Journal of Computer Vision 40(2) (2000) 99–121


