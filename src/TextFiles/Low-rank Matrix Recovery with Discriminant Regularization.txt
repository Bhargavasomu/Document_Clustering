Low-Rank Matrix Recovery

with Discriminant Regularization

Zhonglong Zheng, Haixin Zhang, Jiong Jia, Jianmin Zhao,

Li Guo, Fangmei Fu, and Mudan Yu

Department of Computer Science, Zhejiang Normal University, Jinhua Zhejiang 321004, China

Abstract. Recently, image classiﬁcation has been an active research topic due
to the urgent need to retrieve and browse digital images via semantic keywords.
Based on the success of low-rank matrix recovery which has been applied to
statistical learning, computer vision and signal processing, this paper presents a
novel low-rank matrix recovery algorithm with discriminant regularization. Stan-
dard low-rank matrix recovery algorithm decomposes the original dataset into a
set of representative basis with a corresponding sparse error for modeling the raw
data. Motivated by the Fisher criterion, the proposed method executes low-rank
matrix recovery in a supervised manner, i.e., taking the with-class scatter and
between-class scatter into account when the whole label information is available.
The paper shows that the formulated model can be solved by the augmented La-
grange multipliers, and provide additional discriminating ability to the standard
low-rank models for improved performance. The representative bases learned by
the proposed method are encouraged to be structural coherence within the same
class, and as independent as possible between classes. Numerical simulations on
face recognition tasks demonstrate that the proposed algorithm is competitive
with the state-of-the-art alternatives.

1 Introduction

With the ever-growing amount of digital image data in multimedia databases, there is
a great requirement for algorithms that can provide eﬀective semantic indexing. Cate-
gorizing digital images only using keywords is the quintessential, but not always ex-
ecutable example in image classiﬁcation tasks. Face recognition (FR) is one typical
image classiﬁcation problem. Several aspects contribute to the diﬃculty of FR prob-
lem including the large variability in variance, illumination, pose, occlusion and even
disguise of diﬀerent subjects.

To design realistic FR systems, researchers usually focus on feature extraction of
facial images and the generalization of classiﬁers. The testing sample from the same
subjects will be used to evaluate the associated identiﬁcation or veriﬁcation perfor-
mance. Although the testing sample might be corrupted, the training data sets are
commonly assumed to be well taken in some desired conditions including reasonable
illumination, pose, variations and without occlusion or disguise. When applying exist-
ing face recognition methods for practical scenarios, we will need to throw away the
corrupted training images, and we might thus encounter small sample size and over-
ﬁtting problems. Moreover, the disregard of corrupted training face images might give

J. Pei et al. (Eds.): PAKDD 2013, Part II, LNAI 7819, pp. 437–448, 2013.
c(cid:2) Springer-Verlag Berlin Heidelberg 2013

438

Z. Zheng et al.

up some valuable information for recognition. Inspired by the sparse coding mechanism
of human vision system [1][2], and with the rapid development of (cid:2)1-norm minimiza-
tion techniques in recent years, the sparse representation classiﬁcation (SRC) ideas have
been successfully used in various machine vision and pattern recognition applications
[3][4][5][6]. Though interesting classiﬁcation results have been reported in documen-
tations, more investigations need to be made in order for a clearer understanding about
the relationship between object representation and classiﬁcation. Since SRC requires
the training images to be well aligned for reconstruction purposes, [7] and [8] further
extend it to deal with face misalignment and illumination variations. [5] also proposes
modiﬁed SRC-based framework to handle outliers such as occlusions in face images.
However, the above methods might not generalize well if both training and testing im-
ages are corrupted.

To address this issue, we propose formulating the face recognition problem under a
matrix completion framework fueled by the recent advances in low-rank (LR) matrix
recovery [9][10][11], together with the discriminant regularization denoted by within-
class scatter and between-class scatter [12]. In this paradigm, low-rank matrix approxi-
mation is solved in a supervised manner as the whole label information of the training
database is accessible. That is, we regularize the representative basis derived from stan-
dard LR matrix recovery using class-speciﬁc discriminant criterion which is motivated
by Fisher criterion, and plays an important role in face recognition tasks [12][13][14].
By introducing this type of regularization, our matrix completion algorithm is able to
capture discriminative portions extracted from diﬀerent classes.

2 Related Works

2.1 Discrimination in Face Recognition

The face recognition literature is fairly dense and diverse and thus cannot be surveyed
in its entirety in this limited space. In this paper, we focus on the class of face recogni-
tion approaches called subspace methods that are more closely related to our method. A
prime instance of such methods is Eigenfaces [15], which attempts to group images by
minimizing data variance. Fisherfaces [12], due to ﬁnding a subspace that minimizes
the within-class distances while maximizing the between-class distances at the same
time, achieves much better classiﬁcation performance than Eigenfaces in face recog-
nition problem. Some other subspace methods are geometrically inspired where the
emphasis is on identifying a low dimensional sub-manifold on which the face images
lie. The most successful of these methods include those which seek to project images
to a lower dimensional subspace such that the local neighborhood structure present
in the training set is maintained. These include Laplacianfaces [16], Locality Preserv-
ing Projections (LPP) [17], Orthogonal Laplacianfaces [18], Marginal Fisher Analysis
(MFA)[19] etc.. Over time, improvements on discrimination of these methods have ap-
peared in [20][21][22][23][24]. These generalizations seriously make the discriminant
regularization as an indispensable part of their models, and therefore great improve-
ments can be witnessed.

Low-Rank Matrix Recovery with Discriminant Regularization

439

2.2 Sparse Representation-Based Classiﬁcation

Recently, Wright et al [4] proposed a sparse representation-based classiﬁcation algo-
rithm for face recognition. In SRC-based algorithms, each testing image is regarded as
a sparse linear combination of the whole training data by solving an (cid:2)1 minimization
problem, and very impressive results were reported in [4]. Several works have been pro-
posed to further extend SRC-based algorithms for improved performance. For example,
[25] utilizes a LASSO type regularization for computing the joint sparse representa-
tion of diﬀerent features for visual signals. Jenatton et al. [26] utilizes a tree-structured
sparse regularization for hierarchical sparse coding. Although promising face recogni-
tion results were reported by SRC-based algorithm, it still requires clean face images
for training and thus might not be preferable for real-world scenarios. If corrupted train-
ing data is presented, SRC-based algorithms tend to recognize testing images with the
same type of corruption and thus lead to poor performance. In the following section, we
will introduce our proposed method for robust face recognition, in which both training
and testing data can be corrupted.

2.3 Matrix Recovery via Rank Minimization

Low-rank matrix recovery is a procedure for reconstructing an unknown matrix with
low-rank or approximately low-rank constraints from a sampling of its entries. This
problem is motivated by the requirement of inferring global structure from a small num-
ber of local observations. [10], a breakthrough in matrix completion algorithms, states
that the minimization of the rank function under broad conditions can be achieved us-
ing the minimizer obtained with the nuclear norm (sum of singular values). Since the
natural reformulation of the nuclear norm gives rise to a semi-deﬁnite program, exist-
ing interior point methods can only handle problems with a number of variables in the
order of the hundreds. Recently, Robust PCA method [9] has been proved to achieve
the state-of-the-art performance using Augmented Lagrange Multipliers (ALM) method
[11]. The proposed algorithm is also solved within the framework of ALM due to its
fast eﬃciency. In the context of computer vision and pattern recognition, minimization
of the nuclear norm in matrix completion has been applied to several problems: struc-
ture from motion [27], RPCA [9][28], subspace alignment [29], subspace segmentation
[30] and signal denoising [31] etc..

3 Proposed Algorithm

3.1 Problem Setting
Given the original dataset X = [x1, x2, . . . , xn] ∈ RD×n consists of n columns, each
column denotes a sample. Low-rank matrix recovery decomposes X into the following
form

X = A + E,

(1)

where A is a low-rank matrix, and E is a sparse matrix. The dimension of matrices A
and E is the same as X. According to [10], the solution of eq(1) can be solved by ALM

440

Z. Zheng et al.

[11] method by optimizing the following model
(cid:4)A(cid:4)∗ + λ(cid:4)E(cid:4)1,

arg min
A,E

where (cid:4)(cid:4)∗ denotes nuclear norm, and (cid:4)(cid:4)1 denotes (cid:2)1 norm.

s.t. X = A + E,

(2)

3.2 Within-Class and Between-Class Scatters
Assume that all the labels of data X are available. Speciﬁcally, let xs
i denote the i-th
sample of the s-th class. We derived with-in class scatter and between class scatter
matrices in the following manner which is diﬀerent from Fisherfaces [12].

Let ws denote the within-class scatter of class s. Deﬁne it as

cs(cid:2)

ws =

i=1

(cid:4)xs

i

− ¯xs(cid:4)2

2

,

s = 1, . . . , c.

(3)

, xs
2

, . . . , xs

Let Xs = [xs
1
in class s, and ecs denote all-one column vector of length cs. Then we have ¯xs = 1
cs
Rewriting eq(3) shows

cs] denote the s-th class data matrix, cs is the number of samples
Xsecs.

cs(cid:2)

ws =

− ¯xs)(xs

i

− ¯xs)T

(xs
i

i=1

= T r{ cs(cid:2)

i=1

= T r(XsXT

i )T} − 2T r{ cs(cid:2)
1
cs
T r{Xsecs(ecs)T XT

xs
i (

i=1

s

i (xs
xs
s ) − 2
cs

X secs)T} + T r{(
} +

(ecs)T ecs

1
cs

X secs)(

1
cs
T r{Xsecs(ecs)T XT

s

},

X secs)T}

c2
s

where T r denotes trace operator of matrix. Thus we have

ws = T r{XsDsXT

},

s

where Ds = Is − 2

cs

ecs(ecs)T + (ecs )T ecs

ecs(ecs)T .

c2
s

Next, we can deﬁne the between-class scatter of s-th class with the other classes

c(cid:2)

βs =

j=1, j(cid:2)s

(cid:4) ¯xs − ¯x j(cid:4)2

2

,

(4)

(5)

(6)

where c is the number of classes. Following similar formulations from eq(3) to (5), we
can rewrite eq(6) as
c(cid:2)

( ¯xs − ¯x j)( ¯xs − ¯x j)T

βs =

=

=

j=1, j(cid:2)s
c(cid:2)

j=1, j(cid:2)s

c − 1
c2
s

T r{ ¯xs ¯(xs)T − 2 ¯xs ¯(x j)T + ¯x j ¯(x j)T}
c(cid:2)

T r{Xsecs(ecs)T XT

s

} − 2T r{ ¯xs

c(cid:2)

¯x j} + T r{

¯x j ¯(x j)T}

(7)

j=1, j(cid:2)s

j=1, j(cid:2)s

= T r{XsB1XT

} − T r{XsB2} + B3,

s

Low-Rank Matrix Recovery with Discriminant Regularization

441

where B1 = c−1

c2
s

ecs(ecs)T , B2 = 2
cs

ecs

(cid:3)

c
j=1, j(cid:2)s

¯x j and B3 = T r{(cid:3)

j=1, j(cid:2)s x j(x j)T}.

c

3.3 Low-Rank Matrix Recovery Discrimination

Although low-rank matrix recovery decomposes the original data X and produces a low-
rank matrix A together with a sparse error matrix E for better representation purpose, as
shown in eq(1), the derived low-rank matrix A might not contain suﬃcient discriminat-
ing information. Assume that the original X represents face image data, we can rewrite
it into class-wise form X = [X1, X2, . . . , Xc].

Based on the within-class scatter and between-class scatter matrices shown in eq(5)
and (7), it is a natural idea of adding a discriminant regularization to the low-rank matrix
recovery problem shown in eq(1)

(cid:3)

arg minA,E

c
s=1

{(cid:4)As(cid:4)∗ + λ(cid:4)Es(cid:4)1 + γ(ws(As) − βs(As))}
s.t. Xs = As + Es,

which is a class-wise optimization problem. In eq(8), ws(As) and βs(As) are the within-
class scatter and between-class scatter of s-th class, respectively. Like LDA or Fish-
erfaces [12], to make projected samples favor of classiﬁcation in feature space, we
expect that the samples within the same class cluster as close as possible and samples
between classes separate as far as possible in the learned low-rank matrix A. The term
(cid:4)As(cid:4)∗+λ(cid:4)Es(cid:4)1 shown in eq(8) performs the standard low-rank decomposition of the data
matrix X. The term γ(ws(As) − βs(As)) is our discriminant regularizer based on within-
class and between-class scatters, which is penalized by the parameter γ balancing the
low-rank matrix approximation and discrimination. We refer to eq(8) as low-rank ma-
trix recovery with discriminant regularization.

Meanwhile, we can rewrite (ws(As) − βs(As)) into the following form

ws(As) − βs(As) = T r{AsDsAT

s

} − T r{AsB1AT

} + T r{AsB2} − B3
= T r{As(Ds − B1)AT
≤ (cid:4)As(cid:4)F(cid:4)(Ds − B1)(cid:4)F(cid:4)As(cid:4)F + (cid:4)As(cid:4)∗(cid:4)B2(cid:4)2 − B3
= b1 < As, As > +b2(cid:4)As(cid:4)∗ − b3,

} + T r{AsB2} − B3

s

s

(8)

(9)

(10)

(11)

where

b1 = (cid:4)(Ds − B1)(cid:4)F , b2 = (cid:4)B2(cid:4)2 and b3 = B3.

As b3 is irrelevant to As, the optimization of eq(8) can be rewritten as
(cid:4)As(cid:4)∗ + λ(cid:4)Es(cid:4)1 + γ(b1 < As, As > +b2(cid:4)As(cid:4)∗)

arg minAs,Es

s.t. Xs = As + Es.

The optimization of eq(11) can be solved by ALM [11]. The general method of ALM

is introduced for solving the following constrained optimization problem

min f (X)

s.t. h(X) = 0.

The corresponding ALM function of eq(12) is deﬁned as
μ
2

L(X, Y, μ) = f (X)+ < Y, h(X) > +

(cid:4)h(X)(cid:4)2

F

(12)

(13)

,

442

Z. Zheng et al.

Algorithm 1 General Method of ALM
1: ρ ≥ 1.
2: while not converged do
3:
4:
5: update μk to μk+1
6: end while
7: Output: Xk

solve Xk+1 = arg minX L(Xk, Yk, μk)
Yk+1 = Yk + μkh(Xk)

where Y is a Lagrange multiplier matrix and μ is a positive scalar. The solution to

eq(13) is outlined as Algorithm1.

In the proposed eq(11), let X = (As, Es), then

f (X) = (cid:4)As(cid:4)∗ + λ(cid:4)Es(cid:4)1 + γ(b1 < As, As > +b2(cid:4)As(cid:4)∗),
h(X) = Xs − As − Es

respectively. The ALM function of our eq(11) is

L(As, Es, Ys, μ, γ) = (cid:4)As(cid:4)∗ + λ(cid:4)Es(cid:4)1 + γ(b1 < As, As > +b2(cid:4)As(cid:4)∗)
(cid:4)Xs − As − Es(cid:4)2

+ < Ys, Xs − As − Es > +

μ
2
To solve eq(15), we can optimize As, Es and Ys iteratively.

F

.

(14)

(15)

– Updating As:

When updating As, we have to ﬁx Es and Ys to solve the following problem based
on eq(15), and the 3rd iteration of Algorithm(1) evolves

Ak+1

s

= arg min
Ak
s

= arg min
Ak
s

= arg min
Ak
s

, Yk
L(Ak
, Ek
, μk, γ)
s
s
s
(1 + b2)(cid:4)Ak
(cid:4)∗ + (γb1 +
(cid:4)Xa − Ak
(cid:4)2
(cid:4)∗ +
(cid:4)Ak
1
2

F

s

s

s

) < As, As > +μk < Xk
s

− Ek

s

+

1
μk Yk

s

, Ak
s

>

μk
2

,

(16)

where  = 1+b2
soft-thresholding operator

2γb1+μk and Xa =

μk

2γb1+μk (Xk

s

− Ek

s

+ 1

μk Yk

s ). Introducing the following

S [x] (cid:3)

⎧⎪⎪⎪⎨⎪⎪⎪⎩

x − , i f x > 
x + , i f x < −
0,
otherwise

,

then we have the solution of eq(16) [11]

Ak+1

s

= US s[S ]VT ,

where US VT is the SVD of Xa.

(17)

(18)

Low-Rank Matrix Recovery with Discriminant Regularization

443

– Updating Es:

When updating Es, we have to ﬁx As and Ys. The eq(15) can be derived as

Ek+1

s

= arg min
Ek
s

η(cid:4)Ek

s

(cid:4)1 +

(cid:4)Xe − Ek

s

(cid:4)2

F

,

1
2

(19)

where η = λ 1

μk and Xe =

μk

2γb1+μk (Xk

s

− Ak+1

s

+ 1

μk Yk
s ).

Once we obtain As and Es, Ys can be updated using the 4th iteration of Algorithm1. The
whole method we proposed is described in Algorithm2.

Ak+1

0

0

∗
0

= Ak∗, Ek+1

= sgn(X)/J(sgn(D)).

= Ek∗, j = 0 and b1,2 shown in eq(10);

Algorithm 2 Low-rank Matrix Recovery with Discrimination
1: Input observation matrix X, λ.
2: Input μ0 > 0, ρ > 1 and η.
3: Compute Y
4: while not converged do
5:
6:
7:
8:
9:
10:
11:
12:
13:
14: end while
15: Output: (Ak∗, Ek∗).

while not converged do
− Ek
(U, S , V) = svd(Xk
s
Ak+1
= US [S ]V T;
− Ak+1
j+1
Ek+1
= S η(Xk
j ← j + 1
s
j+1
end while

+ μk(Xs − Ak+1∗ − Ek+1∗

∗
Y
k+1
update μk to μk+1

= Y

∗
k

+ 1

μk Y k

s );

+ 1

μk Y k
s );

s

s

)

3.4 LR with Discrimination for Face Recognition

Occlusion is a common challenging encountered in face recognition tasks, such as eye-
glasses, sunglasses, scarves and some objects placed in front of the faces. Moreover,
even in the absence of an occluding object, violations of an assumed model for face
appearance may act like occlusions: e.g., shadows due to extreme illumination. Robust-
ness to occlusion is therefore essential to practical face recognition system. If the face
images are partially occluded, popular recognition methods based on holistic features
such Eigenfaces [15], Fisherfaces [12] and Laplacianfaces [16] would lead to unaccept-
able performance due to the corruption of the extracted features. Although SRC-based
algorithm [28] achieves better results in recognizing occluded testing images, it still
requires unoccluded face images for training and thus might not be preferable for real
application scenarios.

Low-rank matrix recovery has been applied to alleviate the aforementioned problems
by decomposing the collected data matrix into two diﬀerent parts, one is a representa-
tion basis matrix of low rank and the other is the corresponding sparse error, as shown
in Fig.1.

444

Z. Zheng et al.

(a) The original face images

(b) The standard low-rank recovery of (a)

(c) The standard sparse error of (a)

(d) The low-rank recovery of our method

(e) The sparse error of our method

Fig. 1. The results of low-rank matrix recovery with and without discrimination

We can ﬁnd out from Fig.1 that when the standard low-rank matrix recovery is com-
bined with discrimination, the face images within the recovered representation basis
matrix tend to be more similar to each other for the same subject, which means more
compactness exists within the same classes and dissimilarity between diﬀerent classes.
In addition, we also can conclude from Fig.1 that the sparse error with discrimination
can remove more sparse noise. As a result, the representation basis matrix of low-rank
recovery with discrimination has a better representative ability than the original version.
Since the face images usually lie in high dimensional spaces, traditional dimensionality
reduction techniques, like PCA or LDA, can be performed on the recovered representa-
tion basis matrix. As a result, the derived subspace can be applied as the dictionary for
training and the testing purposes. In the recognition stage, one can also use SRC-based
classiﬁcation strategy to identify the input image. Our scheme for face recognition is
described as Algorithm3.

Algorithm 3 LR with Discrimination for Face Recognition
1: Input training data X = [X1, X2, . . . , Xc] and a testing image y.
2: Use Algorithm2 on X to compute the representation basis matrix A.
3: Calculate the projection matrix of P of A.
4: Compute the projection of X and y:

5: Perform SRC-based classiﬁcation on yp:

Xp = PT X, and yp = PT y.
arg minα (cid:4)yp − Xpα(cid:4)2
+ λ(cid:4)α(cid:4)1,
for i = 1 : c
αi(cid:4)2
err(i) = (cid:4)yp − Xi
end for

2

p

2

6: Output: label(y) = mini err(i).

4 Experiments

In this section, we perform the proposed method shown in Algorithm3 on publicly
available databases for face recognition to demonstrate the eﬃcacy of the proposed

Low-Rank Matrix Recovery with Discriminant Regularization

445

classiﬁcation algorithm. We will ﬁrst examine the role of feature extraction within our
framework, comparing performance across various feature spaces and feature dimen-
sions, and comparing to several popular methods. Meanwhile, We will then demonstrate
the robustness of the proposed algorithm to corruption and occlusion. Finally, the ex-
perimental results demonstrate the eﬀectiveness of sparsity as a means of validating
testing images.

Besides the standard low-rank matrix recovery without discrimination and our pro-
posed method, we also consider Nearest Neighbor (NN), SRC [4], and LLC [32] for
comparisons. Note that LLC can be regarded as an extended version of SRC exploiting
data locality for improved sparse coding, and the classiﬁcation rule is the same as that of
SRC. To evaluate our recognition performance using data with diﬀerent dimensions, we
project the data onto the eigenspace derived by PCA using our LR with discrimination
models. For the standard LR approach, the eigenspace spanned by LR matrices without
discrimination is considered, while those of other SRC based methods are derived by
the data matrix X directly. We vary the dimension of the eigenspace and compare the
results in this section.

4.1 Two Databases

– The Extended Yale B database consists of 2, 414 frontal face images of 38 indi-
viduals around 59 − 64 images for each person [33]. The cropped and normalized
192 × 168 face images were captured under various laboratory-controlled lighting
conditions. Some sample face images of the Extended Yale B are shown in Fig.2(a).
– The AR database consists of over 4, 000 frontal images for 126 individuals [34].
For each individual, 26 pictures were taken in two separate sessions. In Fig.2(b),
the left and right ones are some images collected in two sessions.

(a)

(b)

Fig. 2. Some sample images of Extended Yale B and AR

4.2 Results

On the Extended Yale B Database. For each subject, we randomly select 10, 20 and
30 images of each subject for training respectively, and the left images for testing. Ran-
domly choosing the training set ensures that our results and conclusions will not depend
on any special choice of the training data. We vary the dimension of the eigenspace as
25, 50, 75, 100, 150, 200, 300 and 400 to compare the recognition performance between

446

Z. Zheng et al.

Table 1. 10 training samples

Table 2. 20 training samples

Table 3. 30 training samples

75

50

SRC

METHOD 25
100 150 200 300
LR+SRC 67.3 75.7 79.1 82.2 83.4 84.6 86.8
61.8 69.6 76.9 82.1 83.3 85.8 86.2
LLC+SRC 44.6 62.5 68.7 73.4 75.7 77.3 78.6
30.7 42.8 45.6 49.7 53.4 56.1 58.3
72.4 77.2 81.4 83.8 86.5 86.9 86.8

OURS

NN

75

50

SRC

METHOD 25
100 150 200 300
LR+SRC 75.4 82.3 85.7 87.2 89.6 90.5 91.9
67.3 74.9 80.6 85.8 87.2 90.5 91.1
LLC+SRC 51.7 66.8 72.4 78.3 81.6 85.8 87.2
38.1 45.7 52.2 58.4 62.4 66.3 69.8
82.4 84.6 86.3 89.1 92.9 93.1 93.5

OURS

NN

75

50

SRC

METHOD 25
100 150 200 300
LR+SRC 86.9 93.3 94.7 95.4 96.1 96.5 96.4
80.3 88.6 92.9 94.5 95.4 96.1 96.6
LLC+SRC 62.4 79.6 86.4 89.7 92.5 93.8 94.5
45.1 56.0 63.3 66.8 69.4 73.2 76.7
90.5 94.3 95.8 96.2 97.1 97.5 97.5

OURS

NN

diﬀerent methods. All experiments run ten times and the average results are shown in
Table1-3.

It is clear from those Tables mentioned above that the proposed method consistently
achieves higher recognition rates than other NN and SRC-based approaches. For exam-
ple, at dimension = 100, our method achieves a better recognition rate at 96.2%, and
those for LR, SRC, LLC, and NN are 95.4%, 94.5%, 89.7%, and 66.8%, respectively
(see Table3). Repeating the above experiments using diﬀerent training images for each
person, we can conﬁrm from these empirical results that the use of LR method allevi-
ates the problem of severe illumination variations even when such noise is presented in
both training and testing data. Furthermore, when discrimination is taken into account
as proposed in the paper, LR method exhibits enhanced classiﬁcation capability and
thus outperforms the standard LR algorithm.

On AR Database. In the experiment, a subset of the dataset consisting of 50 male
subjects and 50 female subjects was chosen. The images are cropped with dimension
165 × 120. Diﬀerent from [4], for each subject, both neutral (four neutral faces with
diﬀerent lighting conditions and three faces with diﬀerent expressions) and corrupted
images (three faces with sunglasses and three faces with scarfs) taken at session 1 are
used for training, and session 2 for testing. Speciﬁcally, we consider the following sam-
ple selection for training: 7 neutral images plus 3 sunglass images; 7 neutral images
plus 3 scarf images; 7 neutral images plus 3 sunglass images and 3 scarf images. We
vary the dimension of the eigenspace as 25, 50, 75, 100, 150, 200, 300 and 400 to com-
pare the recognition performance between diﬀerent methods. The experimental results
are visualized in Fig.3.

%
e

 

t

a
R
n
o

 

i
t
i

n
g
o
c
e
R

90

85

80

75

70

65

60

 
0

 

LR+SRC
SRC
LLC+SRC
NN
OURS

50

100

150

200

250

300

Dimensionality

(a)

%
e

 

t

a
R
n
o

 

i
t
i

n
g
o
c
e
R

85

80

75

70

65

60

55

50

 
0

 

LR+SRC
SRC
LLC+SRC
NN
OURS

50

100

150

200

250

300

Dimensionality

(b)

%
e

 

t

a
R
n
o

 

i
t
i

n
g
o
c
e
R

70

65

60

55

50

45

40

 
0

 

LR+SRC
SRC
LLC+SRC
NN
OURS

50

100

150

200

250

300

Dimensionality

(c)

Fig. 3. Recognition rate on AR. (a)7 neutral + 3 sunglass images. (b)7 neutral + 3 scarf images.
(c)session 1 as training set.

Low-Rank Matrix Recovery with Discriminant Regularization

447

From these three ﬁgures, we see that the proposed method outperforms all other
algorithms across diﬀerent dimensions. It is worth noting that with the increase of oc-
clusion (from sunglass ro scarf), the recognition rates of all the approaches are severely
degraded, which can be seen from Fig.3(a) and Fig.3(b). In addition, with the increase
of occluded images in the training set, the performances of all the approaches are also
severely degraded which can be seen from Fig.3(c). These two cases indicate that the
direct use of corrupted training image data will remarkably make the recognition results
worse.

5 Conclusions

In this paper, a low-rank matrix recovery algorithm with discriminant regularization is
proposed. The discrimination regularizer is motivated by Fisher criterion which plays
an important role in classiﬁcation tasks. The introduction of this kind of regularizer into
low-rank matrix recovery promotes the discrimination power in the learned representa-
tion basis. We also show that the proposed optimization algorithm can be formulated by
augmented Lagrange multipliers. When applied to face recognition problem, the pro-
posed algorithm demonstrates robustness to severe occlusions of face images even in
the training set. The experiments has shown that our method achieves the state-of-the-
art recognition results.

Acknowledgments. The authors conﬁrm that the research was supported by National
Natural Science Foundation (No.61170109, No.61100119 and No.61272468), and Sci-
ence and Technology Planning Project of Zhejiang Province (No.2012C21021), China.

References

1. Olshausen, B.A., Field, D.J.: Sparse coding with an over-complete basis ser: a strategy em-

ployed by v1? Vision Research 37(23), 3311–3325 (1997)

2. Vinje, W.E., Gallant, J.L.: Sparse coding and decorrelation in primary visual cortex during

natural vision. Science 287(5456), 1273–1276 (2000)

3. Wright, J., Ma, Y., Mairal, J., Spairo, G., Huang, T., Yan, S.C.: Sparse representation for
computer vision and pattern recognition. Proceedings of the IEEE 98(6), 1031–1044 (2010)
4. Wright, J., Yang, A.Y., Sastry, A.G.S.S., Ma, Y.: Robust face recognition via sparse repre-

sentation. IEEE PAMI 31(2), 210–227 (2009)

5. Yang, M., Zhang, L., Yang, J., Zhang, D.: Robust sparse coding for face recognition. In:

CVPR (2011)

6. Huang, K., Aviyente, S.: Sparse representation for signal classiﬁcation. In: NIPS (2006)
7. Wagner, A., Wright, J., Ganesh, A., Zhou, Z.H., Ma, Y.: Towards a practical face recognition

system: Robust registration and illumination by sparse representation. In: CVPR (2009)

8. Wagner, A., Wright, J., Ganesh, A., Zhou, Z.H., Ma, Y.: Towards a practical face recognition
system: Robust registration and illumination by sparse representation. IEEE PAMI 34(2),
372–386 (2012)

9. Candes, E., Li, X., Ma, Y., Wright, J.: Robust principal component analysis? Journal of

ACM 58(1), 1–37 (2009)

448

Z. Zheng et al.

10. Candes, E., Recht, B.: Exact low rank matrix completion via convex optimization. In: Aller-

ton (2008)

11. Lin, Z., Chen, M., Ma, Y.: The augmented lagrange multiplier method for exact recovery of

corrupted low-rank matrix. UIUC Technical Report UILU-ENG-09-2215 (2009)

12. Belhumeur, P.N., Hespanha, J.P., Kriegman, D.J.: Eigenfaces vs ﬁsherfaces: recognition us-

ing class speciﬁc linear projection. IEEE PAMI 19(7), 711–720 (1997)

13. Li, Z., Lin, D., Tang, X.: Nonparametric discriminant analysis for face recognition. IEEE

PAMI 31(4), 755–761 (2009)

14. Lu, J., Tan, Y., Wang, G.: Discriminaive multi-manifold analysis for face recognition from a

single trainning sample per person. IEEE PAMI pp(99), 1 (2012)

15. Turk, M., Pentland, A.: Eigenfaces for recognition. Journal of Cognitive Neuroscinces 3,

72–86 (1991)

16. He, X., Yan, S., Hu, Y., Niyogi, P., Zhang, H.: For recognition using laplacianfaces. IEEE

PAMI 27(3), 328–340 (2005)

17. He, X., Cai, D., Niyogi, P.: Locality preserving projections. In: NIPS (2003)
18. Cai, D., He, X., Han, J., Zhang, H.: Orthogonal laplacianfaces for face recognition. IEEE

TIP 15(11), 3608–3614 (2006)

19. Yan, S., Xu, D., Zhang, B., Zhang, H., Yang, Q., Lin, S.: Graph embedding and extension: A

general framework for dimensionality reduction. IEEE PAMI 29(1), 40–51 (2007)

20. Hua, G., Viola, P., Drucker, S.: Face recognition using discriminatively trained orthogonal

rank one tensor projections. In: CVPR (2007)

21. Xue, H., Chen, S., Yang, Q.: Discriminatively regularized least-squares classiﬁcation. Pattern

Recognition 42(1), 93–104 (2009)

22. Si, S., Tao, D., Geng, B.: Bregman divergence-dased regularization for transfer subspace

learning. IEEE TKDE 22(7), 929–942 (2010)

23. Lu, J., Tan, Y.: Cost-sensitive subspace learning for face recognition. In: CVPR (2010)
24. Lu, J., Tan, Y.: Regularized locality preserving projections and its extensions for face recog-

nition. IEEE SMCB 40(3), 958–963 (2010)

25. Yuan, X., Yan, S.: classiﬁcation with multi-task joint sparse representation. In: CVPR (2010)
26. Jenatton, R., Mairal, J., Obozinski, G., Bach, F.: Proximal methods for hierarchical sparse

coding. Journal of Machine Learning Research 12, 2297–2334 (2011)

27. Cabral, R., Costeira, J., Torre, F., Bernardino, A.: Fast incremental method for matrix com-

pletion: an application to trajectory correction. In: ICIP (2011)

28. Wright, J., Ganesh, A., Rao, S., Ma, Y.: Robust principal component analysis: exact recovery

of corrupted low rank matrices by convex optimization. In: NIPS (2009)

29. Peng, Y., Ganesh, A., Wright, J., Xu, W., Ma, Y.: Robust alignment by sparse and low-rank

decomposition for linearly correlated images. In: CVPR (2010)

30. Liu, G., Lin, Z., Yu, Y.: Robust subspace segmentation by low-rank representation. In: ICML

(2010)

31. Ji, H., Liu, C., Shen, Z., Xu, Y.: Robust video denoising using low rank matrix completion.

In: CVPR (2010)

32. Wang, J., Yang, J., Yu, K., Lv, F., Huang, T., Gong, Y.: Locality constrained linear coding for

image classiﬁcation. In: CVPR (2010)

33. Georghiades, A., Belhumeur, P., Kriegman, D.: From few to many: Illumination cone models

for face recognition under variable lighting and pose. IEEE PAMI 23(6), 643–660 (2001)

34. Martinez, A., Benavente, R.: The ar face database. CVC Technical Report 24 (1998)


