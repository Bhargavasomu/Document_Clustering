J Cryptogr Eng (2011) 1:293–302
DOI 10.1007/s13389-011-0023-x

REGULAR PAPER

Machine learning in side-channel analysis: a ﬁrst study
Gabriel Hospodar · Benedikt Gierlichs ·
Elke De Mulder · Ingrid Verbauwhede ·
Joos Vandewalle

Received: 30 September 2011 / Accepted: 1 October 2011 / Published online: 27 October 2011
© Springer-Verlag 2011

Abstract Electronic devices may undergo attacks going
beyond traditional cryptanalysis. Side-channel analysis
(SCA) is an alternative attack that exploits information leak-
ing from physical implementations of e.g. cryptographic
devices to discover cryptographic keys or other secrets.
This work comprehensively investigates the application of
a machine learning technique in SCA. The considered tech-
nique is a powerful kernel-based learning algorithm: the
Least Squares Support Vector Machine (LS-SVM). The cho-
sen side-channel is the power consumption and the target is a
software implementation of the Advanced Encryption Stan-
dard. In this study, the LS-SVM technique is compared to
Template Attacks. The results show that the choice of param-
eters of the machine learning technique strongly impacts the
performance of the classiﬁcation. In contrast, the number of
power traces and time instants does not inﬂuence the results in
the same proportion. This effect can be attributed to the usage
of data sets with straightforward Hamming weight leakages
in this ﬁrst study.

Power analysis · Side-channel analysis ·

Keywords
Cryptography · Support vector machines · Machine learning

This work was supported in part by the European Commission’s
ECRYPT II NoE (ICT-2007-216676), by the Belgian State’s IAP
program P6/26 BCRYPT, by the K.U. Leuven-BOF (OT/06/40) and
by the Research Council K.U. Leuven: GOA TENSE (GOA/11/007).
Benedikt Gierlichs is a Postdoctoral Fellow of the Fund for Scientiﬁc
Research, Flanders (FWO).

G. Hospodar (B) · B. Gierlichs · E. De Mulder · I. Verbauwhede ·

J. Vandewalle
Katholieke Universiteit Leuven, ESAT-SCD-COSIC and IBBT,
Kasteelpark Arenberg 10, 3001 Leuven-Heverlee, Belgium
e-mail: gabriel.hospodar@esat.kuleuven.be

1 Introduction

Security in electronic devices must not only rely on crypto-
graphic algorithms proven to be mathematically secure. An
attacker may use information leaked from side-channels [14]
resulting from physical implementations. This is called side-
channel analysis (SCA) and it may endanger the overall secu-
rity of a system. The ever increasing demand for security on
a number of applications, including the internet of things,
ﬁnancial transactions, electronic communications, and data
storage, should drive designers to strongly consider the pos-
sibility of physical attacks in addition to attacks on crypto-
graphic algorithms, as most of these applications require the
use of embedded devices.

In cryptography, SCA usually aims at revealing crypto-
graphic keys. The underlying hypothesis for SCA assumes
that physical observables carry information about the inter-
nal state of a chip implementing some cryptographic algo-
rithm. In this context, useful key-related information can
often be obtained from side-channels such as processing
time [13], power consumption [14], and electromagnetic
emanation [8,17].

In this work, we focus on power analysis. Given a set of
power traces measured from a chip implementing a crypto-
graphic algorithm, the ultimate goal is to tell which crypto-
graphic key has been processed internally. This problem is
frequently addressed using a divide-and-conquer approach.
This approach breaks down a problem into practically trac-
table sub-problems. For example, parts of the cryptographic
key, also known as subkeys, may be attacked at a time. Each
attack may be formulated as a classiﬁcation problem intend-
ing to discover which subkey is linked to a given power trace.
Machine learning [16] is often used to solve classi-
ﬁcation and regression problems. Machine learning con-
cerns computer algorithms that can automatically learn from

123

294

J Cryptogr Eng (2011) 1:293–302

it

turns out

experience. As a number of SCA-related problems can be
formulated as classiﬁcation problems,
that
machine learning represents a potential, useful tool for SCA.
Machine learning techniques have already been applied
in cryptanalysis [19]. Furthermore, Backes et al. [2] used
machine learning techniques for acoustic side-channel
attacks on printers. To the best of our knowledge, there has
not been further thorough investigation on the application of
machine learning techniques in SCA.

This contribution lies in the context of proﬁled attacks,
such as Template Attacks (TAs) [5]. These attacks com-
prise two phases: proﬁling and classiﬁcation. In machine
learning terms, these phases are respectively known as train-
ing and testing. In TAs, multivariate Gaussian templates
of noise within power traces are generated either for all
possible subkeys or for the results of some particular func-
tion involving them. Subsequently, power traces are clas-
siﬁed under a maximum likelihood approach. The TA is
regarded as the strongest form of side-channel attack pos-
sible in an information theoretic sense [5]. Although TA
can be seen as a machine learning technique, we refer
to TA and machine learning techniques separately in this
work.

In our work, the machine learning technique used is the
Least Squares Support Vector Machines (LS-SVMs) [20],
which is a kernel-based learning algorithm. LS-SVM clas-
siﬁers have achieved considerable results in comparison to
other learning techniques, including standard SVM classiﬁ-
ers, for classiﬁcation on 20 public domain benchmark data
sets [9].

We trained LS-SVM classiﬁers by supervised learning. In
the training phase, a number of power traces were provided
to the classiﬁer to teach it the most important features of the
data set. In the testing phase, one unseen power trace was
presented to the classiﬁer at a time.

To get insight into the behavior of all the LS-SVM param-
eters, we focused on binary classiﬁcation problems. Artiﬁcial
data were created based on power traces extracted from a soft-
ware implementation of the Advanced Encryption Standard
(AES) without countermeasures. We considered one single
S-Box lookup. In this ﬁrst study, there was no actual key
recovery yet.

Three experiments were conducted. We ﬁrst investigated
the inﬂuence of changing the parameters of the LS-SVM
classiﬁers to learn their effects. Analyses varying both the
numbers of power traces and their components (time instants
possibly preprocessed) were also performed. In addition,
we examined the impact of three feature selection tech-
niques [Pearson correlation coefﬁcient approach for com-
ponent selection [18], sum of squared pairwise t-differences
(SOST) of the average signals [10] and principal component
analysis (PCA) [12]] and one preprocessing technique (out-
liers removal).

123

Our approaches were compared to TAs in terms of effec-
tiveness to provide a known reference. TAs strongly rely on
parametric estimations of Gaussian distributions. Machine
learning techniques are able to bypass restrictive assump-
tions about probability density distributions.

This paper is organized as follows: Sect. 2 describes the
feature selection techniques used in this work. In Sect. 3, our
selected machine learning technique is explained. The results
of the experiments are presented in Sect. 4. The preprocessing
technique is also explained in this section. Section 5 presents
the conclusions. Section 6 is dedicated to future work.

2 Feature selection

Generally, machine learning approaches make use of a pre-
liminary step before tackling the classiﬁcation problem to
be solved. This step is called feature selection. It ﬁlters out
and/or preprocesses the components of a given data set in
order to extract the intrinsic parts of the data containing the
most relevant pieces of information under some criteria. The
two main advantages of using the feature selection step are
(1) it allows for the reduction of the computational burden
of classiﬁers with respect to processing and memory issues;
and (2) it avoids confusing or teaching the wrong features of
the data to the classiﬁer. High-dimensional data can prevent
classiﬁers from working in practice.

Power traces have thousands or millions of samples in
time, which are here regarded as components. In this work,
each component is represented by t. Many of them do not
carry relevant information related to the targeted subkeys.
They rather represent noise and ideally should not be pre-
sented to the classiﬁer.

The following sections are presented: the Pearson corre-
lation coefﬁcient approach for component selection (mostly
used in this work); the SOST of the average signals; and the
PCA.

2.1 Pearson correlation coefﬁcient approach

the N most relevant
A straightforward way to select
components of the power traces concerns ﬁnding the N points
which have the largest correlations with respect to a func-
tion of the targeted subkey, as shown by Rechberger and
Oswald [18].

√

cov(x(t ), y)
var(x(t )) · var(y)

The Pearson correlation coefﬁcient is given by
, −1 ≤ ρ(t ) ≤ 1,

ρ(t ) =
where cov(·,·) represents the covariance, var(·) represents
the variance, x(t ) is the vector with the component t of all
power traces in the training set, and y is a vector containing
the target values.

J Cryptogr Eng (2011) 1:293–302

295

2.2 Sum of squared pairwise T-differences (SOST)

Chari et al. [5] originally proposed the following method to
choose the most relevant components of the power traces, as
part of TAs: let Mi be the statistical average of the power
traces associated with the subkey i, i = 1, . . . , S. The N
most relevant components would be those at which large
differences show up after computing the sum of pairwise
differences between the average signals Mi .

Gierlichs et al. [10] showed that this feature selection
method was not optimal. Better results were achieved using
the SOST of the average signals. SOST is based on the
T-Test—a statistical tool to distinguish signals. The T-Test
is expressed by the following ratio:

(cid:2)

T = Mi (t ) − M j (t )
+ σ 2

σ 2
(t )
i
ni

.

(1)

(t )
j
n j

The denominator of the formula weights the difference
between Mi (t ) and M j (t ) according to the variabilities σ 2
(t )
i
and σ 2
(t ), in relation to the number of signals ni and n j asso-
i
ciated with the sets i and j. Equation (1) can also be seen as
an analogy to the signal-to-noise ratio, in which the differ-
ence between the averages is the signal and the denominator
is a measure of dispersion, being interpreted as noise. This
noise may make the distinction between the distributions with
averages Mi (t ) and M j (t ) hard, but it should vanish if ni and
n j are large.

The most relevant components will be those at which the

SOST,

SOST(t ) = S(cid:3)

j >i=0

⎛
⎜⎜⎝ Mi (t ) − M j (t )
(cid:2)
+ σ 2

σ 2
(t )
i
ni

(t )
j
n j

2

⎞
⎟⎟⎠

,

presents the N highest peaks.

2.3 Principal component analysis (PCA)

Principal component analysis [12] is a well-known orthog-
onal, non-parametric transformation that provides a way to
efﬁciently select relevant information from data sets. The
core idea is to compute a new basis that better expresses the
data set, revealing its intrinsic structure.

By assuming linearity, the set of new plausible bases is
significantly reduced and the problem turns into ﬁnding an
appropriate change of basis. PCA also assumes that mean
and variance are sufﬁcient statistics.

Presuming that the directions with largest variances con-
tain most relevant information, PCA sorts the transformed,
most important components of a vector with regard to their
variances.

In addition to maximizing the signal, measured by the
interclass variance, PCA also intends to minimize the redun-
dancy within components of the data set. This can be achieved
by setting all off-diagonal terms of the covariance matrix of
the transformed data to zero, making the components uncor-
related to each other.
After estimating the covariance matrix C of the original
data set x ∈ RM , where M is the number of components of x,
the N < M eigenvectors related to the largest eigenvalues λt
from the eigenvector decomposition Cut = λt ut should be
selected. The transformed, lower dimensional variable will
be given by zt = uT
(xt − μt ), t = 1, . . . , N , where μt is the
mean of the t-th component of x. The error on the new data
set resulting from the dimensionality reduction is determined
λt . For the transformed variable, t does not have
by
a time connotation anymore.

M
t=N+1

(cid:10)

t

3 Classiﬁcation

The classiﬁcation technique used in this work is the LS-SVM.
LS-SVM tackles linear systems rather than solving convex
optimization problems, typically quadratic programs, as in
standard support vector machines (SVM) [7]. This is done
by both introducing a least squares loss function and work-
ing with equalities, instead of the intrinsic inequalities of
SVM formulations. One advantage of this reformulation is
complexity reduction.

(LS-)SVM classiﬁers are originally formulated to perform
binary classiﬁcation. In the training phase, the (LS-)SVM
classiﬁer constructs a hyperplane in a high-dimensional
space aiming to separate the data according to the different
classes. This data separation should occur in such a way that
the hyperplane has the largest distance to the nearest training
data points of any class. These particular training data points
deﬁne the so-called margin.
Let Dn = {(xk , yk ): xk ∈ RN , yk ∈ {−1,+1}; k =
1, . . . , n} be a training set, where xk and yk are, respectively,
the k-th input (power trace after feature selection) and output
(subkey-related) patterns.

The classiﬁer in the primal weight space takes the follow-

ing form:
y = sign[wTϕ(x) + b],
where sign(x) = −1 if x < 0, else sign(x) = 1, and
ϕ(x): RN → RNf maps the N -dimensional input space into
a higher, possibly inﬁnite, Nf -dimensional space. Both the
weights w ∈ RNf and bias b ∈ R are parameters of the classi-
ﬁer. These parameters can be found by solving the following
optimization problem having a quadratic cost function and
equality constraints:

123

296

wTw + γ
2

L(w, e) = 1
2

n(cid:3)
e2
min
k
k=1
w,b,e
s.t. yk[wTϕ(xk ) + b] = 1 − ek ,
(2)
which is a modiﬁcation of the basic SVM formulation. In
Eq. (2), e = [e1, . . . , en]T is a vector of error variables, toler-
ating misclassiﬁcation, and γ is the regularization parameter,
determining the trade-off between the margin size maximi-
zation and the training error minimization.

k = 1, . . . , n,

After constructing the Lagrangian,

L(w, b, e; α) = 1
2

n(cid:3)
k=1

1
2

e2
k

wTw+γ
− n(cid:3)

k=1

αk{yk[wTϕ(xk )+b] − 1 + ek},

∂L
∂αk
(cid:12)

and taking the conditions for optimality, by setting
∂L
∂w

= 0,

= 0,

= 0,

= 0,

∂L
∂b

∂L
∂ek

k = 1, . . . , n,

,

(cid:11)
n(cid:3)
k=1

αk yk K (x, xk ) + b

the classiﬁer formulated in the dual space is given by
y(x) = sign
where K (x, xk ) = ϕ(x)Tϕ(xk ) is a positive definite kernel
matrix, αk ∈ R are the Lagrange multipliers, or support val-
ues. Both αk and b are the solutions of the following linear
system(cid:11)
yT
0
y  + 1
γ In
with 1n = (1, . . . , 1)T and kl = yk yl ϕ(xk )Tϕ(xl ). The
solution is unique when the matrix corresponding to the lin-
ear system has full rank.

(cid:12)(cid:13)

0
1n

(cid:14)

(cid:13)

(cid:14)

=

b
α

,

From ∂L
∂ek

According to Mercer’s theorem [1], a positive definite K
guarantees the existence of the feature map ϕ, which is often
not explicitly known.
= 0, we have αk = γ ek, meaning that the sup-
port values are proportional to the errors corresponding to
the training data points. As αk (cid:6)= 0, k = 1, . . . , n, every
data point is a support vector, implying lack of sparseness.
High αk values suggest high contributions of training data
points on the decision boundary created by the classiﬁer to
distinguish the different classes.

3.1 Practicalities

(cid:15)

(cid:16)

J Cryptogr Eng (2011) 1:293–302

−||x − xk||2

,

2

σ 2

K (x, xk ) = exp
where ||·||2 is the L2-norm and σ 2 ∈ R
+
is a parameter to be
chosen. Other kernel options, such as the polynomial kernel
and the multilayer perceptron (MLP) kernel, are beyond the
scope of this work.

Kernel-based models usually depend on parameters con-
trolling both their accuracies and complexities. When using
linear kernels, the only parameter to be tuned is γ . If γ → 0,
the solution favors margin maximization, putting less empha-
sis on minimizing the misclassiﬁcation error. RBF kernels
require tuning an additional parameter σ 2, which is directly
related to the shape of the decision boundary.

Considering RBF kernels, both parameters γ and σ 2
should be optimized for the classiﬁer to maximize the success
rates concerning the analysis of unknown power traces from
the testing data set. A combination of a cross-validation and
a grid search algorithm is recommended in the literature [20]
for parameter tuning. Cross-validation can help preventing
overﬁtting. However, its computation may be computation-
ally costly.

Many classiﬁcation problems have more than two clas-
ses. As (LS-)SVMs are designed to perform binary classiﬁ-
cation, a typical approach to cope with a multi-class problem
is to split the problem up into binary classiﬁcation prob-
lems using some coding technique. For example, a multi-
class problem comprising p classes may be broken down

into (cid:7)log2 p(cid:8) binary classiﬁcation tasks. Subsequently, the

results of the binary classiﬁers should be combined to recon-
struct a valid result for the original multi-class problem. In
this work we only deal with binary classiﬁcation problems.

4 Results

This section investigates the potential that LS-SVMs have
to work as robust power traces analyzers. We chose to get
started with a thorough investigation on the machine learning
technique behavior. To this end, several tests were performed
using both linear and RBF kernels. We are one step behind of
the actual discovery of cryptographic subkeys. In this work,
our attacks distinguish between two different classes related
to the output of one AES S-Box.

4.1 Experimental settings

Roughly, when working with (LS-)SVMs one usually
chooses the kernel K between the linear kernel,
K (x, xk ) = x T
k x,
and the radial basis function (RBF) kernel,

Preliminary tests were performed on real measurements from
an implementation consisting of the subpart of the AES algo-
rithm composed by the XOR between an 8-bit subkey and the
input word, followed by the application of one S-Box. The
LS-SVM supervised learning classiﬁers have been imple-
mented using the LS-SVMlab1.7 [4].

123

J Cryptogr Eng (2011) 1:293–302

297

Our data set contains 5,000 power traces with 2,000 com-
ponents each. In this work, attacks distinguish only between
two different classes, to help us initially build a solid under-
standing about the techniques involved. These two different
classes were chosen in three ways.

The ﬁrst

two approaches considered the relationship
between the power traces and the internal state of the crypto-
graphic algorithm to be represented by the Hamming weight
model [15]. The threshold approach divided the data set into
two classes depending on the Hamming weights of the out-
puts of the S-Box (less than or greater than 4). The interca-
lated approach divided the data set depending if the Hamming
weights were even or odd. The third approach, named bit(4),
focused on the fourth least significant bit of the output of the
S-Box, since it was the bit leaking more information. Both
intercalated and bit(4) approaches were created so that their
classes would not be as trivially separable as those from the
threshold approach.

In the training phase, a number of inputs from the training
set were provided to the classiﬁer to teach it the most impor-
tant features of the data set. In the testing phase, one input of
the test set was presented to the classiﬁer at a time. Success
rates were calculated as percentages of correct classiﬁcations
among the power traces from the test sets.

Section 4.2 investigates the impact that the variation of the
parameters γ and σ 2 have on the success rates. In Sect. 4.3,
we analyzed the inﬂuence of varying both the numbers of
traces and components on the classiﬁcation. The most rel-
evant components of the power traces were selected by the
Pearson correlation coefﬁcient approach in these two sec-
tions. Lastly, Sect. 4.4 examined whether removing out-
liers or using either the SOST or PCA feature selection
techniques, instead of the Pearson correlation coefﬁcient
approach, would increase the success rates. All these sections
provide a brief comparison of our results to TAs in terms of
effectiveness.

4.2 Inﬂuence of the LS-SVM parameters

In this part, the training and test sets comprised, respectively,
3,000 and 2,000 power traces. At ﬁrst, only two compo-
nents, out of 2,000, were selected by picking those having the
largest Pearson correlation coefﬁcients and not belonging to
the same clock cycle. Working with two-dimensional inputs
allowed visualization of the decision boundaries created by
the classiﬁers with respect to the components.

The parameters γ and σ 2, in case of using the RBF kernel,
assumed the following values: 0.1, 1 and 10. After analyzing
all their combinations, we veriﬁed that the success rates for
the threshold approach were as high as 99.3%, regardless of
the type of kernel used. This is because the classes assigned
by the threshold approach are easily distinguishable.

The intercalated approach led to results sensitive to both
the type of kernel and the tuning parameters. When using
RBF kernels, the success rates showed a direct relation to
σ 2. Success rates as high as 99.0%, 94.0% and 82.7% were
achieved for σ 2 = 0.1, 1 and 10, respectively. Although γ
did not inﬂuence the results as much as σ 2, high values of γ
slightly increased the results. Linear kernels yielded success
rates around 49.9%, performing as well as random guesses.
The reason behind this is that a linear function cannot sepa-
rate nonlinear, intercalated data.

The bit(4) approach achieved success rates of 74.0%,
regardless of both the kernel type and the values of the tun-
ing parameters. These results, despite being worse than those
from both the threshold and intercalated approaches, are yet
clearly better than random, considering that only one out
of eight bits from the output of the S-Box was taken into
account.

Figures 1, 2 and 3 show how the classiﬁcation in the
threshold, intercalated and bit(4) approaches, respectively,
occurs in relation to the tuning parameters. The horizontal
and vertical axes are, respectively, the two considered power
traces components. Figures 1, 2 and 3a present the two clas-
ses for each approach: the square-shaped points belong to
one class, whereas the circle-shaped points belong to the
other class. Figures 1, 2 and 3b concern the decision bound-
aries (dark lines scattering the space in two regions: light
and dark colored) of the linear kernel-based classiﬁers for
γ = 1. Varying γ for the linear kernel does not inﬂu-
ence the decision boundary considerably. Figures 1, 2 and
3c–f concern the decision boundaries of the RBF kernel-
based classiﬁers for the combinations of γ ∈ {0.1, 10} and
σ 2 ∈ {0.1, 10}.

We veriﬁed that underﬁtting arises for low values of γ ,
while overﬁtting occurs for high values of γ . This conclu-
sion is supported by Eq. (2). When using the RBF kernel, low
values of σ 2 make the decision boundary ﬁt the data, while
high values of σ 2 spread the decision boundaries.
Particularly, the orientations of the decision boundaries
shown in Fig. 2d for γ = 0.1 and σ 2 = 10 seem counter-
intuitive. The reason behind this is twofold: (1) the low value
of γ favored underﬁtting; and (2) the relatively high value of
σ 2 favored the spread of the decision boundary in the wrong
direction, which has been poorly chosen due to underﬁtting.
However, as γ increases, the direction of the decision bound-
aries tend to ﬁt the orientation of the data more suitably, as
shown in Fig. 2f in comparison to Fig. 2d.

TAs using two templates led to success rates of 99.6%,
50.3%, and 73.7% for the threshold,
intercalated, and
bit(4) approaches, respectively. Except for the intercalated
approach, results were similar to those obtained with LS-
SVMs using RBF kernels. Results on the intercalated
approach were poor, as in LS-SVMs with linear kernels. The
Gaussian-based templates did not ﬁt well the distributions of

123

298

J Cryptogr Eng (2011) 1:293–302

(a)

(b)

(c)

Fig. 1 Threshold approach: decision boundaries of the LS-SVM classiﬁers

123

(d)

(e)

(f)

J Cryptogr Eng (2011) 1:293–302

299

(a)

(b)

(c)

Fig. 2 Intercalated approach: decision boundaries of the LS-SVM classiﬁers

(d)

(e)

(f)

123

300

J Cryptogr Eng (2011) 1:293–302

(a)

(b)

(c)

Fig. 3 Bit(4) approach: decision boundaries of the LS-SVM classiﬁers

123

(d)

(e)

(f)

J Cryptogr Eng (2011) 1:293–302

301

Table 1 Success rates (SR) for the bit(4) approach

4.4 Outliers removal, SOST and PCA

In this part, we examined whether removing outliers or
using either the SOST or PCA feature selection techniques,
instead of the Pearson correlation coefﬁcient approach,
would increase the success rates.

Traces were drawn as outliers if the value of one of its
selected components subtracted by the mean of this compo-
nent yielded a value larger than 2.7 times the related standard
deviation. The threshold 2.7 was chosen ad hoc. It assured
99.3% of data coverage. Still focusing on the bit(4) approach,
this preprocessing technique did not improve the previous
results.

Concerning the SOST, even with this feature selection
technique not choosing exactly the same components as those
from the previous section, the success rates also did not
change in comparison with the prior results.

The scenario did not change remarkably when using PCA
either. By keeping the features representing more than 98.0%
of the variance of all the features, the results remained similar
to the ones from Sect. 4.2. When saving, respectively, 98.0%,
99.0%, and 99.5% of the variance, the number of selected
features were 4, 5, and 7, respectively.

5 Conclusions

Side-channel analysis is a powerful and feasible way of
attacking secure systems. In this novel work, we started a
comprehensive study on the application of machine learning
in SCA. One of the main motivations to use machine learning
techniques is their outstanding results in different domains.
We focused on power analysis. Power traces often leak
meaningful amounts of information about the processed
cryptographic key. Power traces extracted from a software
implementation of the AES without countermeasures were
used for initial analysis. To help us better understand the
behavior of the machine learning technique, we created
three simple arrangements of class distributions: threshold,
intercalated, and bit(4). All of them contained two different
classes.

In this work, we chose the LS-SVM as our classiﬁcation
technique. We performed three experiments. Firstly, exper-
iments examining the inﬂuence of the parameters of the
machine learning technique on the accuracy of the classiﬁers
were performed on data sets with obvious Hamming weight
leakage. Secondly, we investigated the consequences of vary-
ing both the number of traces and their components. Thirdly,
we ran tests considering feature selection (Pearson correla-
tion coefﬁcient approach, SOST and PCA) and preprocessing
(outliers removal) techniques. The results were compared to
the relatively simple, strong and well-known TAs.

The success rates obtained in the threshold approach were
high regardless of the type of kernel used, mainly because

123

the intercalated data. This was because the intercalated data
were actually drawn from a Gaussian mixture.

4.3 Varying the number of traces and components

Since the classes deﬁned by the threshold approach are triv-
ially separable, this section deals only with the intercalated
and bit(4) approaches. The ﬁrst part of the experiments con-
sisted in varying the number of power traces within the train-
ing and test sets. Out of 5,000, 4,000, 3,000, 2,000, 1,000,
and 500 randomly chosen power traces, 70% were assigned
to the training set and the remaining were assigned to the test
set. The parameters γ and σ 2 were kept constant. For the
intercalated approach: γ = 10 and σ 2 = 1. For the bit(4)
approach: γ = 1 and σ 2 = 0.1.

Interestingly enough, the results did not vary noticeably
when contrasted with those from Sect. 4.2. However, in the
bit(4) case, the success rates dropped approximately 10%
when using the minimum number of power traces, though.
For these ﬁrst experiments with binary classiﬁcation prob-
lems a relatively small amount of power traces seemed to be
enough, as far as the relevant components of the power traces
have been selected. TAs behaved likewise.

The second part of the experiments in this section
attempted to raise the success rates (SR) in the bit(4) approach
by increasing the number of components. Table 1 shows the
result for γ = 1 and σ = 0.1 using, respectively, 3,500 and
1,500 power traces for the training and test sets.

When using the RBF kernel, the success rates dropped
as more components were included. It means that the added
components did not bring additional, valuable information
to the classiﬁer. On the other hand, they represented noise,
making the classiﬁcation task even harder. The contempla-
tion of a search method for more suitable parameters may
help improve the results. In both the linear kernel and TA
cases, the success rates did not vary as a function of the con-
sidered number of components.

302

J Cryptogr Eng (2011) 1:293–302

the data set was easily separable. Results for the interca-
lated approach showed that the RBF kernel is more suitable
for nonlinear problems. Since the choice of the LS-SVM
parameters directly affected the results, applying an auto-
matic parameter tuning technique should be considered in
future works.

The inﬂuence of varying the number of power traces
for training could only be noticed when using as few as
500 power traces, which yielded lower success rates. When
increasing the number of components of the inputs of the
classiﬁers, the results got worse due to lack of valuable
information within the additional components. Likewise, the
application of a preprocessing technique did not contribute.
All the feature selection techniques inspected in this work
performed similarly.

TAs performed similarly to LS-SVM classiﬁers using lin-
ear kernels. This similarity was clariﬁed in the analysis on the
intercalated approach, which generated comparable (poor)
results for both LS-SVM classiﬁers with linear kernels and
TAs. LS-SVM classiﬁers with RBF kernels were able to
produce reasonable results in the intercalated approach.
They outperformed TAs in this case.

6 Future works

LS-SVM It would be interesting to look further at the func-
tioning of kernel-based learning algorithms. It may be that
other possibly tailored kernels improve the results. As a short
reminder, the kernel is responsible for mapping the data into
a feature space to facilitate the distinction between different
classes.
Efﬁciency comparison Future works should include an
assessment of the efﬁciency of machine learning approaches
in comparison to TAs. We observed that LS-SVMs are
significantly heavier than TAs, especially in the training
phase. This observation is likely to hold for other elaborated
machine learning techniques.
Other cryptographic algorithms and implementations Any
implementation of any cryptographic algorithm may be
attacked. The more attacks are performed, the more can be
learned about using machine learning techniques in SCA.
Here the attacker is free to attack either software or hard-
ware implementations of cryptographic algorithms such as
the AES, 3DES, etc.
Attacks on protected implementations A more realistic attack
scenario should consider attacks on protected implementa-
tions. Some designers make use of countermeasures such as
masking [6], for instance. Also, more noisy data should be
considered.
Other machine learning techniques One is basically free to
use any off-the-shelf machine learning technique or even
some other approach. Examples of other machine learning

123

techniques include artiﬁcial neural networks [3,11]. Machine
learning techniques that learn in an unsupervised way (by not
making use of any labeled data, e.g. clustering) are able to
perform non-proﬁled attacks, which were out of the scope of
this work.

References

1. Aizerman, M.A., Braverman, E.A., Rozonoer, L.: Theoretical foun-
dations of the potential function method in pattern recognition
learning. In: Automation and Remote Control, vol. 25, pp. 821–
837 (1964)

2. Backes, M., Dürmuth, M., Gerling, S., Pinkal, M., Sporleder, C.:
Acoustic side-channel attacks on printers. In: USENIX, p. 20 USE-
NIX Association, USA (2010)

3. Bishop, C.: Neural Networks for Pattern Recognition. Oxford

University Press, USA (1995)

4. Brabanter, K.D., Karsmakers, P., Ojeda, F., Alzate, C., Brabanter,
J.D., Pelckmans, K., Moor, B.D., Vandewalle, J., Suykens, J.:
LS-SVMlab toolbox user’s guide version 1.7. http://www.esat.
kuleuven.be/sista/lssvmlab/ (2010)

5. Chari, S., Rao, J.R., Rohatgi, P.: Template attacks. In: CHES, vol.

LCNS 2523, pp. 13–28. Springer, USA (2002)

6. Coron, J.S., Goubin, L.: On boolean and arithmetic masking against
differential power analysis. In: CHES, pp. 231–237. Springer,
London (2000)

7. Cortes, C., Vapnik, V.: Support-vector networks. Mach.

Learn. 20, 273–297 (1995)

8. Gandolﬁ, K., Naccache, D., Paar, C., G, K., Mourtel, C., Olivier,
F.: Electromagnetic analysis: concrete results. In: CHES, vol. 2162,
pp. 251–261. Springer, Berlin (2001)

9. Gestel, T.V., Suykens, J., Baesens, B., Viaene, S., Vanthienen,
J., Dedene, G., Moor, B.D., Vandewalle, J.: Benchmarking least
squares support vector machine classiﬁers. Mach. Learn. 54,
5–32 (2004)

10. Gierlichs, B., Lemke-Rust, K., Paar, C.: Templates vs. stochastic
methods. In: CHES, vol. LCNS 4249, pp. 15–29. Springer, Japan
(2006)

11. Haykin, S.: Neural Networks: A Comprehensive Foundation. Mac-

millan College Publishing Company, Englewood Cliffs (1998)

12. Jolliffe, I.T.: Principal Component Analysis. Springer, Berlin

(1986)

13. Kocher, P.C.: Timing attacks on implementations of Difﬁe-
Hellman, RSA, DSS, and other systems. In: Crypto 96—Advances
in Cryptology, pp. 104–113. Springer, UK (1996)

14. Kocher, P.C., Jaffe, J., Jun, B.: Differential power analysis. In:
Crypto 99—Advances in Cryptology. LCNS, vol. 1666, pp. 388–
397. Springer, USA (1999)

15. Messerges, T.S., Dabbish, E.A., Sloan, R.H.: Examining smart-
card security under the threat of power analysis attacks. IEEE Trans.
Comput. 51, 541–552 (2002)

16. Mitchell, T.M.: Machine Learning. McGraw-Hill, New York

(1997)

17. Quisquater, J.J., Samyde, D.: Electromagnetic analysis (EMA):
measures and counter-measures for smart cards. In: Proc. Smart
Card Programming and Security. LCNS, vol. 2140, pp. 200–210
(2001)

18. Rechberger, C., Oswald, E.: Practical template attacks. In: WISA,

vol. 3325, pp. 440–456. Springer, Korea (2004)

19. Rivest, R.L.: Cryptography and machine learning. In: Advances in

Cryptology ASIACRYPT, pp. 427–439. Springer, Berlin (1993)

20. Suykens, J., Gestel, T.V., Brabanter, J.D., Moor, B.D., Vandewalle,
J.: Least Squares Support Vector Machines. World Scientiﬁc,
Singapore (2002)


