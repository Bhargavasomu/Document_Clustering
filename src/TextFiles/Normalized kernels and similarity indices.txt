Normalized Kernels as Similarity Indices

Julien Ah-Pine

Xerox Research Centre Europe

6 chemin de Maupertuis

38240 Meylan, France

julien.ah-pine@xrce.xerox.com

Abstract. Measuring similarity between objects is a fundamental issue
for numerous applications in data-mining and machine learning domains.
In this paper, we are interested in kernels. We particularly focus on kernel
normalization methods that aim at designing proximity measures that
better ﬁt the deﬁnition and the intuition of a similarity index. To this
end, we introduce a new family of normalization techniques which ex-
tends the cosine normalization. Our approach aims at reﬁning the cosine
measure between vectors in the feature space by considering another ge-
ometrical based score which is the mapped vectors’ norm ratio. We show
that the designed normalized kernels satisfy the basic axioms of a sim-
ilarity index unlike most unnormalized kernels. Furthermore, we prove
that the proposed normalized kernels are also kernels. Finally, we assess
these diﬀerent similarity measures in the context of clustering tasks by
using a kernel PCA based clustering approach. Our experiments employ-
ing several real-world datasets show the potential beneﬁts of normalized
kernels over the cosine normalization and the Gaussian RBF kernel.

Keywords: Kernels normalization, similarity indices, kernel PCA based
clustering.

1 Introduction

Measuring similarity between objects is a fundamental issue for numerous ap-
plications in data-mining and machine learning domains such as in clustering or
in classiﬁcation tasks. In that context, numerous recent approaches that tackle
the latter tasks are based on kernels (see for example [1]). Kernels are special
dot products considered as similarity measures. They are popular because they
implicitly map objects initially represented in an input space, to a higher di-
mensional space, called the feature space. The so-called kernel trick relies on
the fact that they represent dot products of mapped vectors without having to
explicitly represent the latter in the feature space. From a practical standpoint,
kernel methods allow one to deal with data that are not easy to linearly separate
in the input space. In such cases, any clustering or classiﬁcation method that
makes use of dot products in the input space is limited. By mapping the data
to a higher dimensional space, those methods can thus perform much better.
Consequently, many other kinds of complex objects can be eﬃciently treated by

M.J. Zaki et al. (Eds.): PAKDD 2010, Part II, LNAI 6119, pp. 362–373, 2010.
c(cid:2) Springer-Verlag Berlin Heidelberg 2010

Normalized Kernels as Similarity Indices

363

using kernel methods. We have mentioned previously that kernels are generally
introduced as similarity measures but as underlined in [2], dot products in gen-
eral do not necessarily ﬁt one’s intuition of a similarity index. Indeed, one could
ﬁnd in the literature several axioms that clarify the deﬁnition of a similarity
index and in that context, any kernel does not necessarily satisfy all of them.
As an example, one of these conditions that a dot product, and thus a kernel,
does not always respect, is the maximal self-similarity axiom which states that
the object to which any object should be the most similar, is itself.

In this paper, we are interested in designing kernels which respect the basic
axioms of a geometrical based similarity index. In that context, kernels nor-
malization methods are useful. Basically, the most common way to normalize a
kernel so as to have a similarity index, is to apply the cosine normalization. In
that manner, maximal self-similarity for instance, is respected unlike for unnor-
malized kernels. In this work we propose a new family of kernel normalization
methods that generalizes the cosine normalization. Typically, the cosine normal-
ization leads to similarity measures between vectors that are based upon their
angular measure. Our proposal goes beyond the cosine measure by reﬁning the
latter score by using another geometrical based measure which relies on the vec-
tors’ norm ratio in the feature space. We give the following example in order to
motivate such normalized kernels. Let us take two vectors which are positively
colinear in the feature space. In that case, their cosine measure is 1. However,
if their norms are not the same ones therefore, we cannot conclude that these
two mapped vectors are identical. Accordingly, their similarity measure should
be lower than 1. Unlike the cosine normalization, the normalization approaches
that we introduce in this paper aim at taking into consideration this point.

The rest of this paper is organized as follows. In section 2, we formally intro-
duce new normalization methods for kernels. Then, in section 3, we give several
properties of the resulting normalized kernels in the context of similarity indices.
We show that using normalization methods allows one to make any kernel satisfy
the basic axioms of a similarity index. Particularly, we prove that these kernel
normalizations deﬁne metrics. In other words, we show that normalized kernels
are kernels. In section 4, we illustrate the beneﬁts of our proposal in the context
of clustering tasks. The method we use in that regard, relies on kernel PCA
based k-means clustering which can be understood as a combination between
kernel PCA [3] and k-means via PCA [4]. This two step approach is a spectral
clustering like algorithm. Using several datasets from the UCI ML repository
[5], we show that diﬀerent normalizations can better capture the proximity rela-
tionships of objects and improve the clustering results of the cosine measure and
of another widely used normalized kernel, the Gaussian Radial Basis Function
(RBF) kernel. We ﬁnally conclude and sketch some future works in section 5.

2 Kernels and Normalization Methods

We ﬁrst recall some basic deﬁnitions about kernel functions and their cosine
normalization. We then introduce our new kernel normalization methods.

364

J. Ah-Pine

2.1 Kernel Deﬁnition and the Cosine Normalization
Let denote X the set of objects, represented in an input space, that we want to
analyze.
Deﬁnition 1 ((Positive semi-deﬁnite) Kernel). A function K : X ×X → R
is a positive semi-deﬁnite kernel if it is symmetric, that is, K(x, y) = K(y, x)
for any two objects x, y in X and positive semi-deﬁnite, that is:

n(cid:2)

n(cid:2)

cici(cid:2) K(xi, xi(cid:2)) ≥ 0

i=1

i(cid:2)=1

(1)
for any n > 0, any choice of n objects x1, . . . , xn in X and any choice of any
numbers c1, . . . , cn in R.
In the sequel, we will simply use the term kernel instead of positive semi-deﬁnite
kernel. We have the following well-known property.
Theorem 1. For any kernel K on an input space X , there exists a Hilbert space
F, called the feature space, and a mapping φ : X → F such that for any two
objects x, y in X :

K(x, y) = (cid:4)φ(x), φ(y)(cid:5)

where (cid:4)., .(cid:5) is the Euclidean dot product.

When the objects of X are vectors represented in an input space which is
an Euclidean space then, we can mention the following two well-known types of
kernel functions:
– Polynomial kernels: Kp(x, y) = ((cid:4)x, y(cid:5) + c)d, with d ∈ N and c ≥ 0.
– Gaussian RBF kernels: Kg(x, y) = exp

(cid:4)
, where (cid:7).(cid:7) is the Eu-

− (cid:3)x−y(cid:3)2
2σ2

(cid:3)

clidean norm and σ > 0.

(2)

(3)

After having recalled basics about kernels, we recall the deﬁnition of the cosine
normalization of a kernel K, denoted K 0.

K 0(x, y) =

(cid:5)

K(x, y)

(K(x, x)K(x, y))
Since we have K(x, x) = (cid:7)φ(x)(cid:7)2, it is easy to see that:
(cid:7)φ(y)(cid:7)(cid:5)

K 0(x, y) = (cid:4) φ(x)
(cid:7)φ(x)(cid:7) ,

φ(y)

Moreover, we have K 0(x, x) = 1 for all objects in X . This means that the objects
in the feature space are projected on an unit hypersphere. In addition, we have
the following geometrical interpretation from the feature space representation
viewpoint:

K 0(x, y) = cos(θ(φ(x), φ(y)))

(4)
with θ(φ(x), φ(y)) being the angular measure between the vectors in the feature
space. In order to simplify the notations, we will denote θ for θ(x, y) and cos θ
for cos(θ(φ(x), φ(y))), thereafter.

Normalized Kernels as Similarity Indices

365

2.2 Kernel Normalization of Order t

The main purpose of this paper is to introduce a new family of kernel nor-
malization approaches that generalizes the cosine normalization. Our approach
amounts to integrating another geometrical based measure that allows us to re-
ﬁne the cosine measure K 0. This additional feature is related to the diﬀerence
between the norms of the two vectors in the feature space.

These normalization procedures involve generalized mean (also known as
power mean) operators which generalize the classical arithmetic mean. Given
i=1 = {a1, a2, . . . , ap}, the generalized mean with
a sequence of p values {ai}p
exponent t is given by:

Mt(a1, . . . , ap) =

(cid:7) 1

t

(cid:6)

1
p

p(cid:2)

i=1

at
i

(5)

Famous particular cases of (5) are given by t = −1, t → 0 and t = 1 which are
respectively the harmonic, geometric and arithmetic means.

Deﬁnition 2 (Kernels normalization of order t > 0). Given a kernel func-
tion K, the normalized kernel of order t > 0 for any two objects x and y of X ,
is denoted K t(x, y) and is deﬁned as follows:

K(x, y)

K t(x, y) =

Mt(K(x, x), K(y, y))

(6)
Similarly to the cosine normalization, K t(x, x) = 1 for all x ∈ X . As a result,
those normalization methods also amount to projecting the objects from the
feature space to an unit hypersphere. However, this family of normalized ker-
nels goes beyond the cosine measure since it extends the latter which actually
corresponds to the limit case t → 0.

In order to better interpret such measures, let us equivalently formulate
(cid:3)φ(y)(cid:3)
(cid:3)φ(x)(cid:3).

K t(x, y) with respect to the following norm ratio measures,
We can easily show that:

(cid:3)φ(x)(cid:3)
(cid:3)φ(y)(cid:3) and

K t(x, y) =

Mt

(cid:3)

cos θ
(cid:3)φ(x)(cid:3)
(cid:3)φ(y)(cid:3) , (cid:3)φ(y)(cid:3)
(cid:3)φ(x)(cid:3)

(cid:4)

(7)

This formulation expresses K t(x, y) according to geometrical based measures.
However, let us introduce the following notation as well:

(cid:8)(cid:7)φ(x)(cid:7)
(cid:7)φ(y)(cid:7) ,

(cid:7)φ(y)(cid:7)
(cid:7)φ(x)(cid:7)

γ(φ(x), φ(y)) = max

(8)
γ(φ(x), φ(y)) lies within [1, +∞[ and is related to the diﬀerence between the
norm measures of φ(x) and φ(y). γ(φ(x), φ(y)) = 1 means that (cid:7)φ(x)(cid:7) = (cid:7)φ(y)(cid:7)
and the greater the diﬀerence between the norms’ value, the higher γ(φ(x), φ(y)).

=

(cid:9)

max((cid:7)φ(x)(cid:7),(cid:7)φ(y)(cid:7))
min((cid:7)φ(x)(cid:7),(cid:7)φ(y)(cid:7))

366

J. Ah-Pine

Similarly to the angular measure, we will denote γ for γ(φ(x), φ(y)) in order to
simplify the notations. Using γ, we have the diﬀerent formulations below:

K t(x, y) = K t(θ, γ) =

cos θ

Mt(γ, γ−1)

= cos θ

21/tγ

(1 + γ2t)1/t

(9)

(cid:8)

(cid:9)

.

The latter relation expresses K t(x, y) as a multiplication between two factors.
On the one hand, we have the cosine index cos θ and on the other hand, we have
the following term which is only dependent on γ,
Following (9), we observe that ∀θ : cos θ ∈ [−1, 1] and ∀t > 0,∀γ ≥ 1 :
(cid:3)
∈]0, 1]. As a result, one can see that ∀t > 0,∀(x, y)∈X 2 : K t(x, y) ∈
1/tγ
2
[−1, 1].

(1+γ2t)1/t

(1+γ2t)1/t

1/tγ
2

(cid:3)

(cid:4)

(cid:4)

In what follows, we detail the roles the geometrical parameters θ and γ
play with respect to the introduced normalized kernels of order t. First, since
∀(φ(x), φ(y)) ∈ F 2 : γ ≥ 1; K t(θ, γ) is clearly a monotonically increasing func-
tion with respect to cos θ1. Then, using (9), we can better underline the eﬀect
of the norm ratio γ on K t(θ, γ). Indeed, by computing the ﬁrst derivative with
respect to this parameter, we obtain:

∂K t
∂γ = cos θ

⎡
⎣21/t(1 + γ2t)1/t

(cid:3)
1 − 2γ

2t

(1+γ2t)

(cid:4)

⎤
⎦

(1 + γ2t)2/t

(10)

2t

2t

(cid:4)
, is negative since

With the following conditions, γ ≥ 1, t > 0, one can verify that in the numer-
ator of the second term of (10), the ﬁrst factor is positive but the second one,
(cid:3)
1 − 2γ
∂γ is
the same as − cos θ. Therefore, for t > 0, K t(θ, γ) is monotonically decreasing
with respect to γ providing that cos θ > 0 whereas, K t(θ, γ) is monotonically
increasing with respect to γ as long as cos θ < 0 (see Fig. 1).

∈ [1, 2[. Consequently, the sign of ∂K t

(1+γ2t)

(1+γ2t)

2γ

Intuitively, when t > 0, the norm ratio measure aims at reﬁning the cosine
measure considering that the latter is less and less “reliable” for measuring prox-
imity, as the diﬀerence between the vectors’ norms becomes larger and larger.
Thereby, regardless the sign of the cosine index, the greater γ, the closer to 0
K t(θ, γ). More formally, we have: ∀t > 0,∀θ, limγ→+∞ K t(θ, γ) = 0.
Notice that for t < 0, we observe the opposite eﬀect since the sign of the
derivative ∂K t
is the same as cos θ. Thus, in that case, when cos θ > 0 for
example, K t(θ, γ) is monotonically increasing with respect to γ. This is not
a suitable behavior for a similarity measure, that’s the reason why we deﬁne
K t(θ, γ) for t > 0 only.

∂γ

In more general terms, the sign and the value of t respectively express the
nature and the degree of the inﬂuence of γ on K t(θ, γ). First, when t is negative,
it deﬁnes a coeﬃcient which is not appropriate for measuring similarity. On

1 Or a monotonically decreasing function with respect to θ as cos θ is a monotonically

decreasing function of θ.

Normalized Kernels as Similarity Indices

367

K0
K1
K10
K100
K+∞

−0.5

−0.55

−0.6

−0.65

−0.7

−0.75

−0.8

−0.85

−0.9

−0.95

 

1
−
=
 
θ
 
s
o
c
 

h

t
i

w
 
s
e
u
a
v
 
l

l

e
n
r
e
k
 

d
e
z

i
l

a
m
r
o
N

−1
1

1.2

1.4

1.6

γ∈ [1,2]

1.8

2

 

1
=
 
θ
 
s
o
c
 

h

t
i

w
 
s
e
u
a
v
 
l

l

e
n
r
e
k
 

d
e
z

i
l

a
m
r
o
N

1

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55

0.5
1

K0
K1
K10
K100
K+∞

1.2

1.4

1.6

γ∈ [1,2]

1.8

2

Fig. 1. Curves respectively representing diﬀerent values of K t(−1, γ) and K t(1, γ) for
diﬀerent t

the contrary, when t is positive it allows one to reﬁne the cosine measure in
an appropriate way. Second, assuming that t > 0, when the latter increases, it
makes γ have a more and more important impact on K t(θ, γ). In that context,
it is worthwhile to mention the two following limit cases: when t → 0 we obtain
the cosine index which is independent of γ, whereas when t → +∞ we have the
following index2:

K +∞

(x, y) =

(cid:3)

cos θ
(cid:3)φ(x)(cid:3)
(cid:3)φ(y)(cid:3) , (cid:3)φ(y)(cid:3)
(cid:3)φ(x)(cid:3)

(cid:4) =

cos θ

γ

.

max

(11)

To illustrate these points, we plotted in Fig. 1, the graphs corresponding to
K t(θ, γ) for diﬀerent values of t namely the limit when t → 0, t = 1, t = 10,
t = 100 and the limit when t → +∞. In the left-hand side graph, we ﬁxed
cos θ = −1 and in the right-hand side graph, cos θ is set to 1. While the cosine
measure is ﬁxed, γ varies from 1 to 2 (the horizontal axis). The goal of these
graphs is to represent the eﬀects of the norm ratio parameter γ on the normalized
kernel value (the vertical axis) for diﬀerent values of t and depending on the sign
of the cosine measure. For example, when cos θ = 1 and γ = 2, we can observe
that, in comparison with the case t → 0 for which the normalized kernel gives 1,
the value drops to 0.85 when t = 1 and it drops further to 0.5 when t → +∞.

3 Properties of Normalized Kernels as Similarity Indices

In this section, we want to better characterize the family of normalized kernels
that we have introduced. To this end, we give some relevant properties that they
respect. First, we show that K t(x, y) with t > 0 satisﬁes the basic axioms of
geometrical based similarity indices. Second, we show that the normalized kernels
of order t > 0 are kernels. This result is the main theoretical contribution of this
paper.
2 Since we have, limt→+∞ Mt(a1, . . . , ap) = max(a1, . . . , ap)

368

J. Ah-Pine

3.1 Basic Properties of K t

We start by giving some basic properties of K t(x, y) with respect to the general
deﬁnition of similarity indices deﬁned in a metric space (see for example [6]).
∀(x, y) ∈ X 2, we have:
1 ∀t > 0 : |K t(x, y)| ≤ 1
2 ∀t > 0 : K t(x, x) = 1
3 ∀t > 0 : K t(x, y) = K t(y, x)
4 ∀t > 0 : x = y ⇔ K t(x, y) = 1
According to property 1, K t(x, y) is bounded by −1 and 1 which is a common
axiom required for a similarity index. Notice that unnormalized kernels do not
respect this axiom in general.

Properties 2 and 3 respectively state that the normalized kernel of order t

respects the maximal self-similarity axiom3 and the symmetric axiom.

According to property 4, the situation K t(x, y) = 1 corresponds to the case
where the vectors are strictly identical in the feature space. Indeed, considering
(9), we can see that the normalized kernel value is 1 if and only if cos θ = 1 and
γ = 1 which is the same as φ(x) = φ(y). Note that property 4 is not true for
the limit case t → 0 for which we only have φ(x) = φ(y) ⇒ K 0(x, y) = 1. Since
for this case the norm ratio plays no role, it is only suﬃcient for the vectors to
be positively colinear to obtain the maximal similarity value 1. Once again, this
shows that the normalized kernels of order t > 0 are similarity measures that are
more discriminative than the cosine measure. It is also worth mentioning in that
context, the two other particular cases: both vectors are completely opposite to
each other when we observe K t(x, y) = −1 (cos θ = −1 and γ = 1) and they are
geometrically orthogonal when K t(x, y) = 0 (cos θ = 0).
kernels of two distinct orders t and t(cid:6).
5 ∀t ≥ t(cid:6) > 0 : sign(K t(x, y)) = sign(K t
K t(x, y) ≤ K t
6 ∀t ≥ t(cid:6) > 0 :
K t(x, y) ≥ K t
7 ∀t ≥ t(cid:6) > 0 : |K t(x, y)| ≤ |K t
Property 5 states that the sign of the similarity measure is independent of t.
More precisely, the sign is only dependent on the angular measure. This is a
consequence of (9) since Mt(γ, γ−1) is strictly positive.

In what follows, we focus on the diﬀerent relationships between normalized

(x, y))
if cos θ > 0
if cos θ < 0

(x, y)
(x, y)
(x, y)|

(cid:14)

Properties 6 and 7 must be put into relation with the comments we made
in section 2 and Fig. 1. Accordingly, these properties formally claim that as t
grows, K t(x, y) makes the cosine measure less and less “reliable”. We previously
mentioned that, all other things being equal, the greater the diﬀerence between
the vectors’ norms in the feature space, the closer to 0 the normalized kernel

(cid:2)

(cid:2)
(cid:2)

(cid:2)

3 Since property 1 states that 1 is the maximal similarity value. Note that this axiom
is also called minimality when dealing with dissimilarity rather than similarity [6].

Normalized Kernels as Similarity Indices

369

value. These properties express the fact that this convergence is faster and faster
as t grows.
lations between generalized means, ∀0 < t(cid:6) ≤ t < ∞:

These aforementioned properties are direct consequences of the following re-

(cid:15)

(

p
i=1

1
ai)1/p

>

1

Mt(cid:2)({ai}p

i=1)

≥

1

Mt({ai}p

i=1)

>

1

max({ai}p

i=1)

To complete the analysis of the basic properties that K t(x, y) respects with
regards to the general axioms required for a similarity index, we need to better
characterize the metric properties of the latter. We address this issue in the
following subsection.

3.2 Metric Properties of K t
In this paragraph we denote K t the similarity matrix of objects in X .
Theorem 2. The similarity matrix K t with t > 0 and general term given by (6)
is positive semi-deﬁnite. In other words, any normalized kernel K t with t > 0 is
a positive semi-deﬁnite kernel.

Proof (Proof of Theorem 2).
The proof of this result is based on Gershgorin circle theorem. Let A be a (n× n)
complex matrix with general term Aii(cid:2). For each row i = 1, . . . , n, its associated
Gershgorin disk denoted Di is deﬁned in the complex plane as follows:

Di = {z ∈ C : |z − Aii| ≤ (cid:2)
i(cid:2)(cid:7)=i
(cid:16)

|Aii(cid:2)|
} = D(Aii, Ri)
(cid:19)
(cid:17)(cid:18)

Ri

(cid:20)

i

Given this deﬁnition, Gershgorin circle theorem (see for example [7]) states that
Di. Accordingly, if the norms of oﬀ-diagonal
all eigenvalues of A lies within
elements of A are small enough then the eigenvalues are not “far” from the
diagonal elements. In other words, the lower the Ri quantities, the closer to the
diagonal elements the eigenvalues.
Now, let us consider normalized kernel matrices of order t > 0 of objects of
{xi; i = 1, . . . , n}. Let denote K t
ii(cid:2) = K t(xi, xi(cid:2)). We ﬁrst suppose the limit case
t(cid:6) → 0. We know that K 0 is the cosine similarity matrix. As a consequence, K 0 is
positive semi-deﬁnite and its eigenvalues are all non negative. Next, let us denote
ii(cid:2)|. Then according to properties 2 and 7 given in subsection 3.1,
Rt
we have:
– ∀t > 0 and ∀i = 1, . . . , n : K t
ii = 1,
– ∀t > t(cid:6) > 0 and ∀i = 1, . . . , n : Rt
Therefore, when t grows, (6) deﬁnes a continuous and diﬀerentiable operator for
which the absolute value of oﬀ-diagonal elements of K t, and consequently the

≤ Rt
(cid:2)
i .

|K t

i =

i(cid:2)(cid:7)=i

(cid:21)

i

370

J. Ah-Pine

quantities Rt
i; i = 1, . . . , n, are lower and lower while the diagonal entries of K t
remain equal to 1. Thus, applying Gershgorin theorem, we can see that when
t increases, the spectrum of K t is closer and closer to the vector of ones with
dimension n. As a consequence, since the eigenvalues of K 0 are non negative then
so are the eigenvalues of K t with t > t(cid:6) > 0 as the latter are closer to 1 than the
former. Finally, for t > 0, K t is symmetric and has non negative eigenvalues.
These properties are equivalent to the conditions mentioned in Deﬁnition 1 thus,
(cid:13)(cid:14)
for t > 0, we can conclude that K t are positive semi-deﬁnite kernels.
(cid:5)
Finally, a corollary of Theorem 2 [8], is that the related distance Dt(x, y) =
2(1 − K t(x, y)) respects the triangle inequality axiom, ∀(x, y, z) ∈ X 3:
8 ∀t > 0 : Dt(x, y) ≤ Dt(x, z) + Dt(z, y)

4 Applications to Clustering Tasks

In order to illustrate the potential beneﬁts of our proposal, we tested diﬀerent
normalized kernels of order t > 0 in the context of clustering tasks. The clustering
algorithm we used is based on kernel Principal Component Analysis (kernel
PCA) and the k-means algorithm. Our experiments concern 5 real-world datasets
from the UCI ML repository [5]. Our purpose is to show that the normalized
kernels that we have introduced, can better capture the proximity relationships
between objects compared with other state-of-the-art normalized kernels.

4.1 Kernel PCA Based k-means Clustering
Our clustering approach is a spectral clustering like algorithm (see for example
[9]). First, from a kernel matrix K t, we proceed to its eigen-decomposition in
order to extract from the implicit high dimensional feature space F, an explicit
and proper low dimensional representation of the data. Then, we apply a k-means
algorithm in that reduced space in order to ﬁnd a partition of the objects.

√
λ2v2, . . . ,

Principal Component Analysis (PCA) is a powerful and widely used statistical
technique for dimension reduction and features extraction. This technique was
extended to kernels in [3]. Formally, let denote λ1 ≥ λ2 ≥ . . . ≥ λk−1 the leading
k−1 eigenvalues of K t and v1, v2, . . . , vk−1 the corresponding eigenvectors. The
low dimensional space extracted from K t that we used as data representation
√
for the clustering step is spanned according to:
(
λ1v1,
When applying dimension reduction techniques prior to a clustering algo-
rithm, an important issue is the number of dimensions that one has to retain.
In this paper, since we aim at using the k-means algorithm as the clustering
method, we follow the work presented in [4] that concerns the relationship be-
tween k-means and PCA. Accordingly, if k is the number of clusters to ﬁnd then
we retain the k − 1 leading eigenvectors.

(cid:5)

λk−1vk−1).

With regards to related works, we can cite the following papers that use Kernel
PCA based clustering in the contexts of image and text analysis respectively,
[10,11].

Normalized Kernels as Similarity Indices

371

4.2 Experiments Settings

The datasets that we used in our experiments are the following ones [5]:

– Iris (150 objects, 4 features in the input space, 3 clusters)
– Ecoli (336 objects, 7 features in the input space, 8 clusters)
– Pima Indian Diabetes (768 objects, 8 features in the input space, 2 clusters)
– Yeast (1484 objects, 8 features in the input space, 8 clusters)
– Image Segmentation (2310 objects, 18 features in the input space, 7 clusters)

For each data set, we ﬁrst normalized the data in the input space by centering
and standardizing the features. Next, we applied diﬀerent kernels K namely,
the linear kernel Kl(x, y) = (cid:4)x, y(cid:5) which is simply the dot product in the input
space, and the polynomial kernel Kp(x, y) = ((cid:4)x, y(cid:5) + 1)2. For each type of
kernels, we computed diﬀerent normalized kernels K t by varying the value of t:
t → 0, t = 1, t = 10, and the limit t → +∞. To each of those similarity matrices,
we applied the kernel PCA clustering method described previously. Since the k-
means algorithm is sensitive with respect to the initialization, for all cases we
launched the algorithm 5 times with diﬀerent random seeds and took the mean
average value of the assessment measures.

Since we deal with normalized kernels that amount to projecting the objects
on an unit hypersphere, we also tested the kernel PCA based k-means clustering
with the Gaussian RBF kernel which presents the same property. This case is our
ﬁrst baseline. However, when using such a kernel, one has to tune a parameter
σ. In this paper, to get rid of this problem, we applied the approach proposed
in [12] which suggests to use the following aﬃnity measure:

Kg(x, y) = exp

(cid:8)
−(cid:7)x − y(cid:7)2

(cid:9)

σxσy

(12)

where σx is set to the value of the distance between x and its 7th nearest
neighbor.

Besides, as our purpose is to show that taking into account the mapped vec-
tors’ norm ratio in addition to the cosine measure can be beneﬁcial, we took the
case t → 0, which simply corresponds to the cosine index, as a baseline as well.
The assessment measure of the clustering outputs we used is the Normalized
Mutual Information (NMI) introduced in [13]. Let denote U the partition found
by the kernel PCA clustering and V the ground-truth partition. This evaluation
measure is denoted NMI(U, V ) and is given by:

NMI(U, V ) =

(cid:22)(cid:3)(cid:21)

(cid:21)

(cid:21)

k
u=1

k
u=1

Nu. log

k
v=1
(cid:23)

Nuv log
(cid:24)(cid:4) (cid:3)(cid:21)

Nu.

n

(cid:3)

(cid:4)

nNuv
Nu.N.v

k
v=1

N.v log

(cid:24)(cid:4)

(cid:23)

N.v
n

(13)

where N is the contingency table between U and V and n the total number
of objects. NMI(U, V ) lies in [0, 1] and the higher the measure, the better the
clustering output U.

372

J. Ah-Pine

l

e
u
a
v
 
I

M
N

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

Kg
0
Kl
1
Kl
10
Kl
∞
Kl

Iris

Ecoli

Pi. Ind. Diab. Yeast

Img. Seg.

l

e
u
a
v
 
I

M
N

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

Kg
0
Kp
1
Kp
10
Kp
∞
Kp

Iris

Ecoli

Pi. Ind. Diab. Yeast

Img. Seg.

Fig. 2. NMI measures for the diﬀerent datasets with diﬀerent kernels (Kl on the left
and Kp on the right) and diﬀerent normalizations (t → 0,t = 1,t = 10,t → +∞)

4.3 Experiments Results

In Fig. 2, we report the results we obtained for each dataset. On the left-hand
side we give the results related to the linear kernel whereas on the right-hand
side, the NMI values correspond to the polynomial kernel. In both graphs, the
results provided by the Gaussian RBF kernel are shown (1st bar). Compared to
this baseline we can see that the cosine measure and the normalized kernels all
perform better on the datasets used in these experiments.

In comparison with the other baseline K 0, we can observe that in most cases
there are normalized kernels of order t > 0 which can lead to better NMI val-
ues. When using the linear kernel, this is true for all K t except for the Image
Segmentation dataset. Furthermore, for the Iris, Ecoli and Yeast datasets, as
t grows the performances are consistently better. This shows that taking into
account the vectors’ norm ratio in the feature space in order to reﬁne the cosine
measure, is beneﬁcial.

In the case of polynomial kernel, not all normalization techniques are inter-
esting since many normalized kernels do not outperform the cosine measure.
p as a normalization
However, when using this kernel, it seems that taking K 1
method is a good choice. Particularly, K 1
p leads to the best performances for
the Iris and the Pima Indian Diabetes datasets. Besides, concerning the Image
Segmentation dataset, we can see that unlike the linear kernel, the normalized
polynomial kernels can outperform the cosine index since the best result is ob-
p .
tained with K 10

5 Conclusion and Future Work

We have introduced a new family of normalization methods for kernels which
extend the cosine normalization. We have detailed the diﬀerent properties of
such methods and the resulting proximity measures with respect to the basic
axioms of geometrical based similarity indices. Accordingly, we have shown that

Normalized Kernels as Similarity Indices

373

normalized kernels are “proper” similarity indices that amount to projecting the
data on an unit hypersphere. We have, in addition, proved that these normalized
kernels are also kernels. From a practical standpoint, we have also validated the
utility of normalized kernels in the context of clustering tasks using several real-
world datasets. However, one remaining issue is the choice of the order t when
normalizing a kernel. We have shown from a theoretical and a practical point of
view that the norm ratio measure can make the normalized kernel more eﬃcient
but still, the weight one should give to this parameter in comparison with the
angular measure is not straightforward to set. In our future work we intend to
further investigate this problem.

References

1. Scholkopf, B., Smola, A.J.: Learning with Kernels: Support Vector Machines, Reg-

ularization, Optimization, and Beyond. MIT Press, Cambridge (2001)

2. Vert, J., Tsuda, K., Schlkopf, B.: A primer on kernel methods. In: Schlkopf, B.,
Vert, J.P., Tsuda, K. (eds.) Kernel Methods in Computational Biology, pp. 35–70.
MIT Press, Cambridge (2004)

3. Sch¨olkopf, B., Smola, A., M¨uller, K.R.: Nonlinear component analysis as a kernel

eigenvalue problem. Neural Comput. 10(5), 1299–1319 (1998)

4. Ding, C., He, X.: K-means clustering via principal component analysis. In: ICML
’04: Proceedings of the twenty-ﬁrst international conference on Machine learning,
p. 29. ACM, New York (2004)

5. Asuncion, A., Newman, D.: UCI machine learning repository (2007)
6. Santini, S., Jain, R.: Similarity measures. IEEE Transactions on Pattern Analysis

and Machine Intelligence 21(9), 871–883 (1999)

7. Horn, R., Johnson, C.: Matrix analysis. Cambridge University Press, Cambridge

(1985)

8. Gower, J., Legendre, P.: Metric and euclidean properties of dissimilarity coeﬃ-

cients. Journal of classiﬁcation 3, 5–48 (1986)

9. Filippone, M., Camastra, F., Masulli, F., Rovetta, S.: A survey of kernel and spec-

tral methods for clustering. Pattern Recogn. 41(1), 176–190 (2008)

10. Tuytelaars, T., Lampert, C.H., Blaschko, M.B., Buntine, W.: Unsupervised object
discovery: A comparison. International Journal of Computer Vision Epub ahead,
1–19 (July 2009)

11. Minier, Z., Csat´o, L.: Kernel PCA based clustering for inducing features in text

categorization. In: ESANN, pp. 349–354 (2007)

12. Zelnik-Manor, L., Perona, P.: Self-tuning spectral clustering. In: Advances in Neu-

ral Information Processing Systems, vol. 17 (2005)

13. Strehl, A., Strehl, E., Ghosh, J.: Cluster ensembles - a knowledge reuse framework
for combining partitionings. Journal of Machine Learning Research 3, 583–617
(2002)


