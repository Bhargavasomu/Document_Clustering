Online Sampling of High Centrality Individuals

in Social Networks

Arun S. Maiya and Tanya Y. Berger-Wolf

Department of Computer Science
University of Illinois at Chicago
{amaiya2,tanyabw}@uic.edu

851 S. Morgan, Chicago, IL 60607, USA

Abstract. In this work, we investigate the use of online or “crawling”
algorithms to sample large social networks in order to determine the
most inﬂuential or important individuals within the network (by varying
deﬁnitions of network centrality). We describe a novel sampling technique
based on concepts from expander graphs. We empirically evaluate this
method in addition to other online sampling strategies on several real-
world social networks. We ﬁnd that, by sampling nodes to maximize the
expansion of the sample, we are able to approximate the set of most
inﬂuential individuals across multiple measures of centrality.

1 Introduction and Motivation

Given a large or even massive social network, how can one eﬃciently identify
the most important or inﬂuential individuals without complete access to the
entire network at once? This scenario arises when the network is too large for
conventional analysis to be computationally feasible. It may also arise in the
context of mining data from a network whose complete structure is hidden from
public view (such as a friendship network in Web-based social media) or has a
highly distributed structure (such as the network of blogs or the Web itself).
The eﬃcient identiﬁcation of inﬂuential individuals (by varying deﬁnitions of
inﬂuence) in large social networks has many applications, from the prevention
of computer worms to viral marketing. In this work, we investigate the use of
online sampling in identifying such critical individuals.

Online Sampling of Centrality. One of the key tasks in social network analy-
sis is determining the relative importance of individuals based on their positions
in the structure of the network [1, 2]. This is referred to as the centrality of
individuals, and there are many notions of what it means to be central to a
network [2]. In a sampling approach to centrality approximation, a subset of
the individuals in the network is sampled, and an induced subgraph consisting
only of these individuals and the links among them is produced. The centrality
computation, then, is performed on this induced subgraph instead of the entire
network, with the centrality scores of the sample being used as approximations
of the true centrality of sampled individuals. It is clear that, for this approach to

M.J. Zaki et al. (Eds.): PAKDD 2010, Part I, LNAI 6118, pp. 91–98, 2010.
c(cid:2) Springer-Verlag Berlin Heidelberg 2010

92

A.S. Maiya and T.Y. Berger-Wolf

be useful, two criteria must be met. First, the sampling method must produce
an induced subgraph that is representative of centrality in the original network
(e.g. the centrality ranking in the sample should be consistent with that of the
original network). Ideally, the sampling method will quickly ﬁnd high centrality
nodes, and these nodes will also be ranked highly when computing centrality on
the sample. A second criterion is that the sampling method must be an online
algorithm, since the network may be too large for its global structure to be ac-
cessed in its entirety or the global view of the entire network may be limited.
By online, we mean that the sampling is produced in an iterative, sequential
manner, without a priori access to the entire input (i.e., sampling via crawling);
at each iteration the new addition to the sample is based on the properties of the
nodes crawled thus far. The next node selected for inclusion in the sample is al-
ways chosen from the set of nodes connected directly to the current sample. This
is also referred to as snowball sampling or neighborhood sampling. In this work,
we systematically investigate the task of online sampling of centrality in large
social networks. We show that, by sampling nodes to maximize the expansion of
the sample, the set of most inﬂuential individuals can be approximated across
three diﬀerent centrality measures: betweenness, closeness, and eigenvector cen-
trality. Remarkably, these sets of top ranked individuals can be approximated
reasonably well with sample sizes as small as 1%.

2 Background and Related Work

Centrality in Networks. The idea that the structural position of an individual
in a social network may be correlated with the relative inﬂuence or importance
of that individual was ﬁrst postulated by Bavelas in the 1940s in the context of
organizational communication [3,1]. Since then, many notions of what it means
to be important or central in a network have been proposed and applied with
great success in a variety of diﬀerent contexts (e.g. [4,5]). In this paper we focus
on three widely-used measures: betweenness centrality, closeness centrality, and
eigenvector centrality. The betweenness of a node is deﬁned as the fraction of the
overall shortest paths passing through a particular node [6,7,1]. The closeness of
an individual in a network is a function of the inverse of the average distance to
every other individual [1]. Finally, eigenvector centrality is a measure of prestige
or popularity proposed by Bonacich [8]. When respresenting the entire network
(or graph) as an adjacency matrix A, the eigenvector centrality of individuals
in the network is the eigenvector x of matrix A corresponding to the largest
eigenvalue λ [8]. The PageRank measure [5] used by Google to rank search
results is, in fact, simply a variant of eigenvector centrality and has been shown
to be highly eﬀective in approximating the prestige and authority of Web pages.

Related Work. Web crawlers, programs that traverse the Web and index pages
for search engines in an automated manner, can be viewed as online sampling
algorithms. Although the goal of Web crawlers is to collect and store the Web link
graph for oﬄine processing, it is highly advantageous for crawling algorithms to

Online Sampling of High Centrality Individuals in Social Networks

93

seek out high PageRank pages early in the crawl [9]. As a result, there have been
several studies evaluating these algorithms in their ability to sample PageRank
(e.g. [9,10,11,12]). In Section 4.2, we evaluate the performance of several of these
algorithms in the context of undirected social networks and in their ability to
sample alternative notions of network centrality. Finally, also related to this work
are the existing studies on representative subgraph sampling such as [13,14,15].

3 Proposed Method

We employ an online sampling algorithm to sample individuals in the social
network. Let G = (V, E) be a network or graph where V is set of vertices (or
nodes) and E ⊆ V ×V is a set of edges (or links between the nodes). We begin by
selecting a single individual v ∈ V uniformly at random and specifying a desired
sample size k where k (cid:4) V . The sample S ⊂ V , then, is initialized to {v}. Each
subsequent individual selected for inclusion in the sample is chosen from the
current neighborhood N(S), where N(S) = {w ∈ V − S : ∃v ∈ S s.t. (v, w) ∈
E}. Next, upon constructing the sample S, a centrality measure is computed on
the induced subgraph of the sample, G(S). The critical question is how to select
individuals from N(S) such that:

1. The ranking of individuals by centrality scores in G(S) corresponds to the
ranking of individuals in G (most importantly for the highest centrality
individuals, as they are generally of greater interest).

2. The highest centrality individuals are quickly included in the sample.

Moreover, as an online algorithm, these selection decisions must be made solely
on the basis of local information (i.e. information obtained only from those indi-
viduals already crawled). In the following sections, we describe several diﬀerent
approaches to online sampling.

Expansion Sampling. We now describe a novel sampling technique based
on the concept of expansion in graphs [16]. We refer to this method as expan-
sion sampling (XS). The expansion of a sample S is deﬁned as
. In this
approach, we seek out the sample S of size k with the maximal expansion:
argmaxS: |S|=k
. We propose a simple algorithm that greedily selects nodes
in order to maximize the expansion of the current sample. That is, the next
node v selected for inclusion in the sample is chosen based on the expression:
argmaxv∈N (S)

|N({v}) − (N(S) ∪ S)|.

|N (S)|
|S|

|N (S)|
|S|

Web Crawlers. As mentioned previously, the process of web crawling is essen-
tially an online sampling process. There are several crawlers explicitly designed
to include high PageRank nodes into the sample more quickly. One such ap-
proach is referred to as Backlink Count (BLC) in which the next node selected
for inclusion into the sample is the node with the most links to nodes already in
the sample [11]. A second approach is referred as the OPIC algorithm [10]. In
this approach, all individuals are assigned a default “cash” value. When a node

94

A.S. Maiya and T.Y. Berger-Wolf

is included in the sample, its cash is distributed to its neighbors in equal pro-
portion. The next node selected for inclusion into the sample, then, is a function
of the sum of cash a node has received from its neighbors. We evaluate both
these methods not only in their ability to sample PageRank (or Eigenvector
Centrality), but also other centrality measures.

BFS, DFS, and Random Walks. As a basis for comparison, we evaluate
the performance of several basic approaches to online sampling. Speciﬁcally,
we evaluate the breadth-ﬁrst search (BFS), the depth-ﬁrst search (DFS), and
sampling based on a random walk (RW) of the social network.

4 Evaluation

4.1 Experimental Setup

Datasets. We evaluate four diverse, real-world social networks1. These include a
co-authorship network (Cond-Mat [17]), an email network (Enron [18]), an online
trust network (Epinions [19]), and an online social network (Slashdot [20]).

Sampling Methodology. As described earlier, our aim is to sample a minute
fraction of the individuals in the original network in a way that the individuals
ranked highly in the entire network are both present and ranked highly in the
induced subgraph of the sample. We execute each sampling algorithm on each
dataset and sample up to 5% of the nodes. Ten samples are produced by each
algorithm on each dataset. During the sampling process, at one percent inter-
vals (e.g. 1%, 2%, . . . , 5%), we compute centrality and evaluation statistics on
the induced subgraph of the sample and compare those with the original net-
work (evaluation criteria are described in the next section). When evaluating
eigenvector centrality, we employ the PageRank variant (PageRank is a variant
of eigenvector centrality, as described in Section 2).

Measuring Sample Quality. We employ two evaluation criteria to measure
the quality of samples. First, we perform a rank correlation between the central-
ity ranking in the sample and the true centrality ranking in the original network
using Kendall’s Tau [21]. This measure (which ranges in value from −1 to 1)
evaluates the extent to which the relative ordering by centrality of all nodes in
the sample is consistent with that of the original network. A value of 1 indi-
cates the rankings are perfectly consistent, and a value of −1 indicates they are
inversely consistent. However, in most cases, it is the set of high centrality indi-
viduals in the network that is of the most interest. A sample exhibiting a high
rank correlation, but consisting only of low centrality individuals is not of inter-
est in most cases. Therefore, we employ a second, more informative, evaluation
criterion based on the Jaccard measure of set similarity [22], which is the size of

1 For all networks, we extract the giant component as an undirected, unweighted

graph.

Online Sampling of High Centrality Individuals in Social Networks

95

the intersection of two sets divided by the size of the union. A Jaccard similarity
of 1 indicates that all the elements are shared between the sets, and a score of
0 indicates that none of the elements are shared. We take the top k individuals
in the sample centrality ranking and the top k individuals in the ranking of the
original network and measure the Jaccard set similarity. This measure indicates
how well each sampling algorithm is able to determine the identity of the top k
highest centrality individuals. For our experiments, we use k = 50.

4.2 Experimental Results

Identifying High Betweenness Individuals. Figure 1a shows the extent to
which each sampling algorithm is able to identify individuals with the highest
betweenness scores in the network. On all datasets, the expansion sampling al-
gorithm consistently (and signiﬁcantly) outperforms all other approaches. Recall
that, in expansion sampling, nodes selected for inclusion in the sample are those
|N({v})−
that maximize the expansion of the sample at each step: argmaxv∈N (S)
(N(S)∪ S)|. By sampling nodes that contribute most to the expansion, the algo-
rithm seeks out individuals with the most dissimilar neighborhood. These nodes,
then, should be brokers or bridges between diﬀerent neighborhoods or clusters
of individuals, which captures the notion of betweenness.

The random walk sampling, in some cases, also identiﬁes high betweenness
fairly well (though, not as well as expansion sampling). Intuitively, high between-
ness individuals will appear with high frequency on the paths between other
individuals. Performing a random walk, then, should tend to traverse through
high betweenness nodes, thereby, including them in the sample. Finally, we see
that BFS and OPIC sampling perform particularly poorly when being used to
ﬁnd high betweenness individuals.

Identifying High Closeness Individuals. As shown in 1b, expansion sam-
pling also outperforms other methods in its ability to sample high closeness
individuals. Recall that closeness centrality is a measure of how close an individ-
ual is to all other individuals in the network. Why should expansion sampling
also perform best on this centrality measure? Consider a sample S with high
is signiﬁcantly large). This means that the sample S, as a
expansion (i.e.
whole, is one hop away from many other individuals in the network, which, by
deﬁnition, is closeness. In addition, as with betweenness, BFS sampling again
performs poorly in identifying high closeness individuals.

|N (S)|
|S|

Identifying High PageRank Individuals. Figure 1c shows the performance
of each sampling technique in sampling PageRank. Surprisingly, expansion sam-
pling again outperforms other methods (although, not as dramatically as with
betweenness and closeness). This result is quite unexpected, as the Web crawling
algorithms have been speciﬁcally designed to sample high PageRank individuals
quickly. Moreover, one would expect a random walk sampling strategy to per-
form much better (if not the best) because, by the very formulation of PageRank
centrality, high PageRank individuals will tend to be visited more frequently by

96

A.S. Maiya and T.Y. Berger-Wolf

XS
BFS
DFS
RW
BLC
OPIC

XS
BFS
DFS
RW
BLC
OPIC

XS
BFS
DFS
RW
BLC
OPIC

y
t
i
r
a

l
i

i

 

m
S
d
r
a
c
c
a
J

y
t
i
r
a

l
i

i

 

m
S
d
r
a
c
c
a
J

1

0.8

0.6

0.4

0.2

0

 
0

1

0.8

0.6

0.4

0.2

0

 
0

1

0.8

0.6

0.4

0.2

y
t
i
r
a

l
i

i

m
S
 
d
r
a
c
c
a
J

0

 
0

Cond−Mat

 

y
t
i
r
a

l
i

i

 

m
S
d
r
a
c
c
a
J

0.01

0.02
0.03
Sample Size

0.04

0.05

Cond−Mat

 

y
t
i
r
a

l
i

i

 

m
S
d
r
a
c
c
a
J

0.01

0.02
0.03
Sample Size

0.04

0.05

Cond−Mat

 

0.01

0.02
0.03
Sample Size

0.04

0.05

1

0.8

0.6

0.4

0.2

0

0

1

0.8

0.6

0.4

0.2

0

0

Enron

Epinions

Slashdot

y
t
i
r
a

l
i

i

 

m
S
d
r
a
c
c
a
J

1

0.8

0.6

0.4

0.2

0

0

0.01

0.02
0.03
Sample Size

0.04

0.05

0.01

0.02
0.03
Sample Size

0.04

0.05

(a) Betweenness

Enron

Epinions

y
t
i
r
a

l
i

i

 

m
S
d
r
a
c
c
a
J

1

0.8

0.6

0.4

0.2

0

0

0.01

0.02
0.03
Sample Size

0.04

0.05

0.01

0.02
0.03
Sample Size

0.04

0.05

y
t
i
r
a

l
i

i

 

m
S
d
r
a
c
c
a
J

y
t
i
r
a

l
i

i

 

m
S
d
r
a
c
c
a
J

1

0.8

0.6

0.4

0.2

0

0

1

0.8

0.6

0.4

0.2

0

0

0.01

0.02
0.03
Sample Size

0.04

0.05

Slashdot

0.01

0.02
0.03
Sample Size

0.04

0.05

(b) Closeness

Enron

Epinions

Slashdot

1

0.8

0.6

0.4

0.2

y
t
i
r
a

l
i

i

m
S
 
d
r
a
c
c
a
J

0

0

1

0.8

0.6

0.4

0.2

y
t
i
r
a

l
i

i

m
S
 
d
r
a
c
c
a
J

0

0

0.01

0.02
0.03
Sample Size

0.04

0.05

(c) PageRank

1

0.8

0.6

0.4

0.2

y
t
i
r
a

l
i

i

m
S
 
d
r
a
c
c
a
J

0

0

0.01

0.02
0.03
Sample Size

0.04

0.05

0.01

0.02
0.03
Sample Size

0.04

0.05

Fig. 1. Jaccard set similarity between Top 50 of original network and Top 50 of sample
for each centrality measure on each dataset. For all datapoints, standard error is very
low, and, for ease of illustration, the standard error bars are omitted. Key: XS =
Expansion Sampling, BFS = Breadth-First Search, DFS = Depth-First Search, RW =
Random Walk, BLC = Backlink Count, OPIC = OPIC algorithm.

a random walker. But, clearly, this property holds only in the limit and not for
a small sample. Overall, once again, it is expansion sampling that identiﬁes high
PageRank individuals most eﬀectively.

On the Concordance Across Centrality Measures. Thus far, we have
shown that expansion sampling performs best in identifying inﬂuential individ-
uals by all three centrality measures evaluated. A single sample produced by
the expansion sampling algorithm, then, includes individuals ranking highly on
multiple centrality measures. This begs the question: to what extent do these dif-
ferent centrality measures coincide or correspond with one another in real-world,
social networks? That is, does expansion sampling simply ﬁnd individuals that
simultaneously rank highly on all three of the centrality measures? Upon closer
inspection, we ﬁnd this to not always be the case. Although we do ﬁnd some
agreement or overlap across the centrality measures (i.e. there exist individuals
ranking highly on multiple measures), expansion sampling does, in fact, ﬁnd indi-
viduals over and above this overlap. We compute the Jaccard similarity between
the sets of the top 50 ranked individuals in the entire network by each centrality
measure (i.e. the similarity between each of the true, global centrality rankings).
For instance, in the Enron dataset, we ﬁnd that the Jaccard similarity of the

Online Sampling of High Centrality Individuals in Social Networks

97

betweenness ranking and the closeness ranking (both of the entire Enron net-
work) is 0.37. However, as shown in Figure 1b, the Jaccard similarity between
the closeness ranking of the entire network and the closeness ranking of the
sample produced by expansion sampling ranges from 0.6 to 0.75 (over and above
0.37). This indicates that the expansion sampling method does, in fact, ﬁnd in-
dividuals ranked highly only on closeness and not on betweenness. Using similar
analysis, expansion sampling also ﬁnds individuals ranking highly on between-
ness and not on closeness. Although expansion sampling does indeed identify
individuals ranking highly on multiple measures, remarkably, this method also
ﬁnds individuals that each rank highly on diﬀerent measures of centrality. This
is striking, as one would not expect a single, biased sampling strategy to be able
to ﬁnd high ranked nodes across multiple (and diverse) measures of centrality.
The expansion sampling method, however, does just this.

Consistency in Relative Ordering of Sample and Original Network. Fi-
nally, we show the Kendall’s Tau rank correlation between the centrality ranking
of the samples and the true centrality ranking in the original network (over all
individuals in the sample). Due to space constraints, we only show the results for
the Enron network in Figure 2. Although results vary slightly across measures
and datasets, all sampling algorithms seem to exhibit a relatively strong consis-
tency in the relative ordering of sampled individuals by centrality in comparison
to the true centrality ranking. The key to eﬀectively identifying the top ranked
individuals, then, is ﬁnding and including them early in the sampling process.
As discussed in Sections 4.2, 4.2, and 4.2, it is expansion sampling that is most
eﬀective in this regard.

Betweenness

 

1

0.8

0.6

0.4

0.2

u
a
T
 
s

’
l
l

a
d
n
e
K

XS
BFS
DFS
RW
BLC
OPIC

0

 
0

0.01

0.02

0.03

Sample Size

0.04

0.05

u
a
T
 
s

’
l
l

a
d
n
e
K

1

0.8

0.6

0.4

0.2

0

0

Closeness

PageRank

u
a
T
 
s

’
l
l

a
d
n
e
K

1

0.8

0.6

0.4

0.2

0

0

0.01

0.02

0.03

0.04

0.05

Sample Size

0.01

0.02

0.03

0.04

0.05

Sample Size

Fig. 2. [Enron Dataset] Kendall’s Tau rank correlation between the sample centrality
rankings and the true ranking in the original network for each centrality measure

5 Conclusion

In this work, we have studied the use of online sampling to identify the set of
individuals exhibiting the highest centrality in large social networks. We showed
that, by sampling nodes to maximize the expansion of the sample, the set of most
inﬂuential individuals can be approximated across multiple centrality measures.
For future work, we plan to investigate the eﬀect of network and graph-theoretic
properties on the performance of these and other sampling strategies.

98

A.S. Maiya and T.Y. Berger-Wolf

References

1. Freeman, L.C.: Centrality in social networks. Social Networks 1, 215–239 (1979)
2. Wasserman, S., Faust, K., Iacobucci, D.: Social Network Analysis: Methods and

Applications. Cambridge University Press, Cambridge (November 1994)

3. Bavelas, A.: Communication patterns in task-oriented groups. J. Acoustical Soc.

of Am. 22(6), 725–730 (1950)

4. Russo, T., Koesten, J.: Prestige, centrality, and learning: A social network analysis

of an online class. Communication Education 54(3), 254–261 (2005)

5. Page, L., Brin, S., Motwani, R., Winograd, T.: The pagerank citation ranking:

Bringing order to the web. Technical report, Stanford InfoLab (1998)

6. Anthonisse, J.: The rush in a graph. Mathematische Centrum, Amsterdam (1971)
7. Freeman, L.: A set of measures of centrality based on betweenness. Sociometry 40,

35–41 (1977)

8. Bonacich, P.: Power and centrality: A family of measures. American J. Sociol-

ogy 92(5), 1170–1182 (1987)

9. Boldi, P., Santini, M., Vigna, S.: Paradoxical eﬀects in pagerank incremental com-

putations. In: Workshop on Web Graphs (2004)

10. Abiteboul, S., Preda, M., Cobena, G.: Adaptive on-line page importance compu-

tation. In: WWW (2003)

11. Cho, J., Molina, H.G., Page, L.: Eﬃcient crawling through url ordering. Computer

Networks and ISDN Systems 30(1-7), 161–172 (1998)

12. Najork, M.: Breadth-ﬁrst search crawling yields high-quality pages. In: WWW 2001

(2001)

13. Leskovec, J., Kleinberg, J., Faloutsos, C.: Graphs over time: densiﬁcation laws,

shrinking diameters and possible explanations. In: KDD 2005 (2005)

14. Krishnamurthy, V., Faloutsos, M., Chrobak, M., Cui, J., Lao, L., Percus, A.: Sam-
pling large internet topologies for simulation purposes. Computer Networks 51(15),
4284–4302 (2007)

15. Hubler, C., Kriegel, H.P., Borgwardt, K., Ghahramani, Z.: Metropolis algorithms

for representative subgraph sampling. In: ICDM 2008 (2008)

16. Hoory, S., Linial, N., Wigderson, A.: Expander graphs and their applications. Bull.

Amer. Math. Soc. 43, 439–561 (2006)

17. Leskovec, J., Kleinberg, J., Faloutsos, C.: Graph evolution: Densiﬁcation and

shrinking diameters. ACM TKDD 1(1), 2 (2007)

18. Shetty, J., Adibi, J.: Enron email dataset. Technical report (2004)
19. Richardson, M., Agrawal, R., Domingos, P.: Trust Management for the Semantic
Web. In: Fensel, D., Sycara, K., Mylopoulos, J. (eds.) ISWC 2003. LNCS, vol. 2870,
pp. 351–368. Springer, Heidelberg (2003)

20. Leskovec, J., Lang, K.J., Dasgupta, A., Mahoney, M.W.: Statistical properties of
community structure in large social and information networks. In: WWW 2008
(2008)

21. Kendall, M., Gibbons, J.D.: Rank Correlation Methods, 5th edn. (September 1990)
22. Jaccard, P.: ´Etude comparative de la distribution ﬂorale dans une portion des alpes
et des jura. Bulletin del la Soci´et´e Vaudoise des Sciences Naturelles 37, 547–579
(1901)


